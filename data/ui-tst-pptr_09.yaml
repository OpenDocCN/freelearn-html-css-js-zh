- en: '*Chapter 9*: Scraping tools'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 9 章*：抓取工具'
- en: Back in [*Chapter 1*](B16113_01_Final_SK_ePub.xhtml#_idTextAnchor014), *Getting
    started with Puppeteer*, we talked about the different uses for web automation.
    Of all those use cases, web scraping is the one that excites developers the most.
    When I give talks about automation, I know that I will get the crowd's full attention
    when I start talking about task automation, but even more when I get into the
    topic of **web scraping**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第一章*](B16113_01_Final_SK_ePub.xhtml#_idTextAnchor014) *使用 Puppeteer 入门*
    中，我们讨论了网络自动化的不同用途。在所有这些用例中，网络抓取是让开发者最兴奋的一个。当我谈论自动化时，我知道当我开始谈论任务自动化时，我会得到观众的全部注意力，但当我深入到
    **网络抓取** 这个话题时，我会得到更多的关注。
- en: Don't get me wrong, I think that UI testing is important. As we saw in the previous
    chapters, it's not just about running automation tests but also about taking care
    of your customers. But web scraping has that fun spark, a hacker feeling, and
    I didn't want to leave this topic out of the book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要误解我，我认为 UI 测试很重要。正如我们在前面的章节中看到的，这不仅仅是运行自动化测试，也是照顾客户。但网络抓取有那种有趣的火花，一种黑客的感觉，我不想在这个话题上遗漏任何内容。
- en: A few months ago, I read a book about web scraping that included a chapter about
    UI testing at the end of the book. We are going to do the same, but the other
    way around. This is a UI testing book with a scraping chapter at the end.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月前，我读了一本关于网络抓取的书，书中在书的结尾处包含了一个关于 UI 测试的章节。我们将做同样的事情，但方向相反。这是一本以抓取章节结尾的 UI
    测试书。
- en: We will begin this chapter by defining and demystifying web scraping. Is it
    just for hackers? Is it even legal? We will also talk about scraping ethics, such
    as when it is OK to scrape and when it is not.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节的开始，通过定义和揭秘网络抓取。它只是针对黑客的吗？它甚至合法吗？我们还将讨论抓取伦理，比如什么时候可以抓取，什么时候不可以。
- en: The second part of the chapter will discuss the different tools available to
    scrape with Puppeteer.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 章节的第二部分将讨论使用 Puppeteer 可用的不同抓取工具。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Introduction to web scraping
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络抓取简介
- en: Creating scrapers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建抓取器
- en: Running scrapers in parallel
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行运行抓取器
- en: How to avoid being detected as a bot
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何避免被检测为机器人
- en: Dealing with authentication and authorization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理身份验证和授权
- en: By the end of this chapter, you will be able to apply all the concepts you have
    learned during the course of this book in a brand-new field of web automation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够将你在本书的学习过程中学到的所有概念应用于一个全新的网络自动化领域。
- en: Let's get started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will find all the code of this chapter on the GitHub repository ([https://github.com/PacktPublishing/UI-Testing-with-Puppeteer](https://github.com/PacktPublishing/UI-Testing-with-Puppeteer))
    under the `Chapter9` directory. Remember to run `npm install` on that directory.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 仓库（[https://github.com/PacktPublishing/UI-Testing-with-Puppeteer](https://github.com/PacktPublishing/UI-Testing-with-Puppeteer)）的
    `Chapter9` 目录下找到本章的所有代码。请记住在那个目录下运行 `npm install`。
- en: Introduction to web scraping
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络抓取简介
- en: The best way to introduce a new concept is by giving a few concrete and straightforward
    definitions. Let's begin by defining **data scraping**. According to Wikipedia
    ([https://www.hardkoded.com/ui-testing-with-puppeteer/data-scraping](https://www.hardkoded.com/ui-testing-with-puppeteer/data-scraping)),
    "*Data scraping is a technique in which a computer program extracts data from
    human-readable output coming from another program*." Any information coming out
    from a computer can be extracted and processed. The first scrapers were called
    "screen scrapers." A screen scraper is something as simple as an application that
    can capture the screen. Then, by running **Optical Character Recognition** (**OCR**),
    it extracts the text from that image for further processing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍一个新概念的最佳方式是通过给出一些具体和直接的定义。让我们首先定义 **数据抓取**。根据维基百科 ([https://www.hardkoded.com/ui-testing-with-puppeteer/data-scraping](https://www.hardkoded.com/ui-testing-with-puppeteer/data-scraping))，“*数据抓取是一种计算机程序从另一个程序的人类可读输出中提取数据的技术*。”任何从计算机输出的信息都可以被提取和处理。最早的抓取器被称为“屏幕抓取器”。屏幕抓取器就像一个可以捕获屏幕的应用程序。然后，通过运行
    **光学字符识别**（**OCR**），它从该图像中提取文本以进行进一步处理。
- en: Web scraping takes this idea to the next level. *Web scraping is a technique
    used to extract data from one or multiple websites using a piece of software.*
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 网络抓取将这个想法提升到了新的水平。*网络抓取是一种使用软件从一个或多个网站中提取数据的技巧*。
- en: 'You might be wondering: Is that even legal? Amazon is a public site. I can
    freely navigate through the site; why wouldn''t I be able to run a script to extract
    data that is already public? Well, it depends. Let me share with you some scenarios
    from the real world that have similar ethical dilemmas to web scraping.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：这甚至合法吗？亚马逊是一个公开的网站。我可以自由地浏览网站；为什么我不能运行一个脚本来提取已经公开的数据呢？好吧，这取决于。让我与你分享一些现实世界中的场景，这些场景与网络爬取有类似的伦理困境。
- en: '**First scenario**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一种情况**'
- en: A small grocery store owner goes to a big mall to compare their products and
    prices. She can't go and get a box of milk and leave without paying, but she can
    walk around and take notes of the product prices, take that list to her shop,
    and compare their prices. The price is not a product. She's not stealing anything.
    Also, the mall is too big, and they can't control every person walking around
    taking notes. But what if the same person goes to a small grocery store on the
    next block and starts taking notes? I bet the owner already knows her, and it's
    too evident that she's taking notes, and that will probably threaten their business.
    Is it illegal? No. But she might get into a fight.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小型杂货店老板去一个大商场比较他们的产品和价格。她不能拿一箱牛奶走，不付钱，但她可以四处走动，记下产品价格，然后把这份清单带到她的店里，比较价格。价格不是产品。她没有偷任何东西。而且，商场太大，他们无法控制每个四处走动记笔记的人。但如果同一个人去下一个街区的小杂货店开始记笔记呢？我敢打赌店主已经认识她了，而且她记笔记的行为太明显了，这可能会威胁到他们的生意。这是非法的吗？不是。但她可能会陷入纠纷。
- en: '**Second scenario**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二种情况**'
- en: Some exclusive furniture stores won't let you take photos inside the store.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一些高端家具店不允许你在店内拍照。
- en: '**Last scenario**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**最后一种情况**'
- en: You can't count cards in the casino! They will kick you out and ban you for
    life.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你在赌场不能数牌！他们会把你踢出去，并且终身禁止你进入。
- en: These are real-life "scrapers." People are trying to extract information in
    the real world. Scraping the web is quite similar. You will be able to scrape
    a site as long as a) the site welcomes (implicitly or explicitly) scrapers, b)
    your attitude is considerate while scraping, and c) what you are scraping is allowed.
    Let's unpack these concepts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是现实生活中的“爬虫”。人们在现实世界中试图提取信息。爬取网络与此类似。只要满足以下条件，你就能爬取一个网站：a) 网站欢迎（隐式或显式）爬虫，b)
    在爬取时你的态度是考虑周到的，以及 c) 你所爬取的内容是被允许的。让我们来分析这些概念。
- en: Does the site allow scrapers?
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网站是否允许爬虫？
- en: 'The first reaction to this question would be: "No! why would the owner of a
    website allow scrapers?" But that''s not necessarily true. What if you own a hotel?
    If an aggregator website scrapes your booking page, then it shows those results
    on their website with a link back to your site, they will make some profit from
    that, and you will get more customers: win-win. Or, if you own a non-profit site
    such as Wikipedia or a government website, scraping might not be an issue. As
    it''s a non-profit website, you shouldn''t care much about bots coming to your
    site to extract data unless they affect your website''s performance. But if your
    site is about song lyrics, you won''t want anybody to come to your site and extract
    the lyrics. Lyrics are your product, your assets.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，第一个反应可能是：“不！网站所有者为什么要允许爬虫？”但这并不一定正确。如果你拥有一家酒店呢？如果一个聚合网站爬取你的预订页面，然后在他们的网站上显示这些结果，并附上链接回到你的网站，他们将从这中获得一些利润，而你将吸引更多客户：双赢。或者，如果你拥有一家如维基百科或政府网站这样的非营利网站，爬取可能不是问题。作为一个非营利网站，你不应该太在意机器人来到你的网站提取数据，除非它们影响了你网站的性能。但如果你网站的内容是关于歌词，你不会希望任何人来到你的网站提取歌词。歌词是你的产品，你的资产。
- en: 'In this chapter, we will see many techniques to bypass some validations, but
    my personal rule is: If the site doesn''t want to be scraped, I won''t scrape
    it. No means no.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到许多绕过一些验证的技术，但我的个人规则是：如果网站不想被爬取，我就不会爬取它。不意味着不。
- en: So, how do we know whether we can scrape a website? The owner of a website can
    express that in at least four different ways.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何知道我们是否可以爬取一个网站呢？网站的所有者可以通过至少四种不同的方式来表达这一点。
- en: Terms and conditions
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条款和条件
- en: The first thing you should check before scraping a website is the terms and
    conditions. The website owners can make it very clear that they don't want it
    to be scraped.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬取一个网站之前，你应该首先检查其条款和条件。网站所有者可以非常明确地表示他们不希望被爬取。
- en: The terms and conditions is that huge block of text we often ignore when installing
    an app. I bet you've also had emails telling you that a website has changed its
    terms and conditions, and you said "yeah, whatever" and archived the email.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用条款和条件是我们经常在安装应用程序时忽略的那块大文本。我敢打赌你也收到过邮件告诉你一个网站已经更改了其使用条款和条件，而你只是说“随便吧”然后存档了那封邮件。
- en: But we shouldn't do that. According to iubenda ([https://www.hardkoded.com/ui-testing-with-puppeteer/iubenda-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/iubenda-terms)),
    "'*Terms and Conditions' is the document governing the contractual relationship
    between the provider of a service and its user. The Terms and Conditions are nothing
    other than a contract in which the owner clarifies the conditions of use of its
    service*." Sometimes we might think that when we buy digital content (software,
    music, e-books) on a website, we then own that product, when, in fact, if you
    read the terms and conditions, you have bought the right to use the product, but
    you don't own it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不应该这样做。根据 iubenda ([https://www.hardkoded.com/ui-testing-with-puppeteer/iubenda-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/iubenda-terms))，"'*使用条款和条件'是服务提供者与其用户之间合同关系的规范文件。使用条款和条件实际上就是一份合同，其中所有者明确了其服务的使用条件*。"有时我们可能会认为，当我们在一个网站上购买数字内容（软件、音乐、电子书）时，我们就拥有了那个产品，而实际上，如果你阅读了使用条款和条件，你只是购买了使用该产品的权利，但并不拥有它。
- en: 'The terms and conditions also states what they allow you to do on a website.
    Many sites make that very clear. Let''s take the example of the terms and conditions
    of www.ebay.com:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用条款和条件还说明了你可以在网站上做什么。许多网站都对此非常明确。以 www.ebay.com 的使用条款和条件为例：
- en: '*3\. Using eBay*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*3. 使用 eBay*'
- en: '*In connection with using or accessing the Services you will not:*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*在使用或访问服务的过程中，你将不会：*'
- en: '*use any robot, spider, scraper or other automated means to access our Services
    for any purpose;*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用任何机器人、蜘蛛、抓取器或其他自动化手段访问我们的服务以任何目的；*'
- en: '*bypass our robot exclusion headers, interfere with the working of our Services,
    or impose an unreasonable or disproportionately large load on our infrastructure.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*绕过我们的机器人排除头信息，干扰我们服务的运行，或对我们的基础设施施加不合理或不成比例的大负载。*'
- en: As we can see, eBay makes it very clear. You can't scrape their site. eBay v.
    Bidder's Edge's ([https://www.hardkoded.com/ui-testing-with-puppeteer/ebay-vs-edge](https://www.hardkoded.com/ui-testing-with-puppeteer/ebay-vs-edge))
    was a well-known case back in the 2000s. eBay alleged that Bidder's Edge activities
    constituted a trespass of eBay's chattels ([https://www.hardkoded.com/ui-testing-with-puppeteer/Trespass-to-chattels](https://www.hardkoded.com/ui-testing-with-puppeteer/Trespass-to-chattels)).
    In other words, Bidder's Edge's scraping affected the servers that are eBay's
    property. I bet you don't want to go to court against eBay.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，eBay 表述得非常明确。你不能抓取他们的网站。eBay 诉 Bidder's Edge ([https://www.hardkoded.com/ui-testing-with-puppeteer/ebay-vs-edge](https://www.hardkoded.com/ui-testing-with-puppeteer/ebay-vs-edge))
    是 2000 年代的一个著名案例。eBay 指控 Bidder's Edge 的抓取活动构成了对 eBay 财产的侵犯 ([https://www.hardkoded.com/ui-testing-with-puppeteer/Trespass-to-chattels](https://www.hardkoded.com/ui-testing-with-puppeteer/Trespass-to-chattels))。换句话说，Bidder's
    Edge 的抓取影响了 eBay 的服务器。我敢打赌你不想与 eBay 对簿公堂。
- en: 'Now, let''s take a look at Ryanair''s terms and conditions ([https://www.hardkoded.com/ui-testing-with-puppeteer/ryanair-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/ryanair-terms)):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看 Ryanair 的使用条款和条件 ([https://www.hardkoded.com/ui-testing-with-puppeteer/ryanair-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/ryanair-terms))：
- en: '*Use of any automated system or software, whether operated by a third party
    or otherwise, to extract any data from this website for commercial purposes ("screen
    scraping") is strictly prohibited.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用任何自动化系统或软件（无论是由第三方操作还是其他方式），从本网站提取任何数据用于商业目的（“屏幕抓取”）是严格禁止的。*'
- en: Ryanair doesn't like scrapers either, but it says "for commercial purposes,"
    which would mean that you could code your scraper to look for the best price for
    your next vacation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Ryanair 也不喜欢抓取器，但它说“出于商业目的”，这意味着你可以编写你的抓取器代码来寻找你下一次度假的最佳价格。
- en: If the terms and conditions is not clear regarding scrapers, the second way
    a site owner can express their relationship with scrapers is through the `robots.txt`
    file.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用条款和条件没有明确说明抓取器，网站所有者可以通过 `robots.txt` 文件表达他们与抓取器的关系的另一种方式。
- en: robots.txt file
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: robots.txt 文件
- en: Wikipedia, again, has a great definition of the robots file. According to Wikipedia
    ([https://www.hardkoded.com/ui-testing-with-puppeteer/Robots-exclusion-standard](https://www.hardkoded.com/ui-testing-with-puppeteer/Robots-exclusion-standard)),
    the robots exclusion protocol "*is a standard used by websites to communicate
    with web crawlers and other web robots. The standard specifies how to inform the
    web robot about which areas of the website should not be processed or scanned.
    Robots are often used by search engines to categorize websites*."
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科再次给出了对robots文件的精彩定义。根据维基百科([https://www.hardkoded.com/ui-testing-with-puppeteer/Robots-exclusion-standard](https://www.hardkoded.com/ui-testing-with-puppeteer/Robots-exclusion-standard))，robots排除协议"*是网站用来与网络爬虫和其他网络机器人通信的标准。该标准指定了如何通知网络爬虫哪些网站区域不应被处理或扫描。网络爬虫通常被搜索引擎用来对网站进行分类*。”
- en: 'The keyword in that definition is "inform." The website owner can express in
    the robots file which parts of the site can be scraped. Most websites only use
    the `robots.txt` file to tell search engines where they can find the sitemap to
    scrape:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 那个定义中的关键词是“通知”。网站所有者可以在robots文件中表达哪些网站部分可以被抓取。大多数网站只使用`robots.txt`文件来告诉搜索引擎它们可以在哪里找到用于抓取的Sitemap：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With these two simple lines of code, they tell search engines, such as Google,
    to get that Sitemap file and scrape those pages. But you can find more complex
    definitions in that file. For instance, the `robots.txt` file on Wikipedia has
    over 700 lines! That tells us that the site is being scraped quite a lot. Let''s
    see some examples of what we can find in that file:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这两行简单的代码告诉搜索引擎，如谷歌，获取那个Sitemap文件并抓取那些页面。但你可以在这个文件中找到更复杂的定义。例如，维基百科上的`robots.txt`文件有超过700行！这告诉我们该网站被大量抓取。让我们看看这个文件中我们可以找到的一些示例：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'I love that the file begins with a message to us! They expect us to come to
    this page to read it. The next section is interesting:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢这个文件以一条信息开始！他们希望我们来到这个页面阅读它。下一部分很有趣：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, Wikipedia informs that they don''t want scrapers identified with the
    user-agent **UbiCrawler**, **DOC**, **Zao**, and **sitecheck.internetseer.com**
    to scrape the site. And the file closes with general rules for all user-agents:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，维基百科通知说，他们不希望与用户代理**UbiCrawler**、**DOC**、**Zao**和**sitecheck.internetseer.com**相关的抓取器抓取网站。并且文件以适用于所有用户代理的通用规则结束：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'They basically say that all the rest (`User-agent: *`) can scrape the whole
    site except some URLs such as `/w/`, `/api/`, and so on.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '他们基本上说，除了某些URL（如`/w/`、`/api/`等）之外，所有剩余的（`User-agent: *`）都可以抓取整个网站。'
- en: If we don't find anything useful in the terms and conditions or in the `robots.txt`
    file, we might find some hints in the page response.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在条款和条件或`robots.txt`文件中找不到任何有用的信息，我们可能在页面响应中找到一些线索。
- en: Are you a human?
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你是人类吗？
- en: 'I bet no one in real life has asked you whether you are a human, but many websites
    ask us that question all the time:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我打赌现实生活中没有人问过你你是不是人类，但许多网站总是问我们这个问题：
- en: '![Bot check by reCAPTCHA'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![reCAPTCHA机器人检测'
- en: '](img/Figure_9.01_B16113.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.01_B16113.jpg)'
- en: Bot check by reCAPTCHA
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: reCAPTCHA机器人检测
- en: 'What started as a simple "type the word you see" turned into more and more
    complicated challenges:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最初只是一个简单的“输入你看到的单词”的挑战，变成了越来越复杂的挑战：
- en: '![Complex CAPTCHAs'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![复杂的CAPTCHA'
- en: '](img/Figure_9.02_B16113.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.02_B16113.jpg)'
- en: Complex CAPTCHAs
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的CAPTCHA
- en: Should we check the third square of the first row? Who knows… But their goal
    is clear. In a friendly way, using the UI, they want to kick scrapers out.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该检查第一行的第三个方块吗？谁知道……但他们的目标是明确的。以一种友好的方式，使用UI，他们想将抓取器踢出去。
- en: 'When a site puts a CAPTCHA, like the one in the previous screenshot, it''s
    not always because they don''t want to be scraped. Maybe they want to protect
    their users. They don''t want malicious bots to test users and passwords or stolen
    credit card numbers. But the intention is clear: that page is for humans.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个网站放置一个像之前截图中的CAPTCHA时，这并不总是因为他们不想被抓取。也许他们想保护他们的用户。他们不希望恶意机器人测试用户名和密码或被盗的信用卡号码。但意图是明确的：那个页面是为人类准备的。
- en: 'I call these validations a "friendly" way to kick bots out. But you can also
    get some unfriendly responses from the server:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我把这些验证称为“友好”的方式将机器人踢出去。但您也可能从服务器获得一些不友好的响应：
- en: '![Page returning a 403 HTTP error'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![页面返回403 HTTP错误'
- en: '](img/Figure_9.03_B16113.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.03_B16113.jpg)'
- en: Page returning a 403 HTTP error
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 页面返回403 HTTP错误
- en: If a page you would typically be able to access now returns a 403 when you scrape
    it, it means that the server considered your actions as an attack on the server
    and banned your IP.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通常可以访问的页面在抓取时返回403错误，这意味着服务器认为你的行为是对服务器的攻击，并禁止了你的IP。
- en: That's terrible news. It means that you won't be able to access that site using
    your current IP. The ban could be for a few hours or forever. That would also
    mean that if you share your public IP with other computers, for instance, inside
    an organization, no one will be able to access that site. There are two ways to
    fix this issue. One is by reaching out to the site owner, apologizing, and asking
    them to remove your IP from the forbidden list. Or you could keep playing badly,
    trying to change your public IP or using proxies. But if you got caught once,
    you will get caught twice.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个坏消息。这意味着你将无法使用当前的IP访问该网站。禁令可能是几个小时或永远。这也意味着，如果你在组织内部与其他计算机共享公共IP，那么没有人将能够访问该网站。有两种方法可以解决这个问题。一种是通过联系网站所有者，道歉并请求他们从禁止列表中移除你的IP。或者你可以继续糟糕地玩，尝试更改你的公共IP或使用代理。但如果你被抓到一次，你将被抓到两次。
- en: The last way a site can tell you not to scrape is by giving you an API.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 网站告诉你不要抓取的最后一种方式是提供API。
- en: Using the Website API
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用网站API
- en: 'In this context, an API is a set of URLs that the website exposes that instead
    of returning a page, return some data, generally in JSON format:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，API是网站公开的一组URL，这些URL在返回页面而不是返回数据时，通常以JSON格式返回数据：
- en: '![Wikipedia API'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![Wikipedia API](img/Wikipedia_API.jpg)'
- en: '](img/Figure_9.04_B16113.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure_9.04_B16113.jpg](img/Figure_9.04_B16113.jpg)'
- en: Wikipedia API
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科API
- en: The question here is, why would you waste time parsing HTML elements, wait for
    network calls, and so on, if you can hit a URL to get all the information you
    need?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是，如果你可以通过点击URL获取所有需要的信息，为什么还要浪费时间解析HTML元素，等待网络调用等等？
- en: 'I think this is an excellent way for a site to tell you: **hey, don''t scrape
    me, here is the data you might need. You are welcome to come here and fetch the
    information you need**.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一种很好的方式，让网站告诉你：“嘿，别抓取我，这里是你可能需要的数据。欢迎你来这里获取你需要的信息。”
- en: APIs also give the website's owner the chance to set the rules, such as the
    rate limit (how many requests you can make per second or minute), the data the
    API exposes, and what will remain hidden for consumers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: API还给了网站所有者设置规则的机会，例如速率限制（每秒或每分钟可以发送多少请求）、API公开的数据以及对于消费者来说将保持隐藏的内容。
- en: These are some ways the site can communicate what we can and can't do. But our
    attitude toward the site being scraped is also important.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些网站可以传达我们可以做什么和不能做什么的方式。但我们对被抓取网站的看法也很重要。
- en: Our attitude
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的态度
- en: 'This part is quite simple, and it can be reduced to two words: be nice. You
    have to think that on the other side of the page you are trying to scrape, there
    are people like you whose goal is to keep the site up and running. They have to
    pay for a server and network bandwidth. Remember, they are not your enemy. They
    are your friends. They have the data you need, so be nice.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分相当简单，可以简化为两个词：友好。你必须想到，在你试图抓取的页面另一边，有像你一样的人，他们的目标是保持网站运行。他们必须为服务器和网络带宽付费。记住，他们不是你的敌人。他们是你的朋友。他们拥有你需要的数据，所以请友好对待。
- en: You should look at available APIs first. If a website exposes an API with the
    data you want, there is no need to scrape the site.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该首先查看可用的API。如果一个网站公开了包含你想要的数据的API，那么就没有必要抓取该网站。
- en: Next, you should consider your scraping rate/speed. Although you might want
    to scrape a site as fast as possible, I would try to make the scrape process close
    to real user interaction. Later in this chapter, we will see tools to scrape pages
    in parallel, but I would use those tools very carefully.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你应该考虑你的抓取速率/速度。虽然你可能希望尽可能快地抓取网站，但我建议尽量使抓取过程接近真实用户交互。在本章的后面部分，我们将看到用于并行抓取页面的工具，但我会非常小心地使用这些工具。
- en: There is one thing that many scrapers using Puppeteer forget. You should always
    identify yourself as a bot. In the next section, we will see that the idea is
    telling the server that you are a bot and not a real user.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 许多使用Puppeteer的抓取器会忘记一件事。你应该始终表明自己是一个机器人。在下一节中，我们将看到这个想法是告诉服务器你是一个机器人而不是真实用户。
- en: The last thing we need to consider is to evaluate what data we are extracting.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后需要考虑的是评估我们正在提取的数据。
- en: What's the data we are scraping?
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们正在抓取哪些数据？
- en: I think this is common sense. Are we extracting copyrighted data? Are we extracting
    the assets of the site? For instance, if we go to a lyrics website and extract
    the lyrics, we are taking the very purpose of the website. But if we go to an
    airline's website and check for flight prices, the price is not the company's
    asset. They sell flights, not prices.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是常识。我们在提取受版权保护的数据吗？我们在提取网站的资产吗？例如，如果我们去一个歌词网站并提取歌词，我们就是在剥夺网站的目的。但如果我们去航空公司的网站查看航班价格，价格并不是公司的资产。他们卖的是航班，而不是价格。
- en: The other thing to consider is what the data we extracted will be used for.
    We should consider whether our actions will empower the scraped site, for instance,
    in the case of the hotel booking website, they might get more customers, or threaten
    it, for instance, if we scrape lyrics to create our own lyrics site.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件事要考虑的是我们提取的数据将用于什么。我们应该考虑我们的行为是否会增强抓取的网站，例如，在酒店预订网站的情况下，他们可能会获得更多客户，或者威胁它，例如，如果我们抓取歌词来创建我们自己的歌词网站。
- en: Truth be told, lots of the sites we know today used scraping techniques to seed
    their websites. You might see that on real-estate websites. Who would want to
    go to an empty real-estate website? No one. So, these websites would be seeded
    with postings from other websites to make them more appealing to new customers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 诚实地讲，我们今天所知道的许多网站都使用了爬虫技术来播种他们的网站。你可能会在房地产网站上看到这一点。谁会想去一个空荡荡的房地产网站呢？没有人。所以，这些网站会通过其他网站的帖子来播种，以吸引新客户。
- en: Enough theory. Let's create some scrapers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 理论已经足够了。让我们创建一些爬虫。
- en: Creating scrapers
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建爬虫
- en: 'Let''s try to scrape book prices from the Packt site. The terms and conditions
    said nothing about scrapers ([https://www.hardkoded.com/ui-testing-with-puppeteer/packtpub-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/packtpub-terms)).
    But the `robots.txt` file has some clear rules:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试从Packt网站抓取书籍价格。条款和条件对爬虫没有任何说明([https://www.hardkoded.com/ui-testing-with-puppeteer/packtpub-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/packtpub-terms))。但是`robots.txt`文件有一些明确的规则：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'They don''t want us to go to those pages. But the site has a pretty massive
    `sitemap.xml`, with over 9,000 lines. If `robots.txt` is the "don''t go here"
    sign for scrapers, `sitemap.xml` is the "please, check this out" sign. These are
    the first items on the `sitemap.xml` file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 他们不希望我们去那些页面。但是网站有一个相当庞大的`sitemap.xml`，超过9,000行。如果`robots.txt`是爬虫的“不要去这里”的标志，那么`sitemap.xml`就是“请查看这个”的标志。这些是`sitemap.xml`文件上的第一个条目：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Based on this XML, we are going to build a **crawler**. A crawler is a program
    that will navigate through the website scraping all the pages. This will be our
    plan:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个XML，我们将构建一个**爬虫**。爬虫是一个程序，它将导航网站并抓取所有页面。这是我们计划：
- en: We are going to build an array of book category URLs. To make the run shorter,
    we will scrape only category pages, such as [https://www.packtpub.com/web-development](https://www.packtpub.com/web-development).
    We will also limit the list to 10 categories, so we are nice with the server.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将构建一个书籍类别URL数组。为了使运行更短，我们将只抓取类别页面，例如[https://www.packtpub.com/web-development](https://www.packtpub.com/web-development)。我们还将限制列表为10个类别，这样我们就对服务器友好。
- en: Once we get that list, we will navigate each page, getting book links. We don't
    want to get duplicates in there, so we need to be careful.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们得到这个列表，我们将导航每个页面，获取书籍链接。我们不希望其中出现重复，所以我们需要小心。
- en: 'Once we get the list of books, we navigate each book page and get the price
    for the print book plus the e-book and the print book alone:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们得到书籍列表，我们将导航每个书籍页面，获取平装书的定价以及电子书和平装书的单独定价：
- en: '![Book details'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![书籍详情'
- en: '](img/Figure_9.05_B16113.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.05_B16113.jpg)'
- en: Book details
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 书籍详情
- en: Those prices will be collected in a JSON array and sent to disk at the end of
    the process.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些价格将被收集到一个JSON数组中，并在过程结束时发送到磁盘。
- en: 'Let''s get started! You can get the code of this section in the `crawler.js`
    file. We will get the `sitemap.xml` file and get the first 10 categories:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！你可以在这个部分的`crawler.js`文件中找到这段代码。我们将获取`sitemap.xml`文件并获取前10个类别：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There we are using the native `https` package to download the `sitemap.xml`
    file. We create a Promise that will be resolved when we call the `resolve` function.
    Then we call the `get` function to fetch the file. We collect the information
    being downloaded in the `data` event, and when we receive an `end` event, we resolve
    the `Promise` by returning the `body` string we were collecting. It might take
    a while to get the code's flow, but it's very straightforward once you get used
    to it.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用原生的 `https` 包下载 `sitemap.xml` 文件。我们创建一个当调用 `resolve` 函数时将会解决的 Promise。然后我们调用
    `get` 函数来获取文件。我们在 `data` 事件中收集正在下载的信息，并在接收到 `end` 事件时，通过返回我们收集的 `body` 字符串来解决
    Promise。理解代码的流程可能需要一些时间，但一旦习惯了，它就非常直接。
- en: 'That `sitemapxml` variable is a string. We need first to parse the XML and
    then get a JavaScript model from there. `xml2js` will do most of the job for us.
    Let''s install that module using `npm install` in our terminal:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`sitemapxml` 变量是一个字符串。我们首先需要解析 XML，然后从中获取一个 JavaScript 模型。`xml2js` 将为我们完成大部分工作。让我们在终端中使用
    `npm install` 安装该模块：'
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once we have that module, we can start using it in our code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个模块，我们就可以开始在代码中使用它了：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will be able to parse the sitemap by calling the `xmlParser` function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用 `xmlParser` 函数来解析 sitemap：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, we are using the same `Promise` pattern we used before. When
    we call `xmlParser`, we will get the parsed `result` in the result argument of
    the callback we just passed in. Once we get the result, we prepare the output.
    It might be helpful to read the code while looking at the `sitemap.xml` file to
    get more context. We get URL elements from the `result.urlset.url` array. Then
    we `filter` URL elements with a `loc` with three slashes (such as [https://www.packtpub.com/web-development](https://www.packtpub.com/web-development)).
    Then, we grab only the first 10 elements using the `slice` function. Lastly, we
    use the `map` function to return only the resulting URL, returning an array of
    strings containing category URLs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们正在使用之前用过的相同的 `Promise` 模式。当我们调用 `xmlParser` 时，我们将在刚刚传入的回调函数的结果参数中获取解析后的
    `result`。一旦我们得到结果，我们就准备输出。在查看 `sitemap.xml` 文件的同时阅读代码可能会有所帮助，以获取更多上下文。我们从 `result.urlset.url`
    数组中获取 URL 元素。然后，我们使用带有三个斜杠的 `loc`（例如 [https://www.packtpub.com/web-development](https://www.packtpub.com/web-development)）来
    `filter` URL 元素。然后，我们使用 `slice` 函数仅获取前 10 个元素。最后，我们使用 `map` 函数仅返回结果 URL，返回一个包含类别
    URL 的字符串数组。
- en: Now it's time to use Puppeteer. We will navigate each of the categories we grabbed
    from the `sitemap.xml` and return book URLs. We are not going to scrape only the
    first page of the category page. I'll leave that feature to you as homework.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用 Puppeteer 了。我们将导航从 `sitemap.xml` 中获取的每个类别，并返回书籍 URL。我们不会只抓取类别页面的第一页。我将这个功能留给你作为作业。
- en: 'Let''s begin by creating a browser that we are going to use across the program:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个将在整个程序中使用的浏览器开始：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: I bet you are pretty familiar with this piece of code. The first new thing we
    can see here is that we are passing the `slowMo` option. That means that we are
    going to wait 500 milliseconds after each action that Puppeteer will perform.
    We will also use the `userAgent` function to get the user-agent from the browser.
    Then, we grab that string, and we append `' UITestingWithPuppeteerDemoBot'`, so
    the server admins in the publisher will know that's us.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我敢打赌你对这段代码非常熟悉。在这里我们能看到的第一个新事物是我们正在传递 `slowMo` 选项。这意味着在 Puppeteer 执行的每个动作之后，我们将等待
    500 毫秒。我们还将使用 `userAgent` 函数从浏览器中获取用户代理。然后，我们获取那个字符串，并追加 `' UITestingWithPuppeteerDemoBot'`，这样出版商的服务器管理员就会知道那是我们。
- en: Let the scraping begin!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始抓取吧！
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We will iterate through the `categories` list in the main function, and we will
    call `getBooks`, passing `categoryURL` and a Puppeteer page. That function will
    return a list of book URLs that we will append to our `books` array using the
    `push` function. We are using `slice(0, 10)`, so we return only the first 10 items.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在主函数中遍历 `categories` 列表，并调用 `getBooks`，传递 `categoryURL` 和一个 Puppeteer 页面。该函数将返回一个包含书籍
    URL 的列表，我们将使用 `push` 函数将其追加到我们的 `books` 数组中。我们使用 `slice(0, 10)`，所以只返回前 10 个项目。
- en: We wrapped all the code into a `try/catch` block because we don't want the code
    to fail if one category has failed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有代码包裹在一个 `try/catch` 块中，因为我们不希望如果某个类别失败时代码会失败。
- en: Let's now take a look at the `getBooks` function. It looks pretty straightforward.
    We go to `categoryURL` and wait for an element using the `a.card-body` CSS selector.
    That selector will give us book URLs. Once the books are loaded, we will call
    `evaluate` so we can get all the links with `a.card-body`, and then, using the
    `map` function, we will return the `href` attribute of the link, which will give
    us the URL.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看`getBooks`函数。它看起来相当直接。我们进入`categoryURL`，使用`a.card-body` CSS选择器等待一个元素。这个选择器将给我们书籍的URL。一旦书籍加载，我们将调用`evaluate`，这样我们就可以获取所有带有`a.card-body`的链接，然后，使用`map`函数，我们将返回链接的`href`属性，这将给我们URL。
- en: 'Scraping books won''t be that different:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 抓取书籍不会有太大不同：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here we are applying everything we have already learned in this book. We go
    to a page, wait for a selector, and then we will call the `evaluate` function,
    which will return an object. We haven't used the `evaluate` function in this way
    yet.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在应用这本书中学到的所有内容。我们进入一个页面，等待选择器，然后我们将调用`evaluate`函数，它将返回一个对象。我们还没有以这种方式使用`evaluate`函数。
- en: 'Inside `evaluate` we get the prices using the`.price-list__item .price-list__price`
    CSS selector, and we get the book title using the`.product-info__title` CSS selector.
    Then, if the product name is `"Print + eBook"`, because the site also offers videos,
    we return an object with three properties: `book`, `print`, and `ebook`.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在`evaluate`内部，我们使用`.price-list__item .price-list__price` CSS选择器获取价格，使用`.product-info__title`
    CSS选择器获取书籍标题。然后，如果产品名称是`"Print + eBook"`，因为该网站还提供视频，我们返回一个具有三个属性的对象：`book`、`print`和`ebook`。
- en: The last thing to highlight is that we are wrapping the code in a `try/catch`
    block. If we fail in fetching one book, we don't want the entire program to fail.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要强调的是，我们将代码包裹在`try/catch`块中。如果我们抓取一本书失败，我们不希望整个程序失败。
- en: The main function will collect those results and then save them to the file
    using `fs.writeFile`. In order to use that function, you will need to import `fs`
    by adding `const fs = require('fs');` in the first line of the program.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 主要函数将收集这些结果，然后使用`fs.writeFile`将它们保存到文件中。为了使用该函数，您需要在程序的第一行添加`const fs = require('fs');`来导入`fs`。
- en: 'If everything goes as expected, we will get a `prices.json` file with something
    like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切如预期进行，我们将得到一个`prices.json`文件，内容如下：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And we have our first scraper. From there, you have the data in the filesystem
    ready to be analyzed by other tools.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有我们的第一个抓取器。从那里，您在文件系统中有了数据，可以由其他工具进行分析。
- en: Can this be made better? Yes, it can. We could try to see whether we can do
    some parallel scraping.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以做得更好吗？是的，可以。我们可以尝试看看是否可以进行一些并行抓取。
- en: Running scrapers in parallel
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行运行抓取器
- en: I'm not saying this just because I coded it, but our scraper has a pretty good
    structure. Every piece is separated into different functions, making it easy to
    identify which parts can run in parallel.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我不是因为我编写了它才这么说，但我们的抓取器结构相当好。每个部分都被分离到不同的函数中，这使得很容易识别哪些部分可以并行运行。
- en: I don't want to sound repetitive, but remember, the site being scraped, in this
    case, Packt, is our friend and even my publisher. We don't affect the site; we
    want to look like normal users. We don't want to run 1,000 calls in parallel.
    We don't need to do that. So, we will try to run our scraper in parallel but with
    caution.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想重复，但请记住，我们正在抓取的网站，在这个例子中，Packt，是我们的朋友，甚至是我的出版商。我们不会影响网站；我们想看起来像正常用户。我们不希望并行运行1,000次调用。我们不需要这样做。所以，我们将尝试以谨慎的方式并行运行我们的抓取器。
- en: 'The good news is that we don''t have to code a parallel architecture to solve
    this. We will use a package called **puppeteer-cluster** ([https://www.npmjs.com/package/puppeteer-cluster](https://www.npmjs.com/package/puppeteer-cluster)).
    This is what this library does according to the description at npmjs:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们不需要编写并行架构的代码来解决这个问题。我们将使用一个名为**puppeteer-cluster**（[https://www.npmjs.com/package/puppeteer-cluster](https://www.npmjs.com/package/puppeteer-cluster)）的包。这是这个库根据npmjs上的描述所做的工作：
- en: Handles crawling errors
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理爬取错误
- en: Auto restarts the browser in case of a crash
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在崩溃的情况下自动重启浏览器
- en: Can automatically retry if a job fails
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任务失败，可以自动重试
- en: Offers different concurrency models to choose from (pages, contexts, browsers)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供不同的并发模型可供选择（页面、上下文、浏览器）
- en: Is simple to use, small boilerplate
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用简单，小样板
- en: Offers progress view and monitoring statistics (see the following code snippet)
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供进度视图和监控统计信息（见以下代码片段）
- en: 'Sounds pretty promising. Let''s see how we can implement it. First, we need
    to install the package:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很有前景。让我们看看我们如何实现它。首先，我们需要安装这个包：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'That will get us the package ready to be used. You can get the code of this
    section in the `crawler-with-cluster.js` file. Let''s import the cluster in our
    scraper by calling `require` in the first line of our code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们的包准备好使用。你可以在`crawler-with-cluster.js`文件中找到这一节的代码。让我们通过在我们的代码第一行调用`require`来在我们的爬虫中导入集群：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that we have imported the `Cluster` class, we can create a new **cluster**
    in the main function:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经导入了`Cluster`类，我们可以在主函数中创建一个新的**集群**：
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `Cluster.launch` function has many options, but I think that, for now,
    we only need to know about these options:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`Cluster.launch`函数有很多选项，但我认为现在我们只需要了解这些选项：'
- en: '`concurrency` will tell the cluster the level of isolation we want to use.
    The default is `Cluster.CONCURRENCY_CONTEXT`. These are all the options available:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`concurrency`将告诉集群我们想要使用的隔离级别。默认值是`Cluster.CONCURRENCY_CONTEXT`。这些都是可用的选项：'
- en: a) Using `Cluster.CONCURRENCY_CONTEXT`, each job will have its own context.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用`Cluster.CONCURRENCY_CONTEXT`，每个作业将有自己的上下文。
- en: b) Using `Cluster.CONCURRENCY_PAGE`, each job will have its own page, but the
    same context will be shared across all jobs.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 使用`Cluster.CONCURRENCY_PAGE`，每个作业将有自己的页面，但所有作业将共享相同的上下文。
- en: c) Using `Cluster.CONCURRENCY_BROWSER`, each job will have its own browser.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 使用`Cluster.CONCURRENCY_BROWSER`，每个作业将有自己的浏览器。
- en: '`maxConcurrency` will help us set how many tasks we want to run simultaneously.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxConcurrency`将帮助我们设置我们想要同时运行多少个任务。'
- en: With `retryLimit`, we can set how many times the cluster will run a task if
    it fails. The default is 0, but we will give it one more chance to do the task,
    setting this to 1.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`retryLimit`，我们可以设置集群在任务失败时将运行任务多少次。默认值是0，但我们将给它一次重试任务的机会，将其设置为1。
- en: If we set the `monitor` option to `true`, we will get a nice console output,
    showing the current process.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们将`monitor`选项设置为`true`，我们将得到一个漂亮的控制台输出，显示当前进程。
- en: The last option we will cover here is `puppeteerOptions`. The cluster will pass
    this object to the `puppeteer.launch` function.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在这里要讨论的最后一个选项是`puppeteerOptions`。集群将把这个对象传递给`puppeteer.launch`函数。
- en: 'One thing that the package description mentions is that it supports error handling.
    Let''s add the error handling they have in the example:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 包说明中提到的一件事是它支持错误处理。让我们添加示例中他们有的错误处理：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: That looks pretty solid. When a task fails, the cluster will fire a `taskerror`
    event. There we can see the error, the data, and whether the action will be retried.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当稳固。当任务失败时，集群将触发一个`taskerror`事件。在那里我们可以看到错误、数据和是否将重试操作。
- en: 'We don''t need to change how we download and process `sitemap.xml`. There is
    nothing to change there. But once we have the category, instead of calling the
    `getBooks` function, we will use `queue` for that task:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要改变我们下载和处理`sitemap.xml`的方式。那里没有需要改变的地方。但一旦我们有了类别，而不是调用`getBooks`函数，我们将使用`queue`来完成这个任务：
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We are telling the cluster that we need to run `getBooks` by passing that `categoryURL`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在告诉集群我们需要通过传递那个`categoryURL`来运行`getBooks`。
- en: 'I have more good news. Our scraping functions are almost ready to be used in
    a cluster – **almost ready**. We need to change four things:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我还有更多好消息。我们的爬取函数几乎准备好在集群中使用——**几乎准备好**。我们需要改变四件事：
- en: '[PRE19]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: First, we changed the signature a little bit. Instead of expecting `(page, categoryURL)`,
    it will expect an object with a `page` property and a `data` property, where `page`
    will be the page created and managed by the cluster, and the `data` property will
    be the `categoryURL` instance we passed in when we queued the task.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对签名做了一点修改。不再是期望`(page, categoryURL)`，而是期望一个具有`page`属性和`data`属性的对象，其中`page`将是集群创建和管理的页面，而`data`属性将是我们在排队任务时传递的`categoryURL`实例。
- en: Tip
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The first argument to pass to the `queue` function doesn't need to be a URL.
    It doesn't even need to be a string. You can pass in any object, and the function
    will get that object in the `data` property.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`queue`函数的第一个参数不需要是URL。甚至不需要是字符串。你可以传递任何对象，函数将获取该对象在`data`属性中的内容。
- en: The second thing we had to do was to add the call to `setUserAgent` as the page
    is created by the cluster itself.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须做的第二件事是在页面创建时添加对`setUserAgent`的调用，因为页面是由集群本身创建的。
- en: Then, instead of returning the list of books, we added more tasks to the queue,
    but in this case, we enqueued the `getPrice` function, passing the `book` URL.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们不是返回书籍列表，而是向队列中添加了更多任务，但在这个情况下，我们排队的函数是`getPrice`，传递了`book` URL。
- en: The last thing we had to do is remove the `try/catch` block because the cluster
    will handle that for us.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后需要做的就是移除`try/catch`块，因为集群会为我们处理这一点。
- en: 'Now it''s time to update the `getPrice` function:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候更新`getPrice`函数了：
- en: '[PRE20]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We did pretty much the same. We changed the signature, added the call to `setUserAgent`,
    removed `try/catch`, and instead of returning the price, we are pushing to the
    `prices` array inside the function.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎做了同样的事情。我们更改了签名，添加了对`setUserAgent`的调用，移除了`try/catch`，并且不再返回价格，而是在函数内部将价格推送到`prices`数组中。
- en: 'Finally, we need to wait for the cluster to finish its work:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要等待集群完成其工作：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The call to `idle` will wait for all the tasks to complete, and then the `close`
    function will close the browser and the cluster. Let's see if all this works!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对`idle`的调用将等待所有任务完成，然后`close`函数将关闭浏览器和集群。让我们看看这一切是否都能正常工作！
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output of `puppeteer-cluster` is amazing. We can see the elapsed time, the
    progress, and what the workers are processing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`puppeteer-cluster`的输出非常出色。我们可以看到已消耗的时间、进度以及工作者正在处理的内容。'
- en: Until now, we played by the rules. But what if we want to avoid being detected
    as scrapers? Let's find that out.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在遵守规则。但如果我们想避免被检测为抓取器呢？让我们找出答案。
- en: How to avoid being detected as a bot
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何避免被检测为机器人
- en: I hesitated about adding this section after everything I mentioned about scraping
    ethics. I think I made my point clear when I said that when the owner says no,
    it means no. But if I'm writing a chapter about scraping, I think I need to show
    you these tools. It's then up to you what to do with the information you have
    learned so far.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在提到抓取伦理之后，我犹豫是否要添加这一节。我认为我在说当所有者说不的时候，我的观点已经很明确了。但如果我要写一个关于抓取的章节，我认为我需要向你展示这些工具。然后，如何使用你到目前为止所学的信息，就取决于你了。
- en: Websites that don't want to be scraped, and are being actively scraped, will
    invest a good amount of time and money in trying not to be scraped. The effort
    would become even more important if the scrapers damage not only the site's performance
    but also the business.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 不希望被抓取的网站，如果正在被积极抓取，将会投入大量时间和金钱来尝试避免被抓取。如果抓取器不仅损害了网站的性能，还损害了业务，那么这种努力将变得更加重要。
- en: Developers in charge of dealing with bots won't rely only on the user agent
    because, as we saw, that could be easily manipulated. They should rely only on
    evaluating the number of requests from an IP because, as we also saw, scrapers
    can slow down their scripts, simulating an interested user.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 负责处理机器人的开发者不会仅仅依赖于用户代理，因为我们看到，这很容易被操纵。他们应该只依赖于评估来自一个IP地址的请求数量，因为我们也看到，抓取器可以减慢其脚本，模拟一个感兴趣的用户的操作。
- en: If the site can't stop scrapers by checking the user agent and monitoring traffic
    spikes, they would try to catch scrapers using different techniques.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网站不能通过检查用户代理和监控流量峰值来阻止抓取器，他们将会尝试使用不同的技术来捕捉抓取器。
- en: They would begin by introducing CAPTCHAs. But, as we will see in this section,
    scrapers can solve some of them.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 他们会首先引入CAPTCHA。但是，正如我们将在本节中看到的，抓取器可以解决其中的一些。
- en: Then, they would try to evaluate the time between requests. Did you click a
    link after 500 milliseconds? Did you fill a form in less than 1 second? You might
    be a bot.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，他们会尝试评估请求之间的时间间隔。你在500毫秒后点击了链接吗？你在不到1秒内填写了表单吗？你可能是一个机器人。
- en: They could also add JavaScript code to check your browser capabilities. Don't
    you have plugins? Not even the ones that come with Chrome by default? Don't you
    have a language set? You might be a bot.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还可以添加JavaScript代码来检查你的浏览器功能。你没有插件吗？连Chrome默认提供的都没有？你没有设置语言吗？你可能是一个机器人。
- en: Finally, they will try to set traps to catch you. For instance, Packt knows
    that you might be scraping links using the `a.card-body` CSS selector. They could
    add a hidden link with that selector, but in that case, the link's URL could be
    https://www.packtpub.com/bot-detected. If you got to the bot-detected URL, you
    would get caught. In the case of forms, they could add hidden inputs that a typical
    user wouldn't complete because it is hidden. If the server gets a value in that
    hidden input, sorry—you were caught again.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，他们将会设置陷阱来捕捉你。例如，Packt知道你可能正在使用`a.card-body` CSS选择器抓取链接。他们可能会添加一个带有该选择器的隐藏链接，但那样的话，链接的URL可能是https://www.packtpub.com/bot-detected。如果你访问了bot-detected
    URL，你就会被发现。在表单的情况下，他们可能会添加一些隐藏的输入字段，一个典型的用户不会完成这些字段，因为它们是隐藏的。如果服务器收到了那个隐藏输入字段中的值，抱歉——你又再次被捕捉到了。
- en: This is a cat-and-mouse game. The mouse will always try to find new ways to
    sneak in, and the cat will work hard to cover the holes in the wall.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个猫捉老鼠的游戏。老鼠总是会试图找到新的方法偷偷进入，而猫会努力填补墙上的漏洞。
- en: That being said, let's see what tools we have if we are the mouse in this game.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们看看如果我们是游戏中这只老鼠，我们有哪些工具可用。
- en: 'Antoine Vastel has a great bot detection demo page ([https://arh.antoinevastel.com/bots/areyouheadless](https://arh.antoinevastel.com/bots/areyouheadless)).
    You can get the code of this section in the `bot.js` file. Let''s try to take
    a screenshot of that page using Puppeteer:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 安东尼·瓦斯特尔有一个出色的机器人检测演示页面([https://arh.antoinevastel.com/bots/areyouheadless](https://arh.antoinevastel.com/bots/areyouheadless))。你可以在这个部分的`bot.js`文件中找到这段代码。让我们尝试使用
    Puppeteer 截取该页面的屏幕截图：
- en: '[PRE23]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Simple Puppeteer code. We open the browser in headless mode, navigate to the
    page, and take a screenshot. Let''s see what the screenshot looks like:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的 Puppeteer 代码。我们以无头模式打开浏览器，导航到页面，并截图。让我们看看截图的样子：
- en: '![Antoine Vastel’s bot detection'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![安东尼·瓦斯特尔的机器人检测'
- en: '](img/Figure_9.06_B16113.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.06_B16113.jpg)'
- en: Antoine Vastel's bot detection
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 安东尼·瓦斯特尔的机器人检测
- en: Antoine got us. We were detected as a bot, but that's not the end of the game.
    There are a few things we can do. Let's start by incorporating `puppeteer-extra`
    ([https://github.com/berstend/puppeteer-extra](https://github.com/berstend/puppeteer-extra)).
    The `puppeteer-extra` package allows us to add plugins to Puppeteer. This package
    will allow us to use the `puppeteer-extra-plugin-stealth` plugin ([https://www.npmjs.com/package/puppeteer-extra-plugin-stealth](https://www.npmjs.com/package/puppeteer-extra-plugin-stealth)).
    This package is like a mouse master in this game. It will add all the tricks (or
    many tricks), so our code is not detected as a bot.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 安东尼抓住了我们。我们被检测为机器人，但这并不是游戏的结束。我们还有一些事情可以做。让我们首先引入`puppeteer-extra`([https://github.com/berstend/puppeteer-extra](https://github.com/berstend/puppeteer-extra))。这个`puppeteer-extra`包允许我们向
    Puppeteer 添加插件。这个包将允许我们使用`puppeteer-extra-plugin-stealth`插件([https://www.npmjs.com/package/puppeteer-extra-plugin-stealth](https://www.npmjs.com/package/puppeteer-extra-plugin-stealth))。在这个游戏中，这个包就像是一个老鼠大师。它将添加所有技巧（或许多技巧），这样我们的代码就不会被检测为机器人。
- en: 'The first thing we need to do is install those two packages from the terminal:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要做的是在终端中安装这两个包：
- en: '[PRE24]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we can replace this line:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以替换这一行：
- en: '[PRE25]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We replace it with these three lines:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用这三行代码替换它：
- en: '[PRE26]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: There, we import `puppeteer`, but from `puppeteer-extra`. Then, we import the
    stealth plugin and install it using the `use` function. And that's it!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里，我们导入`puppeteer`，但来自`puppeteer-extra`。然后，我们导入隐蔽插件并使用`use`函数安装它。就这样！
- en: '![Antoine Vastel’s bot detection bypassed'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '![安东尼·瓦斯特尔的机器人检测绕过'
- en: '](img/Figure_9.07_B16113.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.07_B16113.jpg)'
- en: Antoine Vastel's bot detection bypassed
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 安东尼·瓦斯特尔的机器人检测绕过
- en: The `puppeteer-extra-plugin-stealth` package is not bulletproof. As I mentioned
    before, it's a cat-and-mouse game. There are many other extras you can use. You
    can see the full list in the package's repository ([https://www.hardkoded.com/ui-testing-with-puppeteer/puppeteer-extra-packages](https://www.hardkoded.com/ui-testing-with-puppeteer/puppeteer-extra-packages)).
    There, you can find `puppeteer-extra-plugin-anonymize-ua`, which will change the
    user agent in all the pages, or `puppeteer-extra-plugin-recaptcha`, which will
    try to solve reCAPTCHA ([https://www.google.com/recaptcha/about/](https://www.google.com/recaptcha/about/))
    challenges.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`puppeteer-extra-plugin-stealth`包并不是坚不可摧的。正如我之前提到的，这是一个猫捉老鼠的游戏。还有很多其他的额外工具可以使用。你可以在包的存储库中看到完整的列表([https://www.hardkoded.com/ui-testing-with-puppeteer/puppeteer-extra-packages](https://www.hardkoded.com/ui-testing-with-puppeteer/puppeteer-extra-packages))。在那里，你可以找到`puppeteer-extra-plugin-anonymize-ua`，它将在所有页面上更改用户代理，或者`puppeteer-extra-plugin-recaptcha`，它将尝试解决reCAPTCHA([https://www.google.com/recaptcha/about/](https://www.google.com/recaptcha/about/))挑战。'
- en: A scraping chapter wouldn't be complete if we didn't talk, at least a little,
    about how to deal with authorization.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不至少稍微谈谈如何处理授权，那么一个爬取章节就不会完整。
- en: Dealing with authorization
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理授权
- en: Authentication and authorization is a vast topic in web development. Authentication
    is how a website can identify you. To make it simple, it's the login. On the other
    hand, authorization is what you can do on the site once you are authenticated,
    for instance, checking whether you have access to a specific page.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 认证和授权是网络开发中的一个广泛话题。认证是网站如何识别你的方式。简单来说，就是登录。另一方面，授权是在你认证后可以在网站上执行的操作，例如，检查你是否可以访问特定页面。
- en: 'There are many types of authentication modes. We covered the simplest one in
    this book: a user and password login page. But things can get more complicated.
    Testing integration with Facebook or single sign-on logins could be quite challenging,
    but they would be about automating user interaction.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多种认证模式。我们在本书中介绍了最简单的一种：用户名和密码登录页面。但事情可能会更复杂。测试与Facebook或单点登录的集成可能相当具有挑战性，但它们将涉及自动化用户交互。
- en: 'There is one authentication method that you won''t be able to perform by automating
    the DOM—**HTTP basic authentication**:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种认证方法，您无法通过自动化DOM来执行——**HTTP基本认证**：
- en: '![HTTP basic authentication'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![HTTP基本认证'
- en: '](img/Figure_9.08_B16113.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.08_B16113.jpg)'
- en: HTTP basic authentication
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP基本认证
- en: 'That login popup is not popular these days. In fact, I don''t think they ever
    were popular. But you might have seen them if you have set up a router. That modal
    is like the dialogs we saw in [*Chapter 5*](B16113_05_Final_SK_ePub.xhtml#_idTextAnchor087),
    *Waiting for elements and network calls*. Puppeteer won''t help us out with this
    authentication because there is no HTML to automate there. Luckily for us, automating
    this is easy. You can get the code of this section in the `authentication.js`
    file:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这种登录弹出窗口现在并不流行。事实上，我认为它们从未流行过。但您可能已经看到过它们，如果您设置过路由器。这个模态框就像我们在[*第5章*](B16113_05_Final_SK_ePub.xhtml#_idTextAnchor087)中看到的对话框，*等待元素和网络调用*。Puppeteer无法帮助我们完成这种认证，因为没有HTML可以自动化。幸运的是，自动化这个过程很容易。您可以在`authentication.js`文件中找到这个部分的代码：
- en: '[PRE27]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The only thing we need to do to authenticate into https://ptsv2.com/t/ui-testing-puppeteer/post
    is to call the `authenticate` function before calling `goto`. The `authenticate`
    function expects an object with two properties: `username` and `password`.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要认证进入https://ptsv2.com/t/ui-testing-puppeteer/post的唯一事情是在调用`goto`之前调用`authenticate`函数。`authenticate`函数期望一个包含两个属性的对象：`username`和`password`。
- en: Once we are authenticated, we need to tell the server who we are on every request,
    so they can authorize us (or not) to perform certain tasks. The web server is,
    in theory, stateless. It doesn't have a way to know who we are unless 1) they
    inject some information in their responses, using cookies, or 2) we tell them.
    The most common way is with HTTP Headers. But that can be solved by passing a
    key as a Query String argument or as part of the HTTP post data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成认证，我们需要在每次请求中告诉服务器我们是谁，这样它们才能授权我们（或拒绝）执行某些任务。从理论上讲，Web服务器是无状态的。除非1）它们在响应中注入一些信息，使用cookie，或者2）我们告诉它们，否则它们没有方法知道我们是谁。最常见的方式是通过HTTP头。但这可以通过传递一个作为查询字符串参数或作为HTTP
    POST数据一部分的键来解决。
- en: When you want to alter the authentication data, you need to get that information
    elsewhere. You might need to open your browser, log in to the site you want to
    scrape, and extract the authentication data from there, so you can then use that
    data in your scraper.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要更改认证数据时，您需要从其他地方获取这些信息。您可能需要打开浏览器，登录您想要抓取的网站，并从那里提取认证数据，然后您可以在您的抓取器中使用这些数据。
- en: 'Let''s say that you want to scrape the Packt website, but this time you want
    to scrape it being logged in. So, you open your browser, log in and then you can
    use a tool such as the *Export cookie JSON file for Puppeteer* extension (you
    can find it with that name in the Chrome web store) to export all the cookies
    generated by the site. Once we have the JSON file named `account.packtpub.com.cookies.json`
    with all the cookies, you can copy that file into your workspace and do something
    like this:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要抓取Packt网站，但这次您想在登录状态下抓取。因此，您打开浏览器，登录，然后您可以使用像*Export cookie JSON file for
    Puppeteer*这样的工具扩展（您可以在Chrome网络商店中用这个名字找到它）来导出网站生成的所有cookie。一旦我们有了包含所有cookie的名为`account.packtpub.com.cookies.json`的JSON文件，您可以将该文件复制到您的工区，并执行如下操作：
- en: '[PRE28]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The new element in this code is a call to the `setCookie` function. That function
    expects a list of cookies. As we have all the cookies in a JSON file, we load
    that JSON file and pass the content to the `setCookie` function. Let''s take a
    look at what a cookie looks like inside that file:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的新元素是对`setCookie`函数的调用。该函数期望一个cookie列表。由于我们所有的cookie都在一个JSON文件中，我们加载该JSON文件并将内容传递给`setCookie`函数。让我们看看文件中cookie的样子：
- en: '[PRE29]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The structure is quite simple and straightforward. You don't need to use an
    extension and load a cookie from a JSON file. You can call the `setCookie` function
    passing an object with the `name`, `value`, `domain`, `path`, and `expires` properties
    (the latter is a Unix time in seconds), whether it's `httpOnly`, and whether it
    should be marked as `secure`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 结构相当简单直接。你不需要使用扩展并从 JSON 文件中加载 cookie。你可以调用 `setCookie` 函数，传递一个包含 `name`、`value`、`domain`、`path`
    和 `expires` 属性的对象（后者是秒为单位的 Unix 时间），以及是否为 `httpOnly`，以及是否应该标记为 `secure`。
- en: 'Now it''s time to see how we can handle authorizations implemented using HTTP
    headers. You might find sites using the `authorization` HTTP header to pass some
    kind of user identifier. The `authorization` header would look something like
    this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看我们如何处理使用 HTTP 头实现的授权了。你可能会发现一些网站使用 `authorization` HTTP 头来传递某种用户标识符。`authorization`
    头看起来可能像这样：
- en: '[PRE30]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'According to MDN ([https://www.hardkoded.com/ui-testing-with-puppeteer/authentication-schemes](https://www.hardkoded.com/ui-testing-with-puppeteer/authentication-schemes)),
    you can find the following types: `Basic`, `Bearer`, `Digest`, `HOBA`, `Mutual`,
    and `AWS4-HMAC-SHA256`. If those names sound scary, don''t worry about that. There
    is a high chance that you will only see the `Bearer` type. What would be the credentials?
    Well, that''s what you will need to find out while coding your scraper. You would
    need to see what information is being sent when you use a website for real and
    try to mimic that.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 MDN ([https://www.hardkoded.com/ui-testing-with-puppeteer/authentication-schemes](https://www.hardkoded.com/ui-testing-with-puppeteer/authentication-schemes))，你可以找到以下类型：`Basic`、`Bearer`、`Digest`、`HOBA`、`Mutual`
    和 `AWS4-HMAC-SHA256`。如果这些名字听起来很吓人，请不要担心。你很可能只会看到 `Bearer` 类型。凭证是什么呢？嗯，这就是你在编写爬虫代码时需要找出的事情。你需要查看当你真正使用网站时发送了什么信息，并尝试模仿它。
- en: 'For our example, we will use `Basic` because that''s the same HTTP basic authentication
    we saw before. When you log in using the authentication popup, the browser will
    send the authorization header passing `basic` and `username:password` in Base64\.
    In our example, the username was `user`, and the password was `password`. So,
    we can use any Base64 encoder available, for instance, [https://www.base64encode.net/](https://www.base64encode.net/),
    and get `user:password` in base64: `dXNlcjpwYXNzd29yZA==`.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将使用 `Basic`，因为这与之前看到的相同 HTTP 基本认证。当你使用认证弹出窗口登录时，浏览器将通过传递 `basic` 和
    `username:password`（Base64 编码）来发送授权头。在我们的例子中，用户名是 `user`，密码是 `password`。因此，我们可以使用任何可用的
    Base64 编码器，例如，[https://www.base64encode.net/](https://www.base64encode.net/)，并将
    `user:password` 编码为 base64：`dXNlcjpwYXNzd29yZA==`。
- en: 'We can inject this header in two ways. The first one is using the `setExtraHTTPHeaders`
    function. You can see this code in the `header-inject.js` file:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式注入这个头信息。第一种是使用 `setExtraHTTPHeaders` 函数。你可以在 `header-inject.js` 文件中看到这个代码：
- en: '[PRE31]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`setExtraHTTPHeaders` expects an object where the property name is the header
    name, and the value is the header value. Here we are adding the `authorization`
    header with the value `''basic dXNlcjpwYXNzd29yZA==''`. And that''s it. Puppeteer
    will add that header to every request that the page will make.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`setExtraHTTPHeaders` 期望一个对象，其中属性名是头名称，值是头值。这里我们添加了 `authorization` 头，其值为 `''basic
    dXNlcjpwYXNzd29yZA==''`。就这样。Puppeteer 将将这个头添加到页面将发出的每一个请求中。'
- en: 'But what if the site we are trying to scrape needs an authorization header
    not in every request, but only in some of them? Well, it will be quite tricky,
    but not that hard. You can follow this code in the `header-inject2.js` file:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们试图抓取的网站需要授权头，而不是每个请求都需要，那会是什么情况？嗯，这会很棘手，但并不难。你可以参考 `header-inject2.js`
    文件中的代码：
- en: '[PRE32]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We are first telling Puppeteer we want to intercept every request the page will
    make. We do that by calling the `setRequestInterception` function with the first
    argument as `true`. Then we start listening to the `request` event. If the request
    meets the condition we need, in this case, if it matches our URL, we create an
    `overrides` object with a `headers` property and then call the `continue` function
    of the request object. We cannot override the headers. The `overrides` object
    can also have the `url`, `method` (the HTTP method), and `postData` properties.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先告诉 Puppeteer 我们想要拦截页面将发出的每一个请求。我们通过调用 `setRequestInterception` 函数，并将第一个参数设置为
    `true` 来做到这一点。然后我们开始监听 `request` 事件。如果请求满足我们需要的条件，在这种情况下，如果它与我们的 URL 匹配，我们创建一个具有
    `headers` 属性的 `overrides` 对象，然后调用请求对象的 `continue` 函数。我们不能覆盖头信息。`overrides` 对象还可以有
    `url`、`method`（HTTP 方法）和 `postData` 属性。
- en: The request object also has a function called `abort`. With this function, you
    can cancel that request. For instance, you could check whether the request is
    an image and `abort` it. The result will be a website with no images.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 请求对象还有一个名为 `abort` 的函数。使用这个函数，你可以取消那个请求。例如，你可以检查请求是否为图片，然后 `abort` 它。结果将是一个没有图片的网站。
- en: Important Note
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you call `setRequestInterception`, you need to implement a `request` event
    listener. And you need to `continue` or `abort` every request you listen to.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你调用 `setRequestInterception`，你需要实现一个 `request` 事件监听器。并且你需要对每个你监听的请求进行 `continue`
    或 `abort`。
- en: As I mentioned when I opened this section, this doesn't cover all the different
    authentication and authorization schemes, but it will have you covered in more
    than 90% of cases. Now it's time for a wrap-up.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在打开这个部分时提到的，这并不涵盖所有不同的身份验证和授权方案，但它将覆盖超过90%的情况。现在是我们总结的时候了。
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Although this is not a scraping book, we covered a lot of ground here. I hope
    the first section gave you a good idea of what scraping is, as well as covering
    what you can do and what you shouldn''t do. We also learned how to create our
    own scrapers. We created a crawler in less than 100 lines. We added two new tools
    to our toolbox: `puppeteer-cluster` and `puppeteer-extra`. We closed this chapter
    learning a little bit about authentication and authorization, giving you almost
    everything you need to get started in the scraping world.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是一本关于爬取的书，但我们在这里覆盖了很多内容。我希望第一章能给你一个关于爬取是什么的好印象，以及涵盖了你可以做什么和不应该做什么。我们还学习了如何创建自己的爬虫。我们在不到100行代码内创建了一个爬虫。我们向工具箱中添加了两个新工具：`puppeteer-cluster`
    和 `puppeteer-extra`。我们在本章的最后学习了一点点关于身份验证和授权，几乎为你提供了开始爬取世界所需的一切。
- en: If you weren't that excited about scraping before this chapter, I hope it is
    the spark that will make you start creating your own scrapers. If you knew about
    scraping, I hope this chapter gave you more tools to scrape as a professional.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在阅读本章之前你对爬取并不那么兴奋，我希望这能成为激发你开始创建自己的爬虫的火花。如果你已经了解爬取，我希望本章能为你提供更多作为专业人士爬取的工具。
- en: Our next and final chapter will be about performance and how we can measure
    it using Puppeteer.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一章和最后一章将关于性能以及我们如何使用 Puppeteer 来衡量它。
