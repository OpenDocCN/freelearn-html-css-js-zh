- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Real-Time Data Streaming Using Microservices
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用微服务进行实时数据流处理
- en: Certain microservice applications, such as financial trading platforms and ride-hailing
    services, demand events to be produced and consumed with minimal latency. Real-time
    data streaming has become increasingly crucial in modern software development
    due to its ability to provide immediate, continuous insights and responses based
    on the most current data. This type of real-time data usage is particularly important
    in industries such as finance, healthcare, and logistics, where delays in data
    processing can lead to significant losses or even life-threatening situations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 某些微服务应用程序，如金融交易平台和叫车服务，需要以最小延迟产生和消费事件。由于实时数据流能够基于最新数据提供即时、连续的洞察和响应，其在现代软件开发中变得越来越重要。这种实时数据使用在金融、医疗保健和物流等行业尤为重要，在这些行业中，数据处理延迟可能导致重大损失甚至危及生命。
- en: Applications that rely on real-time data can offer a more responsive and interactive
    user experience. For example, social media platforms, online gaming, and live
    sports streaming rely on real-time data to keep users engaged and provide a seamless
    experience.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于实时数据的应用程序可以提供更响应和互动的用户体验。例如，社交媒体平台、在线游戏和体育直播都依赖于实时数据来保持用户参与并提供无缝体验。
- en: This chapter is all about real-time streaming with microservices. Our purpose
    is to understand when and how to establish such a type of communication when dealing
    with microservices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章全部关于使用微服务进行实时流处理。我们的目的是了解在处理微服务时何时以及如何建立此类通信。
- en: 'This chapter covers the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: What is real-time streaming?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是实时流处理？
- en: Getting started with the earthquake streaming API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用地震流API
- en: Implementing the earthquake stream consumer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现地震流消费者
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow along with the chapter, you will need an IDE (we prefer Visual Studio
    Code), Postman, Docker, and a browser of your choice.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本章内容，您需要一个IDE（我们推荐Visual Studio Code）、Postman、Docker以及您选择的浏览器。
- en: It is preferable to download our repository from [https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript)
    and open the `Ch08` folder to easily follow along with the code snippets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 建议您从[https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript)下载我们的仓库，并打开`Ch08`文件夹，以便轻松跟随代码片段。
- en: What is real-time streaming?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是实时流处理？
- en: '**Real-time streaming** is a data-processing paradigm where data is continuously
    generated, transmitted, and processed as it is created, with minimal delay. Unlike
    batch processing, which collects and processes data in large groups or batches
    at regular intervals, real-time streaming focuses on the immediate and continuous
    flow of data, enabling instant analysis and response. It’s like watching a live
    stream instead of waiting for a video to download entirely.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**实时流处理**是一种数据处理范式，其中数据在创建时即被连续生成、传输和处理，延迟最小。与定期收集和批量处理大量数据或批次的批量处理不同，实时流处理侧重于数据的即时和连续流动，从而实现即时分析和响应。这就像观看直播流而不是等待视频完全下载一样。'
- en: 'Before we continue any further, let us look at some of the key characteristics
    of real-time streaming:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续前进之前，让我们看看实时流处理的一些关键特性：
- en: '**Continuous data flow**: Real-time streaming is like a never-ending flow of
    information coming in all at once from different places. This information can
    be from sensors, people using things online, money being bought and sold, such
    as Bitcoin, and so on.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续数据流**：实时流处理就像来自不同地方的信息永不停止的流动。这些信息可能来自传感器、在线使用物品的人、买卖货币，如比特币等。'
- en: '**Low latency**: The main goal of real-time streaming is to make the delay
    between information being created and it being used as short as possible.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低延迟**：实时流处理的主要目标是使信息从创建到使用之间的延迟尽可能短。'
- en: '**Event-driven processing**: Real-time streaming works by following events
    as they happen, such as things being created or changing. Each event is dealt
    with on its own or in small batches, so the system can react right away to new
    situations.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件驱动处理**：实时流处理通过跟踪事件的发生来工作，例如事物的创建或变化。每个事件都是单独处理或在小批量中处理的，这样系统就可以立即对新情况做出反应。'
- en: '**Scalability**: Real-time streaming systems can handle different amounts and
    speeds of information, growing bigger or smaller depending on how much information
    is coming in'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：实时流系统可以处理不同数量和速度的信息，根据接收到的信息量的大小进行扩展或缩小。'
- en: '**Fault tolerance**: To ensure continuous operation, real-time streaming systems
    incorporate fault tolerance mechanisms, such as data replication and automatic
    recovery from failures. As we mentioned in previous chapters, this is one of the
    important attributes of Apache Kafka, which we plan to also use for this chapter.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：为确保持续运行，实时流系统集成了容错机制，如数据复制和从故障中自动恢复。正如我们在前面的章节中提到的，这是Apache Kafka的重要属性之一，我们计划在本章中也使用它。'
- en: '**Data consistency**: Maintaining data consistency is important in real-time
    streaming, especially when processing brings multiple distributed components into
    the table. Techniques such as **exactly-once processing** and **idempotency**
    are employed to ensure accuracy. Exactly-once processing ensures that each message
    is processed only once, even in the case of failures or retries, preventing duplicates.
    As we use Apache Kafka for most chapters, you can easily configure idempotency
    and exactly-once behavior in it.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据一致性**：在实时流中维护数据一致性很重要，尤其是在处理涉及多个分布式组件时。采用如**精确一次处理**和**幂等性**等技术来确保准确性。精确一次处理确保即使在失败或重试的情况下，每条消息也只被处理一次，从而防止重复。由于我们大多数章节都使用Apache
    Kafka，你可以在其中轻松配置幂等性和精确一次行为。'
- en: The importance of real-time data in modern applications cannot be overstated.
    In today’s data-driven world, the ability to process and act on data as it is
    generated provides a significant competitive edge.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现代应用中实时数据的重要性不容忽视。在当今以数据驱动的世界中，能够处理和利用生成数据的能力提供了显著的竞争优势。
- en: Why real-time data is essential
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么实时数据是必不可少的
- en: 'Here are some key reasons why real-time data is a must-have for most modern
    applications:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为什么实时数据对于大多数现代应用来说是必不可少的几个关键原因：
- en: '**Enhanced decision-making**: Real-time data can enhance an application’s decision-making
    abilities because of the following:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强决策能力**：实时数据可以通过以下方式增强应用程序的决策能力：'
- en: '**Immediate insights**: Real-time data provides immediate insights, allowing
    businesses to make informed decisions quickly. This is important in dynamic environments
    such as stock trading, where market conditions can change rapidly.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**即时洞察**：实时数据提供即时洞察，使企业能够快速做出明智的决策。这在股票交易等动态环境中尤为重要，因为市场状况可能会迅速变化。'
- en: '**Proactive problem-solving**: By continuously monitoring data, organizations
    can identify and address issues before they escalate, reducing downtime and enhancing
    operational efficiency.'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动问题解决**：通过持续监控数据，组织可以在问题升级之前识别并解决它们，减少停机时间并提高运营效率。'
- en: '**Improved user experience**: Real-time data empowers applications to provide
    a more dynamic and personalized user experience by enhancing interactivity and
    responsiveness while tailoring content to individual preferences.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进的用户体验**：实时数据使应用程序能够通过增强交互性和响应性，并根据个人偏好定制内容，提供更动态和个性化的用户体验。'
- en: '**Operational efficiency**: Organizations can significantly boost their efficiency
    by using real-time data, which enables both real-time monitoring and automation
    of processes, helping to streamline operations and reduce costs.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运营效率**：组织可以通过使用实时数据显著提高效率，这既可以实现实时监控，也可以实现流程的自动化，有助于简化运营并降低成本。'
- en: '**Competitive advantage**: Leveraging real-time data gives businesses a distinct
    edge by enhancing their agility and fostering innovation, allowing them to swiftly
    respond to market changes and create cutting-edge products and services.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**竞争优势**：利用实时数据通过提高灵活性和促进创新，为业务提供独特的优势，使他们能够迅速响应市场变化并创造尖端的产品和服务。'
- en: '**Increased revenue**: Utilizing real-time data enables businesses to enhance
    their revenue streams through optimized marketing strategies and dynamic fraud
    detection, ensuring more effective customer engagement and financial security.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加收入**：利用实时数据通过优化的营销策略和动态欺诈检测，使企业能够增强其收入流，确保更有效的客户参与和财务安全。'
- en: '**Enhanced security**: Real-time data strengthens security by enabling continuous
    monitoring and anomaly detection, allowing organizations to quickly identify and
    respond to potential threats and system irregularities.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强安全性**：实时数据通过实现连续监控和异常检测来加强安全性，使组织能够快速识别和应对潜在的威胁和系统异常。'
- en: '**Scalability and flexibility**: Real-time data systems provide the ability
    to efficiently handle large volumes of data while maintaining adaptability, ensuring
    optimal performance even as data loads and requirements fluctuate.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和灵活性**：实时数据系统提供了高效处理大量数据的能力，同时保持适应性，确保即使在数据负载和要求波动时也能保持最佳性能。'
- en: '**Customer satisfaction**: Real-time data enhances customer satisfaction by
    enabling instant support and immediate feedback, allowing businesses to quickly
    address concerns and continuously improve their products and services.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户满意度**：实时数据通过提供即时支持和即时反馈来提高客户满意度，使企业能够迅速解决关注点并持续改进其产品和服务。'
- en: Real-time streaming allows data to be processed as it’s generated, offering
    immediate insights and responses. This continuous flow of data, coupled with low
    latency and event-driven processing, is crucial in industries such as finance,
    healthcare, and logistics. The ability to make real-time decisions, enhance user
    experiences, and improve operational efficiency provides businesses with a competitive
    edge, fostering innovation and increasing revenue while ensuring system scalability,
    fault tolerance, and enhanced security.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流允许数据在生成时进行处理，提供即时的洞察和响应。这种连续的数据流动，结合低延迟和事件驱动处理，在金融、医疗保健和物流等行业至关重要。实时决策的能力、提升用户体验和改善运营效率为企业提供了竞争优势，促进了创新，增加了收入，同时确保了系统的可扩展性、容错性和增强的安全性。
- en: Understanding use cases
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解使用案例
- en: 'At the beginning of the chapter, I briefly mentioned that the use of real-time
    data can have incredible impacts on some industries. Therefore, it is crucial
    to understand the use cases of real-time data when you design your microservices.
    Let’s look at these use cases here:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我简要提到了实时数据的使用可以对某些行业产生惊人的影响。因此，在设计微服务时理解实时数据的使用案例至关重要。让我们来看看这些案例：
- en: '**Financial services**: Real-time data plays a pivotal role in this industry,
    enabling algorithmic stock trading with split-second decisions and supporting
    continuous risk management to ensure compliance and mitigate potential financial
    threats.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融服务**：实时数据在这一行业中扮演着关键角色，它使算法股票交易能够做出毫秒级的决策，并支持持续的风险管理，以确保合规并减轻潜在的金融威胁。'
- en: '**Healthcare**: Real-time data is transforming healthcare by enabling continuous
    patient monitoring for timely interventions and enhancing telemedicine through
    real-time video consultations and data sharing, improving patient care and accessibility.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗保健**：实时数据通过实现连续的患者监测以进行及时干预，并通过实时视频咨询和数据共享来增强远程医疗，改善患者护理和可及性。'
- en: '**Retail and e-commerce**: Real-time data enhances retail and e-commerce operations
    by optimizing inventory management to prevent shortages and enabling dynamic pricing
    strategies that adjust to demand and competitor activity.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零售与电子商务**：实时数据通过优化库存管理以防止短缺，并启用动态定价策略，这些策略能够根据需求和竞争对手的活动进行调整，从而增强零售和电子商务的运营。'
- en: '**Transportation and logistics**: Real-time data optimizes fleet management
    by improving route planning and delivery times, while real-time traffic data enhances
    traffic management, reducing congestion and improving overall mobility.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交通运输与物流**：实时数据通过优化路线规划和配送时间，优化车队管理，同时实时交通数据增强交通管理，减少拥堵并提高整体流动性。'
- en: '**Telecommunications**: Real-time data enhances network management by ensuring
    continuous performance monitoring for optimal service quality, while also improving
    customer experience through the rapid resolution of network issues'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电信**：实时数据通过确保持续的性能监控以优化服务质量，同时通过快速解决网络问题来改善客户体验。'
- en: In the end, what we can say for sure is that real-time data is a cornerstone
    of modern applications, driving enhanced decision-making, improved user experiences,
    operational efficiency, and competitive advantage. By leveraging real-time data,
    organizations can innovate, adapt, and thrive in a rapidly changing digital landscape.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以肯定地说，实时数据是现代应用的基础，推动决策能力、用户体验、操作效率和竞争优势的提升。通过利用实时数据，组织可以创新、适应并在快速变化的数字领域中蓬勃发展。
- en: Relationship between real-time streaming and microservices
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时流式传输与微服务之间的关系
- en: 'Now that we have understood what real-time data is and exactly why it’s necessary,
    it’s time we understood the relationship between real-time streaming and microservices.
    The union of real-time streaming with microservices is a symbiotic one, which
    extends the power, productivity, and scalability of modern software architectures.
    Systems now, as a service, are more reactive, more adaptable, and faster as a
    result of this integration. Let’s try to understand how real-time streaming and
    microservices play well with each other:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了什么是实时数据以及为什么它是必要的，是时候了解实时流式传输与微服务之间的关系了。实时流式传输与微服务的结合是一种共生关系，它扩展了现代软件架构的力量、生产力和可扩展性。现在，作为服务的系统，由于这种集成，变得更加反应灵敏、更加适应性强，运行速度也更快。让我们尝试了解实时流式传输和微服务是如何相互配合的：
- en: '**Decoupling and scalability**: Real-time streaming complements microservices
    by promoting loose coupling and independent scaling, allowing services to communicate
    asynchronously and scale efficiently based on demand.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解耦和可扩展性**：实时流式传输通过促进松散耦合和独立扩展，补充了微服务，允许服务异步通信并根据需求高效扩展。'
- en: '**Flexibility and agility**: The combination of real-time streaming with microservices
    enhances flexibility and agility, enabling continuous service evolution and real-time
    data processing for applications requiring immediate insights and rapid iteration.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性和敏捷性**：实时流式传输与微服务的结合增强了灵活性和敏捷性，使服务能够持续进化，并为需要即时洞察和快速迭代的实时数据处理应用提供支持。'
- en: '**Resilience and fault tolerance**: Integrating real-time streaming with microservices
    enhances resilience and fault tolerance by isolating failures to individual services
    and ensuring data durability, allowing for seamless recovery and continuous operation
    even in the event of service disruptions.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性和容错性**：将实时流式传输与微服务集成，通过将故障隔离到单个服务并确保数据持久性，增强了弹性和容错性，即使在服务中断的情况下，也能实现无缝恢复和持续运行。'
- en: '**Real-time communication**: Real-time streaming enhances communication within
    microservices by enabling event-driven architecture and immediate data propagation,
    allowing services to interact asynchronously and respond quickly to events, leading
    to more responsive and synchronized systems.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时通信**：实时流式传输通过启用事件驱动架构和立即数据传播，增强了微服务内部的通信，允许服务异步交互并快速响应事件，从而实现更响应和同步的系统。'
- en: '**Operational efficiency**: Combining real-time streaming with microservices
    enhances operational efficiency by optimizing resource utilization and simplifying
    data pipelines, allowing continuous data flow and reducing the complexity of traditional
    batch-processing methods.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作效率**：将实时流式传输与微服务相结合，通过优化资源利用和简化数据管道，提高了操作效率，允许持续数据流动并减少传统批量处理方法的复杂性。'
- en: '**Enhanced monitoring and analytics**: Integrating real-time streaming with
    microservices enables real-time monitoring and analytics, offering immediate visibility
    into service performance and providing actionable insights that allow for the
    proactive management and dynamic optimization of services.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的监控和分析**：将实时流式传输与微服务集成，可以实现实时监控和分析，提供对服务性能的即时可见性，并提供可操作见解，允许主动管理和动态优化服务。'
- en: The synergy between real-time streaming and microservices offers a robust framework
    for building responsive, scalable, and efficient systems. By leveraging the strengths
    of both paradigms, organizations can create applications that are capable of handling
    dynamic workloads, providing real-time insights, and delivering superior user
    experiences. This combination is particularly powerful in environments where rapid
    data processing and immediate reactions are critical to success.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流和微服务之间的协同作用为构建响应迅速、可扩展和高效的系统提供了一个强大的框架。通过利用这两种范例的优势，组织可以创建能够处理动态工作负载、提供实时洞察并交付卓越用户体验的应用程序。这种组合在需要快速数据处理和即时反应以成功的环境中尤其强大。
- en: Microservices we will develop
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们将开发的微服务
- en: 'To make our learning process more interactive and more understandable, we will
    develop two simple microservices. The first microservice will act as a producer
    of stream and the domain of this microservices will be an earthquake. An API that
    streams real-time information about earthquakes can be valuable for several reasons:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的学习过程更加互动和易于理解，我们将开发两个简单的微服务。第一个微服务将充当流的生产者，该微服务的领域是地震。一个提供关于地震实时信息的API可以出于几个原因而变得有价值：
- en: '**Emergency response**: Real-time data can be crucial for emergency responders
    who need to assess damage and deploy resources quickly after an earthquake. The
    API will provide information on the location, magnitude, and depth of the earthquake,
    which can help responders prioritize areas that may be most affected.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应急响应**：对于需要评估地震造成的损害并迅速部署资源的应急响应人员来说，实时数据可能至关重要。API将提供关于地震的位置、震级和深度的信息，这有助于响应人员优先考虑可能受影响最大的地区。'
- en: '**Public awareness**: The API could be used for public awareness to create
    applications that send alerts to people in affected areas. This could help people
    take shelter or evacuate if necessary.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公众意识**：API可以用于公众意识，创建向受影响地区的人们发送警报的应用程序。这有助于人们在必要时寻找避难所或疏散。'
- en: '**Research**: Researchers can use the API to track earthquake activity and
    improve their understanding of earthquake patterns. This data can be used to develop
    better earthquake prediction models and improve building codes.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研究**：研究人员可以使用API跟踪地震活动并提高他们对地震模式的理解。这些数据可以用于开发更好的地震预测模型并改进建筑规范。'
- en: '**News and media**: News organizations can use the API to get real-time updates
    on earthquake activity, which can help them report on the latest developments.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新闻和媒体**：新闻机构可以使用API获取关于地震活动的实时更新，这有助于他们报道最新进展。'
- en: In addition to these, there are commercial applications for such an API as well.
    For instance, insurance companies could use it to assess potential risks and losses,
    or engineering firms could use it to design earthquake-resistant structures.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，此类API还有商业应用。例如，保险公司可以使用它来评估潜在的风险和损失，或者工程公司可以使用它来设计抗震结构。
- en: Of course, when building such type of APIs for production, we will need to choose
    a reliable source of earthquake data; but to demonstrate the purpose and implementation
    of real-time data streaming, our API will act as a source of truth.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在构建此类生产API时，我们需要选择一个可靠的地震数据来源；但为了展示实时数据流的目的和实现，我们的API将充当事实来源。
- en: From a data format perspective, we should select a format for the data that
    is easy to use and integrate with other applications. Common formats include JSON
    and XML. Our choice is JSON.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据格式角度来看，我们应该选择一个易于使用且易于与其他应用程序集成的数据格式。常见的格式包括JSON和XML。我们的选择是JSON。
- en: By providing valuable and timely data, your earthquake API can be a useful tool
    for a variety of users.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供有价值且及时的数据，您的地震API可以成为各种用户的有用工具。
- en: The second microservice is going to be a consumer of the data. Throughout our
    learning process, we have implemented our microservices using different packages
    and frameworks with nearly full skeletons. For the current chapter, our focus
    is streaming rather than building an application skeleton from scratch. Our focus
    is not on implementing any architecture. You can refer to previous chapters if
    you want to add additional functionalities and make it a fully self-contained
    architectural application.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个微服务将成为数据的消费者。在我们学习的过程中，我们已经使用不同的包和框架实现了微服务，几乎包含了完整的骨架。对于当前章节，我们的重点是流处理，而不是从头开始构建应用程序骨架。我们的重点不在于实现任何架构。如果你想添加额外的功能并使其成为一个完全自包含的架构应用程序，可以参考前面的章节。
- en: Getting started with an earthquake streaming API
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用地震流式API
- en: 'In our GitHub repository, in the `Ch08` folder, we have two subfolders: `earthquakeService`,
    the earthquake streaming API, and `earthquakeConsumer`, the consumer API. As we
    mentioned before, our main focus is on implementing streaming. To make this chapter
    more focused on the topic, we haven’t implemented a proper detailed design for
    this API. This is also the case with the consumer API.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的GitHub仓库中，`Ch08` 文件夹下有两个子文件夹：`earthquakeService`，地震流式API，和 `earthquakeConsumer`，消费者API。正如我们之前提到的，我们的主要重点是实现流处理。为了使本章更专注于主题，我们没有为这个API实现详细的设计。消费者API也是如此。
- en: It is best to follow along by creating everything with us from scratch.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的做法是跟随我们从头开始创建所有内容。
- en: '`earthquakeService` has the following dependencies:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`earthquakeService` 有以下依赖项：'
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, you need to generate a `package.json` file that contains all dependencies.
    To create the file, run `npm init` and follow the prompts from the terminal. After
    `package.json` is created, run the `npm install 'your_required_package_names'`
    template command to install packages one by one. For example, to install the `express`
    package, just run `npm install express`, and hit *Enter*. We have already talked
    about `package.json` and the package installation process. You can check the previous
    chapters for more information. While we have reused some of the microservices
    from our previous chapter in our current chapter, we’re also going to use `node-rdkafka`
    package which is new for us.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要生成一个包含所有依赖项的 `package.json` 文件。要创建该文件，请运行 `npm init` 并遵循终端中的提示。在 `package.json`
    创建完成后，运行 `npm install 'your_required_package_names'` 模板命令逐个安装包。例如，要安装 `express`
    包，只需运行 `npm install express`，然后按 *Enter*。我们已经讨论过 `package.json` 和包安装过程。你可以查看前面的章节以获取更多信息。虽然我们在当前章节中重复使用了上一章的一些微服务，但我们还将使用对我们来说全新的
    `node-rdkafka` 包。
- en: '`node-rdkafka` is a Node.js library that provides a wrapper around the native
    librdkafka library, enabling efficient communication with Apache Kafka for high-performance
    data streaming. It leverages the power of `librdkafka` for efficient communication
    with Kafka and handles complexities such as balancing writes and managing brokers,
    making Kafka interaction easier for developers.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`node-rdkafka` 是一个Node.js库，它为原生的 librdkafka 库提供了一个包装器，使得与Apache Kafka进行高效数据流通信成为可能。它利用
    `librdkafka` 的强大功能，以高效的方式与Kafka通信，并处理诸如平衡写入和管理代理等复杂性，使得Kafka交互对开发者来说更加容易。'
- en: 'You can install `node-rdkafka` using `npm`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `npm` 安装 `node-rdkafka`：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is not the only package to use for streaming, and depending on your personal
    preference, you can select any other one. The `node-rdkafka` package supports
    a really easy stream writing and reading process, which is why we prefer to use
    it in this chapter for learning purposes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是用于流处理的唯一包，根据你个人的喜好，你可以选择任何其他包。`node-rdkafka` 包支持非常简单的流写入和读取过程，这就是为什么我们选择在本章中用于学习目的。
- en: Note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You should always try to use official packages for production apps. Using official
    packages helps keep your app safe because trusted developers manage them, and
    they are checked often. They are also more reliable, as they are tested, updated,
    and have good support, which is important for apps in production.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该始终尝试在生产应用程序中使用官方包。使用官方包有助于保持你的应用程序安全，因为可信的开发者管理它们，并且它们经常被检查。它们也更加可靠，因为它们经过测试、更新，并且有良好的支持，这对于生产中的应用程序来说非常重要。
- en: We use Apache Kafka as a streaming platform. So, you need Apache Kafka to be
    running. As before, we plan to use the `docker-compose.yml` file, which should
    be up and running with Apache Kafka. Our `docker-compose.yml` file for this example
    will only contain the services needed for Kafka, excluding unnecessary components
    like PostgreSQL to reduce resource usage. Of course, you can run the `docker-compose.yml`
    file from the previous chapters that use Apache Kafka, but having additional services
    will use up more resources on your PC.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Apache Kafka 作为流平台。因此，你需要确保 Apache Kafka 正在运行。和之前一样，我们计划使用 `docker-compose.yml`
    文件，该文件应该已经启动并运行 Apache Kafka。本例中的 `docker-compose.yml` 文件将只包含 Kafka 所需的服务，排除不必要的组件（如
    PostgreSQL）以减少资源使用。当然，你可以从之前章节中使用的包含 Apache Kafka 的 `docker-compose.yml` 文件运行，但额外的服务将占用你电脑上更多的资源。
- en: 'Here is our `docker-compose.yml` file:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的 `docker-compose.yml` 文件：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this configuration, we define `INTERNAL` and `EXTERNAL` listeners to differentiate
    between connections within the Docker network (`INTERNAL://kafka1:9092`) and connections
    from outside the Docker network, such as your local machine (`EXTERNAL://localhost:29092`).
    This separation ensures that services within the Docker network can use the internal
    address, while external clients (like a Node.js app running on your host) can
    connect using the external port. By doing so, Kafka can properly advertise the
    correct addresses to different clients, avoiding connection issues caused by mismatched
    listener configurations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，我们定义了 `INTERNAL` 和 `EXTERNAL` 监听器以区分 Docker 网络内的连接（`INTERNAL://kafka1:9092`）和来自
    Docker 网络外部的连接，例如你的本地机器（`EXTERNAL://localhost:29092`）。这种分离确保 Docker 网络内的服务可以使用内部地址，而外部客户端（如运行在主机上的
    Node.js 应用程序）可以使用外部端口连接。通过这样做，Kafka 可以正确地向不同的客户端广告正确的地址，避免因监听器配置不匹配而导致的连接问题。
- en: 'This file contains Apache Kafka, the Kafka UI, and ZooKeeper. Just check our
    root folder (`Ch08/earthquakeService`) to find and run it. To run the `docker-compose.yml`
    file, first launch Docker Desktop, ensure it’s running, and then follow these
    steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件包含 Apache Kafka、Kafka UI 和 ZooKeeper。只需检查我们的根文件夹（`Ch08/earthquakeService`）以找到并运行它。要运行
    `docker-compose.yml` 文件，首先启动 Docker Desktop，确保它正在运行，然后按照以下步骤操作：
- en: Pull and open `Ch08` from the repository.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从仓库中拉取并打开 `Ch08`。
- en: Open the project from Visual Studio Code (or any text editor you prefer) and
    navigate to `Ch08`.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Visual Studio Code（或你喜欢的任何文本编辑器）打开项目并导航到 `Ch08`。
- en: If you use Visual Studio Code, then go to **Terminal** | **New Terminal** from
    the **Menu**; otherwise, use the command line to navigate to the root folder.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用 Visual Studio Code，则从 **菜单** 中选择 **终端** | **新建终端**；否则，使用命令行导航到根目录。
- en: Run the `docker-compose up -d` command from the terminal (*Figure 8**.1*).
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从终端运行 `docker-compose up -d` 命令（*图 8**.1*）。
- en: '![Figure 8.1: Docker Desktop after running the docker-compose.yml file](img/B09148_08_001.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1：运行 docker-compose.yml 文件后的 Docker Desktop](img/B09148_08_001.jpg)'
- en: 'Figure 8.1: Docker Desktop after running the docker-compose.yml file'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：运行 docker-compose.yml 文件后的 Docker Desktop
- en: To connect to Apache Kafka, we need to store the required configuration in a
    separate file. That is why we use the `dotenv` package to read configuration information.
    Create a `configs` folder under the root folder (`Ch08/earthquake`) and add a
    `.``env` file.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到 Apache Kafka，我们需要将所需的配置存储在单独的文件中。这就是为什么我们使用 `dotenv` 包来读取配置信息。在根目录下（`Ch08/earthquake`）创建一个
    `configs` 文件夹并添加一个 `.env` 文件。
- en: Note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `config` and `configs` folders are separate and serve different purposes.
    Be sure to use the correct folder to avoid confusion. We store the `.env` file
    under the `configs` folder. On the other hand, we store the `config.js` file under
    the `config` folder, which loads environment variables using the `dotenv` package,
    validates them with `Joi`, and returns a configuration object for a Kafka-based
    microservice, throwing an error if validation fails.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`config` 和 `configs` 文件夹是分开的，用于不同的目的。请确保使用正确的文件夹以避免混淆。我们将 `.env` 文件存储在 `configs`
    文件夹下。另一方面，我们将 `config.js` 文件存储在 `config` 文件夹下，该文件使用 `dotenv` 包加载环境变量，使用 `Joi`
    进行验证，并为基于 Kafka 的微服务返回一个配置对象，如果验证失败则抛出错误。'
- en: 'Here is what the `configs/.env` file should look like:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 `configs/.env` 文件应该看起来像的：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We have Kafka configuration such as client ID, brokers, and topic name with
    port information. As we learned before, all application source code lives under
    the `src` folder. Create the `src` folder on the same level as your `configs`
    folder (*Figure 8**.2*).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有Kafka配置，如客户端ID、代理和主题名称以及端口信息。正如我们之前所学的，所有应用程序源代码都位于 `src` 文件夹下。在 `configs`
    文件夹同一级别创建 `src` 文件夹（*图8**.2*）。
- en: '![Figure 8.2: General structure of earthquakeService](img/B09148_08_002.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2：earthquakeService的一般结构](img/B09148_08_002.jpg)'
- en: 'Figure 8.2: General structure of earthquakeService'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：earthquakeService的一般结构
- en: 'We store configuration information in the `.env` file, but we need to add a
    reading and validating mechanism over our `config`. To implement proper reading
    and validating, we need to create a `configs.js` file under the `src/configc`
    folder. Here is what it looks like:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `.env` 文件中存储配置信息，但我们需要在 `config` 上添加一个读取和验证机制。为了实现正确的读取和验证，我们需要在 `src/configc`
    文件夹下创建一个 `configs.js` 文件。它看起来是这样的：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We are using the same `config` read and validation mechanism as the account
    microservice. We have already explained this file in the [*Chapter 7*](B09148_07.xhtml#_idTextAnchor121).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与账户微服务相同的 `config` 读取和验证机制。我们已经在 [*第7章*](B09148_07.xhtml#_idTextAnchor121)
    中解释了此文件。
- en: 'Our `services` folder is responsible for storing service files. To implement
    real-time streaming functionality, we need to create a new file called `earthquake.js`
    under the `services` folder. Here is what it looks like:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `services` 文件夹负责存储服务文件。为了实现实时流功能，我们需要在 `services` 文件夹下创建一个名为 `earthquake.js`
    的新文件。它看起来是这样的：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code defines a class called `EarthquakeEventProducer` that simulates generating
    and publishing earthquake event data to a Kafka topic. Let’s walk through the
    code’s elements here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个名为 `EarthquakeEventProducer` 的类，该类模拟生成并将地震事件数据发布到Kafka主题。让我们来分析一下代码的各个元素：
- en: '`require(''node-rdkafka'')`: Imports the `node-rdkafka` library for interacting
    with a Kafka cluster.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`require(''node-rdkafka'')`: 导入 `node-rdkafka` 库以与Kafka集群交互。'
- en: '`require(''../config/config'')`: Imports a function (likely from `../config/config.js`)
    that reads configuration settings from a file.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`require(''../config/config'')`: 导入一个函数（可能来自 `../config/config.js`），该函数从文件中读取配置设置。'
- en: '`require(''path''):` Imports the `path` module for file path manipulation.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`require(''path'')`: 导入 `path` 模块以进行文件路径操作。'
- en: '`The EarthquakeEventProducer class`: This class handles earthquake event generation
    and publishing.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`The EarthquakeEventProducer class`: 此类处理地震事件生成和发布。'
- en: '`#generateEarthquakeEvent()`: This private method generates a simulated earthquake
    event object with the following properties:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#generateEarthquakeEvent()`: 这个私有方法生成一个具有以下属性的模拟地震事件对象：'
- en: '`id`: A random unique identifier string.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id`: 一个随机唯一的标识符字符串。'
- en: '`magnitude`: A random floating-point number between `0` and `9` representing
    the earthquake’s magnitude'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`magnitude`: 表示地震强度的介于 `0` 和 `9` 之间的随机浮点数'
- en: '`location`: An object containing the following:'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`location`: 包含以下内容的对象：'
- en: '`latitude`: A random floating-point number between `-90` and `90` representing
    the latitude.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latitude`: 一个介于 `-90` 和 `90` 之间的随机浮点数，表示纬度。'
- en: '`longitude`: A random floating-point number between `-180` and `180` representing
    the longitude.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`longitude`: 一个介于 `-180` 和 `180` 之间的随机浮点数，表示经度。'
- en: '`timestamp`: The current timestamp in milliseconds.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestamp`: 当前时间戳（毫秒）。'
- en: 'Here is how we specify our main method called `runEarthquake`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何指定我们的主方法 `runEarthquake` 的：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s break this code down here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里分解这段代码：
- en: '`runEarthquake()`: This async method is responsible for setting up the Kafka
    producer and publishing earthquake events.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runEarthquake()`: 这个异步方法负责设置Kafka生产者和发布地震事件。'
- en: '`configPath`: This constructs the path to the configuration file using `path.join`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`configPath`: 使用 `path.join` 构建配置文件的路径。'
- en: '`appConfig`: This reads configuration from the file using the imported `createConfig`
    function.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`appConfig`: 使用导入的 `createConfig` 函数从文件中读取配置。'
- en: '`stream`: This creates a Kafka producer write stream using `Kafka.Producer.createWriteStream`.
    The configuration includes the following:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream`: 使用 `Kafka.Producer.createWriteStream` 创建一个Kafka生产者写入流。配置包括以下内容：'
- en: '`''metadata.broker.list''`: A comma-separated list of Kafka broker addresses
    from the configuration'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''metadata.broker.list''`: 从配置中获取的Kafka代理地址的逗号分隔列表'
- en: '`''client.id''`: A unique identifier for this producer client from the configuration'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''client.id''`: 从配置中为这个生产者客户端提供的唯一标识符'
- en: '`Topic`: The exact topic that should get the streamed data'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Topic`: 应该接收流数据的确切主题'
- en: '`stream.on(''error'')`: This attaches an event listener for errors in the Kafka
    stream. It logs the error message to the console.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream.on(''error'')`: 这为 Kafka 流中的错误附加了一个事件监听器。它将错误消息记录到控制台。'
- en: '`setInterval`: This sets up an interval timer to generate and publish events
    every 100 milliseconds (adjustable). Inside the interval callback is the following:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setInterval`: 这设置了一个间隔计时器，每 100 毫秒（可调整）生成和发布事件。在间隔回调中是以下内容：'
- en: '`event`: Generates a new earthquake event object using `#generateEarthquakeEvent`'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event`: 使用 `#generateEarthquakeEvent` 生成一个新的地震事件对象'
- en: '`stream.write`: Attempts to write the event data (converted to a buffer using
    `JSON.stringify`) to the Kafka stream'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stream.write`: 尝试将事件数据（使用 `JSON.stringify` 转换为缓冲区）写入 Kafka 流'
- en: '`queuedSuccess`: Checks the return value from `stream.write`:'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`queuedSuccess`: 检查 `stream.write` 的返回值：'
- en: '`true`: Indicates successful queuing of the message. A success message is logged
    to the console.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`true`: 表示消息成功排队。将成功消息记录到控制台。'
- en: '`false`: Indicates the stream’s queue is full. A message about exceeding the
    queue capacity is logged to the console.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`false`: 表示流的队列已满。将关于超出队列容量的消息记录到控制台。'
- en: 'In order to stop our earthquake service, we need to clear the interval:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了停止我们的地震服务，我们需要清除间隔：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `stopEarthquake()` method stops the ongoing earthquake event stream by checking
    whether there is an active interval running, indicated by the presence of `this.intervalId`.
    If the interval exists, it uses `clearInterval()` to stop the event generation
    and resets `this.intervalId` to `null` to indicate that the stream has stopped.
    A success message is logged when the stream is stopped. If no interval is running
    (i.e., `this.intervalId` is `null`), it logs a message saying there’s no active
    stream to stop. This ensures that the function can only stop an existing stream
    and won’t attempt to stop a non-existent one.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`stopEarthquake()` 方法通过检查是否存在正在运行的间隔（由 `this.intervalId` 的存在表示）来停止正在进行的地震事件流。如果存在间隔，它使用
    `clearInterval()` 停止事件生成并将 `this.intervalId` 重置为 `null` 以指示流已停止。当流停止时，记录一条成功消息。如果没有正在运行的间隔（即
    `this.intervalId` 为 `null`），它记录一条消息说没有活动流可以停止。这确保了该函数只能停止现有流，而不会尝试停止不存在的流。'
- en: In the end, this code simulates earthquake event generation and publishes these
    events to a Kafka topic at regular intervals, demonstrating basic Kafka producer
    usage with error handling and logging.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，此代码模拟地震事件生成，并定期将这些事件发布到 Kafka 主题，展示了带有错误处理和日志记录的基本 Kafka 生产者使用。
- en: 'We plan to launch streaming using an API, but to make things as simple as possible,
    we use a minimal API approach that doesn’t require us to create controllers. This
    behavior is implemented in the `app.js` file. Here is the file:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计划通过 API 启动流式传输，但为了使事情尽可能简单，我们使用了一个不需要我们创建控制器的最小 API 方法。这种行为在 `app.js` 文件中实现。以下是该文件：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The code defines two API endpoints using Express.js to start and stop an earthquake
    event stream. The `/earthquake-events/start` endpoint triggers the `runEarthquake()`
    function from the `EarthquakeEventProducer` class, starting the event stream,
    and responds with a success message. The `/earthquake-events/stop` endpoint calls
    the `stopEarthquake()` function to stop the event stream and also responds with
    a success message. The `earthquakeProducer` object is an instance of the `EarthquakeEventProducer`
    class, which manages the event stream operations. Finally, the Express app is
    exported to be used in other parts of the application. This setup allows external
    clients, such as Postman, to control the Kafka event stream through API calls.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用 Express.js 定义了两个 API 端点以启动和停止地震事件流。`/earthquake-events/start` 端点从 `EarthquakeEventProducer`
    类触发 `runEarthquake()` 函数，启动事件流，并返回成功消息。`/earthquake-events/stop` 端点调用 `stopEarthquake()`
    函数停止事件流，并也返回成功消息。`earthquakeProducer` 对象是 `EarthquakeEventProducer` 类的实例，它管理事件流操作。最后，Express
    应用程序被导出以在其他应用程序的部分中使用。这种设置允许外部客户端，如 Postman，通过 API 调用来控制 Kafka 事件流。
- en: 'In an Express.js application, the `index.js` file in the root directory typically
    serves as the entry point for your server. It acts as the central hub where you
    configure and launch your Express app. Here is our `index.js` file:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Express.js 应用程序中，根目录中的 `index.js` 文件通常作为服务器的入口点。它充当配置和启动 Express 应用的中心枢纽。以下是我们的
    `index.js` 文件：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have the following functionalities in the `index.js` file:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `index.js` 文件中有以下功能：
- en: Imports the Express app (`app.js`) and configuration function (`config.js`).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入Express应用(`app.js`)和配置函数(`config.js`)。
- en: Reads configuration from a file using `createConfig`.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`createConfig`从文件中读取配置。
- en: Starts the server using `app.listen` on the configured port and logs a message.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置的端口通过`app.listen`启动服务器并记录一条消息。
- en: Defines functions to gracefully close the server and handle unexpected errors.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义函数以优雅地关闭服务器和处理意外错误。
- en: Attaches event listeners for uncaught exceptions and unhandled promise rejections,
    calling the error-handler function.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为未捕获的异常和未处理的承诺拒绝附加事件监听器，调用错误处理函数。
- en: Finally, calls the `execute` function to start everything.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，调用`execute`函数以启动所有操作。
- en: 'We have implemented our `earthquakeService`; now it is time to test it. Here’s
    how you can do that:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了`earthquakeService`；现在是时候测试它了。以下是您可以这样做的方法：
- en: Open **Terminal** | **New Terminal** from the menu if you’re using Visual Studio
    Code.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您正在使用Visual Studio Code，请从菜单中选择**终端** | **新终端**。
- en: Navigate to the `src` folder.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`src`文件夹。
- en: 'Run the `node` `index.js` command:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`node index.js`命令：
- en: '[PRE10]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To stop streaming, open Postman and send a POST request to `http://localhost:3001/earthquake-events/stop`
    (*Figure 8**.3*).
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要停止流，打开Postman并向`http://localhost:3001/earthquake-events/stop`发送POST请求（*图8.3*）。
- en: '![Figure 8.3: Stopping event streaming](img/B09148_08_003.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3：停止事件流](img/B09148_08_003.jpg)'
- en: 'Figure 8.3: Stopping event streaming'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3：停止事件流
- en: The topic should automatically be created with some events (*Figure 8**.4*).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 主题应自动创建一些事件（*图8.4*）。
- en: '![Figure 8.4: Apache Kafka event after streaming](img/B09148_08_004.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4：Apache Kafka事件流后](img/B09148_08_004.jpg)'
- en: 'Figure 8.4: Apache Kafka event after streaming'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：Apache Kafka事件流后
- en: We have implemented the streaming API. Now it is time to consume data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了流式API。现在是时候消费数据了。
- en: Implementing the earthquake stream consumer
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现地震流消费者
- en: Producing is not very valuable if you don’t have a consumer to consume data.
    Our second microservice, called `earthquakeConsumer`, is going to consume data
    from Apache Kafka. It has a similar code structure to our streaming API (*Figure
    8**.5*).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有消费者来消费数据，生产就变得没有价值。我们的第二个微服务，称为`earthquakeConsumer`，将从Apache Kafka中消费数据。它与我们的流式API具有相似的代码结构（*图8.5*）。
- en: '![Figure 8.5: Earthquake consumer API structure](img/B09148_08_005.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5：地震消费者API结构](img/B09148_08_005.jpg)'
- en: 'Figure 8.5: Earthquake consumer API structure'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：地震消费者API结构
- en: 'Let’s start from the `configs` folder. As in our first microservice in [*Chapter
    5*](B09148_05.xhtml#_idTextAnchor074), we have a `.env` file inside the folder.
    The responsibility of this folder is to store relevant configurations. Here is
    what it looks like:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`configs`文件夹开始。正如我们在第[*第5章*](B09148_05.xhtml#_idTextAnchor074)中的第一个微服务中一样，文件夹内有一个`.env`文件。这个文件夹的职责是存储相关配置。以下是它的样子：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We introduced an additional configuration, `KAFKA_GROUP_ID`, which identifies
    the consumer group, allowing Kafka to balance partition assignments among consumers.
    It is a string property used to identify a collection of consumer instances and
    acts as the glue that binds consumers together for collaborative consumption.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了一个额外的配置，`KAFKA_GROUP_ID`，它用于标识消费者组，允许Kafka在消费者之间平衡分区分配。它是一个字符串属性，用于标识一组消费者实例，并作为将消费者粘合在一起以进行协作消费的粘合剂。
- en: 'Kafka automatically distributes topic partitions among consumers in the same
    group, allowing parallel processing while ensuring that each partition is consumed
    by only one consumer at a time within the group. If a consumer in the group fails,
    Kafka reassigns its partitions to remaining active consumers, ensuring uninterrupted
    message processing. With proper configuration, consumer groups can achieve exactly-once
    delivery semantics, guaranteeing each message is processed by only one consumer
    exactly once. When working with Kafka consumer groups, it’s essential to understand
    how they manage message consumption and workload distribution across multiple
    consumers. The following are key points to keep in mind when configuring and utilizing
    consumer groups for efficient message processing:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka自动将主题分区分配给同一组中的消费者，允许并行处理，同时确保组内每次只有一个消费者消费一个分区。如果组中的消费者失败，Kafka将其分区重新分配给剩余的活跃消费者，确保消息处理不间断。通过适当的配置，消费者组可以实现一次且仅一次的交付语义，保证每条消息只被一个消费者恰好处理一次。当与Kafka消费者组一起工作时，了解它们如何管理消息消费和跨多个消费者的工作负载分配是至关重要的。以下是在配置和利用消费者组进行高效消息处理时需要记住的关键点：
- en: Only one consumer can process a partition at a time within a group.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组内，一次只有一个消费者可以处理一个分区。
- en: Consumers with different group IDs treat topics as independent streams and don’t
    share the workload.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同组 ID 的消费者将主题视为独立的流，并且不共享工作负载。
- en: Always consider using a meaningful group ID to improve cluster management and
    monitoring.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是考虑使用有意义的组 ID 来提高集群管理和监控。
- en: To read and validate this config, we use the same mechanism as we did for the
    streaming API. We have `src/config/config.js`. It reads and validates our configuration
    with the additional `KAFKA_GROUP_ID`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取和验证此配置，我们使用与流 API 相同的机制。我们有 `src/config/config.js`。它使用额外的 `KAFKA_GROUP_ID`
    读取和验证我们的配置。
- en: 'The main functionality has been implemented inside the `src/service/earthquake.js`
    file. Here is our stream-consuming process:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 主要功能已在 `src/service/earthquake.js` 文件中实现。以下是我们的流消费过程：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This code defines a class named `EarthquakeEventConsumer`, which acts as a
    consumer for messages from a Kafka topic containing earthquake event data. Here’s
    a breakdown of the code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个名为 `EarthquakeEventConsumer` 的类，它作为包含地震事件数据的 Kafka 主题消息的消费者。以下是代码的分解：
- en: '`Kafka` from `node-rdkafka`: This library provides functionalities to interact
    with Kafka as a consumer or producer.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`node-rdkafka` 中的 `Kafka`: 这个库提供了与 Kafka 交互作为消费者或生产者的功能。'
- en: '`createConfig` from `../config/config`: This imports a function from another
    file (`config/config.js`) that reads configuration details.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createConfig` 从 `../config/config`: 这从另一个文件（`config/config.js`）导入了一个函数，用于读取配置详细信息。'
- en: '`path`: This is a built-in Node.js module for manipulating file paths.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path`: 这是一个内置的 Node.js 模块，用于操作文件路径。'
- en: '`EarthquakeEventConsumer`: This class is responsible for consuming earthquake
    event data.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EarthquakeEventConsumer`: 这个类负责消费地震事件数据。'
- en: '`constructor()`: This special method is called when you create a new instance
    of `EarthquakeEventConsumer`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`constructor()`: 这是一个特殊的方法，在创建 `EarthquakeEventConsumer` 的新实例时被调用。'
- en: '`configPath`: This constructs the path to a configuration file (such as a `.env`
    file) containing Kafka connection details such as brokers and group ID.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`configPath`: 这构建了指向包含 Kafka 连接详细信息（如代理和组 ID）的配置文件（如 `.env` 文件）的路径。'
- en: '`appConfig`: This calls the `createConfig` function (imported from another
    file) to read the configuration details from the `.env` file and stores it in
    `this.appConfig`. This makes the configuration accessible throughout the object’s
    lifetime.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`appConfig`: 这调用从另一个文件导入的 `createConfig` 函数，从 `.env` 文件中读取配置详细信息，并将其存储在 `this.appConfig`
    中。这使得配置在整个对象的生命周期内可访问。'
- en: '`this.stream`: This line is the key part. It uses `Kafka.KafkaConsumer.createReadStream`
    to create a stream for reading messages from Kafka. Here’s what the options passed
    to `createReadStream` do:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`this.stream`: 这一行是关键部分。它使用 `Kafka.KafkaConsumer.createReadStream` 来创建一个用于从
    Kafka 读取消息的流。以下是传递给 `createReadStream` 的选项所执行的操作：'
- en: '`''metadata.broker.list''`: This specifies the list of Kafka brokers to connect
    to, obtained from the configuration stored in `this.appConfig`.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''metadata.broker.list''`: 这指定了要连接的 Kafka 代理列表，从存储在 `this.appConfig` 中的配置中获得。'
- en: '`''group.id''`: This sets the consumer group ID, also obtained from the configuration.
    Consumers in the same group will share the messages from a topic among themselves.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''group.id''`: 这设置消费者组 ID，也来自配置。同一组中的消费者将在彼此之间共享主题的消息。'
- en: '`''socket.keepalive.enable''`: This enables a mechanism to keep the connection
    alive with the broker.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''socket.keepalive.enable''`: 这启用了与代理保持连接活跃的机制。'
- en: '`''enable.auto.commit''`: This is set to `true` to enable the automatic committing
    of offsets.'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''enable.auto.commit''`: 这设置为 `true` 以启用自动提交偏移量。'
- en: '`topics`: This specifies the Kafka topic name to consume from, obtained from
    the configuration (likely `librdtesting-01` in this case).'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topics`: 这指定了要消费的 Kafka 主题名称，从配置中获得（在这种情况下可能是 `librdtesting-01`）。'
- en: '`waitInterval`: This is set to `0`, indicating no waiting between attempts
    to receive messages if none are available.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`waitInterval`: 这设置为 `0`，表示如果没有可用的消息，则尝试接收消息之间没有等待时间。'
- en: '`objectMode`: This is set to `false`, meaning the messages received from the
    stream will be raw buffers, not JavaScript objects.'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`objectMode`: 这设置为 `false`，意味着从流中接收到的消息将是原始缓冲区，而不是 JavaScript 对象。'
- en: Crucially, this stream creation happens only once in the constructor, ensuring
    efficiency.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键的是，这个流创建只在构造函数中发生一次，确保效率。
- en: '`async consumeData()`: This is an asynchronous method that initiates the data
    consumption process.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`async consumeData()`: 这是一个异步方法，用于启动数据消费过程。'
- en: '`.on(''data'', ...)`: This sets up a listener for the data event emitted by
    the pre-created stream (`this.stream`). The callback function executes each time
    a new message arrives, logging that a message was received and parsing the JSON-encoded
    data for further handling.The callback function logs a message indicating a new
    message was received. It then parses the raw message buffer (assuming it’s JSON-encoded
    data) using `JSON.parse` and logs the parsed data.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.on(''data'', ...)`: 这设置了一个监听器，用于监听由预先创建的流 (`this.stream`) 发出的数据事件。回调函数在每次收到新消息时执行，记录已收到消息，并解析
    JSON 编码的数据以进行进一步处理。回调函数记录一条消息，表明已收到新消息。然后它使用 `JSON.parse` 解析原始消息缓冲区（假设它是 JSON
    编码的数据），并将解析后的数据记录下来。'
- en: '`module.exports = EarthquakeEventConsumer`: This line exports the `EarthquakeEventConsumer`
    class so it can be used in other parts of your application.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module.exports = EarthquakeEventConsumer`: 这行代码导出 `EarthquakeEventConsumer`
    类，以便在应用程序的其他部分使用。'
- en: To summarize, the code defines a consumer that connects to Kafka, subscribes
    to a specific topic, and listens for incoming earthquake event data. It then parses
    the JSON-encoded messages and logs them to the console. The key improvement here
    is creating the Kafka consumer stream only once in the constructor, making the
    code more efficient.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，代码定义了一个连接到 Kafka、订阅特定主题并监听传入地震事件数据的消费者。然后它解析 JSON 编码的消息并将它们记录到控制台。这里的改进之处在于在构造函数中只创建一次
    Kafka 消费者流，使代码更高效。
- en: To run the service, we have `app.js` and `index.js`, which follow the same structure
    as our streaming API.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行服务，我们有 `app.js` 和 `index.js`，它们的结构与我们流式 API 的结构相同。
- en: 'We have now implemented our `earthquakeConsumer` and it is time to test it:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经实现了 `earthquakeConsumer`，现在是时候对其进行测试了：
- en: Open **Terminal** | **New Terminal** from the menu if you use Visual Studio
    Code.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用 Visual Studio Code，从菜单中选择 **终端** | **新终端**。
- en: Navigate to the `src` folder (`Ch08/earthquakeConsumer/src`).
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 `src` 文件夹 (`Ch08/earthquakeConsumer/src`).
- en: Run the `node` `index.js` command.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `node index.js` 命令。
- en: Note
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You don’t need to manually navigate to the `src` folder and run `node index.js`
    every time you want to start the application. Instead, you can streamline this
    process by configuring a script in your `package.json` file. Simply add the following
    to the `scripts` section of `package.json`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 您不需要每次想要启动应用程序时都手动导航到 `src` 文件夹并运行 `node index.js`。相反，您可以通过在 `package.json`
    文件中配置脚本来自动化此过程。只需将以下内容添加到 `package.json` 的 `scripts` 部分：
- en: '`{`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`{`'
- en: '`“``scripts”: {`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`“``scripts”: {`'
- en: '`“start”: “``node src/index.js”`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`“start”: “``node src/index.js”`'
- en: '`}`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: '`}`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: 'Once this is set up, you can start your application from the root of your project
    by simply running the following:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦设置好，您可以通过简单地运行以下命令从项目的根目录启动应用程序：
- en: '[PRE13]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will automatically launch the application, saving you time and effort
    each time you run the code. When running the earthquake consumer service using
    Node.js, the following output confirms that the service has started successfully
    and is ready for operation:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将自动启动应用程序，每次运行代码时都能节省您的时间和精力。当使用 Node.js 运行地震消费者服务时，以下输出确认服务已成功启动并准备就绪：
- en: '[PRE14]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Go to Postman and hit **Send**.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 Postman 并点击 **发送**。
- en: 'While the `earthquakeService` streaming API prints **The message has been queued!**,
    our consumer API will print consumed data such as that shown here:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然 `earthquakeService` 流式 API 打印 **“消息已入队！”**，但我们的消费者 API 将打印出如以下所示的消费数据：
- en: '[PRE15]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can add some more logic to these services, but this should be enough to
    demonstrate streaming as simply as possible.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以给这些服务添加一些额外的逻辑，但这应该足以尽可能简单地演示流式处理。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explored the concept of real-time data streaming in microservices
    architecture. We used the example of an earthquake data-streaming service to illustrate
    how microservices can efficiently handle continuous flows of information.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了微服务架构中实时数据流的概念。我们使用地震数据流服务的例子来说明微服务如何高效地处理连续的信息流。
- en: Rather than storing data in bulk, the producer service publishes data as a continuous
    stream, allowing immediate processing and analysis as each new data point arrives.
    This approach is beneficial for real-time scenarios where immediate processing
    and analysis are crucial.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 与存储大量数据不同，生产者服务以连续流的形式发布数据，允许在新的数据点到达时立即进行处理和分析。这种方法对于需要立即处理和分析的实时场景非常有用。
- en: Another microservice acts as the consumer in this scenario. It subscribes to
    the earthquake data stream produced by the first service. As new data arrives,
    the consumer microservice receives and processes it in real time.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，另一个微服务充当消费者。它订阅由第一个服务产生的地震数据流。当新数据到达时，消费者微服务会实时接收并处理它。
- en: The consumer microservice can perform various actions based on the earthquake
    data. It might trigger alerts, update dashboards, or integrate with other services
    for further analysis and response.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者微服务可以根据地震数据执行各种操作。它可能会触发警报、更新仪表板，或与其他服务集成以进行进一步分析和响应。
- en: Real-time data streaming with microservices offers a powerful approach to handling
    continuous information flows. In [*Chapter 9*](B09148_09.xhtml#_idTextAnchor147),
    you’ll learn how to secure microservices through authentication, authorization,
    and API protection, while also implementing logging and monitoring tools to proactively
    detect and address potential issues.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微服务进行实时数据流提供了一种强大的方法来处理连续的信息流。在[*第9章*](B09148_09.xhtml#_idTextAnchor147)中，您将学习如何通过身份验证、授权和API保护来保护微服务，同时实施日志记录和监控工具以主动检测和解决潜在问题。
- en: Part 3:Securing, Testing, and Deploying Microservices
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：保护、测试和部署微服务
- en: In this final part, we will focus on the crucial aspects of securing, testing,
    and deploying microservices. We’ll learn about implementing authentication, authorization,
    and monitoring tools to ensure that your microservices are secure and reliable.
    This section also covers the process of building a CI/CD pipeline, which is vital
    for automating the deployment of your microservices, and concludes with strategies
    to deploy our microservices to production.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本最后一部分，我们将重点关注保护、测试和部署微服务的关键方面。我们将学习如何通过实现身份验证、授权和监控工具来确保您的微服务安全可靠。本节还涵盖了构建CI/CD流水线的过程，这对于自动化微服务的部署至关重要，并以将我们的微服务部署到生产环境的策略结束。
- en: 'This part contains the following chapters:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 9*](B09148_09.xhtml#_idTextAnchor147), *Securing Microservices*'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B09148_09.xhtml#_idTextAnchor147), *保护微服务*'
- en: '[*Chapter 10*](B09148_10.xhtml#_idTextAnchor160), *Monitoring Microservices*'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B09148_10.xhtml#_idTextAnchor160), *监控微服务*'
- en: '[*Chapter 11*](B09148_11.xhtml#_idTextAnchor174), *Microservices Architecture*'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B09148_11.xhtml#_idTextAnchor174), *微服务架构*'
- en: '[*Chapter 12*](B09148_12.xhtml#_idTextAnchor196), *Testing Microservices*'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B09148_12.xhtml#_idTextAnchor196), *测试微服务*'
- en: '[*Chapter 13*](B09148_13.xhtml#_idTextAnchor211), *A CI/CD Pipeline for Your
    Microservices*'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第13章*](B09148_13.xhtml#_idTextAnchor211), *为您的微服务构建CI/CD流水线*'
