- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Scalability and Architectural Patterns
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展性与架构模式
- en: In its early days, Node.js was just a non-blocking web server written in C++
    and JavaScript and was called web.js. Its creator, Ryan Dahl, soon realized the
    potential of the platform and started extending it with tools to enable the creation
    of different types of server-side applications on top of JavaScript and the non-blocking
    paradigm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Node.js 的早期，它只是一个用 C++ 和 JavaScript 编写的非阻塞 Web 服务器，被称为 web.js。它的创造者 Ryan Dahl
    很快意识到这个平台的前景，并开始通过工具扩展它，以在 JavaScript 和非阻塞范式之上创建不同类型的后端应用程序。
- en: 'The characteristics of Node.js are perfect for the implementation of distributed
    systems, ranging from a few nodes to thousands of nodes communicating through
    the network: Node.js was born to be distributed.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js 的特性非常适合实现分布式系统，从几个节点到数千个节点通过网络进行通信：Node.js 从诞生之日起就是为了分布式而生的。
- en: Unlike in other web platforms, scalability is a topic that gets explored rather
    quickly in Node.js while developing an application. This is often because of the
    single-threaded nature of Node.js, which is incapable of exploiting all the resources
    of a multi-core machine. But this is just one side of the coin. In reality, there
    are more profound reasons for talking about scalability with Node.js.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他 Web 平台不同，在 Node.js 中开发应用程序时，可扩展性是一个很快就会被探讨的话题。这通常是因为 Node.js 的单线程特性，它无法充分利用多核机器的所有资源。但这只是硬币的一面。实际上，关于
    Node.js 的可扩展性还有更深刻的理由。
- en: 'As we will see in this chapter, scaling an application does not only mean increasing
    its capacity, enabling it to handle more requests faster: it''s also a crucial
    path to achieving high availability and tolerance to errors.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本章中看到的那样，扩展应用程序不仅意味着增加其容量，使其能够更快地处理更多请求：它也是实现高可用性和容错性的关键途径。
- en: Sometimes, we even refer to scalability when we talk about ways to split the
    complexity of an application into more manageable pieces. Scalability is a concept
    with multiple faces, six to be precise, as many as there are faces on a cube—the **scale
    cube**.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们甚至在谈论将应用程序的复杂性分解成更易于管理的部分的方法时，会提到可扩展性。可扩展性是一个具有多个面孔的概念，确切地说，有六个面孔，就像立方体的面一样——**可扩展性立方体**。
- en: 'In this chapter, you will learn the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下主题：
- en: Why you should care about scalability
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么你应该关注可扩展性
- en: What the scale cube is and why it is useful to understand scalability
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性立方体是什么以及为什么理解可扩展性是有用的
- en: How to scale by running multiple instances of the same application
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过运行同一应用程序的多个实例进行扩展
- en: How to leverage a load balancer when scaling an application
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在扩展应用程序时如何利用负载均衡器
- en: What a service registry is and how it can be used
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务注册表是什么以及如何使用它
- en: Running and scaling Node.js applications using container orchestration platforms
    like Kubernetes
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 等容器编排平台运行和扩展 Node.js 应用程序
- en: How to design a microservice architecture out of a monolithic application
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从单体应用程序设计微服务架构
- en: How to integrate a large number of services through the use of some simple architectural
    patterns
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过使用一些简单的架构模式整合大量服务
- en: An introduction to application scaling
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序扩展简介
- en: Scalability can be described as the capability of a system to grow and adapt
    to ever-changing conditions. Scalability is not limited to pure technical growth;
    it is also dependent on the growth of a business and the organization behind it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性可以描述为系统增长和适应不断变化条件的能力。可扩展性不仅限于纯技术增长；它还依赖于业务及其背后的组织的增长。
- en: If you are building the next "unicorn startup" and you expect your product to
    rapidly reach millions of users worldwide, you will face serious scalability challenges.
    How is your application going to sustain ever-increasing demand? Is the system
    going to get slower over time or crash often? How can you store high volumes of
    data and keep I/O under control? As you hire more people, how can you organize
    the different teams effectively and make them able to work autonomously, without
    contention across the different parts of the codebase?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建下一个“独角兽初创公司”，并预期你的产品将迅速覆盖全球数百万用户，你将面临严重的可扩展性挑战。你的应用程序如何才能维持不断增长的需求？系统是否会随着时间的推移而变慢或经常崩溃？你如何存储大量数据并保持
    I/O 在控制之下？随着你雇佣更多的人，你如何有效地组织不同的团队，并使他们能够在代码库的不同部分之间无需争用的情况下独立工作？
- en: Even if you are not working on a high-scale project, that doesn't mean that
    you will be free from scalability concerns. You will just face different types
    of scalability challenges. Being unprepared for these challenges might seriously
    hinder the success of the project and ultimately damage the company behind it.
    It's important to approach scalability in the context of the specific project
    and understand the expectations for current and future business needs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不在一个大规模项目中工作，这并不意味着你将摆脱可扩展性的担忧。你只是会面临不同类型的可扩展性挑战。对这些挑战没有准备可能会严重阻碍项目的成功，并最终损害其背后的公司。在特定项目背景下考虑可扩展性，并理解当前和未来业务需求是至关重要的。
- en: Since scalability is such a broad topic, in this chapter, we will focus our
    attention on discussing the role of Node.js in the context of scalability. We
    will discuss several useful patterns and architectures used to scale Node.js applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可扩展性是一个如此广泛的话题，在本章中，我们将关注讨论Node.js在可扩展性背景下的作用。我们将讨论用于扩展Node.js应用的几个有用的模式和架构。
- en: With these in your toolbelt and a solid understanding of your business context,
    you will be able to design and implement Node.js applications that can adapt and
    satisfy your business needs and keep your customers happy.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的工具箱中有了这些，并且对业务背景有扎实的理解，你将能够设计和实现能够适应并满足你的业务需求，并保持客户满意的Node.js应用。
- en: Scaling Node.js applications
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展Node.js应用
- en: We already know that most of the workload of a typical Node.js application runs
    in the context of a single thread. In *Chapter 1*, *The Node.js Platform*, we
    learned that this is not necessarily a limitation but rather an advantage, because
    it allows the application to optimize the usage of the resources necessary to
    handle concurrent requests, thanks to the non-blocking I/O paradigm. This model
    works wonderfully for applications handling a moderate number of requests per
    second (usually a few hundred per second), especially if the application is mostly
    performing I/O-bound tasks (for example, reading and writing from the filesystem
    and the network) rather than CPU-bound ones (for example, number crunching and
    data processing).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，典型的Node.js应用的大部分工作负载都是在单个线程的上下文中运行的。在*第一章*，*Node.js平台*中，我们了解到这并不一定是一个限制，而是一个优势，因为它允许应用优化处理并发请求所需的资源的使用，这得益于非阻塞I/O范式。这种模式对于每秒处理中等数量请求的应用程序来说非常出色（通常每秒几百个），特别是如果应用程序主要是执行I/O密集型任务（例如，从文件系统和网络中读取和写入）而不是CPU密集型任务（例如，数值计算和数据处理）。
- en: In any case, assuming we are using commodity hardware, the capacity that a single
    thread can support is limited. This is regardless of how powerful a server can
    be, so if we want to use Node.js for high-load applications, the only way is to scale it
    across multiple processes and machines.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，如果我们使用的是通用硬件，单个线程所能支持的容量是有限的。这无论服务器有多强大，所以如果我们想使用Node.js处理高负载应用，唯一的方法就是将其扩展到多个进程和机器上。
- en: However, workload is not the only reason to scale a Node.js application. In
    fact, with the same techniques that allow us to scale workloads, we can obtain
    other desirable properties such as high **availability** and **tolerance to failures**.
    Scalability is also a concept applicable to the size and complexity of an application.
    In fact, building architectures that can grow as much as needed over time is another
    important factor when designing software.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，工作负载并不是扩展Node.js应用的唯一原因。实际上，使用允许我们扩展工作负载的相同技术，我们可以获得其他期望的特性，如高**可用性**和**容错性**。可扩展性也是一个适用于应用程序大小和复杂性的概念。事实上，构建能够随着时间增长而扩展的架构是设计软件时另一个重要的因素。
- en: JavaScript is a tool to be used with caution. The lack of type checking and
    its many gotchas can be an obstacle to the growth of an application, but with
    discipline and accurate design, we can turn some of its downsides into precious
    advantages. With JavaScript, we are often pushed to keep the application simple
    and to split its components it into small, manageable pieces. This mindset can
    make it easier to build applications that are distributed and scalable, but also
    easy to evolve over time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript是一种需要谨慎使用的工具。缺乏类型检查和它的许多陷阱可能会成为应用增长的障碍，但通过纪律和准确的设计，我们可以将一些缺点转化为宝贵的优势。使用JavaScript，我们经常被迫保持应用程序简单，并将其组件拆分成小而易于管理的部分。这种思维方式可以使得构建分布式和可扩展的应用程序变得更容易，同时也便于随着时间的推移进行演变。
- en: The three dimensions of scalability
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性的三个维度
- en: 'When talking about scalability, the first fundamental principle to understand
    is **load distribution**, which is the science of splitting the load of an application
    across several processes and machines. There are many ways to achieve this, and
    the book *The Art of Scalability* by Martin L. Abbott and Michael T. Fisher proposes
    an ingenious model to represent them, called the **scale cube**. This model describes
    scalability in terms of the following three dimensions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到可伸缩性时，首先要理解的基本原则是**负载分布**，这是在多个进程和机器之间分配应用负载的科学。有许多方法可以实现这一点，马丁·L·艾博特（Martin
    L. Abbott）和迈克尔·T·费舍尔（Michael T. Fisher）合著的《可伸缩性艺术》（The Art of Scalability）一书中提出了一种巧妙的模型来表示这些方法，称为**规模立方体**。该模型从以下三个维度描述可伸缩性：
- en: '*X*-axis — Cloning'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*-轴 — 克隆'
- en: '*Y*-axis — Decomposing by service/functionality'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y*-轴 — 通过服务/功能分解'
- en: '*Z*-axis — Splitting by data partition'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Z*-轴 — 通过数据分区拆分'
- en: 'These three dimensions can be represented as a cube, as shown in *Figure 12.1*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个维度可以表示为一个立方体，如图 12.1 所示：
- en: '![](img/B15729_12_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_12_01.png)'
- en: 'Figure 12.1: The scale cube'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：规模立方体
- en: 'The bottom-left corner of the cube (that is, the intersection between the *X*-axis
    and the *Y*-axis) represents the application having all the functionality in a
    single code base and running on a single instance. This is what we generally call
    a **monolithic application**. This is a common situation for applications handling
    small workloads or at the early stages of their development. Given a monolithic
    application, there are three different strategies for scaling it. By looking at
    the scale cube, these strategies are represented as growth along the different
    axes of the cube: *X*, *Y*, and *Z*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 立方体的左下角（即 *X*-轴和 *Y*-轴的交点）表示应用具有所有功能在一个代码库中，并且在一个实例上运行。这是我们通常所说的**单体应用**。这对于处理小工作量或在开发早期阶段的应用来说是一个常见的情况。对于一个单体应用，有三种不同的扩展策略。通过观察规模立方体，这些策略被表示为沿着立方体的不同轴（*X*、*Y*
    和 *Z*）的增长：
- en: '**X****-axis — Cloning**: The most intuitive evolution of a monolithic, unscaled
    application is moving right along the *X*-axis, which is simple, most of the time
    inexpensive (in terms of development cost), and highly effective. The principle
    behind this technique is elementary, that is, cloning the same application *n* times
    and letting each instance handle 1/*n*th of the workload.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X**-轴 — 克隆：单体、未扩展应用的直观演变是沿着 *X*-轴向右移动，这很简单，大多数情况下成本较低（从开发成本的角度来看），并且非常有效。这种技术背后的原理很简单，即克隆相同的应用程序
    *n* 次，让每个实例处理 1/*n* 的工作量。'
- en: '**Y****-axis — Decomposing by service/functionality**: Scaling along the *Y*-axis
    means decomposing the application based on its functionalities, services, or use
    cases. In this instance, *decomposing* means creating different, standalone applications,
    each with its own codebase, possibly with its own dedicated database, and even
    with a separate UI.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Y**-轴 — 通过服务/功能分解：沿着 *Y*-轴进行扩展意味着根据应用的功能、服务或用例来分解应用。在这个例子中，“分解”意味着创建不同的、独立的程序，每个程序都有自己的代码库，可能还有自己的专用数据库，甚至还有独立的用户界面。'
- en: For example, a common situation is separating the part of an application responsible
    for the administration from the public-facing product. Another example is extracting
    the services responsible for user authentication, thereby creating a dedicated authentication server.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，一个常见的情况是将负责管理的一部分应用与面向公众的产品分开。另一个例子是提取负责用户认证的服务，从而创建一个专门的认证服务器。
- en: The criteria to split an application by its functionalities depend mostly on
    its business requirements, the use cases, the data, and many other factors, as we
    will see later in this chapter. Interestingly, this is the scaling dimension with
    the biggest repercussions, not only on the architecture of an application but
    also on the way it is managed from a development and an operational perspective.
    As we will see, **microservice** is a term that is most commonly associated with
    a fine-grained *Y*-axis scaling.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据功能拆分应用的标准主要取决于其业务需求、用例、数据以及许多其他因素，正如我们将在本章后面看到的那样。有趣的是，这是具有最大影响的扩展维度，不仅影响应用的架构，还影响其从开发和运营角度的管理方式。正如我们将看到的，“**微服务**”这个术语最常与细粒度的
    *Y*-轴扩展相关联。
- en: '**Z****-axis — Splitting by data partition**: The last scaling dimension is
    the *Z*-axis, where the application is split in such a way that each instance
    is responsible for only a portion of the whole data. This is a technique often
    used in databases, also known as **horizontal/vertical partitioning**. In this
    setup, there are multiple instances of the same application, each of them operating
    on a partition of the data, which is determined using different criteria.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Z轴 — 数据分区划分**：最后一个缩放维度是*Z*轴，应用程序在这里被分割，使得每个实例只负责整个数据的一部分。这是一种在数据库中常用的技术，也称为**水平/垂直分区**。在这个配置中，有多个相同应用程序的实例，每个实例都在处理数据的一个分区，分区是通过不同的标准确定的。'
- en: For example, we could partition the users of an application based on their country
    (*list partitioning*), or based on the starting letter of their surname (*range
    partitioning*), or by letting a hash function decide the partition each user belongs
    to (*hash partitioning*).
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，我们可以根据用户的国籍（*列表分区*）或根据他们姓氏的首字母（*范围分区*）来划分应用程序的用户，或者让哈希函数决定每个用户所属的分区（*哈希分区*）。
- en: Each partition can then be assigned to a particular instance of the application.
    The use of data partitions requires each operation to be preceded by a lookup
    step to determine which instance of the application is responsible for a given
    datum. As we said, data partitioning is usually applied and handled at the data
    storage level because its main purpose is overcoming the problems related to handling
    large monolithic datasets (limited disk space, memory, and network capacity).
    Applying it at the application level is worth considering only for complex, distributed
    architectures or for very particular use cases such as, for example, when building
    applications relying on custom solutions for data persistence, when using databases
    that don't support partitioning, or when building applications at Google scale.
    Considering its complexity, scaling an application along the *Z*-axis should be
    taken into consideration only after the *X* and *Y* axes of the scale cube have been fully
    exploited.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，每个分区都可以分配给应用程序的特定实例。使用数据分区要求每个操作之前都有一个查找步骤，以确定哪个应用程序实例负责给定的数据项。正如我们所说的，数据分区通常在数据存储级别应用和处理，因为其主要目的是克服处理大型单体数据集（有限的磁盘空间、内存和网络容量）相关的问题。仅在复杂、分布式架构或非常特定的用例（例如，当构建依赖于自定义数据持久性解决方案的应用程序时，当使用不支持分区的数据库时，或当构建在谷歌规模的应用程序时）中，才值得考虑在应用级别应用它。考虑到其复杂性，在充分利用了扩展立方体的*X*和*Y*轴之后，才应考虑沿着*Z*轴扩展应用程序。
- en: In the following sections, we will focus on the two most common and effective
    techniques used to scale Node.js applications, namely, **cloning** and **decomposing**
    by functionality/service.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将重点关注用于扩展Node.js应用程序的两种最常见和最有效的方法，即通过**功能/服务**进行**克隆**和**分解**。
- en: Cloning and load balancing
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克隆和负载均衡
- en: Traditional, multithreaded web servers are usually only scaled horizontally
    when the resources assigned to a machine cannot be upgraded any more, or when
    doing so would involve a higher cost than simply launching another machine.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的、多线程的Web服务器通常只有在分配给机器的资源无法再升级，或者这样做会涉及比简单地启动另一台机器更高的成本时，才会进行水平扩展。
- en: By using multiple threads, traditional web servers can take advantage of all
    the processing power of a server, using all the available processors and memory.
    Conversely, Node.js applications, being single-threaded, are usually scaled much
    sooner compared to traditional web servers. Even in the context of a single machine,
    we need to find ways to "scale" an application in order to take advantage of all
    the available resources.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用多个线程，传统的Web服务器可以利用服务器的所有处理能力，使用所有可用的处理器和内存。相反，Node.js应用程序因为是单线程的，通常比传统的Web服务器更早地进行扩展。即使在单机环境中，我们也需要找到方法来“扩展”应用程序，以便利用所有可用的资源。
- en: 'In Node.js, **vertical scaling** (adding more resources to a single machine)
    and **horizontal scaling** (adding more machines to the infrastructure) are almost
    equivalent concepts: both, in fact, involve similar techniques to leverage all
    the available processing power.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在Node.js中，**垂直扩展**（向单个机器添加更多资源）和**水平扩展**（向基础设施添加更多机器）几乎是等效的概念：事实上，两者都涉及类似的技术来利用所有可用的处理能力。
- en: Don't be fooled into thinking about this as a disadvantage. On the contrary,
    being almost forced to scale has beneficial effects on other attributes of an
    application, in particular, availability and fault-tolerance. In fact, scaling
    a Node.js application by cloning is relatively simple and it's often implemented
    even if there is no need to harvest more resources, just for the purpose of having
    a redundant, fail-tolerant setup.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被这种想法误导，认为这是缺点。相反，几乎被迫进行扩展对应用程序的其他属性有积极的影响，特别是可用性和容错性。实际上，通过克隆来扩展 Node.js
    应用程序相对简单，即使没有收集更多资源的需要，也常常为了拥有冗余、容错的环境而实现。
- en: This also pushes the developer to take into account scalability from the early
    stages of an application, making sure the application does not rely on any resource
    that cannot be shared across multiple processes or machines. In fact, an absolute
    prerequisite to scaling an application is that each instance does not have to
    store common information on resources that cannot be shared, such as memory or
    disk. For example, in a web server, storing the session data in memory or on disk
    is a practice that does not work well with scaling. Instead, using a shared database
    will ensure that each instance will have access to the same session information,
    wherever it is deployed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这也促使开发者从应用程序的早期阶段就考虑可扩展性，确保应用程序不依赖于任何不能在多个进程或机器之间共享的资源。事实上，扩展应用程序的一个绝对前提是每个实例不需要在不能共享的资源上存储公共信息，例如内存或磁盘。例如，在
    Web 服务器中，将会话数据存储在内存或磁盘上是不适合扩展的实践。相反，使用共享数据库将确保每个实例都可以访问相同的会话信息，无论它部署在哪里。
- en: 'Let''s now introduce the most basic mechanism for scaling Node.js applications:
    the `cluster` module.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来介绍扩展 Node.js 应用程序最基本的方法：`cluster` 模块。
- en: The cluster module
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`cluster` 模块'
- en: 'In Node.js, the simplest pattern to distribute the load of an application across
    different instances running on a single machine is by using the `cluster` module,
    which is part of the core libraries. The `cluster` module simplifies the forking of
    new instances of the same application and automatically distributes incoming connections
    across them, as shown in *Figure 12.2*:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Node.js 中，将应用程序的负载分散到单台机器上运行的不同实例的最简单模式是使用 `cluster` 模块，它是核心库的一部分。`cluster`
    模块简化了相同应用程序新实例的派生，并自动将进入的连接分配给它们，如图 *12.2* 所示：
- en: '![](img/B15729_12_02.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_12_02.png)'
- en: 'Figure 12.2: Cluster module schematic'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：`cluster` 模块示意图
- en: The **master process** is responsible for spawning a number of processes (**workers**),
    each representing an instance of the application we want to scale. Each incoming
    connection is then distributed across the cloned workers, spreading the load across them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**主进程**负责启动多个进程（**工作者**），每个进程代表我们想要扩展的应用程序的一个实例。随后，每个进入的连接都会分配到克隆的工作者中，从而在它们之间分散负载。'
- en: Since every worker is an independent process, you can use this approach to spawn
    as many workers as the number of CPUs available in the system. With this approach,
    you can easily allow a Node.js application to take advantage of all the computing
    power available in the system.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个工作者都是一个独立的进程，你可以使用这种方法来启动与系统可用 CPU 数量相同数量的工作者。使用这种方法，你可以轻松地让 Node.js 应用程序利用系统中的所有计算能力。
- en: Notes on the behavior of the cluster module
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于 `cluster` 模块行为的相关说明
- en: In most systems, the `cluster` module uses an explicit round-robin load balancing
    algorithm. This algorithm is used inside the master process, which makes sure
    the requests are evenly distributed across all the workers. Round-robin scheduling
    is enabled by default on all platforms except Windows, and it can be globally
    modified by setting the variable `cluster.schedulingPolicy` and using the constants
    `cluster.SCHED_RR` (round robin) or `cluster.SCHED_NONE` (handled by the operating
    system).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数系统中，`cluster` 模块使用显式的轮询负载均衡算法。该算法在主进程中使用，确保请求在所有工作者之间均匀分布。轮询调度默认在所有平台上启用，除了
    Windows，可以通过设置变量 `cluster.schedulingPolicy` 并使用常量 `cluster.SCHED_RR`（轮询）或 `cluster.SCHED_NONE`（由操作系统处理）来全局修改。
- en: The round-robin algorithm distributes the load evenly across the available servers
    on a rotational basis. The first request is forwarded to the first server, the
    second to the next server in the list, and so on. When the end of the list is
    reached, the iteration starts again from the beginning. In the `cluster` module,
    the round-robin logic is a little bit *smarter* than the traditional implementation.
    In fact, it is enriched with some extra behaviors that aim to avoid overloading
    a given worker process.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 轮询算法基于旋转方式在可用的服务器之间均匀分配负载。第一个请求被转发到第一个服务器，第二个请求转发到列表中的下一个服务器，依此类推。当到达列表的末尾时，迭代从开始处重新开始。在
    `cluster` 模块中，轮询逻辑比传统实现要“聪明”一些。事实上，它增加了一些额外的行为，旨在避免给定的工作进程过载。
- en: 'When we use the `cluster` module, every invocation to `server.listen()` in
    a worker process is delegated to the master process. This allows the master process
    to receive all the incoming messages and distribute them to the pool of workers.
    The `cluster` module makes this delegation process very simple for most use cases,
    but there are several edge cases in which calling `server.listen()` in a worker
    module might not do what you expect:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 `cluster` 模块时，工作进程中对 `server.listen()` 的每次调用都会委托给主进程。这允许主进程接收所有传入的消息并将它们分配给工作进程池。`cluster`
    模块使大多数用例中的委托过程变得非常简单，但也有一些边缘情况，在工作模块中调用 `server.listen()` 可能不会产生预期的结果：
- en: '`server.listen({fd})`: If a worker listens using a specific file descriptor,
    for instance, by invoking `server.listen({fd: 17})`, this operation might produce
    unexpected results. File descriptors are mapped at the process level, so if a
    worker process maps a file descriptor, this won''t match the same file in the
    master process. One way to overcome this limitation is to create the file descriptor
    in the master process and then pass it to the worker process. This way, the worker
    process can invoke `server.listen()` using a descriptor that is known to the master.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`server.listen({fd})`: 如果工作进程使用特定的文件描述符进行监听，例如通过调用 `server.listen({fd: 17})`，此操作可能会产生意外的结果。文件描述符在进程级别进行映射，因此如果工作进程映射了一个文件描述符，这不会与主进程中的相同文件匹配。克服这种限制的一种方法是在主进程中创建文件描述符，然后将其传递给工作进程。这样，工作进程就可以使用主进程已知描述符来调用
    `server.listen()`。'
- en: '`server.listen(handle)`: Listening using `handle` objects (`FileHandle`) explicitly
    in a worker process will cause the worker to use the supplied handle directly,
    rather than delegating the operation to the master process.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`server.listen(handle)`: 在工作进程中显式使用 `handle` 对象（`FileHandle`）进行监听将导致工作进程直接使用提供的句柄，而不是将操作委托给主进程。'
- en: '`server.listen(0)`: Calling `server.listen(0)` will generally cause servers
    to listen on a random port. However, in a cluster, each worker will receive the
    same "random" port each time they call `server.listen(0)`. In other words, the
    port is random only the first time; it will be fixed from the second call on.
    If you want every worker to listen on a different random port, you have to generate
    the port numbers by yourself.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`server.listen(0)`: 调用 `server.listen(0)` 通常会导致服务器监听随机端口。然而，在集群中，每个工作进程在每次调用
    `server.listen(0)` 时都会收到相同的“随机”端口。换句话说，端口在第一次调用时是随机的；从第二次调用开始将是固定的。如果您希望每个工作进程都监听不同的随机端口，您必须自己生成端口号。'
- en: Building a simple HTTP server
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建简单的 HTTP 服务器
- en: Let's now start working on an example. Let's build a small HTTP server, cloned
    and load balanced using the `cluster` module. First of all, we need an application
    to scale, and for this example, we don't need too much, just a very basic HTTP
    server.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始工作在一个示例上。让我们构建一个小的 HTTP 服务器，使用 `cluster` 模块进行克隆和负载均衡。首先，我们需要一个可扩展的应用程序，对于这个例子，我们不需要太多，只需要一个非常基本的
    HTTP 服务器。
- en: 'So, let''s create a file called `app.js` containing the following code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们创建一个名为 `app.js` 的文件，包含以下代码：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The HTTP server we just built responds to any request by sending back a message
    containing its **process identifier** (**PID**); this is useful for identifying
    which instance of the application is handling the request. In this version of
    the application, we have only one process, so the PID that you see in the responses
    and the logs will always be the same.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚构建的 HTTP 服务器通过发送包含其**进程标识符**（**PID**）的消息来响应任何请求；这对于识别正在处理请求的应用程序实例非常有用。在这个应用程序版本中，我们只有一个进程，所以响应和日志中看到的
    PID 总是相同的。
- en: 'Also, to simulate some actual CPU work, we perform an empty loop 10 million
    times: without this, the server load would be almost insignificant and it will
    be quite hard to draw conclusions from the benchmarks we are going to run.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了模拟一些实际的CPU工作，我们执行了一个空循环1000万次：没有这个，服务器的负载几乎可以忽略不计，并且从我们将要运行的基准测试中得出结论将非常困难。
- en: The `app` module we create here is just a simple abstraction for a generic web
    server. We are not using a web framework like Express or Fastify for simplicity,
    but feel free to rewrite these examples using your web framework of choice.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里创建的`app`模块只是一个针对通用Web服务器的简单抽象。出于简洁考虑，我们没有使用Express或Fastify这样的Web框架，但请随意使用您选择的Web框架重写这些示例。
- en: You can now check if all works as expected by running the application as usual
    and sending a request to `http://localhost:8080` using either a browser or `curl`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以通过像往常一样运行应用程序并发送一个请求到`http://localhost:8080`，使用浏览器或`curl`来检查所有操作是否按预期进行。
- en: 'You can also try to measure the requests per second that the server is able
    to handle on one process. For this purpose, you can use a network benchmarking
    tool such as `autocannon` ([nodejsdp.link/autocannon](http://nodejsdp.link/autocannon)):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以尝试测量服务器在一个进程上能够处理的每秒请求数。为此，您可以使用网络基准测试工具，例如`autocannon`（[nodejsdp.link/autocannon](http://nodejsdp.link/autocannon)）：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding command will load the server with 200 concurrent connections for
    10 seconds. As a reference, the result we got on our machine (a 2.5 GHz quad-core
    Intel Core i7 using Node.js v14) is in the order of 300 transactions per second.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令将在10秒内以200个并发连接加载服务器。作为一个参考，我们在我们的机器上（2.5 GHz四核Intel Core i7使用Node.js v14）得到的结果是每秒大约300个事务。
- en: Please remember that the load tests we will perform in this chapter are intentionally
    simple and minimal and are provided only for reference and learning purposes.
    Their results cannot provide a 100% accurate evaluation of the performance of
    the various techniques we are analyzing. When you are trying to optimize a real
    production application, make sure to always run your own benchmarks after every
    change. You might find out that, among the different techniques we are going to
    illustrate here, some can be more effective than others for your specific application.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在本章中我们将进行的负载测试是故意简单和最小的，仅提供参考和学习目的。它们的结果不能提供对我们正在分析的各种技术性能的100%准确评估。当您尝试优化一个真实的生产应用程序时，请确保在每次更改后都运行自己的基准测试。您可能会发现，在我们将要展示的不同技术中，有些可能比其他技术对您的特定应用程序更有效。
- en: Now that we have a simple test web application and some reference benchmarks,
    we are ready to try some techniques to improve the performance of the application.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个简单的测试Web应用程序和一些参考基准，我们准备尝试一些技术来提高应用程序的性能。
- en: Scaling with the cluster module
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用集群模块进行扩展
- en: 'Let''s now update `app.js` to scale our application using the `cluster` module:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在更新`app.js`，使用`cluster`模块来扩展我们的应用程序：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we can see, using the `cluster` module requires very little effort. Let''s
    analyze what is happening:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用`cluster`模块几乎不需要做任何工作。让我们分析一下正在发生的事情：
- en: When we launch `app.js` from the command line, we are actually executing the
    master process. In this case, the `cluster.isMaster` variable is set to `true` and
    the only work we are required to do is forking the current process using `cluster.fork()`.
    In the preceding example, we are starting as many workers as there are logical
    CPU cores in the system to take advantage of all the available processing power.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们从命令行启动`app.js`时，我们实际上是在执行主进程。在这种情况下，`cluster.isMaster`变量被设置为`true`，我们唯一需要做的工作是使用`cluster.fork()`来分叉当前进程。在前面的例子中，我们启动了与系统中逻辑CPU核心数量相同的工人，以充分利用所有可用的处理能力。
- en: When `cluster.fork()` is executed from the master process, the current module
    (`app.js`) is run again, but this time in worker mode (`cluster.isWorker` is set
    to `true`, while `cluster.isMaster` is `false`). When the application runs as
    a worker, it can start doing some actual work. In this case, it starts a new HTTP
    server.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当`cluster.fork()`从主进程执行时，当前模块（`app.js`）再次运行，但这次是在工作模式（`cluster.isWorker`设置为`true`，而`cluster.isMaster`设置为`false`）。当应用程序作为工作进程运行时，它可以开始做一些实际的工作。在这种情况下，它启动了一个新的HTTP服务器。
- en: It's important to remember that each worker is a different Node.js process with
    its own event loop, memory space, and loaded modules.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，每个工作进程都是一个具有自己的事件循环、内存空间和加载的模块的不同Node.js进程。
- en: 'It''s interesting to note that the usage of the `cluster` module is based on
    a recurring pattern, which makes it very easy to run multiple instances of an
    application:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，`cluster` 模块的使用基于一个重复出现的模式，这使得运行应用程序的多个实例变得非常容易：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Under the hood, the `cluster.fork()` function uses the `child_process.fork()` API,
    therefore, we also have a communication channel available between the master and
    the workers. The worker processes can be accessed from the variable `cluster.workers`,
    so broadcasting a message to all of them would be as easy as running the following
    line of code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，`cluster.fork()` 函数使用 `child_process.fork()` API，因此，我们也在主进程和工作者进程之间有一个可用的通信通道。工作者进程可以通过变量
    `cluster.workers` 访问，所以向所有这些进程广播消息就像运行以下代码行一样简单：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let''s try to run our HTTP server in cluster mode. If our machine has
    more than one core, we should see a number of workers being started by the master
    process, one after the other. For example, in a system with four logical cores,
    the terminal should look like this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试以集群模式运行我们的 HTTP 服务器。如果我们的机器有多个核心，我们应该看到主进程依次启动了多个工作者进程。例如，在一个有四个逻辑核心的系统上，终端应该看起来像这样：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If we now try to hit our server again using the URL `http://localhost:8080`,
    we should notice that each request will return a message with a different PID,
    which means that these requests have been handled by different workers, confirming
    that the load is being distributed among them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在再次尝试使用 URL `http://localhost:8080` 来访问我们的服务器，我们应该注意到每个请求都会返回一个包含不同 PID
    的消息，这意味着这些请求是由不同的工作者处理的，这证实了负载正在它们之间分配。
- en: 'Now, we can try to load test our server again:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以再次尝试对我们的服务器进行负载测试：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This way, we should be able to discover the performance increase obtained by
    scaling our application across multiple processes. As a reference, in our machine,
    we saw a performance increase of about 3.3x (1,000 trans/sec versus 300 trans/sec).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们应该能够发现通过扩展我们的应用程序到多个进程所获得的性能提升。作为一个参考，在我们的机器上，我们看到了大约 3.3 倍的性能提升（1,000
    交易/秒与 300 交易/秒相比）。
- en: Resiliency and availability with the cluster module
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用集群模块实现弹性和可用性
- en: Because workers are all separate processes, they can be killed or respawned
    depending on a program's needs, without affecting other workers. As long as there
    are some workers still alive, the server will continue to accept connections.
    If no workers are alive, existing connections will be dropped, and new connections
    will be refused. Node.js does not automatically manage the number of workers;
    however, it is the application's responsibility to manage the worker pool based
    on its own needs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因为工作者都是独立的进程，所以可以根据程序的需求将它们杀死或重新启动，而不会影响其他工作者。只要还有一些工作者仍然存活，服务器将继续接受连接。如果没有工作者存活，现有的连接将被丢弃，新的连接将被拒绝。Node.js
    不会自动管理工作者数量；然而，管理工作者池的责任在于应用程序根据其自身需求进行。
- en: As we already mentioned, scaling an application also brings other advantages,
    in particular, the ability to maintain a certain level of service, even in the
    presence of malfunctions or crashes. This property is also known as **resiliency** and
    it contributes to the availability of a system.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，扩展应用程序也带来了其他优势，特别是能够在出现故障或崩溃的情况下保持一定水平的服务。这种特性也被称为 ****弹性****，它有助于提高系统的可用性。
- en: By starting multiple instances of the same application, we are creating a redundant system,
    which means that if one instance goes down for whatever reason, we still have
    other instances ready to serve requests. This pattern is pretty straightforward
    to implement using the `cluster` module. Let's see how it works!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过启动同一应用程序的多个实例，我们创建了一个冗余系统，这意味着如果其中一个实例因为任何原因而关闭，我们仍然有其他实例准备好处理请求。使用 `cluster`
    模块实现这种模式非常直接。让我们看看它是如何工作的！
- en: 'Let''s take the code from the previous section as a starting point. In particular,
    let''s modify the `app.js` module so that it crashes after a random interval of
    time:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把上一节中的代码作为起点。特别是，让我们修改 `app.js` 模块，使其在随机的时间间隔后崩溃：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With this change in place, our server exits with an error after a random number
    of seconds between 1 and 3\. In a real-life situation, this would eventually cause
    our application to stop serving requests, unless we use some external tool to
    monitor its status and restart it automatically. However, if we only have one
    instance, there may be a non-negligible delay between restarts caused by the startup
    time of the application. This means that during those restarts, the application
    is not available. Having multiple instances instead will make sure we always have
    a backup process to serve an incoming request, even when one of the workers fails.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这一更改后，我们的服务器在 1 到 3 秒之间的随机秒数后以错误退出。在现实情况下，这最终会导致我们的应用程序停止处理请求，除非我们使用一些外部工具来监控其状态并自动重启它。然而，如果我们只有一个实例，由于应用程序的启动时间，重启之间可能会有一个不可忽视的延迟。这意味着在这些重启期间，应用程序是不可用的。拥有多个实例将确保我们始终有一个备份进程来处理传入的请求，即使其中一个工作进程失败。
- en: 'With the `cluster` module, all we have to do is spawn a new worker as soon
    as we detect that one is terminated with an error code. Let''s modify `app.js`
    to take this into account:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `cluster` 模块，我们只需在检测到一个工作进程以错误代码终止时立即启动一个新的工作进程。让我们修改 `app.js` 以考虑这一点：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code, as soon as the master process receives an `'exit'` event,
    we check whether the process is terminated intentionally or as the result of an
    error. We do this by checking the status code and the flag `worker.exitedAfterDisconnect`,
    which indicates whether the worker was terminated explicitly by the master. If
    we confirm that the process was terminated because of an error, we start a new
    worker. It's interesting to note that while the crashed worker gets replaced,
    the other workers can still serve requests, thus not affecting the availability
    of the application.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，一旦主进程收到一个 `'exit'` 事件，我们就检查进程是因错误而有意终止还是意外终止。我们通过检查状态码和标志 `worker.exitedAfterDisconnect`
    来做这件事，该标志指示工作进程是否被主进程明确终止。如果我们确认进程是因为错误而终止的，我们就启动一个新的工作进程。值得注意的是，虽然崩溃的工作进程被替换了，但其他工作进程仍然可以处理请求，因此不会影响应用程序的可用性。
- en: 'To test this assumption, we can try to stress our server again using `autocannon`.
    When the stress test completes, we will notice that among the various metrics
    in the output, there is also an indication of the number of failures. In our case,
    it is something like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个假设，我们可以再次使用 `autocannon` 压力测试我们的服务器。当压力测试完成后，我们会在输出中的各种指标中注意到，也有关于失败次数的指示。在我们的案例中，它可能看起来像这样：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This should amount to about 92% availability. Bear in mind that this result
    can vary a lot as it greatly depends on the number of running instances and how
    many times they crash during the test, but it should give us a good indicator
    of how our solution works. The preceding numbers tell us that despite the fact
    that our application is constantly crashing, we only had 674 failed requests over
    8,000 hits.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该达到大约 92% 的可用性。请记住，这个结果可能会有很大的变化，因为它很大程度上取决于运行实例的数量以及它们在测试期间崩溃的次数，但它应该能给我们提供一个很好的指标，了解我们的解决方案是如何工作的。前面的数字告诉我们，尽管我们的应用程序不断崩溃，但我们只有
    674 次失败的请求在 8,000 次点击中。
- en: In the example scenario that we just built, most of the failing requests will
    be caused by the interruption of already established connections during a crash.
    Unfortunately, there is very little we can do to prevent these types of failures,
    especially when the application terminates because of a crash. Nonetheless, our
    solution proves to be working and its availability is not bad at all for an application
    that crashes so often!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚构建的示例场景中，大多数失败的请求将是由崩溃期间已建立的连接中断引起的。不幸的是，我们几乎无法做些什么来防止这些类型的失败，尤其是在应用程序由于崩溃而终止的情况下。尽管如此，我们的解决方案证明是有效的，并且对于经常崩溃的应用程序来说，其可用性相当不错！
- en: Zero-downtime restart
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无中断重启
- en: A Node.js application might also need to be restarted when we want to release
    a new version to our production servers. So, also in this scenario, having multiple
    instances can help maintain the availability of our application.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要向生产服务器发布新版本时，Node.js 应用程序可能也需要重启。因此，在这种场景下，拥有多个实例可以帮助保持我们应用程序的可用性。
- en: When we have to intentionally restart an application to update it, there is
    a small window in which the application restarts and is unable to serve requests.
    This can be acceptable if we are updating our personal blog, but it's not even
    an option for a professional application with a **service-level agreement** (**SLA**)
    or one that is updated very often as part of a continuous delivery process. The
    solution is to implement a **zero-downtime restart**, where the code of an application
    is updated without affecting its availability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有意重启应用程序以更新它时，应用程序重启并无法处理请求的窗口很小。如果我们正在更新我们的个人博客，这可能可以接受，但对于具有**服务级别协议**（**SLA**）的专业应用程序，或者作为持续交付过程的一部分经常更新的应用程序来说，这甚至不是一个选项。解决方案是实现**零停机时间重启**，这样应用程序的代码更新不会影响其可用性。
- en: 'With the `cluster` module, this is, again, a pretty easy task: the pattern
    involves restarting the workers one at a time. This way, the remaining workers
    can continue to operate and maintain the services of the application available.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cluster`模块，这再次是一个相当简单的任务：模式涉及逐个重启工人。这样，剩余的工人可以继续运行并维护应用程序的服务可用。
- en: 'Let''s add this new feature to our clustered server. All we have to do is add
    some new code to be executed by the master process:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这个新特性添加到我们的集群服务器中。我们只需要添加一些新代码，由主进程执行：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is how the preceding code block works:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是前面的代码块是如何工作的：
- en: The restarting of the workers is triggered on receiving the `SIGUSR2` signal.
    Note that we are using an `async` function to implement the event handler as we
    will need to perform some asynchronous tasks here.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工人的重启是在接收到`SIGUSR2`信号时触发的。请注意，我们正在使用`async`函数来实现事件处理器，因为我们在这里需要执行一些异步任务。
- en: When a `SIGUSR2` signal is received, we iterate over all the values of the `cluster.workers` object.
    Every element is a `worker` object that we can use to interact with a given worker
    currently active in the pool of workers.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当接收到`SIGUSR2`信号时，我们遍历`cluster.workers`对象的所有值。每个元素都是一个`worker`对象，我们可以用它来与当前在工人池中活动的特定工人进行交互。
- en: The first thing we do for the current worker is invoke `worker.disconnect()`,
    which stops the worker gracefully. This means that if the worker is currently
    handling a request, this won't be interrupted abruptly; instead, it will be completed.
    The worker exits only after the completion of all inflight requests.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于当前工人，我们首先调用`worker.disconnect()`，这会优雅地停止工人。这意味着如果工人目前正在处理请求，这将不会突然中断；相反，它将完成。只有当所有进行中的请求完成后，工人才会退出。
- en: When the terminated process exits, we can spawn a new worker.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当终止进程退出时，我们可以启动一个新的工人。
- en: We wait for the new worker to be ready and listening for new connections before
    we proceed with restarting the next worker.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们继续重启下一个工人之前，我们等待新工人准备好并监听新的连接。
- en: Since our program makes use of Unix signals, it will not work properly on Windows
    systems (unless you are using the Windows Subsystem for Linux). Signals are the
    simplest mechanism to implement our solution. However, this isn't the only one.
    In fact, other approaches include listening for a command coming from a socket,
    a pipe, or the standard input.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的程序使用了Unix信号，它不会在Windows系统上正常工作（除非你正在使用Windows子系统中的Linux）。信号是实现我们解决方案的最简单机制。然而，这并不是唯一的方法。实际上，其他方法包括监听来自套接字、管道或标准输入的命令。
- en: 'Now, we can test our zero-downtime restart by running the application and then
    sending a `SIGUSR2` signal. However, we first need to obtain the PID of the master
    process. The following command can be useful to identify it from the list of all
    the running processes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过运行应用程序并发送`SIGUSR2`信号来测试我们的零停机时间重启。然而，我们首先需要获取主进程的PID。以下命令可以用来从所有运行进程的列表中识别它：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The master process should be the parent of a set of `node` processes. Once
    we have the PID we are looking for, we can send the signal to it:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 主进程应该是`node`进程集的父进程。一旦我们找到了我们想要的PID，我们就可以向它发送信号：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, the output of the application should display something like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，应用程序的输出应该显示如下：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can try to use `autocannon` again to verify that we don't have any considerable
    impact on the availability of our application during the restart of the workers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次尝试使用`autocannon`来验证在工人重启期间，我们没有对我们的应用程序的可用性产生任何重大影响。
- en: '`pm2` ([nodejsdp.link/pm2](http://nodejsdp.link/pm2)) is a small utility, based
    on `cluster`, which offers load balancing, process monitoring, zero-downtime restarts,
    and other goodies.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`pm2`（[nodejsdp.link/pm2](http://nodejsdp.link/pm2)）是一个基于`cluster`的小型实用工具，它提供了负载均衡、进程监控、零停机时间重启和其他优点。'
- en: Dealing with stateful communications
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理有状态通信
- en: 'The `cluster` module does not work well with stateful communications where
    the application state is not shared between the various instances. This is because
    different requests belonging to the same stateful session may potentially be handled
    by a different instance of the application. This is not a problem limited only
    to the `cluster` module, but, in general, it applies to any kind of stateless,
    load balancing algorithm. Consider, for example, the situation described by *Figure
    12.3*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`cluster`模块在有状态通信中表现不佳，其中应用程序状态没有在各个实例之间共享。这是因为属于同一有状态会话的不同请求可能由应用程序的不同实例处理。这不仅仅是一个`cluster`模块的问题，但一般来说，它适用于任何无状态、负载均衡算法。例如，考虑*图12.3*中描述的情况：'
- en: '![](img/B15729_12_03.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15729_12_03.png)'
- en: 'Figure 12.3: An example issue with a stateful application behind a load balancer'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：负载均衡器后有状态应用程序的一个示例问题
- en: The user **John** initially sends a request to our application to authenticate
    himself, but the result of the operation is registered locally (for example, in
    memory), so only the instance of the application that receives the authentication
    request (**Instance A**) knows that John is successfully authenticated. When John
    sends a new request, the load balancer might forward it to a different instance
    of the application, which actually doesn't possess the authentication details
    of John, hence refusing to perform the operation. The application we just described
    cannot be scaled as it is, but luckily, there are two easy solutions we can apply
    to solve this problem.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 用户**John**最初向我们的应用程序发送请求以进行身份验证，但操作结果是在本地注册的（例如，在内存中），因此只有接收身份验证请求的应用程序实例（**实例A**）知道John已成功认证。当John发送新的请求时，负载均衡器可能会将其转发到应用程序的不同实例，而这个实例实际上没有John的认证详情，因此拒绝执行操作。我们刚刚描述的应用程序不能按原样扩展，但幸运的是，我们可以应用两个简单的解决方案来解决这个问题。
- en: Sharing the state across multiple instances
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在多个实例之间共享状态
- en: The first option we have to scale an application using stateful communications
    is sharing the state across all the instances.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用有状态通信扩展应用程序的第一个选择是在所有实例之间共享状态。
- en: This can be easily achieved with a shared datastore, such as, for example, a
    database like PostgreSQL ([nodejsdp.link/postgresql](http://nodejsdp.link/postgresql)), MongoDB ([nodejsdp.link/mongodb](http://nodejsdp.link/mongodb)),
    or CouchDB ([nodejsdp.link/couchdb](http://nodejsdp.link/couchdb)), or, even better,
    we can use an in-memory store such as Redis ([nodejsdp.link/redis](http://nodejsdp.link/redis))
    or Memcached ([nodejsdp.link/memcached](http://nodejsdp.link/memcached)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过使用共享数据存储轻松实现，例如，例如，像PostgreSQL（[nodejsdp.link/postgresql](http://nodejsdp.link/postgresql)）、MongoDB（[nodejsdp.link/mongodb](http://nodejsdp.link/mongodb)）或CouchDB（[nodejsdp.link/couchdb](http://nodejsdp.link/couchdb)）这样的数据库，或者，更好的是，我们可以使用内存存储，如Redis（[nodejsdp.link/redis](http://nodejsdp.link/redis)）或Memcached（[nodejsdp.link/memcached](http://nodejsdp.link/memcached)）。
- en: '*Figure 12.4* outlines this simple and effective solution:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.4*概述了这个简单而有效的解决方案：'
- en: '![](img/B15729_12_04.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15729_12_04.png)'
- en: 'Figure 12.4: Application behind a load balancer using a shared data store'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：使用共享数据存储的负载均衡器后的应用程序
- en: The only drawback of using a shared store for the communication state is that
    applying this pattern might require a significant amount of refactoring of the
    code base. For example, we might be using an existing library that keeps the communication
    state in memory, so we have to figure out how to configure, replace, or reimplement
    this library to use a shared store.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享存储进行通信状态的唯一缺点是，应用此模式可能需要大量重构代码库。例如，我们可能正在使用一个将通信状态保存在内存中的现有库，因此我们必须找出如何配置、替换或重新实现这个库以使用共享存储。
- en: 'In cases where refactoring might not be feasible, for instance, because of
    too many changes required or stringent time constraints in making the application
    more scalable, we can rely on a less invasive solution: **sticky load balancing**
    (or **sticky sessions**).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在重构可能不可行的情况下，例如，因为需要太多更改或对提高应用程序可扩展性有严格的时间限制，我们可以依赖一个不那么侵入性的解决方案：**粘性负载均衡**（或**粘性会话**）。
- en: Sticky load balancing
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 粘性负载均衡
- en: The other alternative we have to support stateful communications is having the
    load balancer always routing all of the requests associated with a session to
    the same instance of the application. This technique is also called **sticky load
    balancing**.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们支持有状态通信的另一种替代方案是让负载均衡器始终将所有与会话关联的请求路由到应用程序的同一实例。这种技术也称为**粘性负载均衡**。
- en: '*Figure 12.5* illustrates a simplified scenario involving this technique:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.5*展示了涉及此技术的简化场景：'
- en: '![](img/B15729_12_05.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_12_05.png)'
- en: 'Figure 12.5: An example illustrating how sticky load balancing works'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：说明粘性负载均衡工作原理的示例
- en: As we can see from *Figure 12.5*, when the load balancer receives a request
    associated with a new session, it creates a mapping with one particular instance
    selected by the load balancing algorithm. The next time the load balancer receives
    a request from that same session, it bypasses the load balancing algorithm, selecting
    the application instance that was previously associated with the session. The
    particular technique we just described involves inspecting the session ID associated
    with the requests (usually included in a cookie by the application or the load
    balancer itself).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图12.5*所示，当负载均衡器接收到与新的会话相关联的请求时，它会通过负载均衡算法选择一个特定的实例创建一个映射。下一次负载均衡器接收到来自同一会话的请求时，它会绕过负载均衡算法，选择之前与该会话关联的应用实例。我们刚才描述的特定技术涉及检查与请求关联的会话ID（通常由应用程序或负载均衡器本身包含在cookie中）。
- en: A simpler alternative to associate a stateful connection to a single server
    is by using the IP address of the client performing the request. Usually, the
    IP is provided to a hash function that generates an ID representing the application
    instance designated to receive the request. This technique has the advantage of
    not requiring the association to be remembered by the load balancer. However,
    it doesn't work well with devices that frequently change IP, for example, when
    roaming on different networks.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将有状态的连接关联到单个服务器的简单替代方案是使用执行请求的客户端的IP地址。通常，IP地址提供给哈希函数，该函数生成一个代表指定接收请求的应用实例的ID。这种技术的优点是不需要负载均衡器记住关联。然而，它不适用于经常更改IP的设备，例如在不同网络上漫游时。
- en: Sticky load balancing is not supported by default by the `cluster` module, but
    it can be added with an npm library called `sticky-session` ([nodejsdp.link/sticky-session](http://nodejsdp.link/sticky-session)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`cluster`模块不支持粘性负载均衡，但可以通过名为`sticky-session`的npm库添加（[nodejsdp.link/sticky-session](http://nodejsdp.link/sticky-session)）。
- en: One big problem with sticky load balancing is the fact that it nullifies most
    of the advantages of having a redundant system, where all the instances of the
    application are the same, and where an instance can eventually replace another
    one that stopped working. For these reasons, it is recommended to always try to
    avoid sticky load balancing and building applications that maintain session state
    in a shared store. Alternatively, where feasible, you can try to build applications
    that don't require stateful communications at all; for example, by including the
    state in the request itself.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '粘性负载均衡的一个大问题是它抵消了具有冗余系统的大多数优势，其中所有应用程序的实例都是相同的，并且一个实例最终可以替换停止工作的另一个实例。出于这些原因，建议始终尝试避免粘性负载均衡并构建在共享存储中维护会话状态的应用程序。或者，在可行的情况下，可以尝试构建根本不需要有状态通信的应用程序；例如，通过在请求本身中包含状态。 '
- en: For a real example of a library requiring sticky load balancing, we can mention Socket.IO ([nodejsdp.link/socket-io](http://nodejsdp.link/socket-io)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要粘性负载均衡的库的实际示例，我们可以提到Socket.IO ([nodejsdp.link/socket-io](http://nodejsdp.link/socket-io))。
- en: Scaling with a reverse proxy
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用反向代理进行扩展
- en: The `cluster` module, although very convenient and simple to use, is not the only option
    we have to scale a Node.js web application. Traditional techniques are often preferred
    because they offer more control and power in highly-available production environments.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`cluster`模块虽然非常方便且易于使用，但并不是我们扩展Node.js Web应用程序的唯一选择。在高度可用的生产环境中，传统技术通常更受欢迎，因为它们提供了更多的控制和能力。'
- en: The alternative to using `cluster` is to start multiple standalone instances of
    the same application running on different ports or machines, and then use a **reverse
    proxy** (or gateway) to provide access to those instances, distributing the traffic
    across them. In this configuration, we don't have a master process distributing
    requests to a set of workers, but a set of distinct processes running on the same
    machine (using different ports) or scattered across different machines inside
    a network. To provide a single access point to our application, we can use a reverse
    proxy, a special device or service placed between the clients and the instances
    of our application, which takes any request and forwards it to a destination server,
    returning the result to the client as if it was itself the origin. In this scenario,
    the reverse proxy is also used as a load balancer, distributing the requests among
    the instances of the application.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cluster`的替代方案是启动多个相同应用的单例实例，这些实例在不同的端口或机器上运行，然后使用一个**反向代理**（或网关）来提供对这些实例的访问，并将流量分配到它们之间。在这种配置中，我们没有将请求分配给一组工作进程的主进程，而是一组在相同机器上运行的不同进程（使用不同的端口）或在网络内部不同机器上分散的进程。为了提供一个单一的应用程序访问点，我们可以使用反向代理，这是一种放置在客户端和我们的应用程序实例之间的特殊设备或服务，它接受任何请求并将其转发到目标服务器，然后将结果返回给客户端，就像它本身是原始源一样。在这种情况下，反向代理也被用作负载均衡器，在应用程序的实例之间分配请求。
- en: For a clear explanation of the differences between a reverse proxy and a forward
    proxy, you can refer to the Apache HTTP server documentation at [nodejsdp.link/forward-reverse](http://nodejsdp.link/forward-reverse).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反向代理和正向代理之间差异的清晰解释，您可以参考Apache HTTP服务器文档在[nodejsdp.link/forward-reverse](http://nodejsdp.link/forward-reverse)。
- en: '*Figure 12.6* shows a typical multi-process, multi-machine configuration with
    a reverse proxy acting as a load balancer on the front:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.6*显示了具有反向代理作为前端负载均衡器的典型多进程、多机配置：'
- en: '![](img/B15729_12_06.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6](img/B15729_12_06.png)'
- en: 'Figure 12.6: A typical multi-process, multi-machine configuration with a reverse
    proxy acting as a load balancer'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6：具有反向代理作为负载均衡器的典型多进程、多机配置
- en: 'For a Node.js application, there are many reasons to choose this approach in
    place of the `cluster` module:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Node.js应用程序，有许多原因选择这种方法而不是`cluster`模块：
- en: A reverse proxy can distribute the load across several machines, not just several
    processes.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向代理可以在多台机器之间分配负载，而不仅仅是多个进程。
- en: The most popular reverse proxies on the market support sticky load balancing
    out of the box.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场上最受欢迎的反向代理支持开箱即用的粘性负载均衡。
- en: A reverse proxy can route a request to any available server, regardless of its
    programming language or platform.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向代理可以将请求路由到任何可用的服务器，无论其编程语言或平台如何。
- en: We can choose more powerful load balancing algorithms.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以选择更强大的负载均衡算法。
- en: Many reverse proxies offer additional powerful features such as URL rewrites,
    caching, SSL termination point, security features (for example, denial-of-service
    protection), or even the functionality of fully-fledged web servers that can be
    used to, for example, serve static files.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多反向代理提供额外的强大功能，例如URL重写、缓存、SSL终止点、安全功能（例如，拒绝服务保护），甚至可以用于例如服务静态文件的完整功能的Web服务器。
- en: That said, the `cluster` module could also be easily combined with a reverse
    proxy if necessary, for example, by using `cluster` to scale vertically inside
    a single machine and then using the reverse proxy to scale horizontally across
    different nodes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果需要，`cluster`模块也可以很容易地与反向代理结合使用，例如，通过使用`cluster`在单个机器内部进行垂直扩展，然后使用反向代理在不同节点之间进行水平扩展。
- en: '**Pattern**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式**'
- en: Use a reverse proxy to balance the load of an application across multiple instances
    running on different ports or machines.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用反向代理平衡运行在不同端口或机器上的多个实例的应用程序的负载。
- en: 'We have many options to implement a load balancer using a reverse proxy. The
    following is a list of the most popular solutions:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有许多选项可以使用反向代理实现负载均衡。以下是最受欢迎的解决方案列表：
- en: '**Nginx** ([nodejsdp.link/nginx](http://nodejsdp.link/nginx)): This is a web
    server, reverse proxy, and load balancer, built upon the non-blocking I/O model.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nginx**([nodejsdp.link/nginx](http://nodejsdp.link/nginx))：这是一个基于非阻塞I/O模型的Web服务器、反向代理和负载均衡器。'
- en: '**HAProxy** ([nodejsdp.link/haproxy](http://nodejsdp.link/haproxy)): This is
    a fast load balancer for TCP/HTTP traffic.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HAProxy**([nodejsdp.link/haproxy](http://nodejsdp.link/haproxy))：这是一个用于TCP/HTTP流量的快速负载均衡器。'
- en: '**Node.js-based proxies**: There are many solutions for the implementation
    of reverse proxies and load balancers directly in Node.js. This might have advantages
    and disadvantages, as we will see later.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于 Node.js 的代理**：在 Node.js 中实现反向代理和负载均衡器有许多解决方案。这可能会带来优势和劣势，我们将在稍后看到。'
- en: '**Cloud-based proxies**: In the era of cloud computing, it''s not rare to utilize
    a load balancer as a service. This can be convenient because it requires minimal
    maintenance, it''s usually highly scalable, and sometimes it can support dynamic
    configurations to enable on-demand scalability.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于云的代理**：在云计算时代，利用负载均衡器作为服务并不罕见。这可能会很方便，因为它需要最少的维护，通常具有高度的扩展性，有时它还可以支持动态配置，以实现按需扩展性。'
- en: In the next few sections of this chapter, we will analyze a sample configuration
    using Nginx. Later on, we will work on building our very own load balancer using
    nothing but Node.js!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章接下来的几节中，我们将分析使用 Nginx 的一个示例配置。稍后，我们将使用 Node.js 来构建我们自己的负载均衡器！
- en: Load balancing with Nginx
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Nginx 进行负载均衡
- en: To give you an idea of how reverse proxies work, we will now build a scalable
    architecture based on Nginx, but first, we need to install it. We can do that
    by following the instructions at [nodejsdp.link/nginx-install](http://nodejsdp.link/nginx-install).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您了解反向代理是如何工作的，我们将现在基于 Nginx 构建一个可扩展的架构，但首先，我们需要安装它。我们可以通过遵循 [nodejsdp.link/nginx-install](http://nodejsdp.link/nginx-install)
    中的说明来完成此操作。
- en: On the latest Ubuntu system, you can quickly install Nginx with the command `sudo
    apt-get install nginx`. On macOS, you can use `brew` ([nodejsdp.link/brew](http://nodejsdp.link/brew)): `brew
    install nginx`. Note that for the following examples, we will be using the latest
    version of Nginx available at the time of writing (1.17.10).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在最新的 Ubuntu 系统上，您可以使用命令 `sudo apt-get install nginx` 快速安装 Nginx。在 macOS 上，您可以使用
    `brew` ([nodejsdp.link/brew](http://nodejsdp.link/brew))：`brew install nginx`。请注意，对于以下示例，我们将使用写作时的最新版本的
    Nginx（1.17.10）。
- en: 'Since we are not going to use `cluster` to start multiple instances of our
    server, we need to slightly modify the code of our application so that we can
    specify the listening port using a command-line argument. This will allow us to
    launch multiple instances on different ports. Let''s consider the main module
    of our example application (`app.js`):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不会使用 `cluster` 来启动我们服务器的多个实例，因此我们需要稍微修改我们应用程序的代码，以便我们可以使用命令行参数指定监听端口。这将允许我们在不同的端口上启动多个实例。让我们考虑我们示例应用程序的主要模块（`app.js`）：
- en: '[PRE14]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The only difference between this version and the first version of our web server
    is that here, we are making the port number configurable through the `PORT` environment
    variable or a command-line argument. This is needed because we want to be able
    to start multiple instances of the server and allow them to listen on different
    ports.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们网络服务器的第一个版本相比，这个版本唯一的区别是，在这里，我们通过 `PORT` 环境变量或命令行参数来使端口号可配置。这是必要的，因为我们希望能够启动多个服务器实例，并允许它们监听不同的端口。
- en: 'Another important feature that we won''t have available without `cluster` is
    the automatic restart in case of a crash. Luckily, this is easy to fix by using
    a dedicated supervisor, that is, an external process that monitors our application
    and restarts it if necessary. The following are some possible choices:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的特性，没有 `cluster` 我们将无法使用，就是在崩溃情况下的自动重启。幸运的是，这很容易通过使用专门的监控器来解决，即一个外部进程，它会监控我们的应用程序并在必要时重启它。以下是一些可能的选择：
- en: Node.js-based supervisors such as **forever** ([nodejsdp.link/forever](http://nodejsdp.link/forever))
    or **pm2** ([nodejsdp.link/pm2](http://nodejsdp.link/pm2))
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 Node.js 的监控器，例如 **forever** ([nodejsdp.link/forever](http://nodejsdp.link/forever))
    或 **pm2** ([nodejsdp.link/pm2](http://nodejsdp.link/pm2))。
- en: OS-based monitors such as **systemd** ([nodejsdp.link/systemd](http://nodejsdp.link/systemd)) or **runit** ([nodejsdp.link/runit](http://nodejsdp.link/runit))
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 OS 的监控器，例如 **systemd** ([nodejsdp.link/systemd](http://nodejsdp.link/systemd))
    或 **runit** ([nodejsdp.link/runit](http://nodejsdp.link/runit))。
- en: More advanced monitoring solutions such as **monit** ([nodejsdp.link/monit](http://nodejsdp.link/monit))
    or **supervisord** ([nodejsdp.link/supervisord](http://nodejsdp.link/supervisord))
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高级的监控解决方案，例如 **monit** ([nodejsdp.link/monit](http://nodejsdp.link/monit))
    或 **supervisord** ([nodejsdp.link/supervisord](http://nodejsdp.link/supervisord))。
- en: Container-based runtimes such as **Kubernetes** ([nodejsdp.link/kubernetes](http://nodejsdp.link/kubernetes)),
    **Nomad** ([nodejsdp.link/nomad](http://nodejsdp.link/nomad)), or **Docker Swarm**
    ([nodejsdp.link/swarm](http://nodejsdp.link/swarm)).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于容器的运行时，例如 **Kubernetes** ([nodejsdp.link/kubernetes](http://nodejsdp.link/kubernetes))、**Nomad**
    ([nodejsdp.link/nomad](http://nodejsdp.link/nomad)) 或 **Docker Swarm** ([nodejsdp.link/swarm](http://nodejsdp.link/swarm))。
- en: 'For this example, we are going to use `forever`, which is the simplest and
    most immediate for us to use. We can install it globally by running the following
    command:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用 `forever`，这是最简单、最直接的方法。我们可以通过运行以下命令全局安装它：
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The next step is to start the four instances of our application, all on different
    ports and supervised by `forever`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是启动我们应用程序的四个实例，所有实例都在不同的端口上运行，并由 `forever` 监督：
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can check the list of the started processes using the command:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令检查已启动的进程列表：
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can use `forever stopall` to stop all the Node.js processes previously started
    with `forever`. Alternatively, you can use `forever stop <id>` to stop a specific
    process from the ones shown with `forever list`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `forever stopall` 停止之前使用 `forever` 启动的所有 Node.js 进程。或者，你可以使用 `forever
    stop <id>` 停止 `forever list` 显示的特定进程。
- en: Now, it's time to configure the Nginx server as a load balancer.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候配置 Nginx 服务器作为负载均衡器了。
- en: First, we need to create a minimal configuration file in our working directory
    that we will call `nginx.conf`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在我们的工作目录中创建一个最小的配置文件，我们将称之为 `nginx.conf`。
- en: Note that, because Nginx allows you to run multiple applications behind the
    same server instance, it is more common to use a global configuration file, which,
    in Unix systems, is generally located under `/usr/local/nginx/conf`, `/etc/nginx`
    or `/usr/local/etc/nginx`. Here, by having a configuration file in our working
    folder, we are taking a simpler approach. This is ok for the sake of this demo
    as we want to run just one application locally, but we advise you follow the recommended
    best practices for production deployments.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于 Nginx 允许你在同一个服务器实例后面运行多个应用程序，因此更常见的是使用全局配置文件。在 Unix 系统中，这个配置文件通常位于 `/usr/local/nginx/conf`、`/etc/nginx`
    或 `/usr/local/etc/nginx` 之下。在这里，我们在我们的工作目录中有一个配置文件，我们采取了一种更简单的方法。对于这个演示来说，这是可以的，因为我们只想在本地上运行一个应用程序，但我们建议你在生产部署中遵循推荐的最佳实践。
- en: 'Next, let''s write the `nginx.conf` file and apply the following configuration,
    which is the very minimum required to get a working load balancer for our Node.js
    processes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们编写 `nginx.conf` 文件并应用以下配置，这是获取一个可工作的负载均衡器所需的最小配置：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s discuss this configuration together:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起来讨论这个配置：
- en: The declaration `daemon off` allows us to run Nginx as a standalone process
    using the current unprivileged user and by keeping the process running in the
    foreground of the current terminal (which allows us to shut it down using Ctrl
    + C).
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明 `daemon off` 允许我们以当前非特权用户身份运行 Nginx 作为独立进程，并保持进程在当前终端的前台运行（这允许我们使用 Ctrl +
    C 来关闭它）。
- en: We use `error_log` (and later in the `http` block, `access_log`) to stream errors
    and access logs respectively to the standard output and standard error, so we
    can read the logs in real time straight from our terminal.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `error_log`（稍后在 `http` 块中，`access_log`）将错误日志和访问日志分别流式传输到标准输出和标准错误，这样我们就可以实时从我们的终端读取日志。
- en: The `events` block allows us to configure how network connections are managed
    by Nginx. Here, we are setting the maximum number of simultaneous connections
    that can be opened by an Nginx worker process to `2048`.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`events` 块允许我们配置 Nginx 如何管理网络连接。在这里，我们正在设置 Nginx 工作进程可以打开的最大并发连接数为 `2048`。'
- en: The `http` block allows us to define the configuration for a given application.
    In the `upstream my-load-balanced-app` section, we are defining the list of backend
    servers used to handle the network requests. In the `server` section, we use `listen
    8080` to instruct the server to listen on port `8080` and finally, we specify
    the `proxy_pass` directive, which essentially tells Nginx to forward any request
    to the server group we defined before (`my-load-balanced-app`).
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`http` 块允许我们定义特定应用程序的配置。在 `upstream my-load-balanced-app` 部分中，我们正在定义用于处理网络请求的后端服务器列表。在
    `server` 部分中，我们使用 `listen 8080` 指示服务器监听端口 `8080`，最后，我们指定了 `proxy_pass` 指令，它本质上告诉
    Nginx 将任何请求转发到我们之前定义的服务器组（`my-load-balanced-app`）。'
- en: 'That''s it! Now, we only need to start Nginx using our configuration file with
    the following command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！现在，我们只需要使用以下命令启动 Nginx，并使用我们的配置文件：
- en: '[PRE19]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our system should now be up and running, ready to accept requests and balance
    the traffic across the four instances of our Node.js application. Simply point
    your browser to the address `http://localhost:8080` to see how the traffic is
    balanced by our Nginx server. You can also try again to load test this application
    using `autocannon`. Since we are still running all the processes in one local
    machine, your results should not diverge much from what you got when benchmarking
    the version using the `cluster` module approach.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统现在应该已经启动并运行，准备好接受请求并在我们的Node.js应用程序的四个实例之间平衡流量。只需将你的浏览器指向地址`http://localhost:8080`，就可以看到我们的Nginx服务器如何平衡流量。你也可以再次尝试使用`autocannon`对这个应用程序进行负载测试。由于我们仍然在一个本地机器上运行所有进程，你的结果应该不会与使用`cluster`模块方法进行基准测试时得到的结果相差太大。
- en: 'This example demonstrated how to use Nginx to load balance traffic. For simplicity,
    we kept everything locally on our machine, but nonetheless, this was a great exercise
    to get us ready to deploy an application on multiple remote servers. If you want
    to try to do that, you will essentially have to follow this recipe:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例演示了如何使用Nginx进行流量均衡。为了简单起见，我们将所有内容都保留在我们的机器上，但这仍然是一个很好的练习，让我们为在多个远程服务器上部署应用程序做好准备。如果你想尝试这样做，你基本上必须遵循以下步骤：
- en: Provision *n* backend servers running the Node.js application (running multiple
    instances with a service monitor like `forever` or by using the `cluster` module).
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置*n*个运行Node.js应用程序的后端服务器（通过使用像`forever`这样的服务监控器运行多个实例，或者使用`cluster`模块）。
- en: Provision a load balancer machine that has Nginx installed and all the necessary
    configuration to route the traffic to the *n* backend servers. Every process in
    every server should be listed in the `upstream` block of your Nginx configuration
    file using the correct address of the various machines in the network.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置一个安装了Nginx并具有所有必要配置以将流量路由到*n*个后端服务器的负载均衡器机器。每个服务器上的每个进程都应该使用网络中各种机器的正确地址列在你的Nginx配置文件的`upstream`块中。
- en: Make your load balancer publicly available on the internet by using a public
    IP and possibly a public domain name.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用公共IP地址和可能的一个公共域名，使你的负载均衡器在互联网上公开可用。
- en: Try to send some traffic to the load balancer's public address by using a browser
    or a benchmarking tool like `autocannon`.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用浏览器或像`autocannon`这样的基准测试工具向负载均衡器的公共地址发送一些流量。
- en: For simplicity, you can perform all these steps manually by spinning servers
    through your cloud provider admin interface and by using SSH to log in to those.
    Alternatively, you could choose tools that allow you to automate these tasks by
    writing **infrastructure as code** such as **Terraform** ([nodejsdp.link/terraform](http://nodejsdp.link/terraform)),
    **Ansible** ([nodejsdp.link/ansible](http://nodejsdp.link/ansible)), and **Packer**
    ([nodejsdp.link/packer](http://nodejsdp.link/packer)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，你可以通过在云提供商的管理界面中启动服务器，并使用SSH登录到这些服务器来手动执行所有这些步骤。或者，你也可以选择允许你通过编写**基础设施即代码**（如**Terraform**
    ([nodejsdp.link/terraform](http://nodejsdp.link/terraform))）、**Ansible** ([nodejsdp.link/ansible](http://nodejsdp.link/ansible))和**Packer**
    ([nodejsdp.link/packer](http://nodejsdp.link/packer)))来自动化这些任务的工具。
- en: In this example, we used a predefined number of backend servers. In the next
    section, we will explore a technique that allows us to load balance traffic to
    a dynamic set of backend servers.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了预定义的后端服务器数量。在下一节中，我们将探讨一种技术，它允许我们将流量均衡到一组动态的后端服务器。
- en: Dynamic horizontal scaling
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态水平扩展
- en: One important advantage of modern cloud-based infrastructure is the ability
    to dynamically adjust the capacity of an application based on the current or predicted
    traffic. This is also known as **dynamic scaling**. If implemented properly, this
    practice can reduce the cost of the IT infrastructure enormously while still keeping
    the application highly available and responsive.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现代基于云的基础设施的一个重要优势是能够根据当前的或预测的流量动态调整应用程序的容量。这也被称为**动态扩展**。如果实施得当，这种做法可以极大地降低IT基础设施的成本，同时仍然保持应用程序的高度可用性和响应性。
- en: 'The idea is simple: if our application is experiencing a performance degradation
    caused by a peak in traffic, the system automatically spawns new servers to cope
    with the increased load. Similarly, if we see that the allocated resources are
    underutilized, we can shut some servers down to reduce the cost of the running
    infrastructure. We could also decide to perform scaling operations based on a
    schedule; for instance, we could shut down some servers during certain hours of
    the day when we know that the traffic will be lighter, and restart them again
    just before the peak hours. These mechanisms require the load balancer to always
    be up-to-date with the current network topology, knowing at any time which server
    is up.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说：如果我们的应用程序因流量峰值导致性能下降，系统会自动启动新服务器以应对增加的负载。同样，如果我们看到分配的资源利用率不足，我们可以关闭一些服务器以降低运行基础设施的成本。我们还可以决定根据计划执行扩展操作；例如，我们可以在我们知道流量较轻的某些时段关闭一些服务器，并在高峰时段前再次启动它们。这些机制要求负载均衡器始终了解当前的网络安全拓扑，知道任何时刻哪个服务器是启动的。
- en: Using a service registry
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用服务注册表
- en: A common pattern to solve this problem is to use a central repository called
    a **service registry**, which keeps track of the running servers and the services
    they provide.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的常见模式是使用一个称为**服务注册表**的中央存储库，它跟踪运行的服务器和它们提供的服务。
- en: '*Figure 12.7* shows a multiservice architecture with a load balancer on the
    front, dynamically configured using a service registry:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.7*展示了具有前端负载均衡器的多服务架构，使用服务注册表动态配置：'
- en: '![](img/B15729_12_07.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_12_07.png)'
- en: 'Figure 12.7: A multiservice architecture with a load balancer on the front,
    dynamically configured using a service registry'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7：一个具有前端负载均衡器的多服务架构，使用服务注册表动态配置
- en: The architecture in *Figure 12.7* assumes the presence of two services, **API**
    and **WebApp**. There can be one or many instances of each service, spread across
    multiple servers.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.7*中的架构假设存在两个服务，**API**和**WebApp**。每个服务可以有一个或多个实例，分布在不同服务器上。'
- en: When a request to `example.com` is received, the load balancer checks the prefix
    of the request path. If the prefix is `/api`, the request is load balanced between
    the available instances of the **API** service. In *Figure 12.7*, we have two
    instances running on the server `api1.example.com` and one instance running on
    `api2.example.com`. For all the other path prefixes, the request is load balanced
    between the available instances of the **WebApp** service. In the diagram, we
    have only one **WebApp** instance, which is running on the server `web1.example.com`.
    The load balancer obtains the list of servers and service instances running on
    every server using the service registry.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当接收到对`example.com`的请求时，负载均衡器检查请求路径的前缀。如果前缀是`/api`，则请求在**API**服务的可用实例之间进行负载均衡。在*图12.7*中，我们有两个实例在服务器`api1.example.com`上运行，一个实例在`api2.example.com`上运行。对于所有其他路径前缀，请求在**WebApp**服务的可用实例之间进行负载均衡。在图中，我们只有一个**WebApp**实例，它在服务器`web1.example.com`上运行。负载均衡器使用服务注册表获取每个服务器上运行的列表和服务器上的服务实例。
- en: For this to work in complete automation, each application instance has to register
    itself to the service registry the moment it comes up online and unregister itself
    when it stops. This way, the load balancer can always have an up-to-date view
    of the servers and the services available on the network.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现完全自动化，每个应用程序实例在上线时必须将自己注册到服务注册表，并在停止时注销自己。这样，负载均衡器总能有一个最新的服务器和网络上可用的服务视图。
- en: '**Pattern (service registry)**'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式（服务注册表**）'
- en: Use a central repository to store an always up-to-date view of the servers and
    the services available in a system.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 使用中央存储库存储系统中的服务器和服务的始终最新的视图。
- en: While this pattern is useful to load balance traffic, it has the added benefit
    of being able to decouple service instances from the servers on which they are
    running. We can look at the Service Registry pattern as an implementation of the
    Service Locator Design pattern applied to network services.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种模式对负载均衡流量很有用，但它还有一个额外的优点，即能够将服务实例与其运行的服务器解耦。我们可以将服务注册表模式视为将服务定位器设计模式应用于网络服务的一种实现。
- en: Implementing a dynamic load balancer with http-proxy and Consul
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用http-proxy和Consul实现动态负载均衡器
- en: 'To support a dynamic network infrastructure, we can use a reverse proxy such
    as **Nginx** or **HAProxy**: all we need to do is update their configuration using
    an automated service and then force the load balancer to pick the changes. For
    Nginx, this can be done using the following command line:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持动态网络基础设施，我们可以使用像**Nginx**或**HAProxy**这样的反向代理：我们只需要使用自动化的服务更新它们的配置，然后强制负载均衡器选择更改。对于Nginx，可以使用以下命令行来完成：
- en: '[PRE20]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The same result can be achieved with a cloud-based solution, but we have a third
    and more familiar alternative that makes use of our favorite platform.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于云的解决方案也可以达到相同的结果，但我们还有一个第三种更熟悉的替代方案，它利用了我们最喜欢的平台。
- en: 'We all know that Node.js is a great tool for building any sort of network application
    and, as we said throughout this book, this is exactly one of its main design goals.
    So, why not build a load balancer using nothing but Node.js? This would give us
    much more freedom and power and would allow us to implement any sort of pattern
    or algorithm straight into our custom-built load balancer, including the one we
    are now going to explore: dynamic load balancing using a service registry. Furthermore,
    working on this exercise will definitely help us to understand even better how
    production-grade products such as Nginx and HAProxy actually work.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道Node.js是构建任何类型网络应用的优秀工具，正如我们在本书中所述，这正是其主要设计目标之一。那么，为什么不只用Node.js来构建负载均衡器呢？这将给我们带来更多的自由和权力，并允许我们将任何类型的模式或算法直接实现到我们自定义构建的负载均衡器中，包括我们现在将要探索的：使用服务注册表的动态负载均衡。此外，完成这个练习肯定能帮助我们更好地理解像Nginx和HAProxy这样的生产级产品实际上是如何工作的。
- en: 'In this example, we are going to use **Consul** ([nodejsdp.link/consul](http://nodejsdp.link/consul))
    as the service registry to replicate the multiservice architecture we saw in *Figure
    12.7*. To do that, we are going to mainly use three npm packages:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用**Consul** ([nodejsdp.link/consul](http://nodejsdp.link/consul))作为服务注册表来复制我们在*图12.7*中看到的多元服务架构。为此，我们将主要使用三个npm包：
- en: '`http-proxy` ([nodejsdp.link/http-proxy](http://nodejsdp.link/http-proxy)):
    To simplify the creation of a reverse proxy/load balancer in Node.js'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http-proxy` ([nodejsdp.link/http-proxy](http://nodejsdp.link/http-proxy))：在Node.js中简化反向代理/负载均衡器的创建'
- en: '`portfinder` ([nodejsdp.link/portfinder](http://nodejsdp.link/portfinder)):
    To find a free port in the system'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`portfinder` ([nodejsdp.link/portfinder](http://nodejsdp.link/portfinder))：在系统中查找一个空闲端口'
- en: '`consul` ([nodejsdp.link/consul-lib](http://nodejsdp.link/consul-lib)): To interact
    with Consul'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`consul` ([nodejsdp.link/consul-lib](http://nodejsdp.link/consul-lib))：用于与Consul交互'
- en: Let's start by implementing our services. These are simple HTTP servers like
    the ones we have used so far to test `cluster` and Nginx, but this time, we want
    each server to register itself into the service registry the moment it starts.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先实现我们的服务。这些是像我们之前用来测试`cluster`和Nginx的简单HTTP服务器，但这次，我们希望每个服务器在启动时将自己注册到服务注册表中。
- en: 'Let''s see how this looks (file `app.js`):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个样子（文件`app.js`）：
- en: '[PRE21]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the preceding code, there are some parts that deserve our attention:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，有一些部分值得我们注意：
- en: First, we use `portfinder.getPortPromise()` to discover a free port in the system
    (by default, `portfinder` starts to search from port `8000`). We also allow the
    user to configure the address based on the environment variable `ADDRESS`. Finally,
    we generate a random ID to identify this service using `nanoid` ([nodejsdp.link/nanoid](http://nodejsdp.link/nanoid)).
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用`portfinder.getPortPromise()`来发现系统中的一个空闲端口（默认情况下，`portfinder`从端口`8000`开始搜索）。我们还允许用户根据环境变量`ADDRESS`来配置地址。最后，我们使用`nanoid`
    ([nodejsdp.link/nanoid](http://nodejsdp.link/nanoid))生成一个随机ID来识别这个服务。
- en: Next, we declare the `registerService()` function, which uses the `consul` library
    to register a new service in the registry. The service definition needs several
    attributes: `id` (a unique identifier for the service), `name` (a generic name
    that identifies the service), `address` and `port` (to identify how to access
    the service), and `tags` (an optional array of tags that can be used to filter
    and group services). We are using `serviceType` (which we get from the command-line
    arguments) to specify the service name and to add a tag. This will allow us to
    identify all the services of the same type available in the cluster.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们声明`registerService()`函数，该函数使用`consul`库在注册表中注册一个新的服务。服务定义需要几个属性：`id`（服务的唯一标识符）、`name`（标识服务的通用名称）、`address`和`port`（用于标识如何访问服务），以及`tags`（一个可选的标签数组，可以用于过滤和分组服务）。我们使用`serviceType`（我们从命令行参数中获取）来指定服务名称并添加一个标签。这将使我们能够识别集群中所有相同类型的可用服务。
- en: At this point, we define a function called `unregisterService()`, which allows
    us to remove the service we just registered in Consul.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们定义了一个名为`unregisterService()`的函数，它允许我们从Consul中移除我们刚刚注册的服务。
- en: We use `unregisterService()` as a cleanup function so that when the program
    is closed (either intentionally or by accident), the service is unregistered from
    Consul.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`unregisterService()`作为清理函数，以便当程序关闭（无论是故意还是意外）时，服务将从Consul中注销。
- en: Finally, we start the HTTP server for our service on the port discovered by `portfinder`
    and the address configured for the current service. Note that when the server
    is started, we make sure to invoke the `registerService()` function to make sure
    that the service is registered for discovery.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在`portfinder`发现的端口和为当前服务配置的地址上启动我们的服务的HTTP服务器。请注意，当服务器启动时，我们确保调用`registerService()`函数以确保服务已注册以供发现。
- en: With this script, we will be able to start and register different types of applications.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个脚本，我们将能够启动和注册不同类型的应用程序。
- en: 'Now, it''s time to implement the load balancer. Let''s do that by creating
    a new module called `loadBalancer.js`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候实现负载均衡器了。让我们通过创建一个新的模块`loadBalancer.js`来实现它：
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This is how we implemented our Node.js-based load balancer:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们如何实现基于Node.js的负载均衡器：
- en: First, we define our load balancer routes. Each item in the `routing` array
    contains the `service` used to handle the requests arriving on the mapped `path`.
    The `index` property will be used to **round-robin** the requests of a given service.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义我们的负载均衡器路由。`routing`数组中的每个项目都包含用于处理映射`path`上到达的请求的`service`。`index`属性将用于对特定服务的请求进行**轮询**。
- en: We need to instantiate a `consul` client so that we can have access to the registry.
    Next, we instantiate an `http-proxy` server.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要实例化一个`consul`客户端，以便我们可以访问注册表。接下来，我们实例化一个`http-proxy`服务器。
- en: In the request handler of the server, the first thing we do is match the URL against
    our routing table. The result will be a descriptor containing the service name.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在服务器的请求处理器中，我们首先做的事情是将URL与我们的路由表进行匹配。结果将是一个包含服务名称的描述符。
- en: We obtain from `consul` the list of servers implementing the required service.
    If this list is empty or there was an error retrieving it, then we return an error
    to the client. We use the `Tags` attribute to filter all the available services
    and find the address of the servers that implement the current service type.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`consul`获取实现所需服务的服务器列表。如果这个列表为空或者获取列表时发生错误，那么我们将错误返回给客户端。我们使用`Tags`属性来过滤所有可用的服务，并找到实现当前服务类型的服务器地址。
- en: At last, we can route the request to its destination. We update `route.index` to
    point to the next server in the list, following a round-robin approach. We then
    use the index to select a server from the list, passing it to `proxy.web()`, along
    with the request (`req`) and the response (`res`) objects. This will simply forward
    the request to the server we chose.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以将请求路由到其目的地。我们更新`route.index`以指向列表中的下一个服务器，采用轮询的方式。然后我们使用索引从列表中选择一个服务器，将其传递给`proxy.web()`，同时传递请求（`req`）和响应（`res`）对象。这将简单地将请求转发到我们选择的服务器。
- en: It is now clear how simple it is to implement a load balancer using only Node.js
    and a service registry, as well as how much flexibility we can have by doing so.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经很清楚，仅使用Node.js和服务注册表就可以实现负载均衡器是多么简单，以及通过这种方式我们可以获得多少灵活性。
- en: Note that in order to keep the implementation simple, we intentionally left
    out some interesting optimization opportunities. For instance, in this implementation,
    we are interrogating `consul` to get the list of registered services for every
    single request. This is something that can add a significant overhead, especially
    if our load balancer receives requests with a high frequency. It would be more
    efficient to cache the list of services and refresh it on a regular basis (for
    instance, every 10 seconds). Another optimization could be to use the `cluster`
    module to run multiple instances of our load balancer and distribute the load
    across all the available cores in the machine.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了保持实现简单，我们故意省略了一些有趣的优化机会。例如，在这个实现中，我们每次请求都会查询`consul`以获取注册服务的列表。这可能会增加显著的开销，尤其是如果我们的负载均衡器以高频率接收请求时。更有效的方法是缓存服务列表，并定期刷新它（例如，每10秒刷新一次）。另一个优化可能是使用`cluster`模块来运行我们负载均衡器的多个实例，并将负载分配到机器的所有可用核心上。
- en: Now, we should be ready to give our system a try, but first, let's install the Consul server by
    following the official documentation at [nodejsdp.link/consul-install](http://nodejsdp.link/consul-install).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该准备好尝试我们的系统，但首先，让我们按照官方文档在[nodejsdp.link/consul-install](http://nodejsdp.link/consul-install)中的说明来安装Consul服务器。
- en: 'This allows us to start the Consul service registry on our development machine
    with this simple command line:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们在开发机器上使用以下简单的命令行启动Consul服务注册：
- en: '[PRE23]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we are ready to start the load balancer (using `forever` to make sure
    the application is restarted in case of a crash):'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好启动负载均衡器（使用`forever`确保在崩溃的情况下应用程序会重新启动）：
- en: '[PRE24]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, if we try to access some of the services exposed by the load balancer,
    we will notice that it returns an `HTTP 502` error, because we didn''t start any
    servers yet. Try it yourself:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们尝试访问负载均衡器暴露的一些服务，我们会注意到它返回了一个`HTTP 502`错误，因为我们还没有启动任何服务器。自己试一试：
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding command should return the following output:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令应该返回以下输出：
- en: '[PRE26]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The situation will change if we spawn some instances of our services, for example,
    two `api-service` and one `webapp-service`:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们启动我们服务的某些实例，情况将会改变，例如，两个`api-service`和一个`webapp-service`：
- en: '[PRE27]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, the load balancer should automatically see the new servers and start distributing
    requests across them. Let''s try again with the following command:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，负载均衡器应该自动看到新的服务器，并开始在这些服务器之间分配请求。让我们用以下命令再次尝试：
- en: '[PRE28]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding command should now return this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令现在应该返回以下内容：
- en: '[PRE29]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'By running this again, we should now receive a message from another server,
    confirming that the requests are being distributed evenly among the different
    servers:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行这个命令后，我们应该现在收到来自另一个服务器的消息，确认请求正在不同服务器之间均匀分配：
- en: '[PRE30]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: If you want to see the instances managed by `forever` and stop some of them
    you can use the commands `forever list` and `forever stop`. To stop all running
    instances you can use `forever stopall`. Why don't you try to stop one of the
    running instances of the `api-service` to see what happens to the whole application?
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看`forever`管理的实例并停止其中的一些，可以使用`forever list`和`forever stop`命令。要停止所有正在运行的实例，可以使用`forever
    stopall`。为什么不尝试停止`api-service`的一个运行实例，看看整个应用程序会发生什么？
- en: The advantages of this pattern are immediate. We can now scale our infrastructure
    dynamically, on demand, or based on a schedule, and our load balancer will automatically
    adjust with the new configuration without any extra effort!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式的优点是立即的。现在，我们可以根据需求或计划动态地扩展我们的基础设施，而负载均衡器将自动根据新的配置进行调整，无需额外努力！
- en: Consul offers a convenient web UI available at `localhost:8500` by default.
    Check it out while playing with this example to see how services appear and disappear
    as they get registered or unregistered.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Consul默认提供了一个方便的Web UI，地址为`localhost:8500`。在玩这个示例时检查它，看看服务是如何在注册或注销时出现和消失的。
- en: Consul also offers a health check feature to monitor registered services. This
    feature could be integrated within our example to make our infrastructure even
    more resilient to failures. In fact, if a service does not respond to a health
    check, it gets automatically removed from the registry and therefore, it won't
    receive traffic anymore. If you are curious to see how you can implement this
    feature, you can check out the official documentation for *Checks* at [nodejsdp.link/consul-checks](http://nodejsdp.link/consul-checks).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Consul还提供健康检查功能来监控已注册的服务。这个功能可以集成到我们的示例中，使我们的基础设施对故障的抵抗能力更强。实际上，如果一个服务对健康检查没有响应，它会被自动从注册表中移除，因此，它将不再接收流量。如果你好奇如何实现这个功能，你可以查看官方文档中的*检查*部分，链接为[nodejsdp.link/consul-checks](http://nodejsdp.link/consul-checks)。
- en: Now that we know how to perform dynamic load balancing using a load balancer
    and a service registry, we are ready to explore some interesting alternative approaches,
    like peer-to-peer load balancing.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何使用负载均衡器和服务注册表执行动态负载均衡，我们准备探索一些有趣的替代方法，比如对等式负载均衡。
- en: Peer-to-peer load balancing
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对等式负载均衡
- en: Using a reverse proxy is almost a necessity when we want to expose a complex
    internal network architecture to a public network such as the Internet. It helps
    hide the complexity, providing a single access point that external applications
    can easily use and rely on. However, if we need to scale a service that is for
    internal use only, we can have much more flexibility and control.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要将复杂的内部网络架构暴露给公共网络，如互联网时，使用反向代理几乎是一种必要性。它有助于隐藏复杂性，提供了一个外部应用程序可以轻松使用并依赖的单一点访问。然而，如果我们需要扩展仅用于内部使用的服务，我们可以拥有更多的灵活性和控制。
- en: Let's imagine having a service, **Service A**, that relies on **Service B** to
    implement its functionality. **Service B** is scaled across multiple machines
    and it's available only in the internal network. What we have learned so far is
    that **Service A** will connect to **Service B** using a load balancer, which
    will distribute the traffic to all the servers implementing **Service B**.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设有一个服务，**服务A**，它依赖于**服务B**来实现其功能。**服务B**在多台机器上进行了扩展，并且仅在内部网络中可用。我们到目前为止所学的是，**服务A**将通过负载均衡器连接到**服务B**，该负载均衡器将流量分发到所有实现**服务B**的服务器。
- en: However, there is an alternative. We can remove the load balancer from the picture
    and distribute the requests directly from the client (**Service A**), which now
    becomes directly responsible for load balancing its requests across the various
    instances of **Service B**. This is possible only if **Server A** knows the details
    about the servers exposing **Service B**, and in an internal network, this is
    usually known information. With this approach, we are essentially implementing **peer-to-peer
    load balancing**.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个替代方案。我们可以从画面中移除负载均衡器，并将请求直接从客户端（**服务A**）分发出去，现在**服务A**直接负责在其请求之间负载均衡**服务B**的各种实例。这只有在**服务器A**知道暴露**服务B**的服务器细节的情况下才可能，在一个内部网络中，这通常是已知信息。采用这种方法，我们实际上是在实施**对等式负载均衡**。
- en: '*Figure 12.8* compares the two alternatives we just described:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.8*比较了我们刚刚描述的两个替代方案：'
- en: '![](img/B15729_12_08.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8](img/B15729_12_08.png)'
- en: 'Figure 12.8: Centralized load balancing versus peer-to-peer load balancing'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：集中式负载均衡与对等式负载均衡
- en: 'This is an extremely simple and effective pattern that enables truly distributed
    communications without bottlenecks or single points of failure. Besides that,
    it also has the following properties:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个极其简单且有效的模式，它能够实现真正分布式的通信，没有瓶颈或单点故障。除此之外，它还具有以下特性：
- en: Reduces the infrastructure complexity by removing a network node
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过移除网络节点来降低基础设施复杂性
- en: Allows faster communications because messages will travel through one fewer
    node
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于消息将通过更少的节点传输，因此允许更快的通信
- en: Scales better because performances are not limited by what the load balancer
    can handle
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展性更好，因为性能不受负载均衡器处理能力的限制
- en: On the other hand, by removing the load balancer, we are actually exposing the
    complexity of its underlying infrastructure. Also, each client has to be smarter by
    implementing a load balancing algorithm and, possibly, also a way to keep its
    knowledge of the infrastructure up to date.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通过移除负载均衡器，我们实际上暴露了其底层基础设施的复杂性。此外，每个客户端都需要通过实现负载均衡算法，以及可能的方式保持其对基础设施的了解更新。
- en: Peer-to-peer load balancing is a pattern used extensively in the ZeroMQ ([nodejsdp.link/zeromq](http://nodejsdp.link/zeromq))
    library, which we will use in the next chapter.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对等负载均衡是一种在ZeroMQ ([nodejsdp.link/zeromq](http://nodejsdp.link/zeromq)) 库中广泛使用的模式，我们将在下一章中使用它。
- en: In the next section, we will showcase an example implementing peer-to-peer load
    balancing in an HTTP client.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示一个示例，实现HTTP客户端中的对等负载均衡。
- en: Implementing an HTTP client that can balance requests across multiple servers
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现一个可以跨多个服务器平衡请求的HTTP客户端
- en: 'We already know how to implement a load balancer using only Node.js and distribute
    incoming requests across the available servers, so implementing the same mechanism
    on the client side should not be that different. All we have to do, in fact, is
    wrap the client API and augment it with a load balancing mechanism. Take a look
    at the following module (`balancedRequest.js`):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何仅使用Node.js实现负载均衡器，并将传入的请求分布到可用服务器上，因此，在客户端实现相同的机制不应该有很大不同。实际上，我们只需要包装客户端API，并为其添加负载均衡机制。请看以下模块（`balancedRequest.js`）：
- en: '[PRE31]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding code is very simple and needs little explanation. We wrapped the
    original `http.request` API so that it overrides the `hostname` and `port` of
    the request with those selected from the list of available servers using a round-robin
    algorithm. Note that, for simplicity, we used the module `get-stream` ([nodejsdp.link/get-stream](http://nodejsdp.link/get-stream))
    to "accumulate" the response stream into a buffer that will contain the full response
    body.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码非常简单，不需要太多解释。我们包装了原始的`http.request` API，使其能够使用轮询算法从可用服务器列表中选择那些来覆盖请求的`hostname`和`port`。请注意，为了简单起见，我们使用了`get-stream`模块
    ([nodejsdp.link/get-stream](http://nodejsdp.link/get-stream)) 来“累积”响应流到一个缓冲区，该缓冲区将包含完整的响应体。
- en: 'The new wrapped API can then be used seamlessly (`client.js`):'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 新的包装API可以无缝使用（`client.js`）：
- en: '[PRE32]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To run the preceding code, we have to start two instances of the sample server
    provided:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行前面的代码，我们必须启动两个样本服务器实例：
- en: '[PRE33]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is followed by the client application we just built:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这后面跟着我们刚刚构建的客户端应用程序：
- en: '[PRE34]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We should note that each request is sent to a different server, confirming that
    we are now able to balance the load without a dedicated load balancer!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，每个请求都发送到不同的服务器，这证实我们现在能够在不使用专用负载均衡器的情况下进行负载均衡！
- en: An obvious improvement to the wrapper we created previously would be to integrate
    a service registry directly into the client and obtain the server list dynamically.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们之前创建的包装器的一个明显改进是将服务注册表直接集成到客户端，并动态获取服务器列表。
- en: In the next section, we will explore the field of containers and container orchestration
    and see how, in this specific context, the runtime takes ownership of many scalability
    concerns.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨容器和容器编排领域，并了解在这个特定上下文中，运行时如何接管许多可扩展性问题。
- en: Scaling applications using containers
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用容器扩展应用程序
- en: In this section, we will demonstrate how using containers and container orchestration
    platforms, such as Kubernetes, can help us to write simpler Node.js applications
    that can delegate most of the scaling concerns like load balancing, elastic scaling,
    and high availability to the underlying container platform.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用容器和容器编排平台，如Kubernetes，帮助我们编写更简单的Node.js应用程序，这些应用程序可以将大多数可扩展性问题（如负载均衡、弹性扩展和可用性）委托给底层的容器平台。
- en: Containers and container orchestration platforms constitute a quite broad topic,
    largely outside the scope of this book. For this reason, here, we aim to provide
    only some basic examples to get you started with this technology using Node.js.
    Ultimately, our goal is to encourage you to explore new modern patterns in order
    to run and scale Node.js applications.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 容器和容器编排平台是一个相当广泛的话题，很大程度上超出了本书的范围。因此，在这里，我们的目标是仅提供一些基本示例，帮助您使用Node.js开始这项技术。最终，我们的目标是鼓励您探索新的现代模式，以便运行和扩展Node.js应用程序。
- en: What is a container?
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器是什么？
- en: A **container**, specifically a **Linux container**, as standardized by the
    **Open Container Initiative** (**OCI**) ([nodejsdp.link/opencontainers](http://nodejsdp.link/opencontainers)),
    is defined as "a standard unit of software that packages up code and all its dependencies
    so the application runs quickly and reliably from one computing environment to
    another."
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器**，特别是由**开放容器倡议**（**OCI**）[nodejsdp.link/opencontainers](http://nodejsdp.link/opencontainers)标准化的**Linux容器**，被定义为“一个标准的软件单元，它将代码及其所有依赖项打包在一起，以便应用程序能够快速且可靠地在不同的计算环境中运行。”'
- en: In other words, by using containers, you can seamlessly package and run applications
    on different machines, from a local development laptop on your desk to a production
    server in the cloud.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，通过使用容器，你可以在不同的机器上无缝打包和运行应用程序，从你桌上的本地开发笔记本电脑到云中的生产服务器。
- en: Other than being extremely portable, applications running as containers have
    the advantage of having very little overhead when executed. In fact, containers
    run almost as fast as running the native application directly on the operating
    system.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 除了极其可移植之外，作为容器运行的应用程序还有一个优点，那就是在执行时开销非常小。实际上，容器运行的速度几乎与直接在操作系统上运行原生应用程序一样快。
- en: In simple terms, you can see a container as a standard unit of software that
    allows you to define and run an *isolated* process directly on a Linux operating
    system.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，你可以将容器视为一个标准的软件单元，它允许你直接在Linux操作系统上定义和运行一个**隔离**的过程。
- en: For their portability and performance, containers are considered a huge step
    forward when compared to **virtual machines**.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 与**虚拟机**相比，容器因其可移植性和性能而被认为是一个巨大的进步。
- en: There are different ways and tools to create and run an OCI compliant container
    for an application. The most popular of them is **Docker** ([nodejsdp.link/docker](http://nodejsdp.link/docker)).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法和工具可以创建和运行符合OCI标准的容器，其中最受欢迎的是**Docker**[nodejsdp.link/docker](http://nodejsdp.link/docker)。
- en: 'You can install Docker in your system by following the instructions for your
    operating system on the official documentation: [nodejsdp.link/docker-docs](http://nodejsdp.link/docker-docs).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过遵循官方文档中针对你操作系统的说明来在你的系统中安装Docker：[nodejsdp.link/docker-docs](http://nodejsdp.link/docker-docs)。
- en: Creating and running a container with Docker
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Docker创建和运行容器
- en: 'Let''s rewrite our simple web server application with some minor changes (`app.js`):'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一些小的改动重新编写我们的简单网络服务器应用程序（`app.js`）：
- en: '[PRE35]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Compared to the previous versions of this web server, here, we send the machine
    hostname and the application version back to the user. If you run this server
    and make a request, you should get back something like this:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 与此前的这个网络服务器版本相比，这里我们向用户发送机器主机名和应用程序版本。如果你运行这个服务器并发出请求，你应该得到类似以下的内容：
- en: '[PRE36]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s see how we can run this application as a container. The first thing
    we need to do is create a `package.json` file for the project:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将这个应用程序作为容器运行。首先我们需要为项目创建一个`package.json`文件：
- en: '[PRE37]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In order to *dockerize* our application, we need to follow a two-step process:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的应用程序**容器化**，我们需要遵循两步流程：
- en: Build a container image
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建容器镜像
- en: Run a container instance from the image
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从镜像运行容器实例
- en: 'To create the **container image** for our application, we have to define a
    Dockerfile. A container image (or Docker image) is the actual package and conforms
    to the OCI standard. It contains all the source code and the necessary dependencies
    and describes how the application must be executed. A **Dockerfile** is a file
    (actually named `Dockerfile`) that defines the build script used to build a container
    image for an application. So, without further ado, let''s write the Dockerfile
    for our application:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们应用程序的**容器镜像**，我们必须定义一个Dockerfile。容器镜像（或Docker镜像）是实际的包，符合OCI标准。它包含所有源代码和必要的依赖项，并描述了应用程序必须如何执行。**Dockerfile**是一个文件（实际上命名为`Dockerfile`），它定义了用于构建应用程序容器镜像的构建脚本。因此，无需多言，让我们为我们的应用程序编写Dockerfile：
- en: '[PRE38]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Our Dockerfile is quite short, but there are a lot of interesting things here,
    so let''s discuss them one by one:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Dockerfile相当简短，但这里有很多有趣的东西，所以让我们逐一讨论：
- en: '`FROM node:14-alpine` indicates the base image that we want to use. A base
    image allows us to build "on top" of an existing image. In this specific case,
    we are starting from an image that already contains version 14 of Node.js. This
    means we don''t have to be worried about describing how Node.js needs to be packaged
    into the container image.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FROM node:14-alpine` 指定了我们想要使用的基镜像。基镜像允许我们在现有镜像的基础上构建。在这种情况下，我们从一个已经包含 Node.js
    版本 14 的镜像开始。这意味着我们不必担心描述 Node.js 需要如何打包到容器镜像中。'
- en: '`EXPOSE 8080` informs Docker that the application will be listening for TCP
    connections on the port `8080`.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EXPOSE 8080` 通知 Docker 应用程序将在端口 `8080` 上监听 TCP 连接。'
- en: '`COPY app.js package.json /app/` copies the files `app.js` and `package.json`
    into the `/app` folder of the container filesystem. Containers are isolated, so,
    by default, they can''t share files with the host operating system; therefore,
    we need to copy the project files into the container to be able to access and
    execute them.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`COPY app.js package.json /app/` 将文件 `app.js` 和 `package.json` 复制到容器文件系统的 `/app`
    文件夹中。容器是隔离的，所以默认情况下，它们不能与宿主操作系统共享文件；因此，我们需要将项目文件复制到容器中，以便能够访问和执行它们。'
- en: '`WORKDIR /app` sets the working directory for the container to `/app`.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WORKDIR /app` 设置容器的当前工作目录为 `/app`。'
- en: '`CMD ["npm", "start"]` specifies the command that is executed to start the
    application when we run a container from an image. Here, we are just running `npm
    start`, which, in turn, will run `node app.js`, as specified in our `package.json`.
    Remember that we are able to run both `node` and `npm` in the container only because
    those two executables are made available through the base image.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CMD ["npm", "start"]` 指定了在从镜像运行容器时执行的命令。在这里，我们只是运行 `npm start`，这反过来会运行 `node
    app.js`，正如我们在 `package.json` 中指定的那样。记住，我们能够在容器中运行 `node` 和 `npm`，仅仅是因为这两个可执行文件是通过基础镜像提供的。'
- en: 'Now, we can use the Dockerfile to build the container image with the following
    command:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 Dockerfile 使用以下命令构建容器镜像：
- en: '[PRE39]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This command will look for a Dockerfile in the current working directory and
    execute it to build our image.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将在当前工作目录中查找 Dockerfile 并执行它以构建我们的镜像。
- en: 'The output of this command should be something like this:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令的输出应该是这样的：
- en: '[PRE40]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that if you have never used the `node:14-alpine` image before (or if you
    have recently wiped your Docker cache), you will also see some additional output,
    indicating the download of this container image.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你以前从未使用过 `node:14-alpine` 镜像（或者如果你最近清除了 Docker 缓存），你还会看到一些额外的输出，表明正在下载这个容器镜像。
- en: 'The final hash is the ID of our container image. We can use it to run an instance
    of the container with the following command:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的哈希是容器镜像的 ID。我们可以使用以下命令运行容器的一个实例：
- en: '[PRE41]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This command is essentially telling Docker to run the application from image
    `bb3bd34bac55` in "interactive mode" (which means that it will not go in the background)
    and that port `8080` of the container will be mapped to port `8080` of the host
    machine (our operating system).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令本质上是在告诉 Docker 以“交互模式”运行来自镜像 `bb3bd34bac55` 的应用程序（这意味着它不会进入后台），并且容器上的端口
    `8080` 将映射到宿主机的端口 `8080`（我们的操作系统）。
- en: 'Now, we can access the application at `localhost:8080`. So, if we use `curl`
    to send a request to the web server, we should get a response similar to the following:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在 `localhost:8080` 访问应用程序。所以，如果我们使用 `curl` 向 web 服务器发送请求，我们应该得到以下类似的响应：
- en: '[PRE42]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note that the hostname is now different. This is because every container is
    running in a sandboxed environment that, by default, doesn't have access to most
    of the resources in the underlying operating system.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，主机名现在不同了。这是因为每个容器都在一个沙盒环境中运行，默认情况下，它无法访问底层操作系统的大多数资源。
- en: At this point, you can stop the container by just pressing Ctrl + C in the terminal
    window where the container is running.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你只需在运行容器的终端窗口中按下 Ctrl + C，就可以停止容器。
- en: 'When building an image, we can use the `-t` flag to *tag* the resulting image.
    A tag can be used as a more predictable alternative to a generated hash to identify
    and run container images. For instance, if we want to call our container image
    `hello-web:v1`, we can use the following commands:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建镜像时，我们可以使用 `-t` 标志来 **标记** 生成的镜像。标记可以用作比生成的哈希更可预测的替代品，用于识别和运行容器镜像。例如，如果我们想将我们的容器镜像命名为
    `hello-web:v1`，我们可以使用以下命令：
- en: '[PRE43]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: When using tags, you might want to follow the conventional format of `image-name:version`.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用标记时，你可能想遵循 `image-name:version` 的传统格式。
- en: What is Kubernetes?
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是Kubernetes？
- en: 'We just ran a Node.js application using containers, hooray! Even though this
    seems like a particularly exciting achievement, we have just scratched the surface
    here. The real power of containers comes out when building more complicated applications.
    For instance, when building applications composed by multiple independent services
    that needs to be deployed and coordinated across multiple cloud servers. In this
    situation, Docker alone is not sufficient anymore. We need a more complex system
    that allows us to orchestrate all the running container instances over the available
    machines in our cloud cluster: we need a container orchestration tool.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚使用容器运行了一个Node.js应用程序，太棒了！尽管这看起来像是一个特别令人兴奋的成就，但我们在这里只是触及了表面。容器真正的力量在构建更复杂的应用程序时才显现出来。例如，当构建由多个独立服务组成的应用程序，这些服务需要部署并在多个云服务器上协调时。在这种情况下，仅使用Docker已经不再足够。我们需要一个更复杂的系统，使我们能够编排云集群中所有运行容器实例：我们需要一个容器编排工具。
- en: 'A container orchestration tool has a number of responsibilities:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排工具有多项职责：
- en: It allows us to join multiple cloud servers (nodes) into one logical cluster,
    where nodes can be added and removed dynamically without affecting the availability
    of the services running in every node.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许我们将多个云服务器（节点）组合成一个逻辑集群，其中节点可以动态添加和移除，而不会影响每个节点上运行的服务可用性。
- en: It makes sure that there is no downtime. If a container instance stops or becomes
    unresponsive to health checks, it will be automatically restarted. Also, if a
    node in the cluster fails, the workload running in that node will be automatically
    migrated to another node.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它确保没有停机时间。如果一个容器实例停止或对健康检查无响应，它将被自动重启。此外，如果集群中的某个节点失败，该节点上运行的工作负载将被自动迁移到另一个节点。
- en: Provides functionalities to implement service discovery and load balancing.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供实现服务发现和负载均衡的功能。
- en: Provides orchestrated access to durable storage so that data can be persisted
    as needed.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供对持久存储的编排访问，以便根据需要持久化数据。
- en: Automatic rollouts and rollbacks of applications with zero downtime.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序的零停机时间自动部署和回滚。
- en: Secret storage for sensitive data and configuration management systems.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于敏感数据和配置管理系统的秘密存储。
- en: One of the most popular container orchestration systems is Kubernetes ([nodejsdp.link/kubernetes](http://nodejsdp.link/kubernetes)),
    originally open sourced by Google in 2014\. The name Kubernetes originates from
    the Greek ![](img/kubernetes.png), meaning "helmsman" or "pilot", but also "governor"
    or more generically, "the one in command". Kubernetes incorporates years of experience
    from Google engineers running workloads in the cloud at scale.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的容器编排系统之一是Kubernetes ([nodejsdp.link/kubernetes](http://nodejsdp.link/kubernetes))，它最初由谷歌在2014年开源。Kubernetes这个名字来源于希腊语
    ![kubernetes](img/kubernetes.png)，意为“舵手”或“飞行员”，但也指“管理者”或更普遍的“指挥者”。Kubernetes结合了谷歌工程师在云中大规模运行工作负载多年的经验。
- en: One of its peculiarities is the declarative configuration system that allows
    you to define an "end state" and let the orchestrator figure out the sequence
    of steps necessary to reach the desired state, without disrupting the stability
    of the services running on the cluster.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 它的一个特点是声明性配置系统，允许您定义一个“最终状态”，并让编排器找出达到所需状态的必要步骤序列，而不会破坏集群上运行的服务稳定性。
- en: 'The whole idea of Kubernetes configuration revolves around the concept of "objects".
    An object is an element in your cloud deployment, which can be added, removed,
    and have its configuration changed over time. Some good examples of Kubernetes
    objects are:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes配置的整体思想围绕着“对象”这一概念。对象是您云部署中的一个元素，它可以被添加、移除，并且其配置可以在一段时间内发生变化。Kubernetes对象的一些好例子包括：
- en: Containerized applications
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器化应用程序
- en: Resources for the containers (CPU and memory allocations, persistent storage,
    access to devices such as network interfaces or GPU, and so on)
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器的资源（CPU和内存分配、持久存储、访问网络接口或GPU等设备等）
- en: Policies for the application behavior (restart policies, upgrades, fault-tolerance)
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序行为的策略（重启策略、升级、容错）
- en: A Kubernetes object is a sort of "record of intent", which means that once you
    create one in a cluster, Kubernetes will constantly monitor (and change, if needed)
    the state of the object to make sure it stays compliant with the defined expectation.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes对象是一种“意图记录”，这意味着一旦在集群中创建了一个对象，Kubernetes将不断监控（并在需要时更改）对象的状态，以确保它符合定义的期望。
- en: A Kubernetes cluster is generally managed through a command-line tool called
    `kubectl` ([nodejsdp.link/kubectl-install](http://nodejsdp.link/kubectl-install)).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群通常通过一个名为`kubectl`的命令行工具进行管理([nodejsdp.link/kubectl-install](http://nodejsdp.link/kubectl-install))。
- en: There are several ways to create a Kubernetes cluster for development, testing,
    and production purposes. The easiest way to start experimenting with Kubernetes
    is through a local single-node cluster, which can be easily created by a tool
    called `minikube` ([nodejsdp.link/minikube-install](http://nodejsdp.link/minikube-install)).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方式可以创建用于开发、测试和生产目的的Kubernetes集群。开始实验Kubernetes最简单的方式是通过一个本地单节点集群，这可以通过一个名为`minikube`的工具轻松创建([nodejsdp.link/minikube-install](http://nodejsdp.link/minikube-install))。
- en: Make sure to install both `kubectl` and `minikube` on your system, as we will
    be deploying our sample containerized app on a local Kubernetes cluster in the
    next section!
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保在你的系统上安装了`kubectl`和`minikube`，因为我们在下一节将部署我们的示例容器化应用程序到本地Kubernetes集群！
- en: Another great way to learn about Kubernetes is by using the official interactive
    tutorials ([nodejsdp.link/kubernetes-tutorials](http://nodejsdp.link/kubernetes-tutorials)).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种了解Kubernetes的绝佳方式是通过使用官方的交互式教程([nodejsdp.link/kubernetes-tutorials](http://nodejsdp.link/kubernetes-tutorials))。
- en: Deploying and scaling an application on Kubernetes
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署和扩展应用程序
- en: In this section, we will be running our simple web server application on a local
    `minikube` cluster. So, make sure you have `kubectl` and `minikube` correctly
    installed and started.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在本地的`minikube`集群上运行我们的简单Web服务器应用程序。所以，请确保你已经正确安装并启动了`kubectl`和`minikube`。
- en: On macOS and Linux environments, make sure to run `minikube start` and `eval
    $(minikube docker-env)` to initialize the working environment. The second command
    makes sure that when you use `docker` and `kubectl` in your current terminal you
    will interact with the local Minikube cluster. If you open multiple terminals
    you should run `eval $(minikube docker-env)` on every terminal. You can also run
    `minikube dashboard` to run a convenient web dashboard that allows you to visualize
    and interact with all the objects in your cluster.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在macOS和Linux环境中，请确保运行`minikube start`和`eval $(minikube docker-env)`来初始化工作环境。第二个命令确保当你当前终端中使用`docker`和`kubectl`时，你会与本地Minikube集群交互。如果你打开了多个终端，你应该在每个终端上运行`eval
    $(minikube docker-env)`。你也可以运行`minikube dashboard`来运行一个方便的Web仪表板，它允许你可视化并交互你的集群中的所有对象。
- en: 'The first thing that we want to do is build our Docker image and give it a
    meaningful name:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是构建我们的Docker镜像，并给它一个有意义的名字：
- en: '[PRE44]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If you have configured your environment correctly, the `hello-web` image will
    be available to be used in your local Kubernetes cluster.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经正确配置了你的环境，`hello-web`镜像将可用于在你的本地Kubernetes集群中使用。
- en: Using local images is sufficient for local development. When you are ready to
    go to production, the best option is to publish your images to a Docker container
    registry such as Docker Hub ([nodejsdp.link/docker-hub](http://nodejsdp.link/docker-hub)),
    Docker Registry ([nodejsdp.link/docker-registry](http://nodejsdp.link/docker-registry)),
    Google Cloud Container Registry ([nodejsdp.link/gc-container-registry](http://nodejsdp.link/gc-container-registry)),
    or Amazon Elastic Container Registry ([nodejsdp.link/ecr](http://nodejsdp.link/ecr)).
    Once you have your images published to a container registry, you can easily deploy
    your application to different hosts without having to rebuild the corresponding
    images each time.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地开发中使用本地镜像就足够了。当你准备进入生产环境时，最佳选择是将你的镜像发布到Docker容器注册库，例如Docker Hub ([nodejsdp.link/docker-hub](http://nodejsdp.link/docker-hub))、Docker
    Registry ([nodejsdp.link/docker-registry](http://nodejsdp.link/docker-registry))、Google
    Cloud Container Registry ([nodejsdp.link/gc-container-registry](http://nodejsdp.link/gc-container-registry))
    或 Amazon Elastic Container Registry ([nodejsdp.link/ecr](http://nodejsdp.link/ecr))。一旦你的镜像发布到容器注册库，你就可以轻松地将你的应用程序部署到不同的主机，而无需每次都重新构建相应的镜像。
- en: Creating a Kubernetes deployment
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建Kubernetes部署
- en: 'Now, in order to run an instance of this container in the Minikube cluster,
    we have to create a **deployment** (which is a Kubernetes object) using the following
    command:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了在Minikube集群中运行这个容器的实例，我们必须使用以下命令创建一个**部署**（这是一个Kubernetes对象）：
- en: '[PRE45]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This should produce the following output:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '[PRE46]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This command is basically telling Kubernetes to run an instance of the `hello-web:v1`
    container as an application called `hello-web`.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令基本上是在告诉Kubernetes运行一个`hello-web:v1`容器的实例，作为名为`hello-web`的应用程序。
- en: 'You can verify that the deployment is running with the following command:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令来验证部署是否正在运行：
- en: '[PRE47]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This should print something like this:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打印出类似以下内容：
- en: '[PRE48]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This table is basically saying that our `hello-web` deployment is alive and
    that there is one **pod** allocated for it. A pod is a basic unit in Kubernetes
    and represents a set of containers that have to run together in the same Kubernetes
    node. Containers in the same pod have shared resources like storage and network.
    Generally, a pod contains only one container, but it's not uncommon to see more
    than one container in a pod when these containers are running tightly coupled
    applications.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格基本上是说我们的`hello-web`部署是活跃的，并且有一个**Pod**为其分配。Pod是Kubernetes中的基本单元，代表一组必须在同一Kubernetes节点上一起运行的容器。同一Pod中的容器共享资源，如存储和网络。通常，一个Pod只包含一个容器，但当一个容器运行紧密耦合的应用程序时，在Pod中看到多个容器并不罕见。
- en: 'You can list all the pods running in the cluster with:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令列出集群中运行的所有Pod：
- en: '[PRE49]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This should print something like:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打印出类似以下内容：
- en: '[PRE50]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, in order to be able to access the web server from our local machine, we
    need to *expose* the deployment:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了能够从我们的本地机器访问Web服务器，我们需要将部署*暴露*出来：
- en: '[PRE51]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The first command tells Kubernetes to create a `LoadBalancer` object that exposes
    the instances of the `hello-web` app, connecting to port `8080` of every container.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令告诉Kubernetes创建一个`LoadBalancer`对象，该对象暴露了`hello-web`应用的实例，连接到每个容器的`8080`端口。
- en: 'The second command is a `minikube` helper command that allows us to get the
    local address to access the load balancer. This command will also open a browser
    window for you, so now you should see the container response in the browser, which
    should look like this:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个命令是一个`minikube`辅助命令，它允许我们获取访问负载均衡器的本地地址。此命令还会为您打开一个浏览器窗口，因此现在您应该在浏览器中看到容器响应，它应该看起来像这样：
- en: '[PRE52]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Scaling a Kubernetes deployment
  id: totrans-396
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubernetes部署的扩展
- en: 'Now that our application is running and is accessible, let''s actually start
    to experiment with some of the capabilities of Kubernetes. For instance, why not
    try to scale our application by running five instances instead of just one? This
    is as easy as running:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应用程序正在运行并且可以访问，让我们实际上开始尝试一些Kubernetes的功能。例如，为什么不尝试通过运行五个实例而不是一个来扩展我们的应用程序？这就像运行以下命令一样简单：
- en: '[PRE53]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, `kubectl get deployments` should show us the following status:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`kubectl get deployments`应该会显示以下状态：
- en: '[PRE54]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'And `kubectl get pods` should produce something like this:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 并且`kubectl get pods`应该会显示类似以下内容：
- en: '[PRE55]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: If you try to hit the load balancer now, chances are you will see different
    hostnames as the traffic gets distributed across the available instances. This
    should be even more apparent if you try to hit the load balancer while putting
    the application under stress, for instance, by running an `autocannon` load test
    against the load balancer URL.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在尝试访问负载均衡器，很可能会看到不同的主机名，因为流量被分配到可用的实例上。如果你在给应用程序施加压力的同时尝试访问负载均衡器，例如，通过运行针对负载均衡器URL的`autocannon`负载测试，这种情况应该会更加明显。
- en: Kubernetes rollouts
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Kubernetes滚动更新
- en: 'Now, let''s try out another feature of Kubernetes: rollouts. What if we want
    to release a new version of our app?'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试Kubernetes的另一个功能：滚动更新。如果我们想发布应用程序的新版本怎么办？
- en: 'We can set `const version = 2` in our `app.js` file and create a new image:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在`app.js`文件中设置`const version = 2`并创建一个新的镜像：
- en: '[PRE56]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'At this point, in order to upgrade all the running pods to this new version,
    we have to run the following command:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，为了将所有正在运行的Pod升级到这个新版本，我们必须运行以下命令：
- en: '[PRE57]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output of this command should be as follows:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令的输出应该是这样的：
- en: '[PRE58]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'If everything worked as expected, you should now be able to refresh your browser
    page and see something like the following:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切按预期工作，你现在应该能够刷新浏览器页面并看到以下内容：
- en: '[PRE59]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note the **v2** flag there.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 注意那里的**v2**标志。
- en: What just happened behind the scenes is that Kubernetes started to roll out
    the new version of our image by replacing the containers one by one. When a container
    is replaced, the running instance is stopped gracefully. This way requests that
    are currently in progress can be completed before the container is shut down.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后发生的事情是Kubernetes开始通过逐个替换容器来推出我们镜像的新版本。当一个容器被替换时，正在运行的实例会优雅地停止。这样，在容器关闭之前，当前正在进行的请求可以被完成。
- en: This completes our mini Kubernetes tutorial. The lesson here is that, when using
    a container orchestrator platform like Kubernetes, we can keep our application
    code quite simple, as we won't have to include concerns such as scaling to multiple
    instances or deal with soft rollouts and application restarts. This is the major
    advantage of this approach.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们的迷你 Kubernetes 教程。这里的教训是，当使用像 Kubernetes 这样的容器编排平台时，我们可以保持应用程序代码相当简单，因为我们不需要包括诸如扩展到多个实例或处理软发布和应用程序重启等问题。这是这种方法的主要优势。
- en: Of course, this simplicity does not come for free. It is paid by having to learn
    and manage the orchestration platform. If you are running small applications in
    production, it is probably not worth to incur the complexity and the cost of having
    to install and manage a container orchestrator platform like Kubernetes. However,
    if you are serving millions of users every day, there is definitely a lot of value
    in building and maintaining such a powerful infrastructure.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种简单性并非免费获得。它是通过学习和管理编排平台来支付的。如果你在生产中运行小型应用程序，可能不值得承担安装和管理像 Kubernetes 这样的容器编排平台的复杂性和成本。然而，如果你每天为数百万人提供服务，构建和维护这样一个强大的基础设施肯定有很大的价值。
- en: Another interesting observation is that, when running containers in Kubernetes,
    containers are often considered "disposable," which basically means that they
    could be killed and restarted at any time. While this might seem like a non-relevant
    detail, you should actually take this behavior into account and try to keep your
    applications as stateless as possible. In fact, containers, by default, won't
    retain any change in the local filesystem, so every time you have to store some
    persistent information, you will have to rely on external storage mechanisms such
    as databases or persistent volumes.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的观察是，当在 Kubernetes 中运行容器时，容器通常被认为是“可丢弃的”，这基本上意味着它们可以在任何时候被终止并重新启动。虽然这看起来可能是一个无关紧要的细节，但实际上你应该考虑这种行为，并尽量使你的应用程序尽可能无状态。事实上，容器默认情况下不会保留本地文件系统中的任何更改，所以每次你需要存储一些持久信息时，你将不得不依赖外部存储机制，如数据库或持久卷。
- en: 'If you want to clean up your system from the containers you just ran in the
    preceding examples and stop `minikube`, you can do so with the following commands:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要清理你刚刚在前面示例中运行的容器，并停止 `minikube`，你可以使用以下命令：
- en: '[PRE60]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In the next and last part of this chapter, we will explore some interesting
    patterns to decompose a monolithic application into a set of decoupled microservices,
    something that is critically important if you have built a monolithic application
    and are now suffering from scalability issues.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一部分和最后一部分，我们将探讨一些有趣的模式，将这些单体应用程序分解成一系列解耦的微服务，这对于你已经构建了单体应用程序并且现在正遭受可扩展性问题的人来说至关重要。
- en: Decomposing complex applications
  id: totrans-422
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解构复杂应用程序
- en: So far in this chapter, we have mainly focused our analysis on the *X*-axis
    of the scale cube. We saw how it represents the easiest and most immediate way
    to distribute the load and scale an application, also improving its availability.
    In the following section, we are going to focus on the *Y*-axis of the scale cube,
    where applications are scaled by **decomposing** them by functionality and service.
    As we will learn, this technique allows us to scale not only the capacity of an
    application, but also, and most importantly, its complexity.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们主要关注了规模立方体的 *X* 轴。我们看到了它如何代表分布负载和扩展应用程序的最简单和最直接的方式，同时也提高了其可用性。在接下来的部分，我们将关注规模立方体的
    *Y* 轴，其中应用程序通过按功能和服务分解来扩展。正如我们将学到的，这种技术不仅允许我们扩展应用程序的容量，而且更重要的是，它的复杂性。
- en: Monolithic architecture
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单体架构
- en: The term monolithic might make us think of a system without modularity, where
    all the services of an application are interconnected and almost indistinguishable.
    However, this is not always the case. Often, monolithic systems have a highly
    modular architecture and a good level of decoupling between their internal components.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 单体这个词可能会让我们想到一个没有模块化的系统，其中应用程序的所有服务都是相互连接的，几乎无法区分。然而，这并不总是如此。通常，单体系统具有高度模块化的架构，以及它们内部组件之间良好的解耦程度。
- en: A perfect example is the Linux OS kernel, which is part of a category called **monolithic
    kernels** (in perfect opposition with its ecosystem and the Unix philosophy).
    Linux has thousands of services and modules that we can load and unload dynamically,
    even while the system is running. However, they all run in kernel mode, which
    means that a failure in any of them could bring the entire OS down (have you ever
    seen a kernel panic?). This approach is opposite to the microkernel architecture,
    where only the core services of the operating system run in kernel mode, while
    the rest run in user mode, usually each one with its own process. The main advantage
    of this approach is that a problem in any of these services would more likely
    cause it to crash in isolation, instead of affecting the stability of the entire system.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完美的例子是Linux操作系统内核，它是被称为**单体内核**的类别的一部分（与它的生态系统和Unix哲学完美对立）。Linux有成千上万的服务和模块，我们可以在系统运行时动态地加载和卸载它们。然而，它们都在内核模式下运行，这意味着任何其中一个的故障都可能使整个操作系统崩溃（你见过内核恐慌吗？）。这种方法与微内核架构相反，在微内核架构中，只有操作系统的核心服务在内核模式下运行，其余的则在用户模式下运行，通常每个服务都有自己的进程。这种方法的主要优势是，任何这些服务的问题更有可能单独崩溃，而不是影响整个系统的稳定性。
- en: The Torvalds-Tanenbaum debate on kernel design is probably one of the most famous *flame
    wars* in the history of computer science, where one of the main points of dispute
    was exactly monolithic versus microkernel design. You can find a web version of
    the discussion (it originally appeared on Usenet) at [nodejsdp.link/torvalds-tanenbaum](http://nodejsdp.link/torvalds-tanenbaum).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 托瓦兹-坦能鲍姆关于内核设计的辩论可能是计算机科学史上最著名的*火焰战*之一，其中主要的争议点正是单体与微内核设计。你可以在[nodejsdp.link/torvalds-tanenbaum](http://nodejsdp.link/torvalds-tanenbaum)找到讨论的网页版本（它最初出现在Usenet上）。
- en: 'It''s remarkable how these design principles, which are more than 30 years
    old, can still be applied today and in totally different environments. Modern
    monolithic applications are comparable to monolithic kernels: if any of their
    components fail, the entire system is affected, which, translated into Node.js
    terms, means that all the services are part of the same code base and run in a
    single process (when not cloned).'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设计原则已经超过30年，但今天仍然可以应用于完全不同的环境中，这一点非常引人注目。现代单体应用程序可以与单体内核相提并论：如果它们的任何组件失败，整个系统都会受到影响，用Node.js的话来说，这意味着所有服务都是同一代码库的一部分，并且在一个单独的进程中运行（除非克隆）。
- en: '*Figure 12.9* shows an example monolithic architecture:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.9* 展示了一个示例单体架构：'
- en: '![](img/B15729_12_09.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15729_12_09.png)'
- en: 'Figure 12.9: Example of a monolithic architecture'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9：单体架构示例
- en: '*Figure 12.9* shows the architecture of a typical e-commerce application. Its
    structure is modular: we have two different frontends, one for the main store
    and another for the administration interface. Internally, we have a clear separation
    of the services implemented by the application. Each service is responsible for
    a specific portion of the application business logic: **Products**, **Cart**, **Checkout**, **Search**,
    and **Authentication and Users**. However, the preceding architecture is monolithic
    since every module is part of the same codebase and runs as part of a single application.
    A failure in any of its components can potentially tear down the entire online
    store.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.9* 展示了一个典型电子商务应用的架构。其结构是模块化的：我们有两个不同的前端，一个用于主商店，另一个用于管理界面。内部，应用程序实现的服务有明确的分离。每个服务负责应用程序业务逻辑的特定部分：**产品**、**购物车**、**结账**、**搜索**和**认证与用户**。然而，前面的架构是单体的，因为每个模块都是同一代码库的一部分，并且作为单个应用程序的一部分运行。任何组件的故障都可能潜在地摧毁整个在线商店。'
- en: 'Another problem with this type of architecture is the interconnection between
    its modules; the fact that they all live inside the same application makes it
    very easy for a developer to build interactions and coupling between modules.
    For example, consider the use case of when a product is being purchased: the **Checkout**
    module has to update the availability of a **Product** object, and if those two
    modules are in the same application, it''s too easy for a developer to just obtain
    a reference to a **Product** object and update its availability directly. Maintaining
    a low coupling between internal modules is very hard in a monolithic application,
    partly because the boundaries between them are not always clear or properly enforced.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型架构的另一个问题是其模块之间的互连；它们都生活在同一个应用程序中的事实使得开发者很容易构建模块之间的交互和耦合。例如，考虑一个产品被购买的使用案例：**结账**模块必须更新**产品**对象的可用性，如果这两个模块在同一个应用程序中，开发者直接获取一个**产品**对象的引用并更新其可用性就太容易了。在单体应用程序中保持内部模块之间的低耦合非常困难，部分原因是它们之间的边界并不总是清晰或得到适当的执行。
- en: 'A **high coupling** is often one of the main obstacles to the growth of an
    application and prevents its scalability in terms of complexity. In fact, an intricate
    dependency graph means that every part of the system is a liability, it has to
    be maintained for the entire life of the product, and any change should be carefully
    evaluated because every component is like a wooden block in a Jenga tower: moving
    or removing one of them can cause the entire tower to collapse. This often results
    in building conventions and development processes to cope with the increasing
    complexity of the project.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '**高耦合**通常是阻碍应用程序增长的主要障碍之一，并阻止其在复杂性方面的可扩展性。实际上，一个复杂的依赖图意味着系统的每个部分都是一个负担，它必须在整个产品生命周期内维护，任何更改都应仔细评估，因为每个组件都像是一个Jenga塔中的木块：移动或移除其中一个都可能使整个塔倒塌。这通常会导致建立惯例和开发流程来应对项目日益增加的复杂性。'
- en: The microservice architecture
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微服务架构
- en: 'Now, we are going to reveal the most important pattern in Node.js for writing
    big applications: avoid writing big applications. This seems like a trivial statement,
    but it''s an incredibly effective strategy to scale both the complexity and the
    capacity of a software system. So, what''s the alternative to writing big applications?
    The answer is in the *Y*-axis of the scale cube: decomposition and splitting by
    service and functionality. The idea is to break down an application into its essential
    components, creating separate, independent applications. It is practically the
    opposite of a monolithic architecture. This fits perfectly with the Unix philosophy
    and the Node.js principles we discussed at the beginning of the book; in particular,
    the motto "make each program do one thing well."'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将揭示Node.js编写大型应用程序最重要的模式：避免编写大型应用程序。这似乎是一个显而易见的陈述，但它是一种非常有效的策略，可以扩展软件系统的复杂性和容量。那么，编写大型应用程序的替代方案是什么？答案是位于“*Y*”轴的规模立方体的分解和按服务和功能拆分。想法是将应用程序分解为其基本组件，创建独立的、独立的应用程序。这实际上与单体架构相反。这与Unix哲学以及我们在本书开头讨论的Node.js原则完美契合；特别是口号“让每个程序做好一件事”。
- en: '**Microservice architecture** is, today, the main reference pattern for this
    type of approach, where a set of self-sufficient services replace big monolithic
    applications. The prefix "micro" means that the services should be as small as
    possible, but always within reasonable limits. Don''t be misled by thinking that
    creating an architecture with a hundred different applications exposing only one
    web service is necessarily a good choice. In reality, there is no strict rule
    on how small or big a service should be. It''s not the size that matters in the
    design of a microservice architecture; instead, it''s a combination of different
    factors, mainly **loose coupling**, **high cohesion**, and **integration complexity**.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '**微服务架构**是目前此类方法的主要参考模式，其中一组自给自足的服务取代了庞大的单体应用程序。前缀“微”意味着服务应该尽可能小，但始终在合理的范围内。不要被认为创建一个包含一百个不同应用程序（仅暴露一个网络服务）的架构必然是一个好选择所误导。实际上，并没有严格的规定服务应该有多小或多大。在微服务架构的设计中，重要的是尺寸，而是一个由不同因素组合而成的，主要是**松耦合**、**高内聚**和**集成复杂性**。'
- en: An example of a microservice architecture
  id: totrans-438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个微服务架构的例子
- en: 'Let''s now see what the monolithic e-commerce application would look like using
    a microservice architecture:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看使用微服务架构的单一电子商务应用将是什么样子：
- en: '![](img/B15729_12_10.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15729_12_10.png)'
- en: 'Figure 12.10: An example implementation of an e-commerce system using the Microservice
    pattern'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10：使用微服务模式实现的电子商务系统示例
- en: As we can see from *Figure 12.10*, each fundamental component of the e-commerce
    application is now a self-sustaining and independent entity, living in its own
    context, with its own database. In practice, they are all independent applications
    exposing a set of related services.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图12.10*所示，电子商务应用的基本组成部分现在都是一个自我维持和独立的实体，生活在自己的环境中，拥有自己的数据库。在实践中，它们都是独立的应用程序，提供一系列相关的服务。
- en: The **data ownership** of a service is an important characteristic of the microservice
    architecture. This is why the database also has to be split to maintain the proper
    level of isolation and independence. If a unique shared database is used, it would
    become much easier for the services to work together; however, this would also
    introduce a coupling between the services (based on data), nullifying some of
    the advantages of having different applications.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的**数据所有权**是微服务架构的一个重要特征。这就是为什么数据库也必须分割，以保持适当的隔离和独立性。如果使用唯一的共享数据库，服务之间协作将变得更加容易；然而，这也会在服务之间（基于数据）引入耦合，从而抵消了拥有不同应用程序的一些优势。
- en: The dashed lines connecting all the nodes tells us that, in some way, they have
    to communicate and exchange information for the entire system to be fully functional.
    As the services do not share the same database, there is more communication involved
    to maintain the consistency of the whole system. For example, the **Checkout** service
    needs to know some information about **Products**, such as the price and restrictions
    on shipping, and at the same time, it needs to update the data stored in the **Products** service
    such as the product's availability when the checkout is complete. In *Figure 12.10*,
    we tried to represent the way the nodes communicate generic. Surely, the most
    popular strategy is using web services, but as we will see later, this is not
    the only option.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 连接所有节点的虚线告诉我们，以某种方式，它们必须进行通信和交换信息，以便整个系统完全功能正常。由于服务不共享相同的数据库，因此需要更多的通信来维护整个系统的一致性。例如，**Checkout**服务需要了解有关**产品**的一些信息，例如价格和运输限制，同时，它还需要在结账完成后更新存储在**产品**服务中的数据，例如产品的可用性。在*图12.10*中，我们试图以通用方式表示节点之间的通信方式。当然，最流行的策略是使用Web服务，但正如我们稍后将看到的，这并非唯一的选择。
- en: '**Pattern (microservice architecture)**'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式（微服务架构**）'
- en: Split a complex application by creating several small, self-contained services.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建几个小型、自包含的服务来分割复杂的应用程序。
- en: Microservices – advantages and disadvantages
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微服务 - 优点和缺点
- en: In this section, we are going to highlight some of the advantages and disadvantages
    of implementing a microservice architecture. As we will see, this approach promises to
    bring a radical change in the way we develop our applications, revolutionizing
    the way we see scalability and complexity, but on the other hand, it introduces
    new nontrivial challenges.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将突出实施微服务架构的一些优缺点。正如我们将看到的，这种方法承诺将彻底改变我们开发应用程序的方式，彻底改变我们看待可扩展性和复杂性的方式，但另一方面，它也引入了新的非平凡挑战。
- en: Martin Fowler wrote a great article about microservices that you can find at [nodejsdp.link/microservices](http://nodejsdp.link/microservices).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: Martin Fowler撰写了一篇关于微服务的优秀文章，您可以在[nodejsdp.link/microservices](http://nodejsdp.link/microservices)找到。
- en: Every service is expendable
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个服务都是可替换的
- en: The main technical advantage of having each service living in its own application
    context is that crashes do not propagate to the entire system. The goal is to
    build truly independent services that are smaller, easier to change, or can even
    be rebuilt from scratch. If, for example, the **Checkout** service of our e-commerce
    application suddenly crashes because of a serious bug, the rest of the system would continue
    to work as normal. Some functionality may be affected; for example, the ability
    to purchase a product, but the rest of the system would continue to work.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务都生活在自己的应用程序上下文中的主要技术优势是，崩溃不会传播到整个系统。目标是构建真正独立的、更小、更容易更改，甚至可以从头开始重建的服务。例如，如果我们的电子商务应用的**结账**服务突然因为一个严重的错误而崩溃，系统的其余部分将继续正常工作。一些功能可能会受到影响；例如，购买产品的能力，但系统的其余部分将继续工作。
- en: Also, imagine if we suddenly realized that the database or the programming language
    we used to implement a component was not a good design decision. In a monolithic
    application, there would be very little we could do to change things without affecting
    the entire system. Instead, in a microservice architecture, we could more easily
    reimplement the entire service from scratch, using a different database or platform,
    and the rest of the system would not even notice it, as long as the new implementation
    maintains the same interface to the rest of the system.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，设想如果我们突然意识到我们用来实现组件的数据库或编程语言不是一个好的设计决策。在单体应用程序中，我们几乎无法在不影响整个系统的情况下改变任何事情。相反，在微服务架构中，我们可以更容易地从零开始重新实现整个服务，使用不同的数据库或平台，只要新的实现保持与系统其余部分相同的接口，系统的其余部分甚至不会注意到这一点。
- en: Reusability across platforms and languages
  id: totrans-453
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 横跨平台和语言的复用性
- en: Splitting a big monolithic application into many small services allows us to
    create independent units that can be reused much more easily. **Elasticsearch** ([nodejsdp.link/elasticsearch](http://nodejsdp.link/elasticsearch))
    is a great example of a reusable search service. **ORY** ([nodejsdp.link/ory](http://nodejsdp.link/ory))
    is another example of a reusable open source technology that provides a complete
    authentication and authorization service that can be easily integrated into a
    microservice architecture.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 将大型单体应用程序拆分为许多小型服务，使我们能够创建可以更容易复用的独立单元。**Elasticsearch** ([nodejsdp.link/elasticsearch](http://nodejsdp.link/elasticsearch))
    是一个可复用搜索服务的绝佳例子。**ORY** ([nodejsdp.link/ory](http://nodejsdp.link/ory)) 是另一个可复用开源技术的例子，它提供了一个完整的身份验证和授权服务，可以轻松集成到微服务架构中。
- en: The main advantage of the microservice approach is that the level of information
    hiding is usually much higher compared to monolithic applications. This is possible
    because the interactions usually happen through a remote interface such as a web
    API or a message broker, which makes it much easier to hide implementation details
    and shield the client from changes in the way the service is implemented or deployed.
    For example, if all we have to do is invoke a web service, we are shielded from
    the way the infrastructure behind is scaled, from what programming language it
    uses, from what database it uses to store its data, and so on. All these decisions
    can be revisited and adjusted as needed, with potentially no impact on the rest
    of the system.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务方法的主要优势是，与单体应用程序相比，信息隐藏的水平通常要高得多。这是可能的，因为交互通常通过远程接口，如Web API或消息代理进行，这使得隐藏实现细节和屏蔽客户端免受服务实现或部署方式变化的影响变得容易得多。例如，如果我们只需要调用一个Web服务，我们就被屏蔽了基础设施扩展的方式、它使用的编程语言、它用来存储数据的数据库等等。所有这些决策都可以根据需要重新审视和调整，可能对系统的其余部分没有影响。
- en: A way to scale the application
  id: totrans-456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一种扩展应用程序的方法
- en: Going back to the scale cube, it's clear that microservices are equivalent to
    scaling an application along the *Y*-axis, so it's already a solution for distributing
    the load across multiple machines. Also, we should not forget that we can combine
    microservices with the other two dimensions of the cube to scale the application
    even further. For example, each service could be cloned to handle more traffic,
    and the interesting aspect is that they can be scaled independently, allowing
    better resource management.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 回到规模立方体，很明显，微服务相当于沿着*Y*轴扩展应用程序，因此它已经是一个在多台机器之间分配负载的解决方案。此外，我们不应忘记我们可以将微服务与其他两个立方体维度结合起来，以进一步扩展应用程序。例如，每个服务都可以复制以处理更多的流量，有趣的是，它们可以独立扩展，从而实现更好的资源管理。
- en: At this point, it would look like microservices are the solution to all our
    problems. However, this is far from being true. Let's see the challenges we face
    using microservices.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，看起来微服务是解决我们所有问题的方案。然而，这远非事实。让我们看看我们使用微服务时面临的挑战。
- en: The challenges of microservices
  id: totrans-459
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微服务的挑战
- en: 'Having more nodes to manage introduces a higher complexity in terms of integration,
    deployment, and code sharing: it fixes some of the pains of traditional architectures,
    but it also opens up many new questions. How do we make the services interact?
    How can we keep sanity with deploying, scaling, and monitoring such a high number
    of applications? How can we share and reuse code between services?'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 管理更多的节点引入了更高的复杂性，这在集成、部署和代码共享方面尤为明显：它解决了传统架构的一些痛点，但也提出了许多新问题。我们如何使服务交互？我们如何保持部署、扩展和监控如此众多应用程序的理智？我们如何在服务之间共享和重用代码？
- en: Fortunately, cloud services and modern DevOps methodologies can provide some
    answers to those questions, and also, using Node.js can help a lot. Its module
    system is a perfect companion to share code between different projects. Node.js
    was made to be a node in a distributed system such as those of a microservice
    architecture.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，云服务和现代DevOps方法可以为这些问题提供一些答案，而且使用Node.js也能大有帮助。它的模块系统是不同项目之间共享代码的完美伴侣。Node.js被设计成分布式系统中的一个节点，如微服务架构中的节点。
- en: In the following sections, we will introduce some integration patterns that
    can help with managing and integrating services in a microservice architecture.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍一些可以帮助在微服务架构中管理和集成服务的集成模式。
- en: Integration patterns in a microservice architecture
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微服务架构中的集成模式
- en: One of the toughest challenges of microservices is connecting all the nodes
    to make them collaborate. For example, the **Cart** service of our e-commerce
    application would make little sense without some **Products** to add, and the **Checkout** service
    would be useless without a list of products to buy (a cart). As we already mentioned,
    there are also other factors that necessitate an interaction between the various
    services. For example, the **Search** service has to know which **Products** are
    available and must also ensure it keeps its information up to date. The same can
    be said about the **Checkout** service, which has to update the information about **Product** availability
    when a purchase is completed.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务中最具挑战性的问题之一是连接所有节点以使它们协作。例如，我们电子商务应用的**购物车**服务如果没有一些**产品**添加进去就几乎没有什么意义，而**结账**服务如果没有要购买的产品列表（购物车）也将毫无用处。正如我们之前提到的，还有其他因素需要各种服务之间的交互。例如，**搜索**服务需要知道哪些**产品**是可用的，并且必须确保它保持其信息是最新的。同样，对于**结账**服务，当购买完成时，它必须更新关于**产品**可用性的信息。
- en: When designing an integration strategy, it's also important to consider the coupling that
    it's going to introduce between the services in the system. We should not forget
    that designing a distributed architecture involves the same practices and principles
    we use locally when designing a module or subsystem. Therefore, we also need to
    take into consideration properties such as the reusability and extensibility of
    the service.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计集成策略时，考虑它将在系统中的服务之间引入的耦合也很重要。我们不应忘记，设计分布式架构涉及我们在本地设计模块或子系统时使用的相同实践和原则。因此，我们还需要考虑服务的可重用性和可扩展性等属性。
- en: The API proxy
  id: totrans-466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: API代理
- en: The first pattern we are going to show makes use of an **API proxy** (also commonly
    identified as an **API gateway**), a server that proxies the communications between a
    client and a set of remote APIs. In a microservice architecture, its main purpose
    is to provide a single access point for multiple API endpoints, but it can also
    offer load balancing, caching, authentication, and traffic limiting, all of which
    are features that prove to be very useful to implement a solid API solution.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要展示的第一个模式是使用**API代理**（也常被称为**API网关**），这是一个代理客户端和一组远程API之间通信的服务器。在微服务架构中，其主要目的是为多个API端点提供一个单一的访问点，但它还可以提供负载均衡、缓存、身份验证和流量限制等功能，所有这些功能在实现一个坚实的API解决方案中都非常有用。
- en: 'This pattern should not be new to us since we already saw it in action in this
    chapter when we built the custom load balancer with `http-proxy` and `consul`. For
    that example, our load balancer was exposing only two services, and then, thanks
    to a service registry, it was able to map a URL path to a service and hence to
    a list of servers. An API proxy works in the same way; it is essentially a reverse
    proxy and often also a load balancer, specifically configured to handle API requests.
    *Figure 12.11* shows how we can apply such a solution to our e-commerce application:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式对我们来说可能并不陌生，因为我们已经在本章中通过使用`http-proxy`和`consul`构建自定义负载均衡器时看到了它的实际应用。在那个例子中，我们的负载均衡器只暴露了两个服务，然后，多亏了服务注册表，它能够将URL路径映射到服务，进而映射到服务器列表。API代理的工作方式与此相同；它本质上是一个反向代理，通常也是一个负载均衡器，专门配置来处理API请求。*图12.11*展示了我们如何将这种解决方案应用到我们的电子商务应用中：
- en: '![](img/B15729_12_11.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15729_12_11.png)'
- en: 'Figure 12.11: Using the API Proxy pattern in an e-commerce application'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11：在电子商务应用中使用API代理模式
- en: From the preceding diagram, it should be clear how an API proxy can hide the
    complexity of its underlying infrastructure. This is really handy in a microservice
    infrastructure, as the number of nodes may be high, especially if each service
    is scaled across multiple machines. The integration achieved by an API proxy is
    therefore only structural since there is no semantic mechanism. It simply provides
    a familiar monolithic view of a complex microservice infrastructure.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，应该很清楚一个API代理如何隐藏其底层基础设施的复杂性。这在微服务基础设施中非常有用，因为节点数量可能很高，尤其是如果每个服务都扩展到多台机器上。因此，API代理实现的集成仅是结构性的，因为没有语义机制。它仅仅提供了一个熟悉的大型单体视图，用于复杂的微服务基础设施。
- en: Since the API Proxy pattern essentially abstracts the complexity of connecting
    to all the different APIs in the system, it might also allow for some freedom
    to restructure the various services. Maybe, as your requirements change, you will
    need to split an existing microservice into two or more decoupled microservices
    or, conversely, you might realize that, in your business context, it's better
    to join two or more services together. In both cases, the API Proxy pattern will
    allow you to make all the necessary changes with potentially no impact on the
    upstream systems accessing the data through the proxy.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 由于API代理模式本质上抽象了连接到系统中所有不同API的复杂性，它可能也允许对各种服务进行重构的自由。也许，随着您需求的改变，您可能需要将现有的微服务拆分为两个或更多个解耦的微服务，或者相反，您可能意识到，在您的业务环境中，将两个或更多服务合并在一起可能更好。在这两种情况下，API代理模式将允许您进行所有必要的更改，可能不会对通过代理访问数据的上游系统产生影响。
- en: 'The ability to enable incremental change in an architecture over time is a
    very important characteristic in modern distributed systems. If you are interested
    in studying this broad subject in greater depth, we recommend the book *Building
    Evolutionary Architectures*: [nodejsdp.link/evolutionary-architectures](http://nodejsdp.link/evolutionary-architectures).'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在一段时间内使架构能够实现增量变化是现代分布式系统的一个重要特性。如果您对深入研究这个广泛的主题感兴趣，我们推荐阅读书籍《构建可演化架构》：[nodejsdp.link/evolutionary-architectures](http://nodejsdp.link/evolutionary-architectures)。
- en: API orchestration
  id: totrans-474
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: API编排
- en: 'The pattern we are going to describe next is probably the most natural and
    explicit way to integrate and compose a set of services, and it''s called **API
    orchestration**. Daniel Jacobson, VP of Engineering for the Netflix API, in one
    of his blog posts ([nodejsdp.link/orchestration-layer](http://nodejsdp.link/orchestration-layer)),
    defines API orchestration as follows:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要描述的模式可能是集成和组合一组服务最自然和最明确的方式，它被称为**API编排**。Netflix API工程副总裁Daniel Jacobson在他的博客文章([nodejsdp.link/orchestration-layer](http://nodejsdp.link/orchestration-layer))中定义了API编排如下：
- en: '"An API **Orchestration Layer** (**OL**) is an abstraction layer that takes
    generically-modeled data elements and/or features and prepares them in a more
    specific way for a targeted developer or application."'
  id: totrans-476
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一个API **编排层**（**OL**）是一个抽象层，它将通用的建模数据元素和/或特性以更具体的方式准备，以便为特定的开发人员或应用服务。”
- en: The "generically modeled elements and/or features" fit the description of a
    service in a microservice architecture perfectly. The idea is to create an abstraction
    to connect those bits and pieces to implement new services specific to a particular
    application.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: “通用的建模元素和/或特性”完美地符合微服务架构中服务的描述。想法是创建一个抽象层，将这些零散的部分连接起来，以实现针对特定应用的新服务。
- en: 'Let''s see an example using the e-commerce application. Refer to *Figure 12.12*:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过电子商务应用的示例来了解一下。参考 *图12.12*：
- en: '![](img/B15729_12_12.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_12_12.png)'
- en: 'Figure 12.12: An example usage of an orchestration layer to interact with multiple
    microservices'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12：使用编排层与多个微服务交互的示例
- en: '*Figure 12.12* shows how the **Store frontend** application uses an orchestration
    layer to build more complex and specific features by composing and orchestrating
    existing services. The described scenario takes, as an example, a hypothetical
    `completeCheckout()` service that is invoked the moment a customer clicks the **Pay** button
    at the end of the checkout.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.12* 展示了 **商店前端** 应用如何通过组合和编排现有服务来构建更复杂和特定的功能。所描述的场景以一个假设的 `completeCheckout()`
    服务为例，该服务在客户在结账末尾点击 **支付** 按钮时被调用。'
- en: 'The figure shows how `completeCheckout()` is a composite operation made of
    three different steps:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了 `completeCheckout()` 是由三个不同步骤组成的复合操作：
- en: First, we complete the transaction by invoking `checkoutService/pay`.
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过调用 `checkoutService/pay` 来完成交易。
- en: Then, when the payment is successfully processed, we need to tell the **Cart**
    service that the items were purchased and that they can be removed from the cart.
    We do that by invoking `cartService/delete`.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，当支付成功处理时，我们需要通知 **购物车** 服务，所购商品已被购买，并且可以从购物车中移除。我们通过调用 `cartService/delete`
    来完成这一操作。
- en: Also, when the payment is complete, we need to update the availability of the
    products that were just purchased. This is done through `productsService/update`.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，当支付完成时，我们需要更新刚刚购买的产品可用性。这是通过 `productsService/update` 来完成的。
- en: As we can see, we took three operations from three different services and we
    built a new API that coordinates the services to maintain the entire system in
    a consistent state.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们从三个不同的服务中提取了三个操作，并构建了一个新的API来协调这些服务，以保持整个系统的一致状态。
- en: Another common operation performed by the **API Orchestration Layer** is **data
    aggregation**, or in other words, combining data from different services into
    a single response. Imagine we wanted to list all the products contained in a cart.
    In this case, the orchestration would need to retrieve the list of product IDs
    from the **Cart** service, and then retrieve the complete information about the
    products from the **Products** service. The ways in which we can combine and coordinate
    services is infinite, but the important pattern to remember is the role of the
    orchestration layer, which acts as an abstraction between a number of services
    and a specific application.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '**API编排层** 执行的另一个常见操作是 **数据聚合**，换句话说，就是将来自不同服务的数据组合成一个单一响应。想象一下，如果我们想列出购物车中包含的所有产品。在这种情况下，编排需要从
    **购物车** 服务中检索产品ID列表，然后从 **产品** 服务中检索关于产品的完整信息。我们可以组合和协调服务的方式是无限的，但需要记住的重要模式是编排层的作用，它作为多个服务与特定应用程序之间的抽象。'
- en: The orchestration layer is a great candidate for a further functional splitting.
    It is, in fact, very common to have it implemented as a dedicated, independent
    service, in which case it takes the name of **API Orchestrator**. This practice
    is perfectly in line with the microservice philosophy.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 编排层是进一步功能拆分的绝佳候选者。实际上，将其实现为一个专用、独立的服务是非常常见的，在这种情况下，它被称为 **API编排器**。这种做法与微服务哲学完全一致。
- en: '*Figure 12.13* shows this further improvement of our architecture:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.13* 展示了我们对架构的进一步改进：'
- en: '![](img/B15729_12_13.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_12_13.png)'
- en: 'Figure 12.13: An application of the API Orchestrator pattern for our e-commerce
    example'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13：API编排器模式在我们的电子商务示例中的应用
- en: 'Creating a standalone orchestrator, as shown in the previous figure, can help
    in decoupling the client application (in our case, the **Store frontend**) from
    the complexity of the microservice infrastructure. This is similar to the API
    proxy, but there is a crucial difference: an orchestrator performs a *semantic*
    integration of the various services, it''s not just a naïve proxy, and it often
    exposes an API that is different from the one exposed by the underlying services.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，创建一个独立的编排器可以帮助将客户端应用程序（在我们的案例中是 **商店前端**）从微服务基础设施的复杂性中解耦。这与API代理类似，但有一个关键的区别：编排器执行各种服务的
    *语义整合*，它不仅仅是一个简单的代理，并且它通常暴露一个与底层服务暴露的API不同的API。
- en: Integration with a message broker
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与消息代理的集成
- en: The Orchestrator pattern gave us a mechanism to integrate the various services
    in an explicit way. This has both advantages and disadvantages. It is easy to
    design, easy to debug, and easy to scale, but unfortunately, it has to have a
    complete knowledge of the underlying architecture and how each service works.
    If we were talking about objects instead of architectural nodes, the orchestrator
    would be an anti-pattern called **God object**, which defines an object that knows
    and does too much, which usually results in high coupling, low cohesion, but most
    importantly, high complexity.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '**编排器**模式为我们提供了一种以显式方式集成各种服务的机制。这既有优点也有缺点。它易于设计、易于调试和易于扩展，但遗憾的是，它必须对底层架构以及每个服务的工作方式有完整的了解。如果我们谈论的是对象而不是架构节点，那么编排器将是一个被称为**上帝对象**的反模式，它定义了一个知道并做得太多的对象，这通常会导致高耦合、低内聚，但最重要的是，高复杂性。'
- en: 'The pattern we are now going to show tries to distribute, across the services,
    the responsibility of synchronizing the information of the entire system. However,
    the last thing we want to do is create direct relationships between services,
    which would result in high coupling and a further increase in the complexity of
    the system, due to the increasing number of interconnections between nodes. The
    goal is to keep every service decoupled: every service should be able to work,
    even without the rest of the services in the system or in combination with new
    services and nodes.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将要展示的模式试图将同步整个系统信息的责任分散到各个服务中。然而，我们最不想做的事情就是创建服务之间的直接关系，这将导致高耦合，并由于节点之间交互数量的增加，进一步增加系统的复杂性。目标是保持每个服务解耦：每个服务都应能够工作，即使没有系统中的其他服务或与新的服务和节点结合。
- en: 'The solution is to use a message broker, a system capable of decoupling the
    sender from the receiver of a message, allowing us to implement a Centralized Publish/Subscribe
    pattern. This is, in practice, an implementation of the Observer pattern for distributed
    systems. We will talk more about this pattern later in *Chapter 13*, *Messaging
    and Integration Patterns*. *Figure 12.14* shows an example of how this applies
    to the e-commerce application:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用消息代理，这是一个能够解耦消息发送者和接收者的系统，使我们能够实现集中式发布/订阅模式。在实践中，这是分布式系统中观察者模式的实现。我们将在第13章“消息和集成模式”中更详细地讨论这种模式。*图12.14*展示了这种模式如何应用于电子商务应用：
- en: '![](img/B15729_12_14.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_12_14.png)'
- en: 'Figure 12.14: Using a message broker to distribute events in our e-commerce
    application'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：在电子商务应用中使用消息代理来分发事件
- en: As we can see from *Figure 12.14*, the client of the **Checkout** service, which
    is the frontend application, does not need to carry out any explicit integration
    with the other services.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图12.14*所示，**结账**服务的客户端，即前端应用，不需要与其他服务进行任何显式的集成。
- en: 'All it has to do is invoke `checkoutService/pay` to complete the checkout process
    and take the money from the customer; all the integration work happens in the
    background:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 它所需要做的只是调用`checkoutService/pay`来完成结账过程并从客户那里收取款项；所有集成工作都在后台进行：
- en: The **Store frontend** invokes the `checkoutService/pay` operation on the **Checkout** service.
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**商店前端**调用**结账**服务的`checkoutService/pay`操作。'
- en: When the operation completes, the **Checkout** service generates an event, attaching
    the details of the operation, that is, the `cartId` and the list of `products` that
    were just purchased. The event is published into the message broker. At this point,
    the **Checkout** service does not know who is going to receive the message.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当操作完成时，**结账**服务生成一个事件，附加操作详情，即`cartId`和刚刚购买的`products`列表。事件被发布到消息代理。此时，**结账**服务并不知道谁将接收这条消息。
- en: The **Cart** service is subscribed to the broker, so it's going to receive the `purchased` event
    that was just published by the **Checkout** service. The **Cart** service reacts
    by removing the cart identified with the ID contained in the message from its
    database.
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**购物车**服务订阅了代理，因此它将接收由**结账**服务刚刚发布的`purchased`事件。**购物车**服务通过从其数据库中删除包含在消息中的ID所标识的购物车来做出反应。'
- en: The **Products** service was subscribed to the message broker as well, so it
    receives the same `purchased` event. It then updates its database based on this
    new information, adjusting the availability of the products included in the message.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**产品**服务也订阅了消息代理，因此它接收相同的`购买`事件。然后根据这个新信息更新其数据库，调整消息中包含的产品可用性。'
- en: This whole process happens without any explicit intervention from external entities
    such as an orchestrator. The responsibility of spreading the knowledge and keeping
    information in sync is distributed across the services themselves. There is no
    *god* service that has to know how to move the gears of the entire system, since
    each service is in charge of its own part of the integration.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程无需外部实体（如编排器）的明确干预。传播知识和保持信息同步的责任分布在各个服务本身。没有必须知道如何移动整个系统齿轮的“上帝”服务，因为每个服务负责其自身的集成部分。
- en: The message broker is a fundamental element used to decouple the services and
    reduce the complexity of their interaction. It might also offer other interesting
    features, such as persistent message queues and guaranteed ordering of the messages.
    We will talk more about this in the next chapter.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 消息代理是一个用于解耦服务和减少它们交互复杂性的基本元素。它还可能提供其他有趣的功能，例如持久消息队列和消息的保证顺序。我们将在下一章中详细讨论这一点。
- en: Summary
  id: totrans-507
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to design Node.js architectures that scale both
    in capacity and complexity. We saw how scaling an application is not only about
    handling more traffic or reducing the response time, but it's also a practice
    to apply whenever we want better availability and tolerance to failures. We saw
    how these properties often are on the same wavelength, and we understood that
    scaling early is not a bad practice, especially in Node.js, which allows us to
    do it easily and with few resources.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何设计既能在容量上又能在复杂性上扩展的Node.js架构。我们了解到扩展应用程序不仅关乎处理更多流量或减少响应时间，而且是一种实践，无论何时我们想要更好的可用性和对失败的容忍度，都应该应用。我们了解到这些属性通常处于同一波长上，并理解到早期扩展不是一种坏习惯，尤其是在Node.js中，它允许我们轻松且资源较少地完成扩展。
- en: The scale cube taught us that applications can be scaled across three dimensions.
    Throughout this chapter, we focused on the two most important dimensions, the *X*-and *Y*-axes,
    allowing us to discover two essential architectural patterns, namely, load balancing
    and microservices. You should now know how to start multiple instances of the
    same Node.js application, how to distribute traffic across them, and how to exploit
    this setup for other purposes, such as fail tolerance and zero-downtime restarts.
    We also analyzed how to handle the problem of dynamic and auto-scaled infrastructures.
    With this, we saw that a service registry can really come in useful for those
    situations. We learned how to achieve these goals by using plain Node.js, external
    load balancers like Nginx, and service discovery systems like Consul. We also
    learned the basics of Kubernetes.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 规模立方体告诉我们，应用程序可以在三个维度上进行扩展。在本章中，我们专注于两个最重要的维度，即X轴和Y轴，使我们能够发现两个基本架构模式，即负载均衡和微服务。你现在应该知道如何启动同一Node.js应用程序的多个实例，如何在这些实例之间分配流量，以及如何利用这种设置来实现其他目的，如容错和零停机时间重启。我们还分析了如何处理动态和自动扩展的基础设施问题。通过这一点，我们看到了服务注册表在这些情况下确实非常有用。我们学习了如何通过使用纯Node.js、外部负载均衡器（如Nginx）和服务发现系统（如Consul）来实现这些目标。我们还学习了Kubernetes的基础知识。
- en: At this point, we should have got to grips with some very practical approaches
    to be able to face scalability much more fearlessly then before.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该已经掌握了应对可扩展性的实用方法，可以比以前更加无畏地面对。
- en: However, cloning and load balancing cover only one dimension of the scale cube,
    so we moved our analysis to another dimension, studying in more detail what it
    means to split an application by its constituent services by building a microservice
    architecture. We saw how microservices enable a complete revolution in how a project
    is developed and managed, providing a natural way to distribute the load of an
    application and split its complexity. However, we learned that this also means
    shifting the complexity from *how to build a big monolithic application* to *how
    to integrate a set of services*. This last aspect is where we focused the last
    part of our analysis, showing some of the architectural solutions to integrate
    a set of independent services.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，克隆和负载均衡仅覆盖了规模立方体的一个维度，因此我们将分析转向了另一个维度，更详细地研究通过构建微服务架构来按其组成部分服务拆分应用程序意味着什么。我们看到了微服务如何使项目开发和管理的革命性变革成为可能，提供了一种自然的方式来分配应用程序的负载并分割其复杂性。然而，我们了解到这也意味着将复杂性从
    *如何构建一个大型的单体应用程序* 转移到 *如何集成一组服务*。这个最后方面是我们分析的重点，展示了将一组独立服务集成的某些架构解决方案。
- en: In the next and last chapter of this book, we will have the chance to complete
    our *Node.js Design Patterns* journey by analyzing the messaging patterns we discussed
    in this chapter, in addition to more advanced integration techniques that are
    useful when implementing complex distributed architectures.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章和最后一章中，我们将有机会通过分析本章讨论的消息模式，以及更多在实现复杂分布式架构时有用的先进集成技术，来完成我们的 *Node.js 设计模式*
    之旅。
- en: Exercises
  id: totrans-513
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: '**12.1 A scalable book library**: Revisit the book library application we built
    in *Chapter 10*, *Universal JavaScript for Web Applications*, reconsidering it
    after what we learned in this chapter. Can you make our original implementation
    more scalable? Some ideas might be to use the `cluster` module to run multiple
    instances of the server, making sure you handle failures by restarting workers
    that might accidentally die. Alternatively, why not try to run the entire application
    on Kubernetes?'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**12.1 可扩展的书库**：回顾我们在 *第10章*，*Web应用程序的通用JavaScript* 中构建的书库应用程序，在了解本章内容后重新考虑它。你能否使我们的原始实现更具可扩展性？一些想法可能包括使用
    `cluster` 模块来运行服务器的多个实例，确保通过重启可能意外死亡的worker来处理故障。或者，为什么不尝试在 Kubernetes 上运行整个应用程序呢？'
- en: '**12.2 Exploring the** **Z****-axis**: Throughout this chapter, we did not
    show you any examples about how to shard data across multiple instances, but we
    explored all the necessary patterns to build an application that achieves scalability
    along the *Z*-axis of the scale cube. In this exercise, you are challenged to
    build a REST API that allows you to get a list of (randomly generated) people
    whose first name starts with a given letter. You could use a library like `faker`
    ([nodejsdp.link/faker](http://nodejsdp.link/faker)) to generate a sample of random
    people, and then you could store this data in different JSON files (or different
    databases), splitting the data into three different groups. For instance, you
    might have three groups called A-D, E-P, and Q-Z. *Ada* will go in the first group,
    *Peter* in the second, and *Ugo* in the third. Now, you can run one or more instances
    of a web server for every group, but you should expose only one public API endpoint
    to be able to retrieve all the people whose names starts with a given letter (for
    instance, `/api/people/byFirstName/{letter}`). Hint: You could use just a load
    balancer and map all the possible letters to the respective backend of the instances
    that are responsible for the associated group. Alternatively, you could create
    an API orchestration layer that encodes the mapping logic and redirects the traffic
    accordingly. Can you also throw a service discovery tool into the mix and apply
    dynamic load balancing, so that groups receiving more traffic can scale as needed?'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**12.2 探索 Z 轴**：在本章中，我们没有向你展示如何跨多个实例分片数据的任何示例，但我们探讨了构建一个沿规模立方体 Z 轴实现可扩展性的所有必要模式。在这个练习中，你被挑战构建一个
    REST API，允许你获取以给定字母开头的（随机生成的）人名列表。你可以使用像 `faker`（[nodejsdp.link/faker](http://nodejsdp.link/faker)）这样的库来生成随机人的样本，然后你可以将这些数据存储在不同的
    JSON 文件中（或不同的数据库中），将数据分成三个不同的组。例如，你可能有三个组，分别称为 A-D、E-P 和 Q-Z。*Ada* 将进入第一个组，*Peter*
    进入第二个组，*Ugo* 进入第三个组。现在，你可以为每个组运行一个或多个 Web 服务器实例，但你应该只暴露一个公共 API 端点，以便能够检索以给定字母开头（例如，`/api/people/byFirstName/{letter}`）的所有人名。提示：你可以使用负载均衡器，并将所有可能的字母映射到负责相关组的实例的后端。或者，你可以创建一个
    API 调度层，该层编码映射逻辑并相应地重定向流量。你还能将服务发现工具加入其中，并应用动态负载均衡，以便接收更多流量的组能够按需扩展吗？'
- en: '**12.3 Music addiction**: Imagine you have to design the architecture of a
    service like Spotify or Apple Music. Can you try to design this service as a collection
    of microservices by applying some of the principles discussed in this chapter?
    Bonus points if you can actually implement a minimal version of this idea with
    Node.js! If this turns out to be the next big startup idea and makes you a millionaire,
    well… don''t forget to thank the authors of this book. :)'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**12.3 音乐成瘾**：想象一下，你需要设计类似 Spotify 或 Apple Music 这样的服务架构。你能尝试应用本章讨论的一些原则，将这个服务设计为一组微服务吗？如果你能用
    Node.js 实现这个想法的最小版本，那么你将获得额外的分数！如果这个想法成为下一个大型的创业点子，让你成为百万富翁，那么……别忘了感谢这本书的作者们。
    :)'
