- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Advanced Recipes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级食谱
- en: In this chapter, we'll take a problem-solution approach and, like in a cookbook,
    we'll present a set of ready-to-use *recipes* to solve some common Node.js programming
    problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将采用问题-解决方案的方法，就像在烹饪书中一样，我们将展示一系列现成的*食谱*来解决一些常见的Node.js编程问题。
- en: You shouldn't be surprised by the fact that most of the problems presented in
    this chapter arise when we try to do things asynchronously. In fact, as we've
    seen repeatedly in the previous chapters of this book, tasks that are trivial
    in traditional synchronous programming can become more complicated when applied
    to asynchronous programming. A typical example is trying to use a component that
    requires an asynchronous initialization step. In this case, we have the inconvenience
    of delaying any attempt to use the component until the initialization completes.
    We'll show you how to solve this elegantly later.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试异步做事时，本章中提出的许多问题并不令人惊讶。事实上，正如我们在本书的前几章中反复看到的，在传统同步编程中简单易行的任务，当应用于异步编程时可能会变得更加复杂。一个典型的例子是尝试使用需要异步初始化步骤的组件。在这种情况下，我们不得不推迟使用组件的任何尝试，直到初始化完成。我们将在稍后向您展示如何优雅地解决这个问题。
- en: But this chapter is not just about recipes involving asynchronous programming.
    You will also learn the best ways to run CPU-intensive tasks in Node.js.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但这一章不仅仅关于涉及异步编程的食谱。您还将了解在Node.js中运行CPU密集型任务的最佳方式。
- en: 'These are the recipes you will learn in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中您将学习的食谱包括：
- en: Dealing with asynchronously initialized components
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理异步初始化组件
- en: Asynchronous request batching and caching
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步请求批处理和缓存
- en: Canceling asynchronous operations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取消异步操作
- en: Running CPU-bound tasks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行CPU密集型任务
- en: Let's get started.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Dealing with asynchronously initialized components
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理异步初始化组件
- en: One of the reasons for the existence of synchronous APIs in the Node.js core
    modules and many npm packages is because they are handy to use for implementing
    initialization tasks. For simple programs, using synchronous APIs at initialization
    time can streamline things a lot and the drawbacks associated with their use remain
    contained because they are used only once, which is when the program or a particular
    component is initialized.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 同步API存在于Node.js核心模块和许多npm包中的原因之一是因为它们在实现初始化任务时非常方便使用。对于简单的程序，在初始化时使用同步API可以大大简化事情，并且与它们的使用相关的缺点仍然被限制在很小的范围内，因为它们只在使用一次时使用，即程序或特定组件初始化时。
- en: Unfortunately, this is not always possible. A synchronous API might not always
    be available, especially for components using the network during their initialization
    phase to, for example, perform handshake protocols or to retrieve configuration
    parameters. This is the case for many database drivers and clients for middleware
    systems such as message queues.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这并不总是可能的。同步API可能并不总是可用，尤其是在组件初始化阶段使用网络时，例如执行握手协议或检索配置参数。这种情况适用于许多数据库驱动程序和中间件系统（如消息队列）的客户端。
- en: The issue with asynchronously initialized components
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步初始化组件的问题
- en: 'Let''s consider an example where a module called `db` is used to interact with
    a remote database. The `db` module will accept API requests only after the connection and
    handshake with the database server have been successfully completed. Therefore,
    no queries or other commands can be sent until the initialization phase is complete.
    The following is the code for such a sample module (the `db.js` file):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，其中使用名为`db`的模块与远程数据库进行交互。`db`模块只有在与数据库服务器成功建立连接和握手之后才会接受API请求。因此，在初始化阶段完成之前，不能发送查询或其他命令。以下是一个这样的示例模块（`db.js`文件）的代码：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a typical example of an asynchronously initialized component. Under
    these assumptions, we usually have two quick and easy solutions to this problem,
    which we can call *local initialization check* and *delayed startup*. Let's analyze
    them in more detail.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个异步初始化组件的典型例子。在这些假设下，我们通常有两个快速且简单的解决方案来解决这个问题，我们可以称之为*本地初始化检查*和*延迟启动*。让我们更详细地分析它们。
- en: Local initialization check
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地初始化检查
- en: 'The first solution makes sure that the module is initialized before any of
    its APIs are invoked; otherwise, we wait for its initialization. This check has
    to be done every time we want to invoke an operation on the asynchronous module:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案确保在调用模块的任何API之前，模块已经初始化；否则，我们等待其初始化。每次我们想要在异步模块上执行操作时，都必须进行此检查：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we already anticipated, any time we want to invoke the `query()` method on
    the `db` component, we have to check if the module is initialized; otherwise,
    we wait for its initialization by listening for the `'connected'` event. A variation
    of this technique performs the check inside the `query()` method itself, which
    shifts the burden of the boilerplate code from the consumer to the provider of
    the service.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前预料的，每次我们想要在`db`组件上调用`query()`方法时，我们必须检查模块是否已经初始化；否则，我们通过监听`'connected'`事件来等待其初始化。这种技术的变体将检查操作放在`query()`方法内部本身，这样就将样板代码的负担从消费者转移到了服务提供者。
- en: Delayed startup
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟启动
- en: 'The second quick and dirty solution to the problem of asynchronously initialized
    components involves delaying the execution of any code relying on the asynchronously
    initialized component until the component has finished its initialization routine.
    We can see an example of such a technique in the following code fragment:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 解决异步初始化组件问题的第二个快速且简单的方法是在组件完成其初始化例程之前，延迟依赖于该异步初始化组件的任何代码的执行。我们可以在以下代码片段中看到一个这样的技术示例：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see from the preceding code, we first wait for the initialization
    to complete, and then we proceed with executing any routine that uses the `db`
    object.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中我们可以看到，我们首先等待初始化完成，然后继续执行任何使用`db`对象的常规操作。
- en: The main disadvantage of this technique is that it requires us to know, in advance,
    which components will make use of the asynchronously initialized component, which
    makes our code fragile and exposed to mistakes. One solution to this problem is
    delaying the startup of the entire application until all the asynchronous services
    are initialized. This has the advantage of being simple and effective; however,
    it can add a significant delay to the overall startup time of the application
    and moreover, it won't take into account the case in which the asynchronously
    initialized component has to be reinitialized.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的缺点是它要求我们事先知道哪些组件将使用异步初始化组件，这使得我们的代码脆弱且容易出错。解决这个问题的一个方法是将整个应用程序的启动延迟到所有异步服务都初始化完成。这种方法的优势在于简单且有效；然而，它可能会显著增加应用程序的整体启动时间，而且它不会考虑到异步初始化组件需要重新初始化的情况。
- en: As we will see in the next section, there is a third alternative that allows
    us to transparently and efficiently delay every operation until the asynchronous
    initialization step has completed.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们将在下一节中看到的，还有第三个替代方案，它允许我们透明且高效地延迟每个操作，直到异步初始化步骤完成。
- en: Pre-initialization queues
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预初始化队列
- en: Another recipe to make sure that the services of a component are invoked only
    after the component is initialized involves the use of queues and the Command
    pattern. The idea is to queue the method invocations (only those requiring the
    component to be initialized) received while the component is not yet initialized,
    and then execute them as soon as all the initialization steps have been completed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 确保组件的服务仅在组件初始化之后被调用的另一个方法涉及使用队列和命令模式。想法是在组件尚未初始化时，将接收到的方法调用（仅限于需要组件初始化的调用）排队，然后在所有初始化步骤完成后立即执行它们。
- en: 'Let''s see how this technique can be applied to our sample `db` component:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这种技术如何应用于我们的示例`db`组件：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we already mentioned, the technique described here consists of two parts:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，这里描述的技术包括两个部分：
- en: If the component has not been initialized—which, in our case, is when the `connected`
    property is `false`—we create a command from the parameters received with the
    current invocation and push it to the `commandsQueue` array. When the command
    is executed, it will run the original `query()` method again and forward the result
    to the `Promise` we are returning to the caller.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果组件尚未初始化——在我们的例子中，这是当`connected`属性为`false`时——我们使用当前调用接收到的参数创建一个命令，并将其推送到`commandsQueue`数组。当命令执行时，它将再次运行原始的`query()`方法，并将结果转发到我们返回给调用者的`Promise`。
- en: When the initialization of the component is completed—which, in our case, means
    that the connection with the database server is established—we go through the
    `commandsQueue`, executing all the commands that have been previously queued.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当组件的初始化完成时——在我们的例子中，这意味着与数据库服务器的连接已建立——我们遍历`commandsQueue`，执行之前排队的所有命令。
- en: With the `DB` class we just implemented, there is no need to check if the component
    is initialized before invoking its methods. In fact, all the logic is embedded
    in the component itself and any consumer can just transparently use it without
    worrying about its initialization status.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚实现的`DB`类中，在调用其方法之前不需要检查组件是否已初始化。实际上，所有逻辑都嵌入在组件本身中，任何消费者都可以透明地使用它，无需担心其初始化状态。
- en: 'We can also go a step further and try to reduce the boilerplate of the `DB`
    class we just created and, at the same time, improve its modularity. We can achieve
    that by applying the State pattern, which we learned about in *Chapter 9*, *Behavioral
    Design Patterns*, with two states:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更进一步，尝试减少我们刚刚创建的`DB`类的样板代码，同时提高其模块化。我们可以通过应用我们在第9章“行为设计模式”中学到的状态模式，使用两个状态来实现这一点：
- en: The first state implements all the methods that require the component to be
    initialized, and it's activated only when there is a successful initialization.
    Each of these methods implements its own business logic without worrying about
    the initialization status of the `db` component
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个状态实现了所有需要组件初始化的方法，并且只有在初始化成功时才会被激活。这些方法中的每一个都实现了自己的业务逻辑，而不必担心`db`组件的初始化状态。
- en: The second state is activated before the initialization has completed and it
    implements the same methods as the first state, but their only role here is to
    add a new command to the queue using the parameters passed to the invocation.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个状态在初始化完成之前被激活，并实现了与第一个状态相同的方法，但它们在这里的唯一角色是使用传递给调用的参数将新命令添加到队列中。
- en: 'Let''s see how we can apply the structure we just described to our `db` component.
    First, we create the `InitializedState`, which implements the actual business
    logic of our component:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将我们刚刚描述的结构应用到我们的`db`组件中。首先，我们创建`InitializedState`，它实现了我们组件的实际业务逻辑：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, the only method that we need to implement in the `InitializedState`
    class is the `query()` method, which will print a message to the console when
    it receives a new query.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在`InitializedState`类中，我们只需要实现一个方法，即`query()`方法，当它接收到新的查询时，会在控制台打印一条消息。
- en: 'Next, we implement the `QueuingState`, the core of our recipe. This state implements
    the queuing logic:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现`QueuingState`，这是我们的核心。这个状态实现了排队逻辑：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It's interesting to note how the `QueuingState` is mostly built dynamically
    at creation time. For each method that requires an active connection, we create
    a new method for the current instance, which queues a new command representing
    the function invocation. When the command is executed at a later time, when a
    connection is established, the result of the invocation of the method on the `db`
    instance is forwarded to the caller (through the returned promise).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，注意到`QueuingState`在创建时主要是动态构建的。对于每个需要活跃连接的方法，我们为当前实例创建一个新的方法，该方法排队一个新的表示函数调用的命令。当命令在稍后执行，即连接建立时，`db`实例上方法调用的结果会被转发给调用者（通过返回的承诺）。
- en: The other important part of this state class is `[deactivate]()`. This method
    is invoked when the state is deactivated (which is when the component is initialized)
    and it executes all the commands in the queue. Note how we used a `Symbol` to
    name the method.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个状态类中的另一个重要部分是`[deactivate]()`方法。当状态被停用时（即组件初始化时）会调用此方法，并执行队列中的所有命令。注意我们是如何使用`Symbol`来命名这个方法的。
- en: This will avoid any name clashes in the future if we add more methods to the
    state (for example, what if we need to decorate a hypothetical `deactivate()`
    method of the `DB` class?).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这将避免在未来添加更多方法到状态时发生名称冲突（例如，如果我们需要装饰`DB`类的假设`deactivate()`方法怎么办？）。
- en: 'Now, it''s time to reimplement the `DB` class using the two states we just
    described:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候使用我们刚刚描述的两个状态来重新实现`DB`类了：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s further analyze the most important parts of the new `DB` class:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步分析新`DB`类最重要的部分：
- en: In the constructor, we initialize the current `state` of the instance. It's
    going to be the `QueuingState` as the asynchronous initialization of the component
    hasn't been completed yet.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构造函数中，我们初始化实例的当前 `状态`。由于组件的异步初始化尚未完成，它将是 `QueuingState`。
- en: The only method of our class implementing some (stub) business logic is the
    `query()` method. Here, all we have to do is invoke the homonymous method on the
    currently active state.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们类实现一些（占位符）业务逻辑的唯一方法是 `query()` 方法。在这里，我们只需调用当前活动状态的同名方法。
- en: Finally, when we establish the connection with the database (initialization
    complete), we switch the current state to the `InitializedState` and we deactivate
    the old one. The effect of deactivating the `QueuedState`, as we've seen previously,
    is that any command that had been queued is now executed.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，当我们与数据库建立连接（初始化完成）时，我们将当前状态切换到 `InitializedState` 并停用旧的。停用 `QueuedState`
    的效果，正如我们之前看到的，是任何已排队的命令现在都会被执行。
- en: We can immediately see how this approach allows us to reduce the boilerplate
    and, at the same time, create a class that is purely business logic (the `InitializedState`)
    free from any repetitive initialization check.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即看到这种方法如何使我们减少样板代码，同时创建一个纯粹的业务逻辑类（`InitializedState`），不受任何重复初始化检查的影响。
- en: The approach we've just seen will only work if we can modify the code of our
    asynchronously initialized component. In all those cases in which we can't make
    modifications to the component, we will need to create a wrapper or proxy, but
    the technique will be mostly similar to what we've seen here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到的方法只有在我们可以修改异步初始化的组件代码时才会有效。在所有那些我们无法修改组件的案例中，我们需要创建一个包装器或代理，但技术将主要与这里看到的方法相似。
- en: In the wild
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在野外
- en: The pattern we just presented is used by many database drivers and ORM libraries.
    The most notable is Mongoose ([nodejsdp.link/mongoose](http://nodejsdp.link/mongoose)),
    which is an ORM for **MongoDB**. With Mongoose, it's not necessary to wait for
    the database connection to open in order to be able to send queries. This is because
    each operation is queued and then executed later when the connection with the
    database is fully established, exactly as we've described in this section. This
    is clearly a must for any API that wants to provide a good **developer experience**
    (**DX**).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才提出的模式被许多数据库驱动程序和 ORM 库所使用。最著名的是 Mongoose（[nodejsdp.link/mongoose](http://nodejsdp.link/mongoose)），它是
    MongoDB 的 ORM。使用 Mongoose，不需要等待数据库连接打开就可以发送查询。这是因为每个操作都会排队，然后在数据库连接完全建立后执行，正如我们在本节中描述的那样。这对于任何希望提供良好
    **开发者体验**（**DX**）的 API 来说显然是必须的。
- en: Take a look at the code of Mongoose to see how every method in the native driver
    is proxied to add the pre-initialization queue. This also demonstrates an alternative
    way of implementing the recipe we presented in this section. You can find the
    relevant code fragment at [nodejsdp.link/mongoose-init-queue](http://nodejsdp.link/mongoose-init-queue).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 Mongoose 的代码，看看原生驱动程序中的每个方法是如何通过代理添加预初始化队列的。这也展示了实现本节中提出的配方的一种替代方法。相关代码片段可以在
    [nodejsdp.link/mongoose-init-queue](http://nodejsdp.link/mongoose-init-queue)
    找到。
- en: Similarly, the `pg` package ([nodejsdp.link/pg](http://nodejsdp.link/pg)), which
    is a client for the PostgreSQL database, leverages pre-initialization queues,
    but in a slightly different fashion. `pg` queues every query, regardless of the
    initialization status of the database, and then immediately tries to execute all
    the commands in the queue. Take a look at the relevant code line at [nodejsdp.link/pg-queue](http://nodejsdp.link/pg-queue).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`pg` 包（[nodejsdp.link/pg](http://nodejsdp.link/pg)），它是 PostgreSQL 数据库的客户端，利用了预初始化队列，但方式略有不同。`pg`
    会将每个查询排队，无论数据库的初始化状态如何，然后立即尝试执行队列中的所有命令。请查看相关的代码行在 [nodejsdp.link/pg-queue](http://nodejsdp.link/pg-queue)。
- en: Asynchronous request batching and caching
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步请求批处理和缓存
- en: In high-load applications, **caching** plays a critical role and it's used almost
    everywhere on the web, from static resources such as web pages, images, and stylesheets,
    to pure data such as the result of database queries. In this section, we are going
    to learn how caching applies to asynchronous operations and how a high request
    throughput can be turned to our advantage.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在高负载应用中，**缓存**起着关键作用，并且在网络上几乎无处不在，从静态资源如网页、图像和样式表，到纯数据如数据库查询的结果。在本节中，我们将学习缓存如何应用于异步操作，以及如何将高请求吞吐量转化为我们的优势。
- en: What's asynchronous request batching?
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是异步请求批处理？
- en: 'When dealing with asynchronous operations, the most basic level of caching
    can be achieved by **batching** together a set of invocations to the same API.
    The idea is very simple: if we invoke an asynchronous function while there is
    still another one pending, we can piggyback on the already running operation instead
    of creating a brand new request. Take a look at the following diagram:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理异步操作时，最基本级别的缓存可以通过将一组对同一 API 的调用 **批处理** 在一起来实现。这个想法非常简单：如果我们在一个异步函数仍在挂起时调用它，我们可以利用已经运行的操作，而不是创建一个新的请求。看看下面的图示：
- en: '![A screenshot of a social media post  Description automatically generated](img/B15729_11_01.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![社交媒体帖子截图  自动生成的描述](img/B15729_11_01.png)'
- en: 'Figure 11.1: Two asynchronous requests with no batching'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：没有批处理的两个异步请求
- en: The previous diagram shows two clients invoking the same asynchronous operation
    with *exactly the same input*. Of course, the natural way to picture this situation
    is with the two clients starting two separate operations that will complete at
    two different moments.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的图示显示了两个客户端以 *完全相同的输入* 调用相同的异步操作。当然，描绘这种情况的自然方式是两个客户端启动两个不同的操作，这些操作将在不同的时刻完成。
- en: 'Now, consider the following scenario:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑以下场景：
- en: '![A screenshot of a social media post  Description automatically generated](img/B15729_11_02.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![社交媒体帖子截图  自动生成的描述](img/B15729_11_02.png)'
- en: 'Figure 11.2: Batching of two asynchronous requests'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：两个异步请求的批处理
- en: '*Figure 11.2* shows us how two identical requests—which invoke the same API
    with the same input—can be batched, or in other words, appended to the same running
    operation. By doing this, when the operation completes, both clients are notified,
    even though the async operation is actually executed only once. This represents
    a simple, yet extremely powerful, way to optimize the load of an application while
    not having to deal with more complex caching mechanisms, which usually require
    an adequate memory management and invalidation strategy.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11.2* 展示了两个相同的请求——它们使用相同的输入调用相同的 API——是如何进行批处理的，换句话说，是将它们附加到同一个正在运行的操作中。通过这样做，当操作完成时，两个客户端都会收到通知，尽管异步操作实际上只执行了一次。这代表了一种简单而又极其强大的优化应用程序负载的方法，同时无需处理更复杂的缓存机制，这些机制通常需要足够的内存管理和失效策略。'
- en: Optimal asynchronous request caching
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳异步请求缓存
- en: Request batching is less effective if the operation is fast enough or if matching
    requests are spread across a longer period of time. Also, most of the time, we
    can safely assume that the result of two identical API invocations will not change
    so often, so simple request batching will not provide the best performance. In
    all these circumstances, the best candidate to reduce the load of an application
    and increase its responsiveness is definitely a more aggressive caching mechanism.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果操作足够快或者匹配的请求分布在更长的时间内，请求批处理的效果就会降低。此外，大多数情况下，我们可以安全地假设两个相同的 API 调用的结果不会经常改变，因此简单的请求批处理不会提供最佳性能。在这些所有情况下，减少应用程序负载并提高其响应性的最佳选择无疑是更激进的缓存机制。
- en: 'The idea is simple: as soon as a request completes, we store its result in
    the cache, which can be an in-memory variable or an item in a specialized caching
    server (such as Redis). Hence, the next time the API is invoked, the result can
    be retrieved immediately from the cache, instead of spawning another request.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单：一旦请求完成，我们就将其结果存储在缓存中，这可以是一个内存变量或一个专门的缓存服务器（如 Redis）中的项目。因此，下次调用 API
    时，结果可以立即从缓存中检索，而不是启动另一个请求。
- en: The idea of caching should not be new to an experienced developer, but what
    makes this technique different in asynchronous programming is that it should be
    combined with request batching to be optimal. The reason for this is because multiple
    requests might run concurrently while the cache is not set and when those requests
    complete, the cache would be set multiple times.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存的想法对于一个经验丰富的开发者来说可能并不陌生，但使这种技术在异步编程中与众不同的地方在于，它应该与请求批处理相结合以达到最佳效果。这样做的原因是因为在缓存未设置的情况下，可能会有多个请求同时运行，而当这些请求完成时，缓存会被设置多次。
- en: 'Based on these assumptions, we can illustrate the Combined Request Batching
    and Caching pattern as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些假设，我们可以如下说明组合请求批处理和缓存模式：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B15729_11_03.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图  自动生成的描述](img/B15729_11_03.png)'
- en: 'Figure 11.3: Combined batching and caching'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：结合批处理和缓存
- en: 'The preceding figure shows the two phases of an optimal asynchronous caching
    algorithm:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了最优异步缓存算法的两个阶段：
- en: The first phase is totally identical to the batching pattern. Any request received
    while the cache is not set will be batched together. When the request completes,
    the cache is set, once.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个阶段完全等同于批处理模式。当缓存未设置时接收到的任何请求都将一起批处理。当请求完成时，缓存设置一次。
- en: When the cache is finally set, any subsequent request will be served directly
    from it.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当缓存最终设置后，任何后续请求都将直接从缓存中提供服务。
- en: Another crucial detail to consider is the *Zalgo* anti-pattern (as we saw in *Chapter
    3*, *Callbacks and Events*). Since we are dealing with asynchronous APIs, we must
    be sure to always return the cached value asynchronously, even if accessing the
    cache involves only a synchronous operation, such as in the case in which the
    cached value is retrieved from an in-memory variable.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的关键细节是*Zalgo*反模式（如我们在*第3章*，*回调和事件*中看到的）。由于我们处理的是异步API，我们必须确保始终异步返回缓存值，即使访问缓存只涉及同步操作，例如，当从内存变量中检索缓存值时。
- en: An API server without caching or batching
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有缓存或批处理的API服务器
- en: Before we start diving into this new challenge, let's implement a small demo server that
    we will use as a reference to measure the impact of the various techniques we
    are going to implement.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入这个新挑战之前，让我们实现一个小型的演示服务器，我们将使用它作为参考来衡量我们将要实施的各项技术的效果。
- en: 'Let''s consider an API server that manages the sales of an e-commerce company.
    In particular, we want to query our server for the sum of all the transactions
    of a particular type of merchandise. For this purpose, we are going to use a LevelUP
    database through the `level` npm package ([nodejsdp.link/level](http://nodejsdp.link/level)).
    The data model that we are going to use is a simple list of transactions stored
    in the `sales` sublevel (a subsection of the database), which is organized in
    the following format:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个管理电子商务公司销售的API服务器。特别是，我们希望查询服务器以获取特定类型商品所有交易的总和。为此，我们将通过`level` npm包（[nodejsdp.link/level](http://nodejsdp.link/level)）使用LevelUP数据库。我们将使用的数据模型是一个简单的交易列表，存储在`sales`子级别（数据库的一个子部分）中，其组织格式如下：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The key is represented by `transactionId` and the value is a JSON object that
    contains the amount of the sale (`amount`) and the product type (`product`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 键由`transactionId`表示，值是一个包含销售金额（`amount`）和产品类型（`product`）的JSON对象。
- en: 'The data to process is really basic, so let''s implement a simple query over
    the database that we can use for our experiments. Let''s say that we want to get
    the total amount of sales for a particular product. The routine would look as
    follows (file `totalSales.js`):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理的数据非常基础，所以让我们实现一个简单的数据库查询，我们可以用它来进行实验。假设我们想要获取特定产品的总销售额。该程序将如下所示（文件`totalSales.js`）：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `totalSales()` function iterates over all the transactions of the `sales`
    sublevel and calculates the sum of the amounts of a particular product. The algorithm
    is intentionally slow as we want to highlight the effect of batching and caching
    later on. In a real-world application, we would have used an index to query the
    transactions by `product` or, even better, we could have used an incremental map/reduce
    algorithm to continuously calculate the sum for every product
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`totalSales()`函数遍历`sales`子级别的所有交易，并计算特定产品的金额总和。算法故意设计得较慢，以便我们稍后强调批处理和缓存的效果。在实际应用中，我们会使用索引通过`product`查询交易，甚至更好的方法是使用增量map/reduce算法来持续计算每个产品的总和。'
- en: 'We can now expose the `totalSales()` API through a simple HTTP server (the
    `server.js` file):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过一个简单的HTTP服务器（`server.js`文件）公开`totalSales()` API：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Before we start the server for the first time, we need to populate the database
    with some sample data. We can do this with the `populateDb.js` script, which can
    be found in this book''s code repository in the folder dedicated to this section.
    This script creates 100,000 random sales transactions in the database so that
    our query spends some time crunching data:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们第一次启动服务器之前，我们需要用一些示例数据填充数据库。我们可以使用`populateDb.js`脚本来完成这项工作，该脚本位于本书代码库中专门为此部分创建的文件夹中。此脚本在数据库中创建100,000个随机的销售交易，以便我们的查询花费一些时间处理数据：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Okay! Now, everything is ready. Let''s start the server:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！现在，一切准备就绪。让我们启动服务器：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To query the server, you can simply navigate with a browser to the following
    URL:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询服务器，您只需使用浏览器导航到以下 URL：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'However, to have a better idea of the performance of our server, we will need
    more than one request. So, we will use a small script named `loadTest.js`, which
    sends 20 requests at intervals of 200 ms. The script can be found in the code
    repository of this book and it''s already configured to connect to the local URL
    of the server, so, to run it, just execute the following command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了更好地了解我们服务器的性能，我们需要不止一个请求。因此，我们将使用一个名为 `loadTest.js` 的小脚本，该脚本每隔 200 毫秒发送
    20 个请求。该脚本可以在本书的代码仓库中找到，并且已经配置为连接到服务器的本地 URL，因此，要运行它，只需执行以下命令：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We will see that the 20 requests will take a while to complete. Take note of
    the total execution time of the test. Now, we are going to apply our optimizations
    and measure how much time we can save. We'll start by implementing both batching
    and caching by leveraging the properties of promises.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到，20 个请求需要一段时间才能完成。请注意测试的总执行时间。现在，我们将应用我们的优化并测量我们可以节省多少时间。我们将首先通过利用承诺的特性来实现批量处理和缓存。
- en: Batching and caching with promises
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用承诺进行批量处理和缓存
- en: Promises are a great tool for implementing asynchronous batching and caching
    of requests. Let's see why.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 承诺是实现异步批量处理和请求缓存的一个很好的工具。让我们看看为什么。
- en: 'If we recall what we learned about promises in *Chapter 5*, *Asynchronous Control
    Flow Patterns with Promises and Async/Await*, there are two properties that can
    be exploited to our advantage in this circumstance:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下我们在 *第五章* 中学到的关于承诺的知识，*使用承诺和 Async/Await 的异步控制流模式*，在这种情况下，有两个属性可以为我们所用：
- en: Multiple `then()` listeners can be attached to the same promise.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将多个 `then()` 监听器附加到同一个承诺上。
- en: The `then()` listener is guaranteed to be invoked (only once), and it works
    even if it's attached after the promise is already resolved. Moreover, `then()` is
    guaranteed to always be invoked asynchronously.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`then()` 监听器保证会被调用（只调用一次），即使它在承诺已经解决之后附加，它也能正常工作。此外，`then()` 保证总是异步调用。'
- en: In short, the first property is exactly what we need for batching requests,
    while the second means that a promise is already a cache for the resolved value
    and offers a natural mechanism for returning a cached value in a consistent, asynchronous
    way. In other words, this means that batching and caching become extremely simple
    and concise with promises.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，第一个属性正是我们用于批量请求所需的东西，而第二个属性意味着承诺已经是一个已解析值的缓存，并提供了一种在一致、异步的方式返回缓存值的自然机制。换句话说，这意味着使用承诺，批量处理和缓存变得极其简单和简洁。
- en: Batching requests in the total sales web server
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在总销售额网络服务器中批量请求
- en: 'Let''s now add a batching layer on top of our `totalSales` API. The pattern
    we are going to use is very simple: if there is another identical request pending
    when the API is invoked, we will wait for that request to complete instead of
    launching a new one. As we will see, this can easily be implemented with promises.
    In fact, all we have to do is save the promise in a map, associating it to the
    specified request parameters (the `product` type, in our case) every time we launch
    a new request. Then, at every subsequent request, we check if there is already
    a promise for the specified `product` and if there is one, we just return it;
    otherwise, we launch a new request.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在我们的 `totalSales` API 之上添加一个批量层。我们将使用的模式非常简单：如果 API 调用时还有另一个相同请求挂起，我们将等待该请求完成而不是启动一个新的请求。我们将看到，这可以通过承诺轻松实现。事实上，我们所有要做的就是将承诺保存在一个映射中，每次我们启动一个新的请求时，都将其与指定的请求参数（在我们的例子中是
    `product` 类型）关联起来。然后，在后续的每个请求中，我们检查是否已经存在针对指定 `product` 的承诺，如果存在，我们就返回它；否则，我们启动一个新的请求。
- en: 'Now, let''s see how this translates into code. Let''s create a new module named `totalSalesBatch.js`.
    Here, we''re going to implement a batching layer on top of the original `totalSales()` API:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这如何转化为代码。让我们创建一个名为 `totalSalesBatch.js` 的新模块。在这里，我们将在原始 `totalSales()`
    API 之上实现一个批量层：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `totalSales()` function of the `totalSalesBatch` module is a proxy for
    the original `totalSales()` API, and it works as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`totalSalesBatch` 模块的 `totalSales()` 函数是原始 `totalSales()` API 的代理，它的工作方式如下：'
- en: If a promise for the given `product` already exists, we just return it. This
    is where we *piggyback* on an already running request.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果已经存在针对给定 `product` 的承诺，我们只需返回它。这就是我们利用已运行的请求来 * piggyback * 的地方。
- en: If there is no request running for the given `product`, we execute the original
    `totalSales()` function and we save the resulting promise into the `runningRequests`
    map. Next, we make sure to remove the same promise from the `runningRequests`
    map as soon as the request completes.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有针对给定 `product` 的请求正在运行，我们将执行原始的 `totalSales()` 函数，并将结果承诺保存到 `runningRequests`
    映射中。接下来，我们确保在请求完成时立即从 `runningRequests` 映射中移除相同的承诺。
- en: The behavior of the new `totalSales()` function is identical to that of the
    original `totalSales()` API, with the difference that, now, multiple calls to
    the API using the same input are batched, thus saving us time and resources.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 `totalSales()` 函数的行为与原始的 `totalSales()` API 完全相同，区别在于现在，使用相同输入对 API 进行多次调用将被批量处理，从而节省我们的时间和资源。
- en: 'Curious to know what the performance improvement compared to the raw, non-batched
    version of the `totalSales()` API is? Let''s then replace the `totalSales` module
    used by the HTTP server with the one we just created (the `app.js` file):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 想知道与未经批量的原始 `totalSales()` API 相比，性能提升了多少吗？那么，我们将 HTTP 服务器使用的 `totalSales` 模块替换为我们刚刚创建的（`app.js`
    文件）：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If we now try to start the server again and run the load test against it, the
    first thing we will see is that the requests are returned in *batches*. This is
    the effect of the recipe we just implemented, and it's a great practical demonstration of
    how it works.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在再次尝试启动服务器并对其运行负载测试，我们首先会看到请求是以 **批量** 形式返回的。这正是我们刚刚实施的方案的效应，并且是它如何工作的一个很好的实际演示。
- en: Besides that, we should also observe a considerable reduction in the total time
    for executing the test. It should be at least four times faster than the original
    test performed against the plain `totalSales()` API!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还应该观察到执行测试的总时间有相当大的减少。它应该至少比针对原始的纯 `totalSales()` API 执行的测试快四倍！
- en: This result substantiates the huge performance boost that we can obtain by just
    applying a simple batching layer, without all the complexity of managing a full-fledged
    cache and, more importantly, without worrying about invalidation strategies.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果证实了，仅通过应用一个简单的批量层，我们就能获得巨大的性能提升，而不需要管理完整的缓存的所有复杂性，更重要的是，无需担心失效策略。
- en: The Request Batching pattern reaches its best potential in high-load applications
    and with slow APIs. This is because it's exactly in these circumstances that we
    can batch together a high number of requests.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请求批量模式在高负载应用和缓慢的 API 中达到最佳潜力。这是因为正是在这些情况下，我们可以将大量请求批量在一起。
- en: Let's now see how we can implement both batching and caching using a slight
    variation of the technique we've just explored.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何通过稍微改变我们刚刚探索的技术来实现批量和缓存。
- en: Caching requests in the total sales web server
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在总销售额网络服务器中缓存请求
- en: Adding a caching layer to our batching API is straightforward, thanks to promises.
    All we have to do is leave the promise in our request map, even after the request
    has completed.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于承诺的存在，将缓存层添加到我们的批量 API 中非常简单。我们只需在请求完成后将承诺留在我们的请求映射中即可。
- en: 'Let''s implement the `totalSalesCache.js` module straightaway:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即实现 `totalSalesCache.js` 模块：
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The relevant code that enables caching is highlighted. All we have to do is
    remove the promise from the cache after a certain time (`CACHE_TTL`) after the
    request has completed, or immediately if the request has failed. This is a very
    basic cache invalidation technique, but it works perfectly for our demonstration.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 允许缓存的相应代码被突出显示。我们只需在请求完成后的一定时间（`CACHE_TTL`）内从缓存中移除承诺，或者在请求失败时立即移除。这是一个非常基本的缓存失效技术，但对我们这个演示来说效果完美。
- en: 'Now, we are ready to try the `totalSales()` caching wrapper we just created.
    To do that, we only need to update the `app.js` module, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好尝试我们刚刚创建的 `totalSales()` 缓存包装器。为此，我们只需要更新 `app.js` 模块，如下所示：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, the server can be started again and profiled using the `loadTest.js` script,
    as we did in the previous examples. With the default test parameters, we should
    see a 10% reduction in the execution time compared to simple batching. Of course,
    this is highly dependent on a lot of factors; for example, the number of requests
    received and the delay between one request and the other. The advantages of using
    caching over batching will be much more substantial when the number of requests
    is higher and spans a longer period of time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以再次启动服务器并使用`loadTest.js`脚本进行性能分析，就像我们在前面的示例中所做的那样。使用默认的测试参数，我们应该看到与简单的批处理相比，执行时间减少了10%。当然，这高度依赖于许多因素；例如，接收到的请求数量和请求之间的延迟。当请求数量更多且跨越更长的时间段时，使用缓存而不是批处理的优势将更加显著。
- en: Notes about implementing caching mechanisms
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于实现缓存机制的注意事项
- en: 'We must remember that in real-life applications, we may want to use more advanced
    cache invalidation techniques and storage mechanisms. This is necessary for the
    following reasons:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须记住，在实际应用中，我们可能需要使用更高级的缓存失效技术和存储机制。这是出于以下原因必要的：
- en: A large amount of cached values can easily consume a lot of memory. In this
    case, a **least recently used** (**LRU**) or a **first in first out** (**FIFO**)
    policy can be applied to maintain constant memory utilization.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量的缓存值很容易消耗大量内存。在这种情况下，可以应用**最近最少使用**（**LRU**）或**先进先出**（**FIFO**）策略来维持恒定的内存利用率。
- en: When the application is distributed across multiple processes, keeping the cache
    in memory may produce different results across each server instance. If that's
    undesired for the particular application we are implementing, the solution is
    to use a shared store for the cache. This is also more performant than a simple
    in-memory solution as the cache is shared across multiple instances. Popular caching
    solutions include Redis ([nodejsdp.link/redis](http://nodejsdp.link/redis)) and
    Memcached ([nodejsdp.link/memcached](http://nodejsdp.link/memcached)).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当应用程序分布在多个进程之间时，将缓存保存在内存中可能会在每个服务器实例上产生不同的结果。如果我们正在实施的应用程序不希望出现这种情况，解决方案是使用共享存储来保存缓存。这比简单的内存解决方案性能更好，因为缓存是在多个实例之间共享的。流行的缓存解决方案包括Redis
    ([nodejsdp.link/redis](http://nodejsdp.link/redis)) 和Memcached ([nodejsdp.link/memcached](http://nodejsdp.link/memcached))。
- en: 'A manual cache invalidation (for example, when the related non-cached value
    changes), as opposed to a timed expiry, can enable a longer-living cache and at
    the same time provide more up-to-date data, but, of course, it would be a lot
    more complex to manage. Let''s not forget the famous quote by Phil Karlton (principal
    engineer at Netscape, Silicon Graphics, and more): "There are only two hard things
    in Computer Science: cache invalidation and naming things."'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与基于时间的过期时间相比，手动缓存失效（例如，当相关的非缓存值发生变化时）可以使缓存的生命周期更长，同时提供更及时的数据，但当然，管理起来会更复杂。我们不要忘记Phil
    Karlton（Netscape、Silicon Graphics等公司的首席工程师）的著名引言：“在计算机科学中，只有两件难事：缓存失效和命名事物。”
- en: 'With this, we conclude this section on request batching and caching. Next,
    we are going to learn how to tackle a tricky business: canceling asynchronous
    operations.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们结束了关于请求批处理和缓存的这一节。接下来，我们将学习如何处理一个棘手的问题：取消异步操作。
- en: Canceling asynchronous operations
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 取消异步操作
- en: Being able to stop a long-running operation is particularly useful if the operation
    has been canceled by the user or if it has become redundant. In multithreaded
    programming, we can just terminate the thread, but on a single-threaded platform
    such as Node.js, things can get a little bit more complicated.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 能够停止长时间运行的操作特别有用，如果操作被用户取消或变得冗余。在多线程编程中，我们只需终止线程即可，但在单线程平台如Node.js上，事情可能会变得稍微复杂一些。
- en: In this section, we'll be talking about canceling asynchronous operations and
    not about canceling promises, which is a different matter altogether. By the way,
    the Promises/A+ standard doesn't include an API for canceling promises. However,
    you can use a third-party promise library such as bluebird if you need such a
    feature (more at [nodejsdp.link/bluebird-cancelation](http://nodejsdp.link/bluebird-cancelation)).
    Note that canceling a promise doesn't mean that the operation the promise refers
    to will also be canceled; in fact, bluebird offers an `onCancel` callback in the
    promise constructor, in addition to `resolve` and `reject`, which can be used
    to cancel the underlying async operation when the promise is canceled. This is
    actually what this section is about.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论取消异步操作，而不是取消承诺，这两者完全是不同的事情。顺便说一句，Promises/A+标准并没有包括用于取消承诺的API。然而，如果您需要这样的功能，可以使用像bluebird这样的第三方承诺库（更多信息请参阅[nodejsdp.link/bluebird-cancelation](http://nodejsdp.link/bluebird-cancelation)）。请注意，取消承诺并不意味着与之相关的操作也会被取消；实际上，bluebird在承诺构造函数中提供了一个`onCancel`回调，除了`resolve`和`reject`之外，可以在承诺被取消时用来取消底层的异步操作。这正是本节要讨论的内容。
- en: A basic recipe for creating cancelable functions
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建可取消函数的基本配方
- en: 'Actually, in asynchronous programming, the basic principle for canceling the
    execution of a function is very simple: we check if the operation has been canceled
    after every asynchronous call, and if that''s the case, we prematurely quit the
    operation. Consider, for example, the following code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在异步编程中，取消函数执行的基本原则非常简单：我们在每次异步调用后检查操作是否已被取消，如果是这样，我们就提前退出操作。例如，考虑以下代码：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `cancelable()` function receives, as input, an object (`cancelObj`) containing
    a single property called `cancelRequested`. In the function, we check the `cancelRequested`
    property after every asynchronous call, and if that's `true`, we throw a special
    `CancelError` exception to interrupt the execution of the function.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`cancelable()`函数接收一个包含单个属性`cancelRequested`的对象（`cancelObj`）作为输入。在函数中，我们在每次异步调用后检查`cancelRequested`属性，如果它是`true`，则抛出一个特殊的`CancelError`异常来中断函数的执行。'
- en: The `asyncRoutine()` function is just a demo function that prints a string to
    the console and returns another string after 100 ms. You will find its full implementation,
    along with that of `CancelError`, in the code repository for this book.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`asyncRoutine()`函数只是一个演示函数，它将字符串打印到控制台，并在100毫秒后返回另一个字符串。您可以在本书的代码库中找到它的完整实现，以及`CancelError`的实现。'
- en: It's important to note that any code external to the `cancelable()` function
    will be able to set the `cancelRequested` property only after the `cancelable()`
    function gives back control to the event loop, which is usually when an asynchronous
    operation is awaited. This is why it's worth checking the `cancelRequested` property
    only after the completion of an asynchronous operation and not more often.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，任何在`cancelable()`函数外部编写的代码只能在`cancelable()`函数将控制权交还给事件循环之后设置`cancelRequested`属性，这通常发生在异步操作被等待时。这就是为什么我们只在异步操作完成后而不是更频繁地检查`cancelRequested`属性是值得的。
- en: 'The following code demonstrates how we can cancel the `cancelable()` function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何取消`cancelable()`函数：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As we can see, all we have to do to cancel the function is set the `cancelObj.cancelRequested`
    property to `true`. This will cause the function to stop and throw a `CancelError`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，要取消函数，我们只需将`cancelObj.cancelRequested`属性设置为`true`。这将导致函数停止并抛出一个`CancelError`。
- en: Wrapping asynchronous invocations
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装异步调用
- en: Creating and using a basic asynchronous cancelable function is very easy, but
    there is a lot of boilerplate involved. In fact, it involves so much extra code
    that it becomes hard to identify the actual business logic of the function.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和使用基本异步可取消函数非常简单，但其中涉及大量的模板代码。实际上，它涉及了如此多的额外代码，以至于很难识别函数的实际业务逻辑。
- en: We can reduce the boilerplate by including the cancelation logic inside a wrapping
    function, which we can use to invoke asynchronous routines.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在包装函数中包含取消逻辑来减少模板代码，我们可以使用这个函数来调用异步例程。
- en: 'Such a wrapper would look as follows (the `cancelWrapper.js` file):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的包装器看起来如下（`cancelWrapper.js`文件）：
- en: '[PRE20]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our wrapper is created through a factory function called `createCancelWrapper()`.
    The factory returns two functions: the wrapper function (`cancelWrapper`) and
    a function to trigger the cancelation of the asynchronous operation (`cancel`).
    This allows us to create a wrapper function to wrap multiple asynchronous invocations
    and then use a single `cancel()` function to cancel all of them.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个名为 `createCancelWrapper()` 的工厂函数创建包装器。工厂返回两个函数：包装函数（`cancelWrapper`）和一个用于触发异步操作取消的函数（`cancel`）。这允许我们创建一个包装函数来包装多个异步调用，然后使用单个
    `cancel()` 函数来取消所有这些调用。
- en: The `cancelWrapper()` function takes, as input, a function to invoke (`func`)
    and a set of parameters to pass to the function (`args`). The wrapper simply checks
    if a cancelation has been requested and if positive, it will return a promise
    rejected with a `CancelError` object as the rejection reason; otherwise, it will
    invoke `func`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`cancelWrapper()` 函数接收一个要调用的函数（`func`）和一组要传递给函数的参数（`args`）作为输入。包装器简单地检查是否请求了取消操作，如果是的话，它将返回一个以
    `CancelError` 对象作为拒绝原因的拒绝承诺；否则，它将调用 `func`。'
- en: 'Let''s now see how our wrapper factory can greatly improve the readability
    and modularity of our `cancelable()` function:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看我们的包装器工厂如何极大地提高我们的 `cancelable()` 函数的可读性和模块化：
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can immediately see the benefits of using a wrapper function for implementing
    our cancelation logic. In fact, the `cancelable()` function is now much more concise
    and readable.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即看到使用包装函数实现我们的取消逻辑的好处。事实上，`cancelable()` 函数现在更加简洁和可读。
- en: Cancelable async functions with generators
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有生成器的可取消异步函数
- en: 'The `cancelable` wrapper function we just created is already a big step ahead
    compared to embedding the cancelation logic directly in our code. However, it''s
    still not ideal for two reasons: it is error prone (what if we forget to wrap
    one function?) and it still affects the readability of our code, which makes it
    not ideal for implementing cancelable asynchronous operations that are already
    large and complex.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建的 `cancelable` 包装函数与直接在我们的代码中嵌入取消逻辑相比已经前进了一大步。然而，它仍然不是理想的，原因有两个：它容易出错（如果我们忘记包装一个函数怎么办？）并且它仍然会影响我们代码的可读性，这使得它不适合实现已经很大很复杂的可取消异步操作。
- en: An even neater solution involves the use of generators. In *Chapter 9*, *Behavioral
    Design Patterns*, we introduced generators as a means to implement iterators.
    However, they are a very versatile tool and can be used to implement all sorts
    of algorithms. In this case, we will be using generators to build a supervisor
    to control the asynchronous flow of a function. The result will be an asynchronous
    function that is transparently cancelable, whose behavior resembles an async function
    in which the `await` instruction is replaced by `yield`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更简洁的解决方案涉及到生成器的使用。在 *第9章*，*行为设计模式* 中，我们介绍了生成器作为实现迭代器的手段。然而，它们是一个非常通用的工具，可以用来实现各种算法。在这种情况下，我们将使用生成器来构建一个监督器来控制函数的异步流程。结果将是一个透明可取消的异步函数，其行为类似于一个异步函数，其中
    `await` 指令被 `yield` 替换。
- en: 'Let''s see how we can implement this cancelable function using generators (the
    `createAsyncCancelable.js` file):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何使用生成器实现这个可取消函数（`createAsyncCancelable.js` 文件）：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `createAsyncCancelable()` function may seem complex, so let''s analyze
    it in more detail:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`createAsyncCancelable()` 函数可能看起来很复杂，所以让我们更详细地分析它：'
- en: First, we should note that the `createAsyncCancelable()` function takes, as
    input, a generator function (the supervised function) and returns another function
    (`asyncCancelable()`) that wraps the generator function with our supervising logic.
    The `asyncCancelable()` function is what we will use to invoke the asynchronous
    operation.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们应该注意，`createAsyncCancelable()` 函数接收一个生成器函数（受监督的函数）作为输入，并返回另一个函数（`asyncCancelable()`），该函数将生成器函数用我们的监督逻辑包装起来。`asyncCancelable()`
    函数是我们用来调用异步操作的工具。
- en: 'The `asyncCancelable()` function returns an object with two properties:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`asyncCancelable()` 函数返回一个包含两个属性的对象：'
- en: The `promise` property, which contains the promise representing the eventual
    resolution (or rejection) of the asynchronous operation.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`promise` 属性，它包含表示异步操作最终解决（或拒绝）的承诺。'
- en: The `cancel` property, which is a function that can be used to cancel the supervised
    asynchronous flow.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cancel` 属性，它是一个可以用来取消受监督的异步流程的函数。'
- en: When invoked, the first task of `asyncCancelable()` is to invoke the generator
    function with the arguments received as input (`args`) and obtain a generator
    object, which we can use to control the execution flow of the running coroutine.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当调用`asyncCancelable()`时，其首要任务是使用作为输入的参数（`args`）调用生成器函数，并获取一个生成器对象，我们可以使用它来控制正在运行的协程的执行流程。
- en: The entire logic of the supervisor is implemented within the `nextStep()` function,
    which is responsible for iterating over the values yielded by the supervised coroutine
    (`prevResult`). Those can be actual values or promises. If a cancelation is requested,
    we throw the usual `CancelError`; otherwise, if the coroutine has been terminated
    (for example, `prevResult.done` is `true`), we immediately resolve the outer promise
    and complete the return.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督器的整个逻辑都实现在`nextStep()`函数中，该函数负责遍历由被监督的协程（`prevResult`）产生的值。这些可以是实际值或承诺。如果请求取消，我们抛出常规的`CancelError`；否则，如果协程已被终止（例如，`prevResult.done`为`true`），我们立即解析外部承诺并完成返回。
- en: The core part of the `nextStep()` function is where we retrieve the next value
    yielded by the supervised coroutine (which, let's not forget, it's a generator).
    We `await` on that value so we can make sure we get the actual resolution value
    in case we are dealing with a promise. This also makes sure that if `prevResult.value`
    is a promise and it rejects, we end up in the `catch` statement. We can end up
    in the `catch` statement even if the supervised coroutine actually throws an exception.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nextStep()`函数的核心部分是我们检索由被监督的协程（别忘了，它是一个生成器）产生的下一个值。我们在该值上`await`，以确保在处理承诺的情况下我们得到实际的解析值。这也确保了如果`prevResult.value`是一个承诺并且它被拒绝，我们将结束在`catch`语句中。即使被监督的协程实际上抛出了异常，我们也可以进入`catch`语句。'
- en: In the `catch` statement, we throw the caught error inside the coroutine. This
    is redundant if that error was already thrown by the coroutine, but not if it's
    the result of a promise rejection. Even if not optimal, this trick can simplify
    our code a bit for the sake of this demonstration. We invoke `nextStep()` using
    whatever value is yielded next after throwing it inside the coroutine, but if
    the result is another exception (for example, the exception is not caught inside
    the coroutine or another one is thrown), we immediately reject the outer promise
    and complete the asynchronous operation.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`catch`语句中，我们在协程内部抛出捕获到的错误。如果该错误已经被协程抛出，这将是多余的，但如果它是承诺拒绝的结果，则不是。即使不是最优的，这个技巧可以为了这个演示的目的稍微简化我们的代码。我们使用抛出后在协程内部抛出的任何值来调用`nextStep()`，但如果结果是另一个异常（例如，异常没有被协程捕获或抛出了另一个异常），我们立即拒绝外部承诺并完成异步操作。
- en: As we saw, there are a lot of moving parts in the `createAsyncCancelable()`
    function. But we should appreciate the fact that, in just a few lines of code,
    we were able to create a cancelable function that doesn't require any manual cancelation
    logic. As we will see now, the results are impressive.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，`createAsyncCancelable()`函数中有很多动态部分。但我们应该赞赏这样一个事实，即仅仅几行代码，我们就能够创建一个不需要任何手动取消逻辑的可取消函数。正如我们现在将要看到的，结果是令人印象深刻的。
- en: 'Let''s rewrite our sample asynchronous cancelable operation using the supervisor
    we implemented in the `createAsyncCancelable()` function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用在`createAsyncCancelable()`函数中实现的监督器重写我们的示例异步可取消操作：
- en: '[PRE23]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can immediately see that the generator wrapped by `createAsyncCancelable()`
    closely resembles an async function, but we are using `yield` instead of `await`.
    Also, there is no visible cancelation logic at all. The generator function maintains
    the excellent properties of async functions (for example, to make asynchronous
    code look like synchronous code), but unlike the async function and thanks to
    the supervisor introduced by `createAsyncCancelable()`, it's also possible to
    cancel the operation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即看出，`createAsyncCancelable()`包裹的生成器非常类似于异步函数，但我们使用`yield`而不是`await`。而且，根本看不到任何取消逻辑。生成器函数保持了异步函数的优秀特性（例如，使异步代码看起来像同步代码），但与异步函数不同，多亏了`createAsyncCancelable()`引入的监督器，我们还可以取消操作。
- en: The second interesting aspect is that `createAsyncCancelable()` creates a function
    (called `cancelable`) that can be invoked like any other function but at the same
    time returns a promise representing the result of the operation and a function
    to cancel the operation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个有趣的特点是，`createAsyncCancelable()`创建了一个函数（称为`cancelable`），它可以像任何其他函数一样被调用，但同时返回一个表示操作结果的承诺和一个用于取消操作的功能。
- en: This technique of using generators represents the best option we have to implement
    cancelable asynchronous operations.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成器的这种技术代表了我们实现可取消异步操作的最佳选择。
- en: For use in production, most of the time, we can rely on a widely used package
    from the Node.js ecosystem such as `caf` (the acronym means Cancelable Async Flows),
    which you can find at [nodejsdp.link/caf](http://nodejsdp.link/caf).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产使用，大多数时候，我们可以依赖Node.js生态系统中的一个广泛使用的包，如`caf`（该缩写代表可取消异步流），您可以在[nodejsdp.link/caf](http://nodejsdp.link/caf)找到它。
- en: Running CPU-bound tasks
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行CPU-bound任务
- en: 'The `totalSales()` API that we implemented in the *Asynchronous request batching
    and caching* section was (intentionally) expensive in terms of resources and took
    a few hundred milliseconds to run. Nonetheless, invoking the `totalSales()` function
    did not affect the ability of the application to process concurrent incoming requests.
    What we learned about the event loop in *Chapter 1*, *The Node.js Platform*, should
    explain this behavior: invoking an asynchronous operation always causes the stack
    to unwind back to the event loop, leaving it free to handle other requests.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在*异步请求批处理和缓存*部分中，我们实现的`totalSales()` API（故意）在资源方面很昂贵，运行需要几百毫秒。尽管如此，调用`totalSales()`函数并没有影响应用程序处理并发传入请求的能力。我们在*第一章*，*Node.js平台*中关于事件循环的了解应该可以解释这种行为：调用异步操作总是导致堆栈回滚到事件循环，使其能够处理其他请求。
- en: But what happens when we run a synchronous task that takes a long time to complete
    and that never gives back the control to the event loop until it has finished?
    This kind of task is also known as **CPU-bound**, because its main characteristic
    is that it is heavy on CPU utilization rather than being heavy on I/O operations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们运行一个耗时很长且在完成前永远不会将控制权交还给事件循环的任务时，会发生什么？这类任务也被称为**CPU-bound**，因为其主要特点是CPU利用率高，而不是I/O操作量大。
- en: Let's work immediately on an example to see how these types of task behave in
    Node.js.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即通过一个例子来看看这些类型任务在Node.js中的表现。
- en: Solving the subset sum problem
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决子集和问题
- en: Let's now choose a computationally expensive problem to use as a base for our
    experiment. A good candidate is the **subset sum** problem, which decides whether
    a set (or multiset) of integers contains a non-empty subset that has a sum equal
    to zero. For example, if we had as input the set [1, 2, –4, 5, –3], the subsets
    satisfying the problem are [1, 2, –3] and [2, –4, 5, –3].
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在选择一个计算上昂贵的难题作为实验的基础。一个好的候选者是**子集和**问题，它决定一个整数集（或多重集）是否包含一个非空子集，其和等于零。例如，如果我们输入的集合是[1,
    2, –4, 5, –3]，满足该问题的子集是[1, 2, –3]和[2, –4, 5, –3]。
- en: 'The simplest algorithm is the one that checks every possible combination of
    subsets of any size and has a computational cost of *O*(2^n), or in other words,
    it grows exponentially with the size of the input. This means that a set of 20
    integers would require up to 1,048,576 combinations to be checked, which is not
    bad for testing our assumptions. For our example, we are going to consider the
    following variation of the subset sum problem: given a set of integers, we want
    to calculate all the possible combinations whose sum is equal to a given arbitrary
    integer, not just zero.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的算法是检查任何大小的子集的所有可能组合，其计算成本为*O*(2^n)，换句话说，它随着输入大小的增加呈指数增长。这意味着一个包含20个整数的集合需要检查高达1,048,576种组合，这对于测试我们的假设来说还不错。在我们的例子中，我们将考虑以下子集和问题的变体：给定一个整数集合，我们想要计算所有可能的组合，其和等于一个给定的任意整数，而不仅仅是零。
- en: 'Now, let''s work to implement such an algorithm. First, let''s create a new
    module called `subsetSum.js`. We will start by creating a class called `SubsetSum`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们努力实现这样一个算法。首先，让我们创建一个名为`subsetSum.js`的新模块。我们将首先创建一个名为`SubsetSum`的类：
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `SubsetSum` class is extending `EventEmitter`. This allows us to produce
    an event every time we find a new subset matching the sum received as input. As
    we will see, this will give us a lot of flexibility.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`SubsetSum`类扩展了`EventEmitter`。这允许我们在找到与输入接收到的和匹配的新子集时产生一个事件。正如我们将看到的，这将给我们带来很大的灵活性。'
- en: 'Next, let''s see how we can generate all the possible combinations of subsets:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们如何生成所有可能的子集组合：
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will not go into too much detail about the algorithm, but there are two
    important things to notice:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会过多地详细介绍算法，但有几点需要注意：
- en: The `_combine()` method is completely synchronous. It recursively generates
    every possible subset without ever giving back control to the event loop.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_combine()`方法完全是同步的。它递归地生成每个可能的子集，而不会将控制权交还给事件循环。'
- en: Every time a new combination is generated, we provide it to the `_processSubset()` method
    for further processing.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次生成一个新的组合时，我们都会将其提供给`_processSubset()`方法进行进一步处理。
- en: 'The `_processSubset()` method is responsible for verifying that the sum of
    the elements of the given subset is equal to the number we are looking for:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`_processSubset()`方法负责验证给定子集的元素之和是否等于我们正在寻找的数字：'
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Trivially, the `_processSubset()` method applies a `reduce` operation to the
    subset in order to calculate the sum of its elements. Then, it emits an event
    of the type `match` when the resulting sum is equal to the one we are interested
    in finding (`this.sum`).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，`_processSubset()`方法对子集应用`reduce`操作以计算其元素之和。然后，当结果之和等于我们感兴趣寻找的值（`this.sum`）时，它发出一个类型为`match`的事件。
- en: 'Finally, the `start()` method puts all the preceding pieces together:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`start()`方法将所有前面的部分组合在一起：
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `start()` method triggers the generation of all the combinations by invoking
    `_combine()`, and lastly, emits an `end` event, signaling that all the combinations
    were checked and any possible match has already been emitted. This is possible
    because `_combine()` is synchronous; therefore, the `end` event is emitted as
    soon as the function returns, which means that all the combinations have been
    calculated.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`start()`方法通过调用`_combine()`触发所有组合的生成，最后发出一个`end`事件，表示所有组合都已检查，并且任何可能的匹配都已发出。这是可能的，因为`_combine()`是同步的；因此，`end`事件在函数返回时立即发出，这意味着所有组合都已计算。'
- en: Next, we have to expose the algorithm we just created over the network. As always,
    we can use a simple HTTP server for this task. In particular, we want to create
    an endpoint that accepts requests in the format `/subsetSum?data=<Array>&sum=<Integer>` that
    invokes the `SubsetSum` algorithm with the given array of integers and sum to match.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须将我们刚刚创建的算法暴露在网络上。像往常一样，我们可以使用简单的HTTP服务器来完成这个任务。特别是，我们想要创建一个端点，该端点接受格式为`/subsetSum?data=<Array>&sum=<Integer>`的请求，该请求使用给定的整数数组和总和来调用`SubsetSum`算法以匹配。
- en: 'Let''s implement this simple server in a module named `index.js`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在名为`index.js`的模块中实现这个简单的服务器：
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Thanks to the fact that the `SubsetSum` object returns its results using events,
    we can stream the matching subsets as soon as they are generated by the algorithm,
    in real time. Another detail to mention is that our server responds with the text `I'm
    alive!` every time we hit a URL different than `/subsetSum`. We will use this
    for checking the responsiveness of our server, as we will see in a moment.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`SubsetSum`对象使用事件返回其结果，我们可以实时地流式传输由算法生成的匹配子集。另一个需要提及的细节是，我们的服务器在每次我们访问不同于`/subsetSum`的URL时都会以文本`I'm
    alive!`响应。我们将使用此来检查我们服务器的响应性，就像我们一会儿会看到的那样。
- en: 'We are now ready to try our subset sum algorithm. Curious to know how our server
    will handle it? Let''s fire it up, then:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备尝试我们的子集和算法。好奇我们的服务器会如何处理它？让我们启动它，然后：
- en: '[PRE29]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As soon as the server starts, we are ready to send our first request. Let''s
    try it with a multiset of 17 random numbers, which will result in the generation
    of 131,071 combinations, a nice amount to keep our server busy for a while:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器启动后，我们就可以发送我们的第一个请求了。让我们用一个包含17个随机数字的多集来尝试，这将生成131,071个组合，这是一个让我们的服务器忙碌一段时间的好数量：
- en: '[PRE30]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After a few seconds, we should see the results coming from the server. But
    if we try the following command in another terminal while the first request is
    still running, we will spot a huge problem:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，我们应该看到来自服务器的结果。但如果我们在第一个请求仍在运行的同时在另一个终端尝试以下命令，我们会发现一个巨大的问题：
- en: '[PRE31]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will immediately see that this last request hangs until the subset sum algorithm
    of the first request has finished: the server is unresponsive! This was actually
    expected. The Node.js event loop runs in a single thread, and if this thread is
    blocked by a long synchronous computation, it will be unable to execute even a single
    cycle in order to respond with a simple `I''m alive!`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会立即看到最后一个请求会挂起，直到第一个请求的子集和算法完成：服务器无响应！这实际上是预期的。Node.js事件循环在单个线程中运行，如果这个线程被长时间同步计算阻塞，它将无法执行单个周期来简单地响应`I'm
    alive!`
- en: We quickly understand that this behavior does not work for any kind of application
    meant to process multiple concurrent requests. But don't despair. In Node.js,
    we can tackle this type of situation in several ways. So, let's analyze the three
    most popular methods, which are "interleaving with `setImmediate`," "using external
    processes," and "using worker threads."
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会明白，这种行为对于任何旨在处理多个并发请求的应用程序都不适用。但不要绝望。在 Node.js 中，我们可以用几种方式处理这种类型的场景。因此，让我们分析三种最流行的方法，它们是“使用
    `setImmediate` 交错”、“使用外部进程”和“使用工作线程”。
- en: Interleaving with setImmediate
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 `setImmediate` 交错
- en: Usually, a CPU-bound algorithm is built upon a set of steps. This can be a set
    of recursive invocations, a loop, or any variation/combination of these. So, a
    simple solution to our problem would be to give back the control to the event
    loop after each of these steps completes (or after a certain number of them).
    This way, any pending I/O can still be processed by the event loop in those intervals
    in which the long-running algorithm yields the CPU. A simple way to achieve this
    is to schedule the next step of the algorithm to run after any pending I/O requests.
    This sounds like the perfect use case for the `setImmediate()` function (we already
    introduced this API in *Chapter 3*, *Callbacks and Events*).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个 CPU 密集型算法建立在一系列步骤之上。这可以是一系列递归调用、循环，或者这些步骤的任何变化/组合。因此，解决我们问题的简单方法是在这些步骤的每个步骤完成后（或完成一定数量的步骤后）将控制权交还给事件循环。这样，任何挂起的
    I/O 仍然可以在长时间运行的算法释放 CPU 的那些间隔内由事件循环处理。实现这一点的一个简单方法是在处理完任何挂起的 I/O 请求后安排算法的下一步运行。这似乎是
    `setImmediate()` 函数（我们已经在 *第3章*，*回调和事件* 中介绍了这个 API）的完美用例。
- en: Interleaving the steps of the subset sum algorithm
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子集和算法的步骤交错
- en: Let's now see how this technique applies to our subset sum algorithm. All we
    have to do is slightly modify the `subsetSum.js` module. For convenience, we are
    going to create a new module called `subsetSumDefer.js`, taking the code of the
    original `subsetSum` class as a starting point.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这种技术是如何应用于我们的子集和算法的。我们只需要稍微修改一下 `subsetSum.js` 模块。为了方便起见，我们将创建一个新的模块，名为
    `subsetSumDefer.js`，以原始 `subsetSum` 类的代码作为起点。
- en: 'The first change we are going to make is to add a new method called `_combineInterleaved()`,
    which is the core of the technique we are implementing:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要进行的第一个改变是添加一个名为 `_combineInterleaved()` 的新方法，这是我们正在实施技术的核心：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As we can see, all we had to do is defer the invocation of the original (synchronous)
    `_combine()` method with `setImmediate()`. However, now, it becomes more difficult
    to know when the function has finished generating all the combinations, because
    the algorithm is not synchronous anymore.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们所需做的只是使用 `setImmediate()` 延迟调用原始（同步）的 `_combine()` 方法。然而，现在，知道函数何时完成生成所有组合变得更加困难，因为算法不再是同步的。
- en: To fix this, we have to keep track of all the running instances of the `_combine()`
    method using a pattern very similar to the asynchronous parallel execution flow
    that we saw in *Chapter 4*, *Asynchronous Control Flow Patterns with Callbacks*.
    When all the instances of the `_combine()` method have finished running, we can
    emit the `end` event, notifying any listener that the process has completed.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们必须使用与我们在 *第4章*，*使用回调的异步控制流模式* 中看到的异步并行执行流程非常相似的模式来跟踪所有正在运行 `_combine()`
    方法的实例。当 `_combine()` 方法的所有实例都运行完成后，我们可以发出 `end` 事件，通知任何监听器该过程已完成。
- en: 'To finish refactoring the subset sum algorithm, we need to make a couple more
    tweaks. First, we need to replace the recursive step in the `_combine()` method
    with its deferred counterpart:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成子集和算法的重构，我们需要进行一些额外的调整。首先，我们需要将 `_combine()` 方法中的递归步骤替换为其延迟对应的版本：
- en: '[PRE33]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: With the preceding change, we make sure that each step of the algorithm will
    be queued in the event loop using `setImmediate()` and, therefore, executed after
    any pending I/O request instead of being run synchronously.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的更改，我们确保算法的每个步骤都将使用 `setImmediate()` 在事件循环中排队，因此将在任何挂起的 I/O 请求之后而不是同步执行。
- en: 'The other small tweak is in the `start()` method:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个小调整是在 `start()` 方法中：
- en: '[PRE34]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the preceding code, we initialized the number of running instances of the
    `_combine()` method to `0`. We also replaced the call to `_combine()` with a call
    to `_combineInterleaved()` and removed the emission of the `end` event because
    this is now handled asynchronously in `_combineInterleaved()`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将 `_combine()` 方法的运行实例数初始化为 `0`。我们还用对 `_combineInterleaved()` 的调用替换了对
    `_combine()` 的调用，并移除了 `end` 事件的发射，因为现在这已经在 `_combineInterleaved()` 中异步处理了。
- en: With this last change, our subset sum algorithm should now be able to run its
    CPU-bound code in steps interleaved by intervals, where the event loop can run
    and process any other pending request.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个最后的更改，我们的子集和算法现在应该能够以交错的方式分步骤运行其 CPU 密集型代码，在这些间隔期间，事件循环可以运行并处理任何其他挂起的请求。
- en: 'The last missing bit is updating the `index.js` module so that it can use the
    new version of the `SubsetSum` API. This is actually a trivial change:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后缺少的部分是更新 `index.js` 模块，使其能够使用 `SubsetSum` API 的新版本。实际上，这是一个微不足道的更改：
- en: '[PRE35]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We are now ready to try this new version of the subset sum server. Start the
    server again and then try to send a request to calculate all the subsets matching
    a given sum:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以尝试这个子集和服务器的新版本了。再次启动服务器，然后尝试发送一个请求来计算所有匹配给定总和的子集：
- en: '[PRE36]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'While the request is running, check again whether the server is responsive:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在请求运行期间，再次检查服务器是否响应：
- en: '[PRE37]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Cool! The second request should return almost immediately, even while the `SubsetSum` task
    is still running, confirming that our technique is working well.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！第二个请求应该几乎立即返回，即使 `SubsetSum` 任务仍在运行，这也证实了我们的技术运行良好。
- en: Considerations on the interleaving approach
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于交错方法的考虑
- en: As we saw, running a CPU-bound task while preserving the responsiveness of an application
    is not that complicated; it just requires the use of `setImmediate()` to schedule
    the next step of an algorithm to run after any pending I/O. However, this is not
    the best recipe in terms of efficiency. In fact, deferring a task introduces a
    small overhead that, multiplied by all the steps that an algorithm has to run,
    can have a significant impact on the overall running time. This is usually the
    last thing we want when running a CPU-bound task. A possible solution to mitigate
    this problem would be using `setImmediate()` only after a certain number of steps—instead
    of using it at every single step—but still, this would not solve the root of the
    problem.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，在保持应用程序响应性的同时运行 CPU 密集型任务并不复杂；只需使用 `setImmediate()` 来安排算法的下一步在所有挂起的
    I/O 之后运行。然而，从效率的角度来看，这并不是最佳方案。事实上，延迟一个任务会引入一点开销，当乘以算法必须运行的步骤总数时，可能会对总运行时间产生重大影响。当我们运行
    CPU 密集型任务时，这通常是我们最不希望看到的事情。缓解这个问题的可能解决方案是在一定数量的步骤之后使用 `setImmediate()`——而不是在每一步都使用它——但即便如此，这也不会解决问题的根源。
- en: Also, this technique doesn't work very well if each step takes a long time to
    run. In this case, in fact, the event loop would lose responsiveness, and the
    whole application would start lagging, which is undesirable in a production environment.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果每一步运行时间都很长，这种技术效果并不好。在这种情况下，实际上，事件循环会失去响应性，整个应用程序开始滞后，这在生产环境中是不理想的。
- en: Bear in mind that this does not mean that the technique we have just seen should
    be avoided at all costs. In certain situations in which the synchronous task is
    executed sporadically and doesn't take too long to run, using `setImmediate()` to
    interleave its execution is sometimes the simplest and most effective way to avoid
    blocking the event loop.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这并不意味着我们应该不惜一切代价避免我们刚刚看到的技巧。在某些情况下，如果同步任务执行得零星且运行时间不长，使用 `setImmediate()`
    来交错其执行有时是避免阻塞事件循环的最简单和最有效的方法。
- en: Note that `process.nextTick()` cannot be used to interleave a long-running task.
    As we saw in *Chapter 3*, *Callbacks and Events*, `nextTick()` schedules a task
    to run before any pending I/O, and this will cause I/O starvation in case of repeated
    calls. You can verify this yourself by replacing `setImmediate()` with `process.nextTick()` in
    the previous example.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，不能使用 `process.nextTick()` 来交错执行一个长时间运行的任务。正如我们在 *第3章*，*回调和事件* 中所看到的，`nextTick()`
    会安排一个任务在所有挂起的 I/O 之前运行，如果重复调用，这将会导致 I/O 饥饿。您可以通过在之前的示例中将 `setImmediate()` 替换为
    `process.nextTick()` 来自行验证这一点。
- en: Using external processes
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用外部进程
- en: 'Deferring the steps of an algorithm is not the only option we have for running
    CPU-bound tasks. Another pattern for preventing the event loop from blocking is
    using **child processes**. We already know that Node.js gives its best when running
    I/O-intensive applications such as web servers, which allows us to optimize resource
    utilization thanks to its asynchronous architecture. So, the best way we have
    to maintain the responsiveness of an application is to not run expensive CPU-bound
    tasks in the context of the main application and, instead, use separate processes.
    This has three main advantages:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟算法步骤不是我们运行CPU密集型任务的唯一选择。另一种防止事件循环阻塞的模式是使用 **子进程**。我们已经知道Node.js在运行I/O密集型应用程序，如Web服务器时表现最佳，这使得我们可以通过其异步架构优化资源利用。因此，我们保持应用程序响应性的最佳方式是在主应用程序上下文中不运行昂贵的CPU密集型任务，而是使用单独的进程。这有三个主要优势：
- en: The synchronous task can run at full speed, without the need to interleave the
    steps of its execution.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步任务可以全速运行，无需在执行步骤中交错。
- en: Working with processes in Node.js is simple, probably easier than modifying
    an algorithm to use `setImmediate()`, and allows us to easily use multiple processors without
    the need to scale the main application itself.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Node.js中与进程一起工作很简单，可能比修改算法以使用 `setImmediate()` 更容易，并且允许我们轻松地使用多个处理器，而无需扩展主应用程序本身。
- en: If we really need maximum performance, the external process could be created
    in lower-level languages, such as good old C or more modern compiled languages
    like Go or Rust. Always use the best tool for the job!
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们真的需要最大性能，外部进程可以用底层语言创建，例如古老的C语言或更现代的编译语言，如Go或Rust。始终使用最适合工作的工具！
- en: Node.js has an ample toolbelt of APIs for interacting with external processes.
    We can find all we need in the `child_process` module. Moreover, when the external
    process is just another Node.js program, connecting it to the main application
    is extremely easy and allows seamless communication with the local application.
    This magic happens thanks to the `child_process.fork()` function, which creates
    a new child Node.js process and also automatically creates a communication channel
    with it, allowing us to exchange information using an interface very similar to
    the `EventEmitter`. Let's see how this works by refactoring our subset sum server again.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Node.js有一套丰富的API用于与外部进程交互。我们可以在 `child_process` 模块中找到我们所需的一切。此外，当外部进程只是另一个Node.js程序时，将其连接到主应用程序非常简单，并允许与本地应用程序进行无缝通信。这种魔法得益于
    `child_process.fork()` 函数，它创建一个新的子Node.js进程，并自动创建与它的通信通道，允许我们使用与 `EventEmitter`
    非常相似的接口交换信息。让我们通过再次重构我们的子集和服务器来了解它是如何工作的。
- en: Delegating the subset sum task to an external process
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将子集和任务委托给外部进程
- en: 'The goal of refactoring the `SubsetSum` task is to create a separate child
    process responsible for handling the synchronous processing, leaving the event
    loop of the main server free to handle requests coming from the network. This
    is the recipe we are going to follow to make this possible:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 重构 `SubsetSum` 任务的目的是创建一个负责处理同步处理的单独子进程，从而让主服务器的事件循环空闲，以便处理来自网络的请求。我们将遵循以下配方来实现这一点：
- en: We will create a new module named `processPool.js` that will allow us to create
    a pool of running processes. Starting a new process is expensive and requires
    time, so keeping them constantly running and ready to handle requests allows us
    to save time and CPU cycles. Also, the pool will help us limit the number of processes
    running at the same time to prevent exposing the application to **denial-of-service** (**DoS**)
    attacks.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个名为 `processPool.js` 的新模块，它将允许我们创建一个运行进程池。启动一个新的进程成本高昂且耗时，因此保持它们持续运行并准备好处理请求可以节省时间和CPU周期。此外，该池将帮助我们限制同时运行的进程数量，以防止应用程序暴露于
    **拒绝服务**（**DoS**）攻击。
- en: Next, we will create a module called `subsetSumFork.js` responsible for abstracting
    a `SubsetSum` task running in a child process. Its role will be communicating
    with the child process and forwarding the results of the task as if they were
    coming from the current application.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个名为 `subsetSumFork.js` 的模块，负责封装在子进程中运行的 `SubsetSum` 任务。其角色是与子进程通信，并将任务的输出结果转发，仿佛它们来自当前应用程序。
- en: Finally, we need a **worker** (our child process), a new Node.js program with
    the only goal of running the subset sum algorithm and forwarding its results to
    the parent process.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要一个 **worker**（我们的子进程），一个仅用于运行子集和算法并将结果转发给父进程的新 Node.js 程序。
- en: The purpose of a DoS attack is to make a machine unavailable to its users. This
    is usually achieved by exhausting the capacity of such a machine by exploiting
    a vulnerability or massively overloading it with requests (DDoS – distributed
    DoS).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 拒绝服务攻击（DoS）的目的是使机器对其用户不可用。这通常是通过利用漏洞或通过大量请求（DDoS - 分布式DoS）耗尽该机器的容量来实现的。
- en: Implementing a process pool
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现进程池
- en: 'Let''s start by building the `processPool.js` module piece by piece:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从逐步构建 `processPool.js` 模块开始：
- en: '[PRE38]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the first part of the module, we import the `fork()` function from the `child_process`
    module, which we will use to create new processes. Then, we define the `ProcessPool` constructor,
    which accepts a `file` parameter representing the Node.js program to run, and
    the maximum number of running instances in the pool (`poolMax`). We then define
    three instance variables:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在模块的第一部分，我们导入 `fork()` 函数，来自 `child_process` 模块，我们将使用它来创建新进程。然后，我们定义 `ProcessPool`
    构造函数，它接受一个 `file` 参数，表示要运行的 Node.js 程序，以及池中运行实例的最大数量 (`poolMax`)。然后我们定义三个实例变量：
- en: '`pool` is the set of running processes ready to be used.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool` 是一组准备使用的运行进程。'
- en: '`active` contains the list of the processes currently being used.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`active` 包含当前正在使用的进程列表。'
- en: '`waiting` contains a queue of callbacks for all those requests that could not
    be fulfilled immediately because of the lack of an available process.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`waiting` 包含所有那些由于缺乏可用进程而无法立即满足的请求的回调队列。'
- en: 'The next piece of the `ProcessPool` class is the `acquire()` method, which
    is responsible for eventually returning a process ready to be used, when one becomes
    available:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProcessPool` 类的下一部分是 `acquire()` 方法，它负责在可用时最终返回一个准备使用的进程：'
- en: '[PRE39]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The logic of `acquire()` is very simple and is explained as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`acquire()` 的逻辑非常简单，解释如下：'
- en: If we have a process in the `pool` ready to be used, we simply move it to the `active` list
    and then use it to fulfill the outer promise with `resolve()`.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 `pool` 中有一个准备使用的进程，我们只需将其移动到 `active` 列表，然后使用它通过 `resolve()` 来履行外部承诺。
- en: If there are no available processes in the `pool` and we have already reached
    the maximum number of running processes, we have to wait for one to be available.
    We achieve this by queuing the `resolve()` and `reject()` callbacks of the outer
    promise, for later use.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 `pool` 中没有可用的进程，并且我们已经达到了运行进程的最大数量，我们必须等待一个进程变得可用。我们通过排队外部承诺的 `resolve()`
    和 `reject()` 回调来实现这一点，以供以后使用。
- en: If we haven't reached the maximum number of running processes yet, we create
    a new one using `child_process.fork()`. Then, we wait for the `ready` message
    coming from the newly launched process, which indicates that the process has started
    and is ready to accept new jobs. This message-based channel is automatically provided
    with all processes started with `child_process.fork()`.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们还没有达到运行进程的最大数量，我们使用 `child_process.fork()` 创建一个新的进程。然后，我们等待来自新启动进程的 `ready`
    消息，这表明进程已启动并准备好接受新工作。这个基于消息的通道是自动提供给所有使用 `child_process.fork()` 启动的进程的。
- en: 'The last method of the `ProcessPool` class is `release()`, whose purpose is
    to put a process back into the `pool` once we are done with it:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProcessPool` 类的最后一个方法是 `release()`，其目的是在我们完成使用进程后将其放回 `pool`：'
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This is how the `release()` method works:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 `release()` 方法的工作原理：
- en: If there is a request in the `waiting` list, we simply reassign the `worker` we
    are releasing by passing it to the `resolve()` callback at the head of the `waiting` queue.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 `waiting` 列表中有一个请求，我们只需将释放的 `worker` 通过传递给 `waiting` 队列头部的 `resolve()` 回调来重新分配。
- en: Otherwise, we remove the worker that we are releasing from the `active` list
    and put it back into the `pool`.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，我们将释放的 worker 从 `active` 列表中移除，并将其放回 `pool`。
- en: As we can see, the processes are never stopped but just reassigned, allowing
    us to save time by not restarting them at each request. However, it's important
    to observe that this might not always be the best choice, and this greatly depends
    on the requirements of your application.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，进程永远不会停止，只是重新分配，这样我们就可以通过不在每次请求时重新启动它们来节省时间。然而，重要的是要注意，这并不总是最佳选择，这很大程度上取决于应用程序的需求。
- en: 'Other possible tweaks for reducing long-term memory usage and adding resilience
    to our process pool are:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可能的调整，以减少长期内存使用并增加进程池的弹性：
- en: Terminate idle processes to free memory after a certain time of inactivity.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一段时间不活动后，终止空闲进程以释放内存。
- en: Add a mechanism to kill non-responsive processes or restart those that have
    crashed.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个机制来终止无响应的进程或重启已崩溃的进程。
- en: In this example, we will keep the implementation of our process pool simple
    as the details we could add are really endless.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将保持我们的进程池实现简单，因为我们能添加的细节实际上是无限的。
- en: Communicating with a child process
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与子进程通信
- en: 'Now that our `ProcessPool` class is ready, we can use it to implement the `SubsetSumFork`
    class, whose role is to communicate with the worker and forward the results it
    produces. As we already mentioned, starting a process with `child_process.fork()`
    also gives us a simple message-based communication channel, so let''s see how
    this works by implementing the `subsetSumFork.js` module:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了 `ProcessPool` 类，我们可以用它来实现 `SubsetSumFork` 类，其作用是与工作进程通信并转发它产生的结果。正如我们之前提到的，使用
    `child_process.fork()` 启动进程也给我们提供了一个简单的基于消息的通信通道，所以让我们通过实现 `subsetSumFork.js`
    模块来看看它是如何工作的：
- en: '[PRE41]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The first thing to note is that we created a new `ProcessPool` object using
    the file `./workers/subsetSumProcessWorker.js` as the child worker. We also set
    the maximum capacity of the pool to `2`.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，我们使用文件 `./workers/subsetSumProcessWorker.js` 作为子工作进程创建了一个新的 `ProcessPool`
    对象。我们还设置了池的最大容量为 `2`。
- en: 'Another point worth mentioning is that we tried to maintain the same public
    API of the original `SubsetSum` class. In fact, `SubsetSumFork` is an `EventEmitter` whose
    constructor accepts `sum` and `set`, while the `start()` method triggers the execution of
    the algorithm, which, this time, runs on a separate process. This is what happens
    when the `start()` method is invoked:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得注意的点是，我们试图保持原始 `SubsetSum` 类的相同公共 API。实际上，`SubsetSumFork` 是一个 `EventEmitter`，其构造函数接受
    `sum` 和 `set`，而 `start()` 方法触发算法的执行，这次是在一个单独的进程中运行。这是调用 `start()` 方法时发生的情况：
- en: We try to acquire a new child process from the pool. When the operation completes,
    we immediately use the `worker` handle to send a message to the child process
    with the data of the job to run. The `send()` API is provided automatically by
    Node.js to all processes started with `child_process.fork()`. This is essentially
    the communication channel that we were talking about.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们尝试从池中获取一个新的子进程。当操作完成后，我们立即使用 `worker` 处理器向子进程发送一条消息，其中包含要运行作业的数据。`send()`
    API 是 Node.js 自动提供给所有使用 `child_process.fork()` 启动的进程的。这本质上是我们之前讨论的通信通道。
- en: We then start listening for any message sent by the worker process using the `on()` method
    to attach a new listener (this is also a part of the communication channel provided
    by all processes started with `child_process.fork()`).
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们然后开始使用 `on()` 方法监听工作进程发送的任何消息，以附加一个新的监听器（这也是所有使用 `child_process.fork()` 启动的进程提供的通信通道的一部分）。
- en: In the `onMessage` listener, we first check if we received an `end` event, which
    means that the `SubsetSum` task has finished, in which case we remove the `onMessage` listener
    and release the `worker`, putting it back into the pool.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `onMessage` 监听器中，我们首先检查是否收到了 `end` 事件，这意味着 `SubsetSum` 任务已完成，在这种情况下，我们移除 `onMessage`
    监听器并释放 `worker`，将其放回池中。
- en: The worker produces messages in the format `{event, data}`, allowing us to seamlessly
    forward (re-emit) any event produced by the child process.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作进程以 `{"event", "data"}` 格式产生消息，使我们能够无缝地转发（重新发射）子进程产生的任何事件。
- en: That's it for the `SubsetSumFork` wrapper. Let's now implement the worker (our
    child process).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 `SubsetSumFork` 包装器的全部内容。现在让我们来实现工作进程（我们的子进程）。
- en: It is good to know that the `send()` method available on a child process instance
    can also be used to propagate a socket handle from the main application to a child
    process (look at the documentation at [nodejsdp.link/childprocess-send](http://nodejsdp.link/childprocess-send)).
    This is actually the technique used by the `cluster` module to distribute the
    load of an HTTP server across multiple processes. We will see this in more detail
    in the next chapter.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 值得知道的是，在子进程实例上可用的 `send()` 方法也可以用来从主应用程序传播套接字处理到子进程（请参阅 [nodejsdp.link/childprocess-send](http://nodejsdp.link/childprocess-send)
    上的文档）。这实际上是 `cluster` 模块用来在多个进程中分配 HTTP 服务器负载的技术。我们将在下一章中更详细地了解这一点。
- en: Implementing the worker
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现工作进程
- en: 'Let''s now create the `workers/subsetSumProcessWorker.js` module, our worker
    process:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在创建`workers/subsetSumProcessWorker.js`模块，我们的工作进程：
- en: '[PRE42]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We can immediately see that we are reusing the original (and synchronous) `SubsetSum`
    as it is. Now that we are in a separate process, we don't have to worry about
    blocking the event loop anymore; all the HTTP requests will continue to be handled
    by the event loop of the main application without disruptions.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即看到我们正在重用原始的（和同步的）`SubsetSum`，就像它现在这样。既然我们现在是在一个单独的进程中，我们不再需要担心阻塞事件循环了；所有的HTTP请求将继续由主应用程序的事件循环处理，而不会中断。
- en: 'When the worker is started as a child process, this is what happens:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当工作进程作为子进程启动时，会发生以下情况：
- en: It immediately starts listening for messages coming from the parent process.
    This can be easily done with the `process.on()` function (a part of the communication
    API provided when the process is started with `child_process.fork()`).
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它立即开始监听来自父进程的消息。这可以通过使用`process.on()`函数（当使用`child_process.fork()`启动进程时提供的通信API的一部分）轻松完成。
- en: The only message we expect from the parent process is the one providing the
    input to a new `SubsetSum` task. As soon as such a message is received, we create
    a new instance of the `SubsetSum` class and register the listeners for the `match` and `end` events.
    Lastly, we start the computation with `subsetSum.start()`.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们期望从父进程接收的唯一消息是提供新`SubsetSum`任务的输入的消息。一旦收到这样的消息，我们就创建`SubsetSum`类的新实例，并注册`match`和`end`事件的监听器。最后，我们使用`subsetSum.start()`开始计算。
- en: Every time an event is received from the running algorithm, we wrap it in an
    object having the format `{event, data}` and send it to the parent process. These
    messages are then handled in the `subsetSumFork.js` module, as we have seen in
    the previous section.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次从运行中的算法接收到事件时，我们将其包装在一个具有`{event, data}`格式的对象中，并将其发送到父进程。然后，这些消息在`subsetSumFork.js`模块中处理，正如我们在上一节中看到的。
- en: As we can see, we just had to wrap the algorithm we already built, without modifying
    its internals. This clearly shows that any portion of an application can be easily
    put in an external process by simply using the technique we have just seen.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们只需将我们已构建的算法包装起来，而不需要修改其内部结构。这清楚地表明，应用程序的任何部分都可以通过简单地使用我们刚刚看到的技巧轻松地放入外部进程。
- en: When the child process is not a Node.js program, the simple communication channel
    we just described (`on()`, `send()`) is not available. In these situations, we
    can still establish an interface with the child process by implementing our own
    protocol on top of the standard input and standard output streams, which are exposed
    to the parent process. To find out more about all the capabilities of the `child_process` API,
    you can refer to the official Node.js documentation at [nodejsdp.link/child_process](http://nodejsdp.link/child_process).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 当子进程不是Node.js程序时，我们刚才描述的简单通信通道（`on()`，`send()`）不可用。在这些情况下，我们仍然可以通过在标准输入和标准输出流上实现自己的协议来与子进程建立接口，这些流被暴露给父进程。要了解更多关于`child_process`
    API的所有功能，您可以参考官方Node.js文档[http://nodejsdp.link/child_process](http://nodejsdp.link/child_process)。
- en: Considerations for the multi-process approach
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于多进程方法的考虑
- en: 'As always, to try this new version of the subset sum algorithm, we simply have
    to replace the module used by the HTTP server (the `index.js` file):'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，为了尝试这个新的子集和算法版本，我们只需简单地替换HTTP服务器使用的模块（`index.js`文件）：
- en: '[PRE43]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can now start the server again and try to send a sample request:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以再次启动服务器并尝试发送一个示例请求：
- en: '[PRE44]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As for the interleaving approach that we saw previously, with this new version
    of the `SubsetSum` module, the event loop is not blocked while running the CPU-bound
    task. This can be confirmed by sending another concurrent request, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 至于我们之前看到的交错方法，在这个新的`SubsetSum`模块版本中，在运行CPU密集型任务时事件循环不会被阻塞。这可以通过发送另一个并发请求来确认，如下所示：
- en: '[PRE45]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The preceding command should immediately return the text `I'm alive!`
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令应立即返回文本`I'm alive!`
- en: More interestingly, we can also try to start two `SubsetSum` tasks concurrently
    and we will be able to see that they will use the full power of two different
    processors in order to run (if your system has more than one processor, of course).
    Instead, if we try to run three `SubsetSum` tasks concurrently, the result should
    be that the last one to start will hang. This is not because the event loop of
    the main process is blocked, but because we set a concurrency limit of two processes
    for the `SubsetSum` task, which means that the third request will be handled as
    soon as at least one of the two processes in the pool becomes available again.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的是，我们还可以尝试同时启动两个`SubsetSum`任务，我们将能够看到它们将充分利用两个不同处理器的全部能力来运行（当然，如果你的系统有多个处理器的话）。相反，如果我们尝试同时运行三个`SubsetSum`任务，结果应该是最后一个启动的任务会挂起。这并不是因为主进程的事件循环被阻塞，而是因为我们为`SubsetSum`任务设置了两个进程的并发限制，这意味着第三个请求将至少在池中的两个进程之一再次可用时被处理。
- en: As we saw, the multi-process approach has many advantages compared to the interleaving
    approach. First, it doesn't introduce any computational penalty when running the
    algorithm. Second, it can take full advantage of a multi-processor machine.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，与交错方法相比，多进程方法具有许多优势。首先，在运行算法时不会引入任何计算惩罚。其次，它可以充分利用多处理器机器。
- en: Now, let's see an alternative approach that uses threads instead of processes.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一种使用线程而不是进程的替代方法。
- en: Using worker threads
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用工作线程
- en: Since Node 10.5.0, we have a new mechanism for running CPU-intensive algorithms
    outside of the main event loop called **worker threads**. Worker threads can be
    seen as a lightweight alternative to `child_process.fork()` with some extra goodies.
    Compared to processes, worker threads have a smaller memory footprint and a faster
    startup time since they run within the main process but inside different threads.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 从Node 10.5.0版本开始，我们有一个新的机制，可以在主事件循环之外运行CPU密集型算法，称为**工作线程**。工作线程可以被视为`child_process.fork()`的一个轻量级替代方案，并附带一些额外的好处。与进程相比，工作线程具有更小的内存占用和更快的启动时间，因为它们在主进程中运行，但位于不同的线程中。
- en: Even though worker threads are based on real threads, they don't allow the deep
    synchronization and sharing capabilities supported by other languages such as
    Java or Python. This is because JavaScript is a single-threaded language and it
    doesn't have any built-in mechanism to synchronize access to variables from multiple
    threads. JavaScript with threads simply wouldn't be JavaScript. The solution to
    bring all the advantages of threads within Node.js without actually changing the
    language is worker threads.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管工作线程基于真实的线程，但它们不允许像Java或Python等其他语言支持的深度同步和共享能力。这是因为JavaScript是一种单线程语言，它没有内置的机制来同步多个线程对变量的访问。具有线程的JavaScript简单地就不是JavaScript了。在实际上不改变语言的情况下，将线程的所有优点带入Node.js的解决方案是工作线程。
- en: Worker threads are essentially threads that, by default, don't share anything
    with the main application thread; they run within their own V8 instance, with
    an independent Node.js runtime and event loop. Communication with the main thread
    is possible thanks to message-based communication channels, the transfer of `ArrayBuffer`
    objects, and the use of `SharedArrayBuffer` objects whose synchronization is managed
    by the user (usually with the help of `Atomics`).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 工作线程本质上是在默认情况下不与主应用程序线程共享任何内容的线程；它们在自己的V8实例中运行，具有独立的Node.js运行时和事件循环。通过基于消息的通信通道、`ArrayBuffer`对象的传输以及使用由用户（通常在`Atomics`的帮助下）管理的同步的`SharedArrayBuffer`对象，可以与主线程进行通信。
- en: 'You can read more about `SharedArrayBuffer` and `Atomics` in the following
    article: [nodejsdp.link/shared-array-buffer](http://nodejsdp.link/shared-array-buffer).
    Even though the article focuses on Web Workers, a lot of concepts are similar
    to Node.js''s worker threads.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下文章中了解更多关于`SharedArrayBuffer`和`Atomics`的信息：[nodejsdp.link/shared-array-buffer](http://nodejsdp.link/shared-array-buffer)。尽管这篇文章侧重于Web
    Workers，但其中许多概念与Node.js的工作线程相似。
- en: This extensive level of isolation of worker threads from the main thread preserves
    the integrity of the language. At the same time, the basic communication facilities
    and data-sharing capabilities are more than enough for 99% of use cases.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将工作线程与主线程隔离到广泛程度，保护了语言的完整性。同时，基本的通信设施和数据共享能力对于99%的使用场景来说已经足够。
- en: Now, let's use worker threads in our `SubsetSum` example.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在我们的`SubsetSum`示例中使用工作线程。
- en: Running the subset sum task in a worker thread
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在工作线程中运行子集和任务
- en: The worker threads API has a lot in common with that of `ChildProcess`, so the
    changes to our code will be minimal.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 工作线程API与`ChildProcess`有很多相似之处，因此我们代码的改动将会很小。
- en: 'First, we need to create a new class called `ThreadPool`, which is our `ProcessPool`
    adapted to operate with worker threads instead of processes. The following code
    shows the differences between the new `ThreadPool` class and the `ProcessPool`
    class. There are only a few differences in the `acquire()` method, which are highlighted;
    the rest of the code is identical:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个新的类`ThreadPool`，这是我们针对工作线程而不是进程操作而适配的`ProcessPool`。以下代码显示了新`ThreadPool`类与`ProcessPool`类之间的差异。在`acquire()`方法中只有几个差异被突出显示；其余代码完全相同：
- en: '[PRE46]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we need to adapt the worker and place it in a new file called `subsetSumThreadWorker.js`.
    The main difference from our old worker is that instead of using `process.send()`
    and `process.on()`, we''ll have to use `parentPort.postMessage()` and `parentPort.on()`:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要适配工作线程并将其放置在一个名为`subsetSumThreadWorker.js`的新文件中。与我们的旧工作线程相比，主要区别在于我们不再使用`process.send()`和`process.on()`，而是必须使用`parentPort.postMessage()`和`parentPort.on()`：
- en: '[PRE47]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Similarly, the module `subsetSumThreads.js` is essentially the same as the
    `subsetSumFork.js` module except for a couple of lines of code, which are highlighted
    in the following code fragment:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，`subsetSumThreads.js`模块基本上与`subsetSumFork.js`模块相同，除了几行代码有所不同，这些代码在下面的代码片段中被突出显示：
- en: '[PRE48]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As we've seen, adapting an existing application to use worker threads instead
    of forked processes is a trivial operation. This is because the API of the two
    components are very similar, but also because a worker thread has a lot in common
    with a full-fledged Node.js process.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，将现有应用程序适配为使用工作线程而不是分叉进程是一个简单的操作。这是因为这两个组件的API非常相似，而且也因为工作线程与完整的Node.js进程有很多共同之处。
- en: 'Finally, we need to update the `index.js` module so that it can use the new
    `subsetSumThreads.js` module, as we''ve seen for the other implementations of the algorithm:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要更新`index.js`模块，使其能够使用新的`subsetSumThreads.js`模块，正如我们之前对算法的其他实现所做的那样：
- en: '[PRE49]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now, you can try the new version of the subset sum server using worker threads.
    As for the previous two implementations, the event loop of the main application
    is not blocked by the subset sum algorithm as it runs in a separate thread.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用工作线程尝试子集和服务器的新版本。至于前两种实现，由于子集和算法在单独的线程中运行，主应用程序的事件循环不会被阻塞。
- en: The example we've seen uses only a small subset of all the capabilities offered
    by worker threads. For more advanced topics such as transferring `ArrayBuffer`
    objects or `SharedArrayBuffer` objects, you can read the official API documentation
    at [nodejsdp.link/worker-threads](http://nodejsdp.link/worker-threads).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所看到的示例仅使用了工作线程提供的所有功能中的一小部分。对于更高级的主题，例如传输`ArrayBuffer`对象或`SharedArrayBuffer`对象，您可以在[nodejsdp.link/worker-threads](http://nodejsdp.link/worker-threads)的官方API文档中阅读。
- en: Running CPU-bound tasks in production
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在生产环境中运行CPU密集型任务
- en: The examples we've seen so far should give you an idea of the tools at our disposal
    for running CPU-intensive operations in Node.js. However, components such as process
    pools and thread pools are complex pieces of machinery that require proper mechanisms
    to deal with timeouts, errors, and other types of failures, which, for brevity,
    we left out from our implementation. Therefore, unless you have special requirements,
    it's better to rely on more battle-tested libraries for production use. Two of
    those libraries are `workerpool` ([nodejsdp.link/workerpool](http://nodejsdp.link/workerpool))
    and `piscina` ([nodejsdp.link/piscina](http://nodejsdp.link/piscina)), which are
    based on the same concepts we've seen in this section. They allow us to coordinate
    the execution of CPU-intensive tasks using external processes or worker threads.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所看到的示例应该能给您一个关于我们在Node.js中运行CPU密集型操作所拥有的工具的思路。然而，像进程池和线程池这样的组件是复杂的机械装置，需要适当的机制来处理超时、错误和其他类型的故障，这些我们在实现中为了简洁而省略了。因此，除非您有特殊要求，否则最好依赖于经过更多实战考验的库来用于生产环境。其中两个这样的库是`workerpool`([nodejsdp.link/workerpool](http://nodejsdp.link/workerpool))和`piscina`([nodejsdp.link/piscina](http://nodejsdp.link/piscina))，它们基于我们在本节中看到的概念。它们允许我们使用外部进程或工作线程来协调CPU密集型任务的执行。
- en: One last observation is that we must consider that if we have particularly complex
    algorithms to run or if the number of CPU-bound tasks exceeds the capacity of
    a single node, we may have to think about scaling out the computation across multiple
    nodes. This is a completely different problem and we'll learn more about this
    in the next two chapters.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个观察结果是，如果我们有特别复杂的算法要运行，或者 CPU 密集型任务的数量超过了单个节点的容量，我们可能需要考虑将计算扩展到多个节点。这是一个完全不同的问题，我们将在接下来的两章中了解更多关于这个问题的内容。
- en: Summary
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter added some great new weapons to our toolbelt, and as you can see,
    our journey is getting more focused on advanced problems. Due to this, we have
    started to delve deeply into more complex solutions. This chapter gave us not
    only a set of recipes to reuse and customize for our needs, but also some great
    demonstrations of how mastering a few principles and patterns can help us tackle
    the most complex problems in Node.js development.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为我们工具箱添加了一些全新的武器，正如你所见，我们的旅程正变得更加专注于高级问题。因此，我们开始深入探讨更复杂的解决方案。本章不仅为我们提供了一组可重用和定制的食谱，还展示了掌握一些原则和模式如何帮助我们解决
    Node.js 开发中最复杂的问题。
- en: The next two chapters represent the peak of our journey. After studying the
    various tactics of Node.js development, we are now ready to move on to the strategies
    and explore the architectural patterns for scaling and distributing our Node.js
    applications.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两章代表了我们的旅程的巅峰。在研究了 Node.js 开发的各种策略之后，我们现在准备继续前进，探索扩展和分布式 Node.js 应用程序的架构模式。
- en: Exercises
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: '**11.1 Proxy with pre-initialization queues**: Using a JavaScript Proxy, create
    a wrapper for adding pre-initialization queues to any object. You should allow
    the consumer of the wrapper to decide which methods to augment and the name of
    the property/event that indicates if the component is initialized.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**11.1 带有预初始化队列的代理**：使用 JavaScript 代理，为任何对象创建一个包装器，添加预初始化队列。你应该允许包装器的消费者决定要增强哪些方法以及表示组件是否已初始化的属性/事件的名称。'
- en: '**11.2 Batching and caching with callbacks**: Implement batching and caching
    for the `totalSales` API examples using only callbacks, streams, and events (without
    using promises or async/await). Hint: Pay attention to Zalgo when returning cached
    values!'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**11.2 使用回调进行批处理和缓存**：仅使用回调、流和事件（不使用承诺或 async/await）实现 `totalSales` API 示例的批处理和缓存。提示：在返回缓存值时请注意
    Zalgo！'
- en: '**11.3 Deep async cancelable**: Extend the `createAsyncCancelable()` function
    so that it''s possible to invoke other cancelable functions from within the main
    cancelable function. Canceling the main operation should also cancel all nested
    operations. Hint: Allow to `yield` the result of an `asyncCancelable()` from within
    the generator function.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**11.3 深度可取消异步**：扩展 `createAsyncCancelable()` 函数，使其能够在主可取消函数内部调用其他可取消函数。取消主操作也应取消所有嵌套操作。提示：允许在生成器函数内部
    `yield` `asyncCancelable()` 的结果。'
- en: '**11.4 Compute farm**: Create an HTTP server with a `POST` endpoint that receives,
    as input, the code of a function (as a string) and an array of arguments, executes
    the function with the given arguments in a worker thread or in a separate process,
    and returns the result back to the client. Hint: You can use `eval()`, `vm.runInContext()`,
    or neither of the two. Note: Whatever code you produce for this exercise, please
    be aware that allowing users to run arbitrary code in a production setting can
    pose serious security risks, and you should never do it unless you know exactly
    what the implications are.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**11.4 计算农场**：创建一个具有 `POST` 端点的 HTTP 服务器，接收作为输入的函数代码（作为字符串）和参数数组，在工作线程或单独的进程中使用给定的参数执行函数，并将结果返回给客户端。提示：你可以使用
    `eval()`、`vm.runInContext()` 或两者都不用。注意：无论你为这个练习产生什么代码，请务必意识到在生产环境中允许用户运行任意代码可能会带来严重的安全风险，除非你确切知道其影响，否则你绝不应该这样做。'
