- en: Robust Infrastructure with Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有 Kubernetes 的稳健基础设施
- en: In the previous chapter, we used Docker to pre-build and package different parts
    of our application, such as Elasticsearch and our API server, into Docker images.
    These images are portable and can be deployed independently onto any environment.
    Although this revised approach automated some aspects of our workflow, we are
    still **manually** deploying our containers on a **single** server.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用 Docker 预构建和打包应用程序的不同部分，如 Elasticsearch 和我们的 API 服务器，到 Docker 镜像中。这些镜像是可以移植的，并且可以独立部署到任何环境中。尽管这种改进的方法自动化了我们工作流程的一些方面，但我们仍然在**单个**服务器上**手动**部署我们的容器。
- en: This lack of automation presents the risk of human error. Deploying on a single
    server introduces a **single point of failure** (**SPOF**), which reduces the
    reliability of our application.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这种缺乏自动化带来了人为错误的风险。在单个服务器上部署引入了**单点故障**（**SPOF**），这降低了我们应用程序的可靠性。
- en: Instead, we should provide redundancy by spawning multiple instances of each
    service, and deploying them across different physical servers and data centers.
    In other words, we should deploy our application on a cluster.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们应该通过启动每个服务的多个实例，并将它们部署在不同的物理服务器和数据中心来提供冗余。换句话说，我们应该在集群上部署我们的应用程序。
- en: Clusters allow us to have high availability, reliability, and scalability. When
    an instance of a service becomes unavailable, a failover mechanism can redirect
    unfulfilled requests to the still-available instances. This ensures that the application,
    as a whole, remains responsive and functional.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 集群使我们能够拥有高可用性、可靠性和可伸缩性。当某个服务的实例变得不可用时，故障转移机制可以将未满足的请求重定向到仍然可用的实例。这确保了整个应用程序保持响应和功能。
- en: 'However, coordinating and managing this distributed, redundant cluster is non-trivial,
    and requires many moving parts to work in concert. These include the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，协调和管理这个分布式、冗余的集群并非易事，需要许多部分协同工作。这些包括以下内容：
- en: Service discovery tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现工具
- en: Global configuration store
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全球配置存储
- en: Networking tools
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络工具
- en: Scheduling tools
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度工具
- en: Load balancers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: '...and many more'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...以及更多'
- en: Cluster Management Tools is a platform which manages these tools and provides
    a layer of abstraction for developers to work with. A prime example is *Kubernetes*,
    which was open-sourced by Google in 2014.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理工具是一个管理这些工具并提供开发人员工作抽象层的平台。一个典型的例子是2014年由谷歌开源的**Kubernetes**。
- en: Because most Cluster Management Tools also use containers to deploy, they are
    often also called **Container Orchestration** **systems**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因为大多数集群管理工具也使用容器进行部署，所以它们通常也被称为**容器编排**系统。
- en: 'In this chapter, we will learn how to:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何：
- en: Make our application more robust by deploying it with Kubernetes on DigitalOcean
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在 DigitalOcean 上使用 Kubernetes 部署我们的应用程序来使其更加稳健
- en: Understand the features of a robust system; namely **availability**, **reliability**,
    **throughput**, and **scalability**
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解稳健系统的特性；即**可用性**、**可靠性**、**吞吐量**和**可伸缩性**
- en: Examine the types of components a **Cluster Management Tool** would normally
    manage, how they work together, and how they contribute to making our system more
    robust
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查集群管理工具通常会管理的组件类型，它们如何协同工作，以及它们如何有助于使我们的系统更加稳健
- en: Get hands-on and deploy and manage our application as a distributed Kubernetes
    cluster
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过部署和管理工作作为分布式 Kubernetes 集群来获得实践经验
- en: High availability
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性
- en: Availability is a measure of the proportion of time that a system is able to
    fulfill its intended function. For an API, it means the percentage of time that
    the API can respond correctly to a client's requests.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性是衡量系统能够履行其预期功能的比例的时间。对于一个API，这意味着API能够正确响应客户端请求的时间百分比。
- en: Measuring availability
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量可用性
- en: 'Availability is usually measured as the percentage of time the system is functional
    (*Uptime*) over the total elapsed time:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性通常测量为系统功能的时间百分比（*正常运行时间*）与总经过时间的比例：
- en: '![](img/7816e04c-3bb0-484f-8900-0b6d50eff372.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7816e04c-3bb0-484f-8900-0b6d50eff372.png)'
- en: This is typically represented as "nines". For example, a system with an availability
    level of "four nines" will have an uptime of 99.99% or higher.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常表示为“九”。例如，具有“四个九”可用性级别的系统将具有99.99%或更高的正常运行时间。
- en: Following the industry standard
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遵循行业标准
- en: Generally speaking, the more complex a system, the more things can go wrong;
    this translates to a lower availability. In other words, it is much easier to
    have a 100% uptime for a static website than for an API.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，系统越复杂，出错的可能性就越大；这导致可用性降低。换句话说，对于静态网站来说，实现100%的在线时间比API要容易得多。
- en: 'So, what is the industry standard for availability for common APIs? Most online
    platforms offer a **service level agreement** (**SLA**) that includes a clause
    for the minimum availability of the platform. Here are some examples (accurate
    at the time of writing):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，常见API的可用性行业标准是什么？大多数在线平台都提供包含平台最低可用性条款的**服务级别协议**（**SLA**）。以下是一些示例（截至撰写本文时准确）：
- en: 'Google Compute Engine Service Level Agreement (SLA): 99.99%'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌计算引擎服务级别协议（SLA）：99.99%
- en: 'Amazon Compute Service Level Agreement: 99.99%'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊计算服务级别协议：99.99%
- en: 'App Engine Service Level Agreement (SLA): 99.95%'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用引擎服务级别协议（SLA）：99.95%
- en: 'Google Maps—Service Level Agreement (“Maps API SLA”): 99.9%'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌地图——服务级别协议（“地图API SLA”）：99.9%
- en: 'Amazon S3 Service Level Agreement: 99.9%'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊S3服务级别协议：99.9%
- en: Evidently, these SLAs provide minimum availability guarantees that range from
    "three nines" (99.9%) to "four nines" (99.99%); this translates to a maximum downtime
    of between 52.6 minutes and 8.77 hours per year. Therefore, we should also aim
    to provide a similar level of availability for our API.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些服务级别协议提供了从“三个九”（99.9%）到“四个九”（99.99%）的最小可用性保证；这相当于每年最多停机时间为52.6分钟到8.77小时。因此，我们也应该旨在为我们的API提供类似级别的可用性。
- en: Eliminating single points of failure (SPOF)
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消除单点故障（SPOF）
- en: The most fundamental step to ensure high availability is eliminating (SPOF).
    A SPOF is a component within a system which, if fails, causes the entire system
    to fail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 确保高可用性的最基本步骤是消除（SPOF）。单点故障是系统中的一个组件，如果它失败，会导致整个系统失败。
- en: For example, if we deploy only one instance of our backend API, the single Node
    process running the instance becomes a SPOF. If that Node process exits for whatever
    reason, then our whole application goes down.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们只部署一个后端API的实例，运行该实例的单个节点进程就变成了一个单点故障（SPOF）。如果该节点进程因任何原因退出，那么我们的整个应用程序就会崩溃。
- en: Fortunately, eliminating a SPOF is relatively simple—replication; you simply
    have to deploy multiple instances of that component. However, that comes with
    challenges of its own—when a new request is received, which instance should handle
    it?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，消除单点故障相对简单——复制；你只需部署该组件的多个实例。然而，这也带来了自己的挑战——当收到新的请求时，哪个实例应该处理它？
- en: Load balancing versus failover
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡与故障转移
- en: 'Conventionally, there are two methods to route requests to replicated components:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，有两种方法可以将请求路由到复制的组件：
- en: '**Load balancing**: A load balancer sits in-between the client and the server
    instances, intercepting the requests and distributing them among all instances:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**：负载均衡器位于客户端和服务器实例之间，拦截请求并将它们分配到所有实例：'
- en: '![](img/fc306757-1c22-4a71-8d75-6796dfeb55c4.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc306757-1c22-4a71-8d75-6796dfeb55c4.jpg)'
- en: 'The way requests are distributed depends on the load balancing algorithm used.
    Apart from "random" selection, the simplest algorithm is the *round-robin* algorithm.
    This is where requests are sequentially routed to each instance in order. For
    example, if there are two backend servers, A and B, the first request will be
    routed to A, the second to B, the third back to A, the fourth to B, and so on.
    This results in requests being evenly distributed:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请求的分配方式取决于所使用的负载均衡算法。除了“随机”选择之外，最简单的算法是**轮询**算法。这就是请求按顺序依次路由到每个实例。例如，如果有两个后端服务器，A和B，第一个请求将被路由到A，第二个到B，第三个回到A，第四个到B，以此类推。这导致请求均匀分配：
- en: '![](img/340684e2-6628-466d-8e53-ef0fde637b6f.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/340684e2-6628-466d-8e53-ef0fde637b6f.jpg)'
- en: While round-robin is the simplest scheme to implement, it assumes that all nodes
    are equal – in terms of available resources, current load, and network congestion.
    This is often not the case. Therefore, *dynamic round-robin* is often used, which
    will route more traffic to hosts with more available resources and/or lower load.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然轮询是最简单的实现方案，但它假设所有节点都是平等的——在可用资源、当前负载和网络拥塞方面。这通常不是情况。因此，通常使用**动态轮询**，它将更多流量路由到具有更多可用资源或负载较低的宿主。
- en: '**Failover**: Requests are routed to a single *primary* instance. If and when
    the primary instance fails, subsequent requests are routed to a different *secondary*,
    or *standby*, instance:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障转移**：请求被路由到单个 *主* 实例。如果主实例失败，后续请求将被路由到不同的 *辅助* 或 *备用* 实例：'
- en: '![](img/30a4e467-8e2b-4831-bc8f-7d39ebe30ea2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30a4e467-8e2b-4831-bc8f-7d39ebe30ea2.png)'
- en: 'As with all things, there are pros and cons of each method:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就像所有事物一样，每种方法都有其优缺点：
- en: '**Resource Utilization**: With the failover approach, only a single instance
    is running at any one time; this means you''ll be paying for server resources
    that do not contribute to the normal running of your application, nor improve
    its performance or throughput. On the other hand, the objective of load balancing
    is to maximize resource usage; providing high availability is simply a useful
    side effect.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源利用率**：使用故障转移方法，任何时候只有一个实例在运行；这意味着你将支付那些不贡献于你应用程序的正常运行，也不提高其性能或吞吐量的服务器资源。另一方面，负载均衡的目标是最大化资源利用率；提供高可用性只是一个有用的副作用。'
- en: '**Statefulness**: Sometimes, failover is the only viable method. Many real-world,
    perhaps legacy, applications are stateful, and the state can become corrupted
    if multiple instances of the application are running at the same time. Although
    you can refactor the application to cater for this, it''s still a fact that not
    all applications can be served behind a load balancer.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态性**：有时，故障转移是唯一可行的方案。许多现实世界中的，可能是遗留的应用程序是有状态的，如果同时运行多个应用程序实例，状态可能会损坏。尽管你可以重构应用程序以适应这种情况，但仍然是一个事实，并非所有应用程序都可以在负载均衡器后面提供服务。'
- en: '**Scalability**: With failover, to improve performance and throughput, you
    must scale the primary node vertically (by increasing its resources). With load
    balancing, you can scale both vertically and horizontally (by adding more machines).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可伸缩性**：使用故障转移，为了提高性能和吞吐量，你必须垂直扩展（通过增加其资源）主节点。使用负载均衡，你可以垂直和水平扩展（通过添加更多机器）。'
- en: Since our application is stateless, using a distributed load balancer makes
    more sense as it allows us to fully utilize all resources and provide better performance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的应用程序是无状态的，因此使用分布式负载均衡器更有意义，因为它允许我们充分利用所有资源并提供更好的性能。
- en: Load balancing
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Load balancing can be done in multiple ways—using DNS for load distribution,
    or employing a Layer 4 or Layer 7 load balancer.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡可以通过多种方式实现——使用 DNS 进行负载分配，或者使用第四层或第七层负载均衡器。
- en: DNS load balancing
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNS 负载均衡
- en: A domain can configure its DNS settings so that multiple IP addresses are associated
    with it. When a client tries to resolve the domain name to an IP address, it returns
    a list of all IP addresses. Most clients would then send its requests to the first
    IP address in the list.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个域名可以配置其 DNS 设置，使其与多个 IP 地址相关联。当客户端尝试将域名解析为 IP 地址时，它会返回一个包含所有 IP 地址的列表。大多数客户端随后会将请求发送到列表中的第一个
    IP 地址。
- en: DNS load balancing is where the DNS changes the order of these addresses each
    time a new name resolution request is made. Most commonly, this is done in a round-robin
    manner.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 负载均衡是在每次进行新的名称解析请求时，DNS 改变这些地址的顺序。最常见的是，这是以轮询的方式完成的。
- en: 'Using this method, client requests should be distributed equally among all
    backend servers. However, load balancing at the DNS level has some major disadvantages:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，客户端请求应该在所有后端服务器之间均匀分配。然而，在 DNS 层面的负载均衡有一些主要的缺点：
- en: '**Lack of health-checks**: The DNS does not monitor the health of the servers.
    Even if one of the servers in the list goes down, it will still return with the
    same list of IP addresses.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏健康检查**：DNS 不监控服务器的健康状态。即使列表中的某个服务器宕机，它仍然会返回相同的 IP 地址列表。'
- en: Updating and propagating DNS records to all *root servers*, intermediate DNS
    servers (*resolvers*), and clients can take anything from minutes to hours. Furthermore,
    most DNS servers cache their DNS records. This means that requests may still be
    routed to failed servers long after the DNS records are updated.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新和传播 DNS 记录到所有 *根服务器*、中间 DNS 服务器（*解析器*）和客户端可能需要几分钟到几小时的时间。此外，大多数 DNS 服务器都会缓存它们的
    DNS 记录。这意味着在 DNS 记录更新后，请求可能仍然被路由到已失败的服务器。
- en: Layer 4/7 load balancers
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四层/第七层负载均衡器
- en: Another way to load balance client requests is to use a *load balancer*. Instead
    of exposing the backend servers on multiple IP addresses and letting the client
    pick which server to use, we can instead keep our backend servers hidden behind
    a private local network. When a client wants to reach our application, it would
    send the request to the load balancer, which will forward the requests to the
    backends.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种负载均衡客户端请求的方法是使用一个 *负载均衡器*。我们不是在多个 IP 地址上公开后端服务器并让客户端选择使用哪个服务器，而是可以将我们的后端服务器隐藏在私有本地网络后面。当客户端想要访问我们的应用程序时，它会将请求发送到负载均衡器，负载均衡器会将请求转发到后端。
- en: 'Generally speaking, there are two types of load balancers—Layer 4 (L4), Layer
    7 (L7). Their names relate to the corresponding layer inside the **Open Systems
    Interconnection** (**OSI**) reference model—a standard conceptual model that partitions
    a communication system into abstraction layers:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，负载均衡器有两种类型——第 4 层（L4）、第 7 层（L7）。它们的名称与 **开放系统互连**（**OSI**）参考模型中的相应层相关——这是一个将通信系统划分为抽象层的标准概念模型：
- en: '![](img/0bd73b37-b690-4015-a261-411fac6ff45a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0bd73b37-b690-4015-a261-411fac6ff45a.png)'
- en: There are numerous standard protocols at each layer that specify how data should
    be packaged and transported. For example, FTP and MQTT are both application layer
    protocols. FTP is designed for file transfer, whereas MQTT is designed for publish-subscribe-based
    messaging.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都有许多标准协议，它们指定了数据应该如何打包和传输。例如，FTP 和 MQTT 都是应用层协议。FTP 是为文件传输设计的，而 MQTT 是为基于发布/订阅的消息传递设计的。
- en: When a load balancer receives a request, it will make a decision as to which
    backend server to forward a request to. These decisions are made using information
    embedded in the request. An L4 load balancer would use information from the transport
    layer, whereas an L7 load balancer can use information from the application layer,
    including the request body itself.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当负载均衡器收到一个请求时，它将决定将请求转发到哪个后端服务器。这些决策是基于请求中嵌入的信息做出的。L4 负载均衡器会使用传输层的信息，而 L7 负载均衡器可以使用应用层的信息，包括请求体本身。
- en: Layer 4 load balancers
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 层负载均衡器
- en: Generally speaking, L4 load balancers use information defined at the **Transport layer**
    (layer 4) of the OSI model. In the context of the internet, this means that L4
    load balancers should use information from Transmission Control Protocol (TCP)
    data packets. However, as it turns out, L4 load balancers also use information
    from Internet Protocol (IP) packets, which is a layer 3 - the *network* layer.
    Therefore, the name "Layer 4" should be considered a misnomer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，L4 负载均衡器使用在 OSI 模型的传输层（第 4 层）定义的信息。在互联网的上下文中，这意味着 L4 负载均衡器应该使用传输控制协议（TCP）数据包的信息。然而，实际上，L4
    负载均衡器也使用来自互联网协议（IP）数据包的信息，这是第 3 层——*网络*层。因此，“第 4 层”这个名字应该被认为是一个误称。
- en: Specifically, an L4 load balancer routes requests based on the source/destination
    IP addresses and ports, with zero regards to the contents of the packets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，L4 负载均衡器根据源/目的 IP 地址和端口路由请求，对数据包的内容不予理会。
- en: Usually, an L4 load balancer comes in the form of a dedicated hardware device
    running proprietary chips and/or software.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，L4 负载均衡器以运行专有芯片和/或软件的专用硬件设备的形式出现。
- en: Layer 7 load balancing
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 层负载均衡
- en: A Layer 7 (L7) load balancer is similar to an L4 load balancer, but uses information
    from the highest layer on the OSI model – the *application* layer. For web services
    like our API, the Hypertext Transfer Protocol (HTTP) is used.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第 7 层（L7）负载均衡器与 L4 负载均衡器类似，但使用 OSI 模型中最顶层——*应用*层的信息。对于我们的 API 等网络服务，使用的是超文本传输协议（HTTP）。
- en: An L7 load balancer can use information from the URL, HTTP headers (for example, `Content-Type`),
    cookies, contents of the message body, client's IP address, and other information
    to route a request.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: L7 负载均衡器可以使用来自 URL、HTTP 头部（例如，`Content-Type`）、cookies、消息体内容、客户端 IP 地址以及其他信息来路由请求。
- en: 'By working on the application layer, an L7 load balancer has several advantages
    over an L4 load balancer:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在应用层工作，L7 负载均衡器相对于 L4 负载均衡器有以下几个优势：
- en: '**Smarter**: Because L7 load balancers can base their routing rules on more
    information, such as the client''s geolocation data, they can offer more sophisticated
    routing rules than L4 load balancers.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更智能**：因为 L7 负载均衡器可以根据更多信息（如客户端的地理位置数据）制定路由规则，所以它们可以提供比 L4 负载均衡器更复杂的路由规则。'
- en: '**More capabilities**: Because L7 load balancers have access to the message
    content, they are able to alter the message, such as encrypting and/or compressing
    the body.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多功能**：因为L7负载均衡器可以访问消息内容，它们能够修改消息，例如加密和/或压缩正文。'
- en: '**Cloud Load Balancing**: Because L4 load balancers are typically hardware
    devices, cloud providers usually do not allow you to configure them. In contrast,
    L7 load balancers are typically software, which can be fully managed by the developer.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云负载均衡**：因为L4负载均衡器通常是硬件设备，云提供商通常不允许你配置它们。相比之下，L7负载均衡器通常是软件，可以完全由开发者管理。'
- en: '**Ease of debugging**: They can use cookies to keep the same client hitting
    the same backend server. This is a must if you implement stateful logic such as
    "sticky" sessions, but is also otherwise advantageous when debugging—you only
    have to parse logs from one backend server instead of all of them.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调试的简便性**：他们可以使用cookie来确保相同的客户端访问相同的后端服务器。如果你实现了“粘性会话”等有状态逻辑，这是必须的，但在调试时也有优势——你只需要解析一个后端服务器的日志，而不是所有服务器的日志。'
- en: However, L7 load balancers are not always "better" than their L4 counterparts.
    L7 load balancers require more system resources and have high latency, because
    it must take into consideration more parameters. However, this latency is not
    significant enough for us to worry about.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，L7负载均衡器并不总是比它们的L4对应物“更好”。L7负载均衡器需要更多的系统资源，并且具有高延迟，因为它必须考虑更多的参数。然而，这种延迟并不足以让我们担心。
- en: There are currently a few production-ready L7 load balancers on the market—**High
    Availability Proxy** (**HAProxy**), NGINX, and Envoy. We will look into deploying
    a distributed load balancer in front of our backend servers later in this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 目前市场上有一些现成的L7负载均衡器——**高可用代理**（**HAProxy**）、NGINX和Envoy。我们将在本章后面探讨在后台服务器前部署分布式负载均衡器。
- en: High reliability
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可靠性
- en: Reliability is a measure of the confidence in a system, and is inversely proportional
    to the probability of failure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性是衡量对系统信心的指标，并且与故障概率成反比。
- en: 'Reliability is measured using several metrics:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性是通过几个指标来衡量的：
- en: '**Mean time between failures** (**MTBF**): Uptime/number of failures'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均故障间隔时间**（**MTBF**）：正常运行时间/故障次数'
- en: '**Mean time to repair **(**MTTR**): The average time it takes the team to fix
    a failure and return the system online'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均修复时间**（**MTTR**）：团队修复故障并使系统恢复在线的平均时间'
- en: Testing for reliability
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试可靠性
- en: The easiest way to increase reliability is to increase test coverage of the
    system. This is, of course, assuming that those tests are meaningful tests.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 提高可靠性的最简单方法是增加系统的测试覆盖率。当然，这假设那些测试是有意义的测试。
- en: 'Tests increase reliability by:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 测试通过以下方式提高可靠性：
- en: 'Increasing MTBF: The more thorough your tests, the more likely you''ll catch
    bugs before the system is deployed.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高MTBF：你的测试越彻底，你越有可能在系统部署之前捕捉到错误。
- en: 'Reducing MTTR: This is because historical test results inform you of the last
    version which passes all tests. If the application is experiencing a high level
    of failures, then the team can quickly roll back to the last-known-good version.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低MTTR：这是因为历史测试结果会告诉你最后一个通过所有测试的版本。如果应用程序出现高故障率，那么团队可以快速回滚到最后一个已知良好的版本。
- en: High throughput
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高带宽
- en: Throughput is a measure of the number of requests that can be fulfilled in a
    given time interval.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽是衡量在给定时间间隔内可以满足的请求数量的指标。
- en: 'The throughput of a system depends on several factors:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的带宽取决于几个因素：
- en: '**Network Latency**: The amount of time it takes for the message to get from
    the client to our application, as well as between different components of the
    application'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络延迟**：消息从客户端到我们的应用程序所需的时间，以及应用程序不同组件之间所需的时间'
- en: '**Performance**: The computation speed of the program itself'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：程序本身的计算速度'
- en: '**Parallelism**: Whether requests can be processed in parallel'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行性**：请求是否可以并行处理'
- en: 'We can increase throughput using the following strategies:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下策略来提高带宽：
- en: Deploying our application geographically close to the client: Generally, this
    reduces the number of hops that a request must make through proxy servers, and
    thus reduces network latency. We should also deploy components that depend on
    each other close together, preferably within the same data center. This also reduces
    network latency.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在地理上靠近客户端部署我们的应用程序：通常，这减少了请求必须通过代理服务器跳转的次数，从而降低了网络延迟。我们还应该将相互依赖的组件部署在附近，最好是在同一个数据中心内。这也有助于降低网络延迟。
- en: 'Ensure servers have sufficient resources: This makes sure that the CPU on your
    servers are sufficiently fast, and that the servers have enough memory to perform
    their tasks without having to use swap memory.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保服务器拥有足够的资源：这确保了您服务器上的CPU足够快，并且服务器有足够的内存来执行其任务，而无需使用交换内存。
- en: 'Deploy multiple instances of an application behind a load balancer: This allows
    multiple requests to the application to be processed at the same time.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在负载均衡器后面部署应用程序的多个实例：这允许同时处理对应用程序的多个请求。
- en: 'Ensure your application code is non-blocking: JavaScript is an asynchronous
    language. If you write synchronous, blocking code, it will prevent other operations
    from executing while you wait for the synchronous operation to complete.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您的应用程序代码是非阻塞的：JavaScript是一种异步语言。如果您编写同步、阻塞代码，则在等待同步操作完成时，将阻止其他操作执行。
- en: High scalability
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可扩展性
- en: Scalability is a measure of how well a system can grow in order to handle higher
    demands, while still maintaining the same levels of performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性是衡量系统在处理更高需求的同时，仍能保持相同性能水平的能力的指标。
- en: The demand may arise as part of a sustained growth in user uptake, or it may
    be due to a sudden peak of traffic (for example, a food delivery application is
    likely to receive more requests during lunch hours).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 需求可能源于用户采用率的持续增长，也可能是因为流量突然高峰（例如，食品配送应用程序在午餐时间可能会收到更多请求）。
- en: A highly scalable system should constantly monitor its constituent components
    and identify components which are working above a "safe" resource limit, and scale
    that component either *horizontally* or *vertically*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个高度可扩展的系统应不断监控其组成部分，并识别那些工作在“安全”资源限制之上的组件，并对其进行横向或纵向扩展。
- en: 'We can increase scalability in two ways:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式提高可扩展性：
- en: 'Scale Vertically or *scaling Up*: Increase the amount of resources (for example,
    CPU, RAM, storage, bandwidth) to the existing servers'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直扩展或*向上扩展*：增加现有服务器的资源量（例如，CPU、RAM、存储、带宽）
- en: 'Scale Horizontally or *scaling out*: Adding servers to the existing cluster'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 横向扩展或*扩展出去*：向现有集群添加服务器
- en: 'Scaling vertically is simple, but there''ll always be a limit as to how much
    CPU, RAM, bandwidth, ports, and even processes the machine can handle. For example,
    many kernels have a limit on the number of processes it can handle:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直扩展很简单，但机器可以处理的CPU、RAM、带宽、端口甚至进程的数量总是有一个限制。例如，许多内核对其可以处理的进程数量有一个限制：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Scaling horizontally allows you to have higher maximum limits for resources,
    but comes with challenges of its own. An instance of the service may hold some
    temporary state that must be synchronized across different instances.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 横向扩展允许您拥有更高的资源最大限制，但同时也带来了自己的挑战。服务的一个实例可能持有一些必须在不同实例之间同步的临时状态。
- en: However, because our API is "stateless" (in the sense that all states are in
    our database and not in memory), scaling horizontally poses less of an issue.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们的API是“无状态的”（在这种意义上，所有状态都在我们的数据库中，而不是在内存中），横向扩展带来的问题较少。
- en: Clusters and microservices
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群和微服务
- en: 'In order to make our system be highly available, reliable, scalable, and produce
    high throughput, we must design a system that is:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的系统具有高可用性、可靠性、可扩展性和高吞吐量，我们必须设计一个系统：
- en: 'Resilient/Durable: Able to sustain component failures'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性/持久：能够承受组件故障
- en: 'Elastic: Each service and resource can grow and shrink quickly based on demand'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性：每个服务和资源可以根据需求快速增长和缩小
- en: Such systems can be achieved by breaking monolithic applications into many smaller *stateless* components
    (following the microservices architecture) and deploying them in a *cluster*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将单体应用程序分解成许多更小的*无状态*组件（遵循微服务架构）并在集群中部署它们，可以实现这样的系统。
- en: Microservices
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微服务
- en: 'Instead of having a monolithic code base that caters to many concerns, you
    can instead break the application down into many services which, when working
    together, make up the whole application. Each service should:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与提供一个满足许多关注点的单体代码库相比，你可以将应用程序分解成许多服务，当它们协同工作时，就构成了整个应用程序。每个服务应该：
- en: Have one or very few concerns
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个或非常少的关注点
- en: Be de-coupled from other services
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他服务解耦
- en: Be stateless (if possible)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可能的话，保持无状态
- en: '![](img/22f95aca-df87-4acc-913b-40657a9c3cab.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/22f95aca-df87-4acc-913b-40657a9c3cab.jpg)'
- en: With a monolithic application, all the components must be deployed together
    as a single unit. if you want to scale your application, you must scale by deploying
    more instances of the monolith. Furthermore, because there're no clear boundaries
    between different service, you'd often find tightly-coupled code in the code base. On
    the other hand, a microservices architecture places each service as a separate,
    standalone entity. You can scale by replicating only the service that is required.
    Furthermore, you can deploy the services on varied architecture, even using different
    vendors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在单体应用程序中，所有组件必须作为一个单一单元一起部署。如果你想要扩展应用程序，你必须通过部署更多单体实例来扩展。此外，由于不同服务之间没有明确的边界，你经常会发现代码库中存在紧密耦合的代码。另一方面，微服务架构将每个服务作为一个独立的、独立的实体。你可以通过仅复制所需的服务来扩展。此外，你可以在不同的架构上部署服务，甚至可以使用不同的供应商。
- en: A service should expose an API for other services to interact with, but would
    otherwise be independent of other services. This means services could be independently
    deployed and managed.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个服务应该向其他服务公开一个API以进行交互，但除此之外，它应该独立于其他服务。这意味着服务可以独立部署和管理。
- en: Writing an application that allows for a microservice architecture allows us
    to achieve high scalability—administrators can simply spawn more instances of
    an in-demand service. Because the services are independent of each other, they
    can be deployed independently, where the more in-demand services have more instances
    deployed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个允许微服务架构的应用程序可以使我们实现高可扩展性——管理员可以简单地生成更多需求服务的实例。因为服务之间是独立的，所以它们可以独立部署和管理。
- en: We have made our application stateless and containerized, both of which makes
    implementing a microservices architecture much easier.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使我们的应用程序无状态并容器化，这两者都使得实现微服务架构变得更加容易。
- en: Clusters
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群
- en: 'To implement a reliable and scalable infrastructure, we must provide redundancy.
    This means redundancy in:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现可靠和可扩展的基础设施，我们必须提供冗余。这意味着在以下方面的冗余：
- en: '**Hardware**: We must deploy our application across multiple physical hosts,
    each (ideally) at different geographical locations. This is so that if one data
    center is offline or destroyed, services deployed at the other data centers can
    keep our application running.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件**：我们必须在多个物理主机上部署我们的应用程序，每个（理想情况下）在不同的地理位置。这样，如果一个数据中心离线或被摧毁，其他数据中心部署的服务可以保持我们的应用程序运行。'
- en: '**Software**: We must also deploy multiple instances of our services; this
    is so that the load of handling requests can be distributed across them. Consequently,
    this yields the following benefits:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件**：我们必须也部署我们服务的多个实例；这样，处理请求的负载就可以在这些实例之间分配。因此，这带来了以下好处：'
- en: We can route users to the server which provides them with the quickest response
    time (usually the one closest geographically to the user)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将用户路由到提供最快响应时间的服务器（通常是地理位置上离用户最近的服务器）
- en: We can put one service offline, update it, and bring it back online without
    affecting the uptime of the entire application
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以关闭一个服务，更新它，然后将其重新上线，而不会影响整个应用程序的运行时间
- en: Deploying applications on a cluster allows you to have hardware redundancy,
    and load balancers provide software redundancy.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群上部署应用程序可以使你拥有硬件冗余，负载均衡器提供软件冗余。
- en: A cluster consists of a network of hosts/servers (called *nodes*). Once these
    nodes are provisioned, you can then deploy instances of your services inside them.
    Next, you'll need to configure a load balancer that sits in front of the services
    and distribute requests to the node with the most available service.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 集群由一组主机/服务器（称为节点）的网络组成。一旦这些节点被配置，你就可以在它们内部部署你服务的实例。接下来，你需要配置一个负载均衡器，它位于服务之前，并将请求分配给拥有最多可用服务的节点。
- en: 'By deploying redundant services on a cluster, it ensures:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在集群上部署冗余服务，它可以确保：
- en: '**High Availability**: If a server becomes unavailable, either through failure
    or planned maintenance, then the load balancer can implement a *failover* mechanism
    and redistribute requests to the healthy instances.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可用性**：如果服务器变得不可用，无论是由于故障还是计划维护，负载均衡器可以实施**故障转移**机制，并将请求重新分配到健康的实例。'
- en: '**High Reliability**: Redundant instances remove the *single point of failure*.
    It means our whole system becomes *fault-tolerant*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高可靠性**：冗余实例消除了**单点故障**。这意味着我们的整个系统变得**容错**。'
- en: '**High Throughput**: By having multiple instances of the service across geographical
    regions, it allows for low latency.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高吞吐量**：通过在地理区域跨多个服务实例，它允许低延迟。'
- en: This may be implemented as a **Redundant Array Of Inexpensive Servers** (**RAIS**),
    the server equivalent of RAID, or *Redundant Arrays Of Inexpensive Disks*. Whenever
    a server fails, the service will still be available by serving them from the healthy
    servers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能实现为一个**廉价服务器冗余阵列**（**RAIS**），服务器的RAID等效，或者称为**廉价磁盘冗余阵列**。每当服务器出现故障时，服务仍然可以通过从健康服务器提供服务来保持可用。
- en: However, if you are using a cloud provider like DigitalOcean, they would take
    care of the hardware redundancy for you. All that's left for us to do is deploy
    our cluster and configure our load balancer.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你使用像DigitalOcean这样的云服务提供商，他们将为你处理硬件冗余。我们剩下的只是部署我们的集群并配置我们的负载均衡器。
- en: Cluster management
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群管理
- en: Deploying our application in a microservices manner inside a cluster is simple
    enough in principle, but actually quite complex to implement.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群内部以微服务方式部署我们的应用程序在原则上足够简单，但实际上实施起来相当复杂。
- en: 'First, you must *provision* servers to act as nodes inside your cluster. Then,
    we''ll need to set up a handful of tools that work in concert with each other
    to manage your cluster. These tools can be categorized into two groups:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你必须**配置**服务器以作为集群内的节点。然后，我们需要设置一些相互协作以管理集群的工具。这些工具可以分为两组：
- en: '**Cluster-level tools**: Works at the cluster level, and makes global decisions
    that affect the whole cluster'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群级工具**：在集群级别工作，并做出影响整个集群的全局决策。'
- en: '**Node-level tools**: Resides within each node. It takes instructions from,
    and feedback to, cluster-level tools in order to coordinate the management of
    services running inside the node.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点级工具**：位于每个节点内部。它从集群级工具接收指令，并反馈给集群级工具，以协调节点内运行的服务管理。'
- en: 'For the cluster-level tools, you''ll need the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集群级工具，你需要以下工具：
- en: '**A scheduler**: This dictates which node a particular service will be deployed
    on.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器**：这决定了特定服务将部署在哪个节点上。'
- en: '**A Discovery Service**: This keeps a record of how many instances of each
    service are deployed, their states (for example, starting, running, terminating
    and so on.), where they''re deployed, and so on. It allows for *service discovery.*'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发现服务**：记录每个服务的实例数量，它们的状态（例如，启动、运行、终止等），它们部署的位置等。它允许进行**服务发现**。'
- en: '**A Global Configuration Store**: Stores cluster configurations such as common
    environment variables.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局配置存储**：存储集群配置，如常见环境变量。'
- en: 'On the node-level, you''ll need the following tools:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点级别，你需要以下工具：
- en: '**Local configuration management tools**: To keep local configuration states
    and to synchronize with cluster-level configurations. We have our cluster configurations
    stored inside the Global Configuration Store; however, we also need a way to retrieve
    those settings into each node. Furthermore, when those configurations are changed,
    we need a way to fetch the updated configuration and reload the application/services
    if required. `confd` ([https://github.com/kelseyhightower/confd](https://github.com/kelseyhightower/confd)) is
    the most popular tool.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地配置管理工具**：用于保持本地配置状态并与集群级配置同步。我们将集群配置存储在全局配置存储中；然而，我们还需要一种方法将那些设置检索到每个节点。此外，当这些配置发生变化时，我们需要一种方法来获取更新的配置，并在必要时重新加载应用程序/服务。`confd`（[https://github.com/kelseyhightower/confd](https://github.com/kelseyhightower/confd)）是最受欢迎的工具。'
- en: '**Container runtime**: Given that a node is assigned to run a service, it must
    have the necessary programs to do so. Most services deployed on modern microservice
    infrastructures use containers to encapsulate the service. Therefore, all major
    cluster management tools will be bundled with some kind of container runtime,
    such as Docker.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**：由于一个节点被分配来运行一个服务，它必须具备执行此操作所需的程序。大多数在现代微服务基础设施上部署的服务使用容器来封装服务。因此，所有主要的集群管理工具都将捆绑某种类型的容器运行时，例如Docker。'
- en: Now, let's take a look at each cluster-level tool in more detail.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地查看每个集群级工具。
- en: Cluster-level tools
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群级工具
- en: As mentioned previously, cluster-level tools work at the cluster level, and
    make global decisions that affect the whole cluster.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，集群级工具在集群级别工作，并做出影响整个集群的全局决策。
- en: Discovery service
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现服务
- en: 'At the moment, our API container can communicate with our Elasticsearch container
    because, under the hood, they''re connected to the same network, on the same host
    machine:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的API容器可以与我们的Elasticsearch容器通信，因为它们在底层连接到同一网络，在同一台主机机器上：
- en: '[PRE1]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: However, if these containers are deployed on separate machines, using different
    networks, how can they communicate with each other?
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果这些容器部署在不同的机器上，使用不同的网络，它们如何相互通信？
- en: Our API container must obtain network information about the Elasticsearch container
    so that it can send requests to it. One way to do this is by using a *service
    discovery* tool.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的API容器必须获取Elasticsearch容器的网络信息，以便能够向其发送请求。一种方法是使用*服务发现*工具。
- en: With service discovery, whenever a new container (running a service) is initialized,
    it registers itself with the **Discovery Service**, providing information about
    itself, which includes its IP address. The **Discovery Service** then stores this
    information in a simple key-value store.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用服务发现，每当一个新的容器（运行一个服务）初始化时，它会将自己注册到**发现服务**，提供关于自己的信息，包括其IP地址。然后，**发现服务**将此信息存储在简单的键值存储中。
- en: The service should update the **Discovery Service** regularly with its status,
    so that the Discovery Service always has an up-to-date state of the service at
    any time.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 服务应定期更新**发现服务**的状态，以便发现服务在任何时候都能保持服务的最新状态。
- en: 'When a new service is initiated, it will query the **Discovery Service** to
    request information about the services it needs to connect with, such as their
    IP address. Then, the **Discovery Service** will retrieve this information from
    its key-value store and return it to the new service:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个新的服务启动时，它将查询**发现服务**以请求有关它需要连接的服务的信息，例如它们的IP地址。然后，**发现服务**将从其键值存储中检索此信息并将其返回给新的服务：
- en: '![](img/343e6df5-2202-41e1-8c4d-1c71079a7520.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/343e6df5-2202-41e1-8c4d-1c71079a7520.png)'
- en: Therefore, when we deploy our application as a cluster, we can employ a service
    discovery tool to facilitate our API's communication with our Elasticsearch service.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们以集群形式部署我们的应用程序时，我们可以使用服务发现工具来简化我们的API与Elasticsearch服务的通信。
- en: 'Popular service discovery tools include the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的服务发现工具包括以下：
- en: '`etcd`, by CoreOS ([https://github.com/coreos/etcd](https://github.com/coreos/etcd))'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`，由CoreOS提供([https://github.com/coreos/etcd](https://github.com/coreos/etcd))'
- en: Consul, by HashiCorp ([https://www.consul.io/](https://www.consul.io/))
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul，由HashiCorp提供([https://www.consul.io/](https://www.consul.io/))
- en: Zookeeper, by Yahoo, now an Apache Software Foundation ([https://zookeeper.apache.org/](https://zookeeper.apache.org/))
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zookeeper，由Yahoo提供，现在是Apache软件基金会的一部分([https://zookeeper.apache.org/](https://zookeeper.apache.org/))
- en: Scheduler
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度器
- en: 'While the **Discovery Service** holds information about the state and location
    of each service, it does not make the decision of which host/node the service
    should be deployed on. This process is known as **host selection** and is the
    job of a *scheduler*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然**发现服务**持有关于每个服务状态和位置的信息，但它不会决定服务应该部署在哪个主机/节点上。这个过程被称为**主机选择**，是**调度器**的职责：
- en: '![](img/671cb9c9-069b-42b8-8cd6-75dbcc6c0fcd.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/671cb9c9-069b-42b8-8cd6-75dbcc6c0fcd.png)'
- en: 'The scheduler''s decision can be based on a set of rules, called **policies**,
    which takes into account the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器的决策可以基于一组规则，称为**策略**，这些策略考虑以下因素：
- en: The nature of the request.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求的性质。
- en: Cluster configuration/settings.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群配置/设置。
- en: '**Host density**: An indication of how busy a the host system on the node is.
    If there are multiple nodes inside the cluster, we should prefer to deploy any
    new services on a node with the lowest host density. This information can be obtained
    from the Discovery Service, which holds information about all deployed services.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主机密度**：表示节点上主机系统繁忙程度的指标。如果集群内有多个节点，我们应该优先在主机密度最低的节点上部署任何新的服务。此信息可以从发现服务中获得，该服务包含有关所有已部署服务的所有信息。'
- en: '**Service (anti-)affinity**: Whether two services should be deployed together
    on the same host. This depends on:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务（反）亲和性**：是否应将两个服务部署在同一主机上。这取决于：'
- en: '**Redundancy requirements**: The same application should not be deployed on
    the same node(s) if there are other nodes that are not running the service. For
    instance, if our API service has already been deployed on two of three hosts,
    the scheduler may prefer to deploy on the remaining host to ensure maximum redundancy.'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗余需求**：如果存在其他未运行该服务的节点，则不应在同一节点（们）上部署相同的应用程序。例如，如果我们的API服务已经部署在三个主机中的两个上，则调度器可能更喜欢在剩余的主机上部署以确保最大冗余。'
- en: '**Data locality**: The scheduler should try placing computation code next to
    the data it needs to consume to reduce network latency.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据局部性**：调度器应尝试将计算代码放置在它需要消费的数据旁边，以减少网络延迟。'
- en: '**Resource requirements**: Of existing services running on nodes, as well as
    the service to be deployed'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源需求**：节点上运行的现有服务以及要部署的服务'
- en: Hardware/software constraints
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件/软件限制
- en: Other policies/rules set by the cluster administrator
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由集群管理员设定的其他策略/规则
- en: Global configuration store
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全局配置存储
- en: 'Oftentimes, as is the case with our services, environment variables need to
    be set before the service can run successfully. So far, we''ve specified the environment
    variables to use by using the `docker run` `--env-file` flag:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，就像我们的服务一样，在服务成功运行之前需要设置环境变量。到目前为止，我们通过使用`docker run`的`--env-file`标志指定了要使用的环境变量：
- en: '[PRE2]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, when deploying services on a cluster, we no longer run each container
    manually—we let the scheduler and node-level tools do this for us. Furthermore,
    we need all our services to share the same environment variables. Therefore, the
    most obvious solution is to provide a *Global Configuration Store* that stores
    configuration that is to be shared among all of the nodes and services.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当在集群上部署服务时，我们不再手动运行每个容器——我们让调度器和节点级工具为我们做这件事。此外，我们需要所有服务共享相同的环境变量。因此，最明显的解决方案是提供一个*全局配置存储*，该存储存储要在所有节点和服务之间共享的配置。
- en: Provisioning tools
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置工具
- en: Provisioning means starting new hosts (be it physical or virtual) and configuring
    them in a way that allows them to run Cluster Management Tools. After provisioning,
    the host is ready to become a node inside the cluster and can receive work.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 配置意味着启动新的主机（无论是物理的还是虚拟的）并配置它们，以便它们可以运行集群管理工具。配置完成后，主机即可成为集群内的节点并接收工作。
- en: This may involve using Infrastructure Management tools like Terraform to spin
    up new hosts, and Configuration Management tools like Puppet, Chef, Ansible or
    Salt, to ensure the configuration set inside each host are consistent with each
    other.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能涉及使用基础设施管理工具如Terraform来启动新主机，以及配置管理工具如Puppet、Chef、Ansible或Salt，以确保每个主机内部的配置设置彼此一致。
- en: While provisioning can be done before deploying our application, most Cluster
    Management software has a provisioning component built into it.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在部署我们的应用程序之前可以进行配置，但大多数集群管理软件都内置了配置组件。
- en: Picking a cluster management tool
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择集群管理工具
- en: Having to manage these different cluster management components individually
    is tedious and error-prone. Luckily, cluster management tools exist that provides
    a common API that allows us to configure these tools in a consistent and automated
    manner. You'd use the Cluster management tool's API instead of manipulating each
    component individually.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 必须单独管理这些不同的集群管理组件是繁琐且容易出错的。幸运的是，存在集群管理工具，它们提供了一个公共API，允许我们以一致和自动化的方式配置这些工具。您将使用集群管理工具的API而不是单独操作每个组件。
- en: Cluster management tools are also known as *cluster orchestration tools* or *container
    orchestration tools*. Although there may be slight nuances between the different
    terms, we can regard them as the same for the purpose of this chapter.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理工具也被称为*集群编排工具*或*容器编排工具*。尽管不同术语之间可能存在细微差别，但为了本章的目的，我们可以将它们视为相同。
- en: 'There are a few popular cluster management tools available today:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有几种流行的集群管理工具可用：
- en: 'Marathon ([https://mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/)):
    By Mesosphere and runs on Apache Mesos.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marathon ([https://mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/))：由
    Mesosphere 开发，运行在 Apache Mesos 上。
- en: 'Swarm ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/)):
    The Docker engine includes a *swarm mode* that manages Docker containers in clusters
    called *swarms*. You may also group certain containers together using Docker Compose.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/))：Docker
    引擎包含一种**集群模式**，用于管理称为**swarm**的集群中的 Docker 容器。您还可以使用 Docker Compose 将某些容器组合在一起。
- en: 'Kubernetes: The *de facto* cluster management tool.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes：**事实上的**集群管理工具。
- en: We will be using Kubernetes because it has the most mature ecosystem, and is
    the *de facto* industry standard.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Kubernetes，因为它拥有最成熟的生态系统，并且是**事实上的**行业标准。
- en: Control Planes and components
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制平面和组件
- en: The components we described previously—scheduler, Discovery Service, Global
    Configuration Store, and so on—are common to all Cluster Management Tools that
    exist today. The difference between them is how they package these components
    and abstract away the details. In Kubernetes, these components are aptly named
    Kubernetes *Components*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前描述的组件——调度器、发现服务、全局配置存储等——是所有现有集群管理工具的共同点。它们之间的区别在于它们如何打包这些组件并抽象出细节。在 Kubernetes
    中，这些组件被恰当地命名为 Kubernetes **组件**。
- en: We will distinguish between generic "components" with Kubernetes Components
    by using the capital case for the latter.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用大写字母来区分 Kubernetes 组件的通用“组件”。
- en: In Kubernetes terminology, a "component" is a process that implements some part
    of the Kubernetes cluster system; examples include the `kube-apiserver` and `kube-scheduler`.
    The sum of all components forms what you think of as the "Kubernetes system",
    which is formally known as the *Control Plane*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 术语中，一个“组件”是一个实现 Kubernetes 集群系统某个部分的进程；例如包括`kube-apiserver`和`kube-scheduler`。所有组件的总和构成了您所认为的“Kubernetes
    系统”，正式名称为**控制平面**。
- en: Similar to how we categorized the cluster tools into cluster-level tools and
    node-level tools, Kubernetes categorizes Kubernetes Components into *Master Components* and *Node
    Components*, respectively. Node Components operates within the node it is running
    on; Master Components work with multiple nodes or the entire cluster, hold cluster-level
    settings, configuration, and state, and make cluster-level decisions. Master Components
    collectively makes up the *Master Control Plane*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们将集群工具分类为集群级工具和节点级工具的方式类似，Kubernetes 将 Kubernetes 组件分别分类为**主组件**和**节点组件**。节点组件在其运行的节点内操作；主组件与多个节点或整个集群一起工作，持有集群级设置、配置和状态，并做出集群级决策。主组件共同构成了**主控制平面**。
- en: Kubernetes also has *Addons—*components which are not strictly required but
    provide useful features such as Web UI, metrics, and logging.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 还提供了**附加组件**——这些组件不是严格必需的，但提供了诸如 Web UI、指标和日志记录等有用的功能。
- en: With this terminology in mind, let's compare the generic cluster architecture
    we've described with Kubernetes'.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个术语的基础上，让我们比较我们描述的通用集群架构与 Kubernetes 的架构。
- en: Master components
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主组件
- en: 'The discovery service and global configuration store and scheduler are implemented
    with the `etcd` and `kube-scheduler` master components:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 发现服务、全局配置存储和调度器是用`etcd`和`kube-scheduler`主组件实现的：
- en: '`etcd` is a consistent and highly-available **key-value** (**KV**) store used
    as both the Discovery Service and Global Configuration Store.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd`是一个一致且高可用的**键值存储**（**KV**），用作发现服务和全局配置存储。'
- en: Because the discovery service and global configuration store both hold information
    about the services, and each are accessible by all nodes, `etcd` can serve both
    purposes. Whenever a service registers itself with the discovery service, it will
    also be returned a set of configuration settings.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于发现服务和全局配置存储都包含有关服务的信息，并且所有节点都可以访问它们，因此`etcd`可以同时服务于这两个目的。每当服务向发现服务注册时，它也会收到一组配置设置。
- en: '`kube-scheduler` is a scheduler. It keeps track of which applications are unassigned
    to a node (and thus not running) and makes the decision as to which node to assign
    it to.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-scheduler`是一个调度器。它跟踪哪些应用程序尚未分配给节点（因此尚未运行），并决定将其分配给哪个节点。'
- en: In addition to these essential cluster management components, Kubernetes also
    provides additional Master Components to make working with Kubernetes easier.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本的集群管理组件之外，Kubernetes 还提供了额外的 Master 组件，以使使用 Kubernetes 更加容易。
- en: By default, all Master Components run on a single *Master Node*, which runs
    only the Master Components and not other containers/services. However, they can
    be configured to be replicated in order to provide redundancy.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有 Master 组件都在单个 *Master Node* 上运行，该节点只运行 Master 组件而不运行其他容器/服务。然而，它们可以被配置为进行复制，以提供冗余。
- en: kube-apiserver
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kube-apiserver
- en: 'Kubernetes runs as a daemon that exposes a RESTful Kubernetes API server—`kube-apiserver`. `kube-apiserver` acts
    as the interface to the Master Control Plane. Instead of communicating with each
    Kubernetes Component individually, you''d instead make calls to `kube-apiserver`,
    which will communicate with each component on your behalf:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 以守护进程的形式运行，暴露了一个 RESTful Kubernetes API 服务器——`kube-apiserver`。`kube-apiserver` 充当主控制平面的接口。你不需要单独与每个
    Kubernetes 组件通信，而是向 `kube-apiserver` 发起调用，它将代表你与每个组件通信：
- en: '![](img/70dc31f4-3fce-42e9-9937-f0421f46f4ac.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70dc31f4-3fce-42e9-9937-f0421f46f4ac.png)'
- en: 'There are many benefits to this, including the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了许多好处，包括以下内容：
- en: You have a central location where all changes pass through. This allows you
    to record a history of everything that has happened in your cluster.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有一个中心位置，所有更改都会通过这个位置。这允许你记录集群中发生的一切的历史。
- en: The API provides a uniform syntax.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 提供了统一的语法。
- en: kube-control-manager
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kube-control-manager
- en: As we will demonstrate later, a central concept of Kubernetes, and the reason
    why you'd use a Cluster Management Tool in the first place, is that you don't
    have to *manually* manipulate the cluster yourself.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍后将要展示的，Kubernetes 的一个核心概念，以及你最初使用集群管理工具的原因，就是你不需要 *手动* 操作集群。
- en: Doing so will consist of sending a request to one component, receiving a response,
    and based on that response, sending another request to another component. This
    is the *imperative* approach, and is time-consuming because it requires you to
    manually write programs to implement this logic.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做将包括向一个组件发送请求，接收响应，然后根据该响应向另一个组件发送另一个请求。这是 *命令式* 方法，因为它需要你手动编写程序来实现这种逻辑，所以比较耗时。
- en: Instead, Kubernetes allows us to specify the desired state of our cluster using
    configuration files, and Kubernetes will automatically coordinate the different
    Kubernetes Components to make it happen. This is the *declarative *approach and
    is what Kubernetes recommends.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，Kubernetes 允许我们通过配置文件指定我们集群的期望状态，Kubernetes 将自动协调不同的 Kubernetes 组件来实现这一目标。这是一种 *声明式* 方法，也是
    Kubernetes 推荐的方法。
- en: Linking this to what we already know, the job of the whole Kubernetes system
    (the Control Plane) then becomes a system that tries to align the current state
    of the cluster with the desired state.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一点与我们已知的内容联系起来，整个 Kubernetes 系统（控制平面）的工作就变成了一个试图使集群的当前状态与期望状态保持一致的系统。
- en: Kubernetes does this through *Controllers*. Controllers are the processes that
    actually carry out the actions of keeping the state of the cluster with the desired
    state.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 通过 *控制器* 来完成这项工作。控制器是执行保持集群状态与期望状态一致的实际动作的进程。
- en: 'There are many types of controllers; here are two examples:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的控制器；以下有两个例子：
- en: Node Controllers, for ensuring that the cluster has the desired number of nodes.
    For example, when a node fails, the Node Controller is responsible for spawning
    a new node.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点控制器，用于确保集群有期望数量的节点。例如，当一个节点失败时，节点控制器负责启动一个新的节点。
- en: Replication Controllers, for ensuring each application has the desired number
    of replicas.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 副本控制器，用于确保每个应用程序都有期望数量的副本。
- en: The role of controllers for `kube-controller-manager` become clearer once we've
    explained Kubernetes Objects and deploy our first service on Kubernetes.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们解释了 Kubernetes 对象并在 Kubernetes 上部署了第一个服务，`kube-controller-manager` 的控制器角色就会变得更加清晰。
- en: Node components
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点组件
- en: Node-level tools are implement as Node Components in Kubernetes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 节点级工具在 Kubernetes 中作为节点组件实现。
- en: Container runtime
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器运行时
- en: Kubernetes runs applications and services inside containers, and it expects
    that each node in the cluster already has the respective container runtime installed;
    this can be done with a provisioning tool like Terraform.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 在容器内运行应用程序和服务，并期望集群中的每个节点都已经安装了相应的容器运行时；这可以通过像 Terraform 这样的配置工具来完成。
- en: However, it does not dictate any particular container format, as long as it
    is a format that abides by the **Open Container Initiative** (**OCI**)'s runtime
    specification ([https://github.com/opencontainers/runtime-spec](https://github.com/opencontainers/runtime-spec)).
    For instance, you can use Docker, rkt (by CoreOS), or runc (by OCI) / CRI-O (by
    Kubernetes team) as the container format and runtime.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它并不指定任何特定的容器格式，只要它是遵循**开放容器倡议**（**OCI**）的运行时规范（[https://github.com/opencontainers/runtime-spec](https://github.com/opencontainers/runtime-spec)）的格式。例如，你可以使用Docker、rkt（由CoreOS提供）或runc（由OCI提供）/
    CRI-O（由Kubernetes团队提供）作为容器格式和运行时。
- en: kubelet
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kubelet
- en: In the generic cluster architecture, our cluster needs a local configuration
    management tool like `confd` to pull updates from the Discovery Service and Global
    Configuration Stores. This ensures applications running on the node are using
    the most up-to-date parameters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在通用的集群架构中，我们的集群需要一个本地的配置管理工具，如`confd`，从发现服务和全局配置存储中拉取更新。这确保了在节点上运行的应用程序正在使用最新的参数。
- en: In Kubernetes, this is the job of `kubelet`. However, `kubelet` does a lot more
    than just updating the local configuration and restarting services. It also monitors
    each service, make sure they are running *and* healthy, and reports their status
    back to `etcd` via `kube-apiserver`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，这是`kubelet`的工作。然而，`kubelet`不仅仅只是更新本地配置和重启服务。它还监控每个服务，确保它们正在运行并且健康，并通过`kube-apiserver`将它们的状态报告回`etcd`。
- en: kube-proxy
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kube-proxy
- en: Each application (including replicas) deployed in the cluster is assigned virtual
    IPs. However, as applications are shut down and re-deployed elsewhere, their virtual
    IPs can change. We will go into more details later, but Kubernetes provides a *Services* Object
    that provides a static IP address for our end users to call. `kube-proxy` is a
    network proxy that runs on each node, and acts as a simple load balancer that
    forwards (or proxies) requests from the static IP address to the virtual IP address
    of one of the replicated applications.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中部署的每个应用程序（包括副本）都被分配了虚拟IP。然而，随着应用程序的关闭和重新部署到其他地方，它们的虚拟IP可能会改变。我们将在稍后详细介绍，但Kubernetes提供了一个**服务**对象，为我们的最终用户提供了一个静态IP地址进行调用。`kube-proxy`是运行在每个节点上的网络代理，充当一个简单的负载均衡器，将来自静态IP地址的请求转发（或代理）到复制的应用程序之一的虚拟IP地址。
- en: The role of `kube-proxy` will become more apparent when we create services.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建服务时，`kube-proxy`的作用将变得更加明显。
- en: Kubernetes objects
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes对象
- en: Now that you understand the different Components that make up the Kubernetes
    system, let's shift our attention to *Kubernetes API Objects*, or *Objects* (with
    a capital O), for short.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了构成Kubernetes系统的不同组件，让我们将注意力转向**Kubernetes API对象**，或简称**对象**（大写O）。
- en: As you already know, with Kubernetes, you don't need to interact directly with
    individual Kubernetes Components; instead, you interact with `kube-apiserver` and
    the API server will coordinate actions on your behalf.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知，在使用Kubernetes时，你不需要直接与单个Kubernetes组件交互；相反，你与`kube-apiserver`交互，API服务器将代表你协调操作。
- en: The API abstracts away raw processes and entities into abstract concepts called
    Objects. For instance, instead of asking the API server to "Run these groups of
    related containers on a node", you'd instead ask "Add this Pod to the cluster".
    Here, the group of containers is abstracted to a *Pod* Object. When we work with
    Kubernetes, all we're doing is sending requests to the Kubernetes API to manipulate
    these Objects.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: API将原始进程和实体抽象成称为对象的概念。例如，你不会要求API服务器“在节点上运行这些相关容器的组”，而是会要求“将这个Pod添加到集群”。在这里，容器组被抽象为一个**Pod**对象。当我们与Kubernetes一起工作时，我们只是在向Kubernetes
    API发送请求来操作这些对象。
- en: The four basic objects
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 四个基本对象
- en: 'There are four basic Kubernetes Objects:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes有四个基本对象：
- en: '**Pod**: A group of closely-related containers that should be managed as a
    single unit'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod**：一组紧密相关的容器，应该作为一个单一单元进行管理'
- en: '**Service**: An abstraction that proxies requests from a static IP to the dynamic,
    virtual IPs of one of the Pods running the application'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Service**：一个抽象，将来自静态IP的请求代理到运行应用程序的Pod的动态虚拟IP'
- en: '**Volume**: This provides shared storage for all containers inside the same
    Pod'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Volume**：这为同一Pod内的所有容器提供共享存储'
- en: '**Namespace**: This allows you to separate a single physical cluster into multiple
    virtual clusters'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Namespace**：这允许你将单个物理集群分割成多个虚拟集群'
- en: High-level objects
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级对象
- en: 'These basic Objects may then be built upon to form higher-level Objects:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本对象可以在此基础上构建，形成更高级的对象：
- en: '**ReplicaSet**: Manages a set of Pods so that a specified number of replicas
    are maintained within the cluster.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副本集**：管理一组Pod，以确保在集群内保持指定数量的副本'
- en: '**Deployment:** An even higher-level abstraction than ReplicaSet, a Deployment
    Object will manage a ReplicaSet to ensure that the right number of replicas are
    running, but also allows you update your configuration to update/deploy a new
    ReplicaSet.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署**：比副本集更高层次的概念，部署对象将管理副本集以确保运行正确的副本数量，同时允许您更新配置以更新/部署新的副本集。'
- en: '**StatefulSet**: Similar to a Deployment, but in a Deployment, when a Pod restarts
    (for example, due to scheduling), the old Pod is destroyed and a new Pod is created.
    Although these Pods are created using the same specification, they are different
    Pods because data from the previous Pod is not persisted. In a StatefulSet, the
    old Pod can persist its state across restarts.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态集**：类似于部署，但在部署中，当Pod重启（例如，由于调度）时，旧Pod将被销毁并创建一个新的Pod。尽管这些Pod使用相同的规范创建，但它们是不同的Pod，因为前一个Pod的数据没有持久化。在有状态集中，旧Pod可以在重启之间持久其状态。'
- en: '**DaemonSet**: Similar to ReplicaSet, but instead of specifying the number
    of replicas to run, a DaemonSet is intended to run on every node in the cluster.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DaemonSet**：类似于副本集，但与指定要运行的副本数量不同，DaemonSet旨在在集群的每个节点上运行。'
- en: '**Job**: Instead of keeping Pods running indefinitely, a Job object spawns
    new Pods to carry out tasks with a finite timeline, and ensures that the Pods
    terminates successfully after the task completes.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作业**：与无限期运行Pod不同，作业对象会生成新的Pod来执行具有有限时间线的任务，并在任务完成后确保Pod成功终止。'
- en: The aforementioned higher-level Objects rely on the four basic Objects.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 上述高级对象依赖于四个基本对象。
- en: Controllers
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制器
- en: These higher-level Objects are ran and managed by *Controllers*, which actually
    perform the actions that manipulate the Objects.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些高级对象由*控制器*运行和管理，实际上执行操作以操纵对象。
- en: For example, when we create a Deployment, a *Deployment controller* manages
    the Pods and ReplicaSet specified from the configuration. It is the controller
    who is responsible for making changes to get the actual state to the desired state.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们创建一个部署时，*部署控制器*管理配置中指定的Pod和副本集。负责将实际状态更改为所需状态的正是这个控制器。
- en: Most Objects have a corresponding Controller—a ReplicaSet object is managed
    by a ReplicaSet controller, a DaemonSet is managed by the DaemonSet controller,
    and so on.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数对象都有一个相应的控制器——副本集对象由副本集控制器管理，DaemonSet由DaemonSet控制器管理，依此类推。
- en: 'Apart from these, there are numerous other Controllers, with the most common
    ones listed as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，还有许多其他的控制器，其中最常见的列如下：
- en: '**Node Controller**: Responsible for noticing and responding when nodes go
    down'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点控制器**：负责在节点掉线时注意到并做出响应'
- en: '**Replication Controller**: Responsible for maintaining the correct number
    of pods for every replication controller object in the system'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副本控制器**：负责维护系统中每个副本控制器对象正确的Pod数量'
- en: '**Route Controller**'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**路由控制器**'
- en: '**Volume Controller**'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷控制器**'
- en: '**Service Controller**: Works on the load balancer and direct requests to the
    corresponding Pods'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务控制器**：在负载均衡器上工作，并将请求直接发送到相应的Pod'
- en: '**Endpoints Controller**: Populates the Endpoints object that links Service
    Objects and Pods'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端点控制器**：填充端点对象，该对象将服务对象和Pod连接起来'
- en: '**Service Account and Token Controllers**: Creates default accounts and API
    access tokens for new namespaces'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务账户和令牌控制器**：为新的命名空间创建默认账户和API访问令牌'
- en: These higher-level objects, and the Controllers that implements them, manage
    basic Objects on your behalf, providing additional conveniences that you'd come
    to expect when working with Cluster Management Tools. We will demonstrate the
    use of these Objects as we migrate our application to run on Kubernetes later
    in this chapter.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这些高级对象及其控制器代表您管理基本对象，提供在使用集群管理工具时您期望的额外便利。我们将在此章的后面部分演示这些对象的使用，我们将迁移应用程序以在Kubernetes上运行。
- en: Setting up the local development environment
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置本地开发环境
- en: Now that you understand the different Components of Kubernetes and the abstractions
    (Objects) that the API provides, we are ready to migrate the deployment of our
    application to using Kubernetes. In this section, we will learn the basics of
    Kubernetes by running it on our local machine. Later on in this chapter, we will
    build on what we've learned and deploy our application on multiple VPSs, managed
    by a cloud provider.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了Kubernetes的不同组件以及API提供的抽象（对象），我们准备将我们的应用程序部署迁移到使用Kubernetes。在本节中，我们将通过在我们的本地机器上运行Kubernetes来学习Kubernetes的基础知识。在本章的后面部分，我们将基于我们所学的内容，在多个VPS上部署我们的应用程序，这些VPS由云服务提供商管理。
- en: Checking hardware requirements
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查硬件要求
- en: 'To run Kubernetes locally, your machine needs to fulfill the following hardware
    requirements:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本地运行Kubernetes，您的机器需要满足以下硬件要求：
- en: Have 2 GB or more of available RAM
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少有2 GB的可用RAM
- en: Have two or more CPU cores
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有两个或更多CPU核心
- en: Swap space is disabled
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交换空间已禁用
- en: Make sure you are using a machine which satisfies those requirements.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您正在使用满足这些要求的机器。
- en: Cleaning our environment
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理我们的环境
- en: Because Kubernetes manages our application containers for us, we no longer need
    to manage our own Docker containers. Therefore, let's provide a clean working
    environment by removing any Docker containers and images related to our application.
    You can do this by running `docker ps -a` and `docker images` to see a list of
    all containers and images, and then using `docker stop <container>`, `docker rm
    <container>`, and `docker rmi <image>` to remove the relevant ones.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Kubernetes为我们管理应用程序容器，我们不再需要管理自己的Docker容器。因此，让我们通过删除与我们的应用程序相关的任何Docker容器和镜像来提供一个干净的工作环境。您可以通过运行`docker
    ps -a`和`docker images`来查看所有容器和镜像的列表，然后使用`docker stop <container>`、`docker rm <container>`和`docker
    rmi <image>`来删除相关的容器和镜像。
- en: Disabling swap memory
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 禁用交换内存
- en: 'Running Kubernetes locally requires you to turn Swap Memory off. You can do
    so by running `swapoff -a`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行Kubernetes需要您关闭交换内存。您可以通过运行`swapoff -a`来实现：
- en: '[PRE3]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Installing kubectl
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装kubectl
- en: 'Although we can interact with the Kubernetes API by sending raw HTTP requests
    using a program like `curl`, Kubernetes provides a convenient command-line client
    called `kubectl`. Let''s install it:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以通过使用像`curl`这样的程序发送原始HTTP请求与Kubernetes API交互，但Kubernetes提供了一个方便的命令行客户端，称为`kubectl`。让我们来安装它：
- en: '[PRE4]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can find alternate installation methods at [kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/)找到其他安装方法。
- en: 'You can check that the installation was successful by running `kubectl version`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行`kubectl version`来检查安装是否成功：
- en: '[PRE5]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, the `kubectl` provides autocompletion; to activate it, simply run
    the following code:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`kubectl`提供了自动补全功能；要激活它，只需运行以下代码：
- en: '[PRE6]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Installing Minikube
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Minikube
- en: Minikube is a free and open source tool by the Kubernetes team that enables
    you to easily run a single-node Kubernetes cluster locally. Without Minikube,
    you'd have to install and configure `kubectl` and `kubeadm` (used for provisioning)
    yourself.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube是由Kubernetes团队开发的一个免费开源工具，它允许您轻松地在本地运行单个节点Kubernetes集群。如果没有Minikube，您将不得不自己安装和配置`kubectl`和`kubeadm`（用于提供）。
- en: So, let's install Minikube by following the instructions found at [https://github.com/kubernetes/minikube/releases](https://github.com/kubernetes/minikube/releases).
    For Ubuntu, we can choose to either run the install script or install the `.deb` package.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们按照在[https://github.com/kubernetes/minikube/releases](https://github.com/kubernetes/minikube/releases)找到的说明来安装Minikube。对于Ubuntu，我们可以选择运行安装脚本或安装`.deb`包。
- en: 'At the time of writing this book, the `.deb` package installation is still
    experimental, so we will opt for the install script instead. For example, to install
    Minikube v0.27.0, we can run the following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，`.deb`包的安装仍然是实验性的，因此我们将选择安装脚本。例如，要安装Minikube v0.27.0，我们可以运行以下命令：
- en: '[PRE7]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can use the same command to update `minikube`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用相同的命令来更新`minikube`。
- en: Installing a Hypervisor or Docker Machine
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装虚拟机管理程序或Docker Machine
- en: Normally, Minikube runs a single-node cluster inside a virtual machine (VM),
    and this requires the installation of a hypervisor like VirtualBox or KVM. This
    requires a lot of setting up and is not great for performance.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Minikube在虚拟机（VM）内部运行单个节点集群，这需要安装一个虚拟机管理程序，如VirtualBox或KVM。这需要大量的设置，并且对性能不是很好。
- en: 'Instead, we can instruct Minikube to run Kubernetes components directly on
    our machine outside of any VMs. This requires the Docker runtime and Docker Machine
    to be installed on our machine. Docker runtime should already be installed if
    you followed our previous chapter, so let''s install Docker Machine:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以指示 Minikube 在任何 VM 之外直接在我们的机器上运行 Kubernetes 组件。这需要在我们的机器上安装 Docker 运行时和
    Docker Machine。如果你遵循了我们之前的章节，Docker 运行时应该已经安装好了，所以让我们安装 Docker Machine：
- en: '[PRE8]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After installation, run `docker-machine version` to confirm that the installation
    was successful:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，运行 `docker-machine version` 以确认安装成功：
- en: '[PRE9]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Running your cluster with Minikube on Docker is only available on Linux machines.
    If you are not using a Linux machine, go to the Minikube documentation to follow
    instructions on setting up a VM environment and using a VM driver. The rest of
    the chapter will still work for you. Just remember to use the correct `--vm-driver` flag
    when running `minikube start`.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Docker 上使用 Minikube 运行你的集群仅在 Linux 机器上可用。如果你不是使用 Linux 机器，请访问 Minikube 文档，按照设置
    VM 环境和使用 VM 驱动程序的说明操作。本章的其余部分对你仍然适用。只需记住在运行 `minikube start` 时使用正确的 `--vm-driver`
    标志。
- en: Creating our cluster
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的集群
- en: With the Kubernetes daemon (installed and ran by `minikube`) and the Kubernetes
    client (`kubectl`) installed, we can now run `minikube start` to create and start
    our cluster. We'd need to pass in `--vm-driver=none` as we are not using a VM.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 守护进程（由 `minikube` 安装和运行）和 Kubernetes 客户端（`kubectl`）安装后，我们现在可以运行
    `minikube start` 来创建和启动我们的集群。由于我们不使用 VM，我们需要传递 `--vm-driver=none`。
- en: If you are using a VM, remember to use the correct `--vm-driver` flag.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 VM，请记住使用正确的 `--vm-driver` 标志。
- en: We need to run the `minikube start` command as `root` because the `kubeadm` and `kubelet` binaries
    need to be downloaded and moved to `/usr/local/bin`, which requires root privileges.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以 `root` 身份运行 `minikube start` 命令，因为 `kubeadm` 和 `kubelet` 二进制文件需要下载并移动到
    `/usr/local/bin`，这需要 root 权限。
- en: However, this usually means that all the files created and written during the
    installation and initiation process will be owned by `root`. This makes it hard
    for a normal user to modify configuration files.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这通常意味着在安装和初始化过程中创建和写入的所有文件都将由 `root` 拥有。这使得普通用户修改配置文件变得困难。
- en: Fortunately, Kubernetes provides several environment variables that we can set
    to change this.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes 提供了几个环境变量，我们可以设置以更改此设置。
- en: Setting environment variables for the local cluster
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为本地集群设置环境变量
- en: 'Inside `.profile` (or its equivalents, such as `.bash_profile` or `.bashrc`),
    add the following lines at the end:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `.profile`（或其等效文件，如 `.bash_profile` 或 `.bashrc`）中，在末尾添加以下行：
- en: '[PRE10]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`CHANGE_MINIKUBE_NONE_USER` tells `minikube` to assign the current user as
    the owner of the configuration files. `MINIKUBE_HOME` tells `minikube` to store
    the Minikube-specific configuration on `~/.minikube`, and `KUBECONFIG` tells `minikube` to
    store the Kubernetes-specific configuration on `~/.kube/config`.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`CHANGE_MINIKUBE_NONE_USER` 告诉 `minikube` 将当前用户指定为配置文件的拥有者。`MINIKUBE_HOME`
    告诉 `minikube` 将 Minikube 特定的配置存储在 `~/.minikube` 中，而 `KUBECONFIG` 告诉 `minikube`
    将 Kubernetes 特定的配置存储在 `~/.kube/config` 中。'
- en: 'To apply these environment variables to the current shell, run the following
    command:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些环境变量应用到当前 shell 中，请运行以下命令：
- en: '[PRE11]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Lastly, we''ll need to actually create a `.kube/config` configuration file
    to our home directory:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要在主目录下实际创建一个 `.kube/config` 配置文件：
- en: '[PRE12]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Running minikube start
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行 minikube start
- en: 'With our environment variables set, we''re finally ready to run `minikube start`:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好环境变量后，我们终于可以运行 `minikube start`：
- en: '[PRE13]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This command performs several operations under the hood:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令在幕后执行多个操作：
- en: Provisions any VMs (if we're using VM). This is done internally by libmachine
    from Docker Machine.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用 VM，则配置任何 VM。这是由 Docker Machine 的 libmachine 内部完成的。
- en: Sets up configuration files and certificates under `./kube` and `./minikube`.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `./kube` 和 `./minikube` 下设置配置文件和证书。
- en: Starts up the local Kubernetes cluster using `localkube`.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `localkube` 启动本地 Kubernetes 集群。
- en: Configures `kubectl` to communicate with this cluster.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 `kubectl` 以与此集群通信。
- en: 'Since we are developing locally using the `--vm-driver=none` flag, our machine
    becomes the only node within the cluster. You can confirm this by using `kubectl` to
    see whether the node is registered with the Kubernetes API and `etcd`:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在本地开发并使用 `--vm-driver=none` 标志，我们的机器成为集群中唯一的节点。你可以通过使用 `kubectl` 来确认节点是否已注册到
    Kubernetes API 和 `etcd`：
- en: '[PRE14]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'All Master Components, such as the scheduler (`kube-scheduler`), as well as
    Node Components, such as `kubelet`, are running on the same node, inside Docker
    containers. You can check them out by running `docker ps`:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 所有主组件，如调度器（`kube-scheduler`），以及节点组件，如 `kubelet`，都在同一个节点上，在 Docker 容器内运行。你可以通过运行
    `docker ps` 来检查它们：
- en: '[PRE15]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As a last check, run `systemctl status kubelet.service` to ensure that `kubelet` is
    running as a daemon on the node:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的检查，运行 `systemctl status kubelet.service` 来确保 `kubelet` 在节点上作为守护进程运行：
- en: '[PRE16]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Everything is now set up. You can confirm this by running `minikube status` and `kubectl
    cluster-info`:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都已经设置好了。你可以通过运行 `minikube status` 和 `kubectl cluster-info` 来确认：
- en: '[PRE17]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Updating the context
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新上下文
- en: 'If you change the local network that your computer is connected to, the cluster''s
    IP may change. If you try to use `kubectl` to connect to the cluster after this
    change, you''ll see an error saying that the `network is unreachable`:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更改了计算机连接的本地网络，集群的 IP 可能会改变。如果你在此更改后尝试使用 `kubectl` 连接到集群，你会看到一个错误，表明“网络不可达”：
- en: '[PRE18]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Whenever you see an error like this, run `minikube status` to check the state
    of the cluster:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你看到这样的错误时，运行 `minikube status` 来检查集群的状态：
- en: '[PRE19]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In this case, it informs us that `kubectl` is "pointing to the stale `minikube-vm`"
    and we should run `minikube update-context` to update `kubectl` to point to the
    new cluster IP:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，它告诉我们 `kubectl` 正在“指向过时的 `minikube-vm`”，我们应该运行 `minikube update-context`
    来更新 `kubectl` 以指向新的集群 IP：
- en: '[PRE20]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After doing this, check that `kubectl` is able to communicate with the Kubernetes
    API server:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，检查 `kubectl` 是否能够与 Kubernetes API 服务器通信：
- en: '[PRE21]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Resetting the cluster
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重置集群
- en: 'Working with Kubernetes can be tricky, especially at the beginning. If you
    ever get stuck with a problem and can''t resolve it, you can use `kubeadm reset` to
    reset everything related to our Kubernetes cluster, and start again from scratch:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Kubernetes 一起工作可能会很棘手，尤其是在开始时。如果你遇到问题并且无法解决，可以使用 `kubeadm reset` 来重置与我们的 Kubernetes
    集群相关的所有内容，并从头开始：
- en: '[PRE22]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Try it now. Then, run the same `minikube start` command as before to recreate
    the cluster:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下。然后，运行之前相同的 `minikube start` 命令来重新创建集群：
- en: '[PRE23]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Creating our first Pod
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的第一个 Pod
- en: Now that we have a cluster running locally, let's deploy our Elasticsearch service
    on it. With Kubernetes, all services run inside containers. Conveniently for us,
    we are already familiar with Docker, and Kubernetes supports the Docker container
    format.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在本地运行了一个集群，让我们在上面部署我们的 Elasticsearch 服务。使用 Kubernetes，所有服务都在容器内运行。对我们来说，幸运的是，我们已经熟悉
    Docker，而 Kubernetes 支持 Docker 容器格式。
- en: However, Kubernetes doesn't actually deploy containers individually, but rather,
    it deploys *Pods*. As already mentioned, Pods are a type of basic Kubernetes Objects—abstractions
    provided by the Kubernetes API. Specifically, Pods are a logical grouping of containers
    that should be deployed and managed together. In Kubernetes, Pods are also the
    lowest-level unit that Kubernetes manages.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Kubernetes 并不是实际单独部署容器，而是部署 *Pods*。如前所述，Pods 是一种基本的 Kubernetes 对象——由 Kubernetes
    API 提供的抽象。具体来说，Pods 是应该一起部署和管理的容器的逻辑分组。在 Kubernetes 中，Pods 也是 Kubernetes 管理的最低级单元。
- en: 'Containers inside the same Pod share the following:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 同一个 Pod 内的容器共享以下内容：
- en: '**Lifecycle**: All containers inside a Pod are managed as a single unit. When
    a pod starts, all the containers inside the pod will start (this is known as a **shared
    fate**). When a Pod needs to be relocated to a different node, all containers
    inside the pod will relocate (also known as **co-scheduling**).'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生命周期**：Pod 内的所有容器作为一个单一单元进行管理。当 Pod 启动时，Pod 内的所有容器都将启动（这被称为 **共享命运**）。当 Pod
    需要迁移到不同的节点时，Pod 内的所有容器都将迁移（也称为 **协同调度**）。'
- en: '**Context**: A Pod is isolated from other Pods similar to how one Docker container
    is isolated from another Docker container. In fact, Kubernetes uses the same mechanism
    of namespaces and groups to isolate a pod.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文**：Pod 与其他 Pod 的隔离方式类似于一个 Docker 容器与另一个 Docker 容器的隔离方式。事实上，Kubernetes
    使用相同的命名空间和分组机制来隔离 Pod。'
- en: '**Shared network**: All containers within the pod share the same IP address
    and port space, and can communicate with each other using `localhost:<port>`. They
    can also communicate with each other using inter-process communications (IPC).'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享网络**：Pod 内的所有容器共享相同的 IP 地址和端口空间，并且可以使用 `localhost:<port>` 互相通信。它们也可以使用进程间通信（IPC）互相通信。'
- en: '**Shared storage**: Containers can access a shared volume that will be persisted
    outside of the container, and will survive even if the containers restart:'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享存储**：容器可以访问一个将在容器外部持久化的共享卷，即使容器重启，卷也会存在：'
- en: '![](img/0e055736-d842-4cb4-b4e0-a8c0ef3b38c1.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0e055736-d842-4cb4-b4e0-a8c0ef3b38c1.png)'
- en: Running Pods with kubelet
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用kubelet运行Pod
- en: 'Pods are run by the `kubelet` service that runs inside each node. There are
    three ways to instruct `kubelet` to run a Pod:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: Pods由运行在每个节点内部的`kubelet`服务运行。有三种方式可以指导`kubelet`运行一个Pod：
- en: By directly passing it the Pod configuration file (or a directory container
    configuration files) using `kubelet --config <path-to-pod-config>`. `kubelet` will
    poll this directory every 20 seconds for changes, and will start new containers
    or terminate containers based on any changes to the configuration file(s).
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过直接传递Pod配置文件（或包含配置文件的目录）使用`kubelet --config <path-to-pod-config>`。`kubelet`将每20秒轮询此目录以查找更改，并根据配置文件（的）任何更改启动新容器或终止容器。
- en: By specifying an HTTP endpoint which returns with the Pod configuration files.
    Like the file option, `kubelet` polls the endpoint every 20 seconds.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指定一个返回Pod配置文件的HTTP端点。就像文件选项一样，`kubelet`每20秒轮询端点。
- en: By using the Kubernetes API server to send any new pod manifests to `kubelet`.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用Kubernetes API服务器将任何新的Pod清单发送到`kubelet`。
- en: 'The first two options are not ideal because:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个选项并不理想，因为：
- en: It relies on polling, which means that the nodes cannot react quickly to changes
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它依赖于轮询，这意味着节点无法快速响应变化
- en: The Kubernetes API server is not aware of these pods, and thus cannot manage
    them
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes API服务器对这些Pod一无所知，因此无法管理它们
- en: Instead, we should use `kubelet` to communicate our intentions to the Kubernetes
    API server, and let it coordinate how to deploy our Pod.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们应该使用`kubelet`将我们的意图传达给Kubernetes API服务器，并让它协调如何部署我们的Pod。
- en: Running Pods with kubectl run
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`kubectl run`运行Pod
- en: 'First, confirm that no Elasticsearch containers are currently running on our
    machine:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确认我们的机器上没有正在运行Elasticsearch容器：
- en: '[PRE24]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now use `kubectl run` to run an image inside a Pod, and deploy it onto
    our cluster:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`kubectl run`在Pod内运行一个镜像，并将其部署到我们的集群：
- en: '[PRE25]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, when we check the Pods that have been deployed onto our cluster, we can
    see a new `elasticsearch-656d7c98c6-s6v58` Pod:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们检查已部署到我们集群的Pod时，我们可以看到一个名为`elasticsearch-656d7c98c6-s6v58`的新Pod：
- en: '[PRE26]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'It may take some time for the Pod to initiate, especially if the Docker image
    is not available locally and needs to be downloaded. Eventually, you should see
    the `READY` value become `1/1`:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: Pod的初始化可能需要一些时间，尤其是如果Docker镜像不在本地且需要下载。最终，你应该看到`READY`值变为`1/1`：
- en: '[PRE27]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Understanding high-level Kubernetes objects
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解高级Kubernetes对象
- en: 'The more observant of you might have noticed the following output after you
    ran `kubectl`:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 一些细心的你可能已经注意到了你在运行`kubectl`之后的以下输出：
- en: '[PRE28]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'When we run `kubectl run`, Kubernetes does not create a Pod directly; instead,
    Kubernetes automatically creates a Deployment Object that will manage the Pod
    for us. Therefore, the following two commands are functionally equivalent:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行`kubectl run`时，Kubernetes不会直接创建Pod；相反，Kubernetes会自动创建一个Deployment对象来为我们管理Pod。因此，以下两个命令在功能上是等效的：
- en: '[PRE29]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To demonstrate this, you can see a list of active Deployments using `kubectl
    get deployments`:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这一点，你可以使用`kubectl get deployments`查看活动Deployment的列表：
- en: '[PRE30]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The benefit of using a Deployment object is that it will manage the Pods under
    its control. This means that if the Pod fails, the Deployment will automatically
    restart the Pod for us.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Deployment对象的好处是它会管理其控制下的Pod。这意味着如果Pod失败，Deployment将自动为我们重启Pod。
- en: Generally, we should not *imperatively* instruct Kubernetes to create a low-level
    object like Pods, but *declaratively *create a higher-level Kubernetes Object
    and let Kubernetes manage the low-level Objects for us.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不应该*命令式地*指示Kubernetes创建像Pod这样的低级对象，而应该*声明式地*创建一个更高层次的Kubernetes对象，并让Kubernetes为我们管理低级对象。
- en: This applies to ReplicaSet as well—you shouldn't deploy a ReplicaSet; instead,
    deploy a Deployment Object that uses ReplicaSet under the hood.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样适用于ReplicaSet——你不应该部署ReplicaSet；相反，你应该部署一个使用ReplicaSet的Deployment对象。
- en: Declarative over imperative
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 声明式而非命令式
- en: Pods, Deployments, and ReplicaSet are examples of Kubernetes Objects. Kubernetes
    provides you with multiple approaches to run and manage them.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: Pods、Deployments和ReplicaSet是Kubernetes对象的例子。Kubernetes为你提供了多种运行和管理它们的方法。
- en: '`kubectl run`—imperative: You provide instructions through the command line
    to the Kubernetes API to carry out'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl run`—命令式：您通过命令行提供指令给 Kubernetes API 以执行'
- en: '`kubectl create`—imperative: You provide instructions, in the form of a configuration
    file, to the Kubernetes API to carry out'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl create`—命令式：您以配置文件的形式提供指令给 Kubernetes API 以执行'
- en: '`kubectl apply`—declarative: You tell the Kubernetes API the desired state
    of your cluster using configuration file(s), and Kubernetes will figure out the
    operations required to reach that state'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl apply`—声明式：您使用配置文件（s）告诉 Kubernetes API 您集群的期望状态，Kubernetes 将确定达到该状态所需的操作'
- en: '`kubectl create` is a slight improvement to `kubectl run` because the configuration
    file(s) can now be version controlled; however, it is still not ideal due to its
    imperative nature.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl create` 是对 `kubectl run` 的轻微改进，因为配置文件（s）现在可以版本控制；然而，由于其命令式本质，它仍然不是理想的。'
- en: If we use the imperative approach, we'd be manipulating the Kubernetes object(s)
    directly, and thus be responsible for monitoring all Kubernetes objects. This
    essentially defeats the point of having a Cluster Management Tool.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用命令式方法，我们将直接操作 Kubernetes 对象（s），因此需要负责监控所有 Kubernetes 对象。这实际上抵消了拥有集群管理工具的意义。
- en: The preferred pattern is to create Kubernetes Objects in a declarative manner
    using a version-controlled *manif**est *file.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 建议的模式是使用受版本控制的 *清单 *文件以声明方式创建 Kubernetes 对象。
- en: '| Management technique | Operates on | Recommended environment | Supported
    writers | Learning curve |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 管理技术 | 操作对象 | 推荐环境 | 支持的作者 | 学习曲线 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Imperative commands | Live objects | Development projects | 1+ | Lowest |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 命令式命令 | 实时对象 | 开发项目 | 1+ | 最低 |'
- en: '| Imperative object configuration | Individual files | Production projects
    | 1 | Moderate |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 命令式对象配置 | 单个文件 | 生产项目 | 1 | 中等 |'
- en: '| Declarative object configuration | Directories of files | Production projects
    | 1+ | Highest |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 声明式对象配置 | 文件目录 | 生产项目 | 1+ | 最高 |'
- en: You should also note that the imperative and declarative approaches are mutually
    exclusive—you cannot have Kubernetes manage everything based on your configuration,
    and also manipulate objects on your own. Doing so will cause Kubernetes to detect
    the changes you've made as deviations from the desired state, and will work against
    you and undo your changes. Therefore, we should consistently use the declarative
    approach.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该注意，命令式和声明式方法互斥——您不能既让 Kubernetes 根据您的配置管理一切，又自己操作对象。这样做会导致 Kubernetes 将您所做的更改检测为与期望状态的偏差，并对此产生反作用，撤销您的更改。因此，我们应该始终使用声明式方法。
- en: Deleting deployment
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除部署
- en: 'With this in mind, let''s redeploy our Elasticsearch service in a declarative
    manner, using `kubectl apply`. But first, we must delete our existing Deployment.
    We can do that with `kubectl delete`:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们以声明式的方式重新部署我们的 Elasticsearch 服务，使用 `kubectl apply`。但首先，我们必须删除现有的部署。我们可以使用 `kubectl
    delete` 来完成：
- en: '[PRE31]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Creating a deployment manifest
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建部署清单
- en: 'Now, create a new directory structure at `manifests/elasticsearch`, and in
    it, create a new file called `deployment.yaml`. Then, add the following Deployment
    configuration:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在 `manifests/elasticsearch` 目录下创建一个新的目录结构，并在其中创建一个名为 `deployment.yaml` 的新文件。然后，添加以下部署配置：
- en: '[PRE33]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The configuration file consists of several fields (fields marked `*` are required):'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件由几个字段组成（标记为 `*` 的字段是必需的）：
- en: '`apiVersion*`: The version of the API. This affects the scheme expected for
    the configuration file. The API is broken into modular API Groups. This allows
    Kubernetes to develop newer features independently. It also provides Kubernetes
    cluster administrators more fine-grained control over which API features they
    want to be enabled.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apiVersion*`: API 的版本。这会影响配置文件预期的方案。API 被划分为模块化的 API 组。这允许 Kubernetes 独立开发新功能。它还提供了
    Kubernetes 集群管理员对哪些 API 特性想要启用的更细粒度的控制。'
- en: The core Kubernetes objects are available in the *core* group (the *legacy* group),
    and you can specify this by using `v1` as the `apiVersion` property value. Deployments
    are available under the `apps` group, and we can enable this by using `apps/v1` as
    the `apiVersion` property value. Other groups include `batch` (provides the `CronJob` object), `extensions`, `scheduling.k8s.io`, `settings.k8s.io`, and
    many more.
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 核心 Kubernetes 对象位于*核心*组（*传统*组）中，您可以通过使用`v1`作为`apiVersion`属性值来指定。Deployment 位于`apps`组下，我们可以通过使用`apps/v1`作为`apiVersion`属性值来启用它。其他组包括`batch`（提供`CronJob`对象）、`extensions`、`scheduling.k8s.io`、`settings.k8s.io`以及更多。
- en: '`kind*`: The type of resource this manifest is specifying. In our case, we
    want to create a Deployment, so we should specify `Deployment` as the value. Other
    valid values for `kind` include `Pod` and `ReplicaSet`, but for reasons mentioned
    previously, you wouldn''t normally use them.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind*`：此清单指定的资源类型。在我们的情况下，我们想要创建一个 Deployment，因此我们应该将`Deployment`指定为值。`kind`的其他有效值包括`Pod`和`ReplicaSet`，但如前所述，您通常不会使用它们。'
- en: '`metadata`: Metadata about the Deployment, such as:'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata`：Deployment 的元数据，例如：'
- en: '`namespace`: With Kubernetes, you can split a single physical cluster into
    multiple *virtual clusters*. The default namespace is `default`, which is sufficient
    for our use case.'
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namespace`：在 Kubernetes 中，您可以将单个物理集群分割成多个*虚拟集群*。默认命名空间是`default`，对于我们的用例来说已经足够。'
- en: '`name`: A name to identify the Deployment within the cluster.'
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：一个名称，用于在集群中标识 Deployment。'
- en: '`spec`: Details the behavior of the Deployment, such as:'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec`：详细说明了 Deployment 的行为，例如：'
- en: '`replicas`: The number of replica Pods, specified in the `spec.template`, to
    deploy'
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replicas`：副本 Pod 的数量，指定在`spec.template`中部署'
- en: '`template`: The specification for each Pod in the ReplicaSet'
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`template`：ReplicaSet 中每个 Pod 的规范'
- en: '`metadata`: The metadata about the Pod, including a `label` property'
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata`：Pod 的元数据，包括一个`label`属性'
- en: '`spec`: The specification for each individual Pod:'
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec`：每个单独 Pod 的规范：'
- en: '`containers`: A list of containers that belong in the same Pod and should be
    managed together.'
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`containers`：属于同一 Pod 并应一起管理的容器列表。'
- en: '`selector`: The method by which the Deployment controller knows which Pods
    it should manage. We use the `matchLabels` criteria to match all Pods with the
    label `app: elasticsearch`. We then set the label at `spec.template.metadata.labels`.'
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selector`：Deployment 控制器知道它应该管理哪些 Pod 的方法。我们使用`matchLabels`标准来匹配所有带有标签`app:
    elasticsearch`的 Pod。然后我们在`spec.template.metadata.labels`中设置标签。'
- en: A note on labels
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于标签的注意事项
- en: 'In our manifest file, under `spec.template.metadata.labels`, we''ve specified
    that our Elasticsearch Pods should carry the label `app: elasticsearch`.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的清单文件中，在`spec.template.metadata.labels`下，我们指定了我们的 Elasticsearch Pod 应携带标签`app:
    elasticsearch`。'
- en: Label is one of two methods to attach arbitrary metadata to Kubernetes Objects,
    with the other being *annotations*.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是将任意元数据附加到 Kubernetes 对象的两种方法之一，另一种是*注解*。
- en: 'Both labels and annotations are implemented as key-value stores, but they serve
    different purposes:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 标签和注解都作为键值存储实现，但它们有不同的用途：
- en: 'Labels: Used to identify an Object as belonging to a certain group of similar
    Objects. In other words, it can be used to select a subset of all Objects of the
    same type. This can be used to apply Kubernetes commands to only a subset of all
    Kubernetes Objects.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：用于标识一个对象属于一组相似的物体。换句话说，它可以用来选择所有相同类型物体的子集。这可以用来仅对所有 Kubernetes 对象的子集应用 Kubernetes
    命令。
- en: 'Annotations: Any other arbitrary metadata not used to identify the Object.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注解：任何其他未用于标识对象的任意元数据。
- en: A label key consists of two components—an optional prefix, and a name—separated
    by a forward slash (`/`).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 标签键由两个组件组成——一个可选的前缀和一个名称，由正斜杠(`/`)分隔。
- en: The prefix exists as a sort of namespace, and allows third-party tools to select
    only the Objects that it is managing. For instance, the core Kubernetes components
    have a label with a prefix of `kubernetes.io/`.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀作为一种命名空间存在，允许第三方工具仅选择它所管理的对象。例如，核心 Kubernetes 组件有一个前缀为`kubernetes.io/`的标签。
- en: 'Labeled Objects can then be selected using *label selectors*, such as the one
    specified in our Deployment manifest:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*标签选择器*可以选择标记过的对象，例如在我们 Deployment 清单中指定的：
- en: '[PRE34]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This selector instructs the Deployment Controller to manage only these Pods
    and not others.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 此选择器指示 Deployment 控制器仅管理这些 Pod，而不管理其他 Pod。
- en: Running pods declaratively with kubectl apply
  id: totrans-432
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`kubectl apply`声明式地运行 Pod
- en: 'With the Deployment manifest ready, we can run `kubectl apply` to update the
    desired state of our cluster:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 部署清单就绪后，我们可以运行 `kubectl apply` 来更新集群的期望状态：
- en: '[PRE35]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will trigger a set of events:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发一系列事件：
- en: '`kubectl` sends the Deployment manifest to the Kubernetes API server (`kube-apiserver`). `kube-apiserver` will
    assign it a unique ID, and adds it on to `etcd`.'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl` 将 Deployment 清单发送到 Kubernetes API 服务器（`kube-apiserver`）。`kube-apiserver`
    将为其分配一个唯一的 ID，并将其添加到 `etcd`。'
- en: The API server will also create the corresponding ReplicaSet and Pod Objects
    and add it to `etcd`.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: API 服务器还将创建相应的 ReplicaSet 和 Pod 对象，并将其添加到 `etcd`。
- en: The scheduler watches `etcd` and notices that there are Pods that have not been
    assigned to a node. Then, the scheduler will make a decision about where to deploy
    the Pods specified by the Deployment.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调度器监视 `etcd` 并注意到有一些 Pods 尚未分配给节点。然后，调度器将决定将 Deployment 指定的 Pods 部署在哪里。
- en: Once a decision is made, it will inform `etcd` of its decision; `etcd` records
    the decision.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦做出决定，它将通知 `etcd` 其决定；`etcd` 记录该决定。
- en: The `kubelet` service running on each node will notice this change on `etcd`,
    and pull down a PodSpec – the Pod's manifest file. It will then run and manage
    a new Pod according to the PodSpec.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个节点上运行的 `kubelet` 服务将注意到 `etcd` 上的此变化，并拉取 PodSpec — Pod 的清单文件。然后，它将根据 PodSpec
    运行和管理一个新的 Pod。
- en: During the entire process, the scheduler and kubelets keep `etcd` up to date *at
    all times* via the Kubernetes API.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，调度器和 kubelets 通过 Kubernetes API 在所有时间保持 `etcd` 的更新状态。
- en: 'If we query for the state of the Deployment in the first few seconds after
    we run `kubectl apply`, we will see that `etcd` has updated its records with our
    desired state, but the Pods and containers will not be available yet:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在运行 `kubectl apply` 后的几秒钟内查询 Deployment 的状态，我们将看到 `etcd` 已经更新了其记录以反映我们的期望状态，但
    Pods 和容器尚不可用：
- en: '[PRE36]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '**What do the numbers m****ean?** `DESIRED`—the desired number of replicas; `CURRENT`—the
    current number of replicas; `UP-TO-–` the current number of replicas that has
    the most up-to-date configuration (has the copy of the latest Pod template/manifest); `AVAILABLE`—the
    number of replicas available to users'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '**这些数字代表什么？** `DESIRED` — 所需的副本数；`CURRENT` — 当前副本数；`UP-TO-–` — 具有最新配置的当前副本数（具有最新
    Pod 模板/清单的副本）；`AVAILABLE` — 可供用户使用的副本数'
- en: 'We can then run `kubectl rollout status` to be notified, in real-time, when
    each Pod is ready:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以运行 `kubectl rollout status` 来实时通知，当每个 Pod 准备就绪时：
- en: '[PRE37]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we can check the deployment again, and we can see that all three replica
    Pods are available:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以再次检查部署，并可以看到所有三个副本 Pod 都是可用的：
- en: '[PRE38]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We have now successfully switched our approach from an imperative one (using `kubectl
    run`), to a declarative one (using manifest files and `kubectl apply`).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已成功将方法从命令式（使用 `kubectl run`）切换到声明式（使用清单文件和 `kubectl apply`）。
- en: Kubernetes Object management hierarchy
  id: totrans-450
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 对象管理层次结构
- en: 'To solidify your understanding that our Deployment object is managing a ReplicaSet
    object, you can run `kubectl get rs` to get a list of ReplicaSet in the cluster:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固你对 Deployment 对象正在管理 ReplicaSet 对象的理解，你可以运行 `kubectl get rs` 来获取集群中的 ReplicaSet
    列表：
- en: '[PRE39]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The name of a ReplicaSet is automatically generated from the name of the Deployment
    object that manages it, and a hash value derived from the Pod template:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet 的名称自动从管理它的 Deployment 对象的名称生成，以及从 Pod 模板派生出的哈希值：
- en: '[PRE40]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Therefore, we know that the `elasticsearch-699c7dd54f` ReplicaSet is managed
    by the `elasticsearch` Deployment.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们知道 `elasticsearch-699c7dd54f` ReplicaSet 由 `elasticsearch` Deployment 管理。
- en: 'Using the same logic, you can run `kubectl get pods` to see a list of Pods:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的逻辑，你可以运行 `kubectl get pods` 来查看 Pod 列表：
- en: '[PRE41]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Again, the name of the Pod is the name of its controlling ReplicaSet and a unique
    hash.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，Pod 的名称是其控制 ReplicaSet 的名称和一个唯一的哈希值。
- en: You can also see that the Pods have a `pod-template-hash=2557388109` label applied
    to them. The Deployment and ReplicaSet use this label to identify which Pods it
    should be managing.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到，Pods 被应用了一个 `pod-template-hash=2557388109` 标签。Deployment 和 ReplicaSet
    使用此标签来识别它应该管理哪些 Pods。
- en: 'To find out more information about an individual Pod, you can run `kubectl
    describe pods <pod-name>`, which will produce a human-friendly output:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取有关单个 Pod 的更多信息，你可以运行 `kubectl describe pods <pod-name>`，这将生成一个易于理解的结果：
- en: '[PRE42]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Alternatively, you can get information about a Pod in a more structured JSON
    format by running `kubectl get pod <pod-name>`.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以通过运行 `kubectl get pod <pod-name>` 来以更结构化的 JSON 格式获取有关 Pod 的信息。
- en: Configuring Elasticsearch cluster
  id: totrans-463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Elasticsearch集群
- en: From the output of `kubectl describe pods` (or `kubectl get pod`), we can see
    that the IP address of the Pod named `elasticsearch-699c7dd54f-n5tmq` is listed
    as `172.17.0.5`. Since our machine is the node that this Pod runs on, we can access
    the Pod using this private IP address.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 从`kubectl describe pods`（或`kubectl get pod`）的输出中，我们可以看到名为`elasticsearch-699c7dd54f-n5tmq`的Pod的IP地址列出来为`172.17.0.5`。由于我们的机器是这个Pod运行的节点，我们可以使用这个私有IP地址访问Pod。
- en: 'The Elasticsearch API should be listening to port `9200`. Therefore, if we
    make a `GET` request to `http://172.17.0.5:9200/`, we should expect Elasticsearch
    to reply with a JSON object:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch API应该监听端口`9200`。因此，如果我们向`http://172.17.0.5:9200/`发送`GET`请求，我们应该期望Elasticsearch以JSON对象的形式回复：
- en: '[PRE43]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can do the same for Pods `elasticsearch-699c7dd54f-pft9k` and `elasticsearch-699c7dd54f-pm2wz`,
    which have the IPs `172.17.0.4` and `172.17.0.6`, respectively:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为Pod `elasticsearch-699c7dd54f-pft9k` 和 `elasticsearch-699c7dd54f-pm2wz`
    做同样的事情，它们的IP地址分别是`172.17.0.4` 和 `172.17.0.6`：
- en: '[PRE44]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Although these Elasticsearch instances are deployed inside the same Kubernetes
    cluster, they are each inside their own Elasticsearch cluster (there are currently
    three Elasticsearch clusters, running independently from each other). We know
    this because the value of `cluster_uuid` for the different Elasticsearch instances
    are all different.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些Elasticsearch实例都部署在同一个Kubernetes集群中，但它们各自位于自己的Elasticsearch集群中（目前有三个Elasticsearch集群，相互独立运行）。我们知道这一点是因为不同Elasticsearch实例的`cluster_uuid`值都不同。
- en: However, we want our Elasticsearch nodes to be able to communicate with each
    other, so that data written to one instance will be propagated to, and accessible
    from, other instances.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们希望我们的Elasticsearch节点能够相互通信，以便将写入一个实例的数据传播到其他实例，并从其他实例访问。
- en: 'Let''s confirm that this is not the case with our current setup. First, we
    will index a simple document:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认我们的当前设置不是这种情况。首先，我们将索引一个简单的文档：
- en: '[PRE45]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Already, we can see that the desired total number of shards is `2`, but we only
    have one shard.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 已经，我们可以看到期望的总分片数是`2`，但我们只有一个分片。
- en: 'We can confirm that the document is now indexed and accessible from the same
    Elasticsearch instance (running at `172.17.0.6:9200`), but not from any other
    Elasticsearch instances on our Kubernetes cluster:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确认文档现在已被索引，并且可以从同一Elasticsearch实例（在`172.17.0.6:9200`运行）访问，但不能从我们的Kubernetes集群中的任何其他Elasticsearch实例访问：
- en: '[PRE46]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Before we continue, it's important to make the distinction between an Elasticsearch
    cluster and a Kubernetes cluster. Elasticsearch is a distributed data storage
    solution, where all data is distributed among one or more shards, deployed among
    one or more nodes. An Elasticsearch cluster can be deployed on any machines, and
    is completely unrelated to a Kubernetes cluster. However, because we are deploying
    a distributed Elasticsearch services on Kubernetes, the Elasticsearch cluster
    now resides within the Kubernetes cluster.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，区分Elasticsearch集群和Kubernetes集群是很重要的。Elasticsearch是一个分布式数据存储解决方案，其中所有数据都分布在一个或多个分片中，这些分片部署在一个或多个节点上。Elasticsearch集群可以部署在任何机器上，并且与Kubernetes集群完全无关。然而，由于我们正在Kubernetes上部署分布式Elasticsearch服务，Elasticsearch集群现在位于Kubernetes集群内部。
- en: Networking for distributed databases
  id: totrans-477
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式数据库的网络
- en: Due to the ephemeral nature of Pods, the IP addresses for Pods running a particular
    service (such as Elasticsearch) may change. For instance, the scheduler may kill
    Pods running on a busy node, and redeploy it on a more available node.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Pods的短暂性，运行特定服务（如Elasticsearch）的Pod的IP地址可能会改变。例如，调度器可能会杀死在繁忙节点上运行的Pod，并将其重新部署到更可用的节点上。
- en: 'This poses a problem for our Elasticsearch deployment because:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们Elasticsearch部署提出了一个问题，因为：
- en: An Elasticsearch instance running on one Pod would not know the IP addresses
    of other instances running on other Pods
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个Pod上运行的Elasticsearch实例将不知道在其他Pod上运行的其他实例的IP地址
- en: Even if an instance obtains a list of IP addresses of other instances, this
    list will quickly become obsolete
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使一个实例获得了其他实例的IP地址列表，这个列表也会很快过时
- en: This means that Elasticsearch nodes cannot discover each other (this process
    is called **Node Discovery**), and is the reason why changes applied to one Elasticsearch
    node is not propagated to the others.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着Elasticsearch节点无法发现彼此（这个过程称为**节点发现**），这也是为什么对一个Elasticsearch节点所做的更改没有传播到其他节点的原因。
- en: To resolve this issue, we must understand how Node Discovery works in Elasticsearch,
    and then figure out how we can configure Kubernetes to enable discovery for Elasticsearch.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，我们必须了解Elasticsearch中的节点发现是如何工作的，然后找出我们如何配置Kubernetes以启用Elasticsearch的发现。
- en: Configuring Elasticsearch's Zen discovery
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Elasticsearch的Zen发现
- en: Elasticsearch provides a discovery module, called **Zen Discovery**, that allows
    different Elasticsearch nodes to find each other.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch提供了一个名为**Zen Discovery**的发现模块，允许不同的Elasticsearch节点相互发现。
- en: By default, Zen Discovery achieves this by pinging ports `9300` to `9305` on
    each loopback address (`127.0.0.0/16`), and tries to find Elasticsearch instances
    that respond to the ping. This default behavior provides auto-discovery for all
    Elasticsearch nodes running on the same machine.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Zen Discovery通过在每个环回地址（`127.0.0.0/16`）上ping端口`9300`到`9305`来实现这一点，并尝试找到响应ping的Elasticsearch实例。这种默认行为为同一台机器上运行的Elasticsearch所有节点提供了自动发现。
- en: However, if the nodes reside on different machines, they won't be available
    on the loopback addresses. Instead, they will have IP addresses that are private
    to their network. For Zen Discovery to work here, we must provide a *seed list* of
    hostnames and/or IP addresses that other Elasticsearch nodes are running on.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果节点位于不同的机器上，它们将不会在环回地址上可用。相反，它们将具有仅限于其网络的IP地址。为了使Zen Discovery在这里工作，我们必须提供其他Elasticsearch节点上运行的*种子列表*中的主机名和/或IP地址。
- en: 'This list can be specified under the `discovery.zen.ping.unicast.hosts` property
    inside Elasticsearch''s configuration file `elasticsearch.yaml`. But this is difficult
    because:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表可以在Elasticsearch配置文件`elasticsearch.yaml`中的`discovery.zen.ping.unicast.hosts`属性下指定。但这很困难，因为：
- en: The Pod IP address that these Elasticsearch nodes will be running on is very
    likely to change
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些Elasticsearch节点将要运行的Pod IP地址很可能发生变化
- en: Every time the IP changes, we'd have to go inside each container and update `elasticsearch.yaml`
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次IP更改时，我们都需要进入每个容器并更新`elasticsearch.yaml`
- en: 'Fortunately, Elasticsearch allows us to specify this setting as an environment
    variable. Therefore, we can modify our `deployment.yaml` and add an `env` property
    under `spec.template.spec.containers`:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Elasticsearch允许我们将此设置作为环境变量指定。因此，我们可以修改我们的`deployment.yaml`文件，并在`spec.template.spec.containers`下添加一个`env`属性：
- en: '[PRE47]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Attaching hostnames to Pods
  id: totrans-493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将主机名附加到Pods
- en: But what should the value of this environment variable be? Currently, the IP
    addresses of the Elasticsearch Pods is random (within a large range) and may change
    at any time.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个环境变量的值应该是多少？目前，Elasticsearch Pods的IP地址是随机的（在很大范围内），并且可能随时更改。
- en: To resolve this issue, we need to give each Pod a unique hostname that sticks
    to the Pod, even if it gets rescheduled.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，我们需要为每个Pod提供一个独特的、即使它被重新调度也会坚持的hostname。
- en: When you visit a website, you usually won't type the site's IP address directly
    onto the browser; instead, you'd use the website's domain name. Even if the host
    of the website changes to a different IP address, the website will still be reachable
    on the same domain name. This is similar to what happens when we attach a hostname
    to a Pod.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 当你访问一个网站时，你通常不会直接在浏览器中键入网站的IP地址；相反，你会使用网站的域名。即使网站的托管者更改为不同的IP地址，网站仍然可以通过相同的域名访问。这与我们将主机名附加到Pod时发生的情况类似。
- en: 'To achieve this, we need to do two things:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，我们需要做两件事：
- en: Provide each Pod with an identity using another Kubernetes Object called *StatefulSet.*
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用另一个称为*StatefulSet*的Kubernetes对象为每个Pod提供一个身份。
- en: Attach a DNS subdomain to each Pod using a *Headless Service*, where the value
    of the subdomain is based on the Pod's identity.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*Headless Service*将DNS子域附加到每个Pod，其中子域的值基于Pod的身份。
- en: Working with StatefulSets
  id: totrans-500
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与StatefulSets一起工作
- en: So far, we've been using the Deployment object to deploy our Elasticsearch service.
    The Deployment Controller will manage the ReplicaSets and Pods under its control
    and ensure that the correct numbers are running and healthy.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用Deployment对象来部署我们的Elasticsearch服务。Deployment控制器将管理其控制下的ReplicaSets和Pods，并确保运行和健康的正确数量。
- en: However, a Deployment assumes that each instance is stateless and works independently
    from each other. More importantly, it assumes that instances are fungible—that
    one instance is interchangeable with any other. **Pods managed by a Deployment
    have identical identities.**
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Deployment假设每个实例是无状态的，并且独立于彼此工作。更重要的是，它假设实例是可互换的——一个实例可以与任何其他实例互换。**由Deployment管理的Pod具有相同的身份。**
- en: This is not the case for Elasticsearch, or other distributed databases, which
    must hold stateful information that distinguishes one Elasticsearch node from
    another. These Elasticsearch nodes need individual identities so that they can
    communicate with each other to ensure data is consistent across the cluster.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Elasticsearch或其他分布式数据库来说，情况并非如此，它们必须持有区分不同Elasticsearch节点状态的信息。这些Elasticsearch节点需要具有个别身份，以便它们可以相互通信，确保集群中数据的一致性。
- en: Kubernetes provides another API Object called **StatefulSet**. Like the Deployment
    object, StatefulSet manages the running and scaling of Pods, but it also guarantees
    the ordering and uniqueness of each Pod. **Pods managed by a StatefulSet have
    individual identities.**
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了一个名为**StatefulSet**的另一个API对象。与Deployment对象类似，StatefulSet管理Pod的运行和扩展，但它还保证了每个Pod的排序和唯一性。**由StatefulSet管理的Pod具有个别身份**。
- en: 'StatefulSets are similar to Deployments in terms of definition, so we only
    need to make minimal changes to our `manifests/elasticsearch/deployment.yaml`.
    First, change the filename to `stateful-set.yaml`, and then change the `kind` property
    to StatefulSet:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets在定义上与Deployments相似，因此我们只需要对我们的`manifests/elasticsearch/deployment.yaml`进行最小改动。首先，将文件名改为`stateful-set.yaml`，然后将`kind`属性更改为StatefulSet：
- en: '[PRE48]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, all the Pods within the StatefulSet can be identified with a name. The
    name is composed of the name of the StatefulSet, as well as the *ordinal index* of
    the Pod:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，StatefulSet中的所有Pod都可以通过名称来识别。该名称由StatefulSet的名称以及Pod的*序号索引*组成：
- en: '[PRE49]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Ordinal index
  id: totrans-509
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序号索引
- en: The ordinal index, also known as **ordinal number** in set theory, is simply
    a set of numbers that are used to order a collection of objects, one after the
    other. Here, Kubernetes is using them to order, as well as identify each Pod.
    You can think of it akin to an auto-incrementing index in a SQL column.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 序号索引，也称为集合论中的**序号**，简单地说是一组用于按顺序排列一组对象的数字。在这里，Kubernetes正在使用它们进行排序以及识别每个Pod。你可以将其类比为SQL列中的自增索引。
- en: The "first" Pod in the StatefulSet has an ordinal number of `0`, the "second"
    Pod has the ordinal number of `1`, and so on.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet中的“第一个”Pod的序号是`0`，“第二个”Pod的序号是`1`，以此类推。
- en: Our StatefulSet is named `elasticsearch` and we indicated `3` replicas, so our
    Pods will now be named `elasticsearch-0`, `elasticsearch-1`, and `elasticsearch-2`.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的状态集命名为`elasticsearch`，并指示了`3`个副本，因此我们的Pod现在将被命名为`elasticsearch-0`、`elasticsearch-1`和`elasticsearch-2`。
- en: Most importantly, a Pod's cardinal index, and thus its identity, is *sticky—*if
    the Pod gets rescheduled onto another Node, it will keep this same ordinal and
    identity.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，Pod的序号索引，以及其身份，是*粘性的*——如果Pod被重新调度到另一个节点，它将保持相同的序号和身份。
- en: Working with services
  id: totrans-514
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与服务一起工作
- en: By using a StatefulSet, each Pod can now be uniquely identified. However, the
    IP of each Pod is still randomly assigned; we want our Pods to be accessible from
    a stable IP address. Kubernetes provides the *Service* Object to achieve this.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用StatefulSet，每个Pod现在都可以被唯一标识。然而，每个Pod的IP仍然是随机分配的；我们希望我们的Pod可以通过一个稳定的IP地址访问。Kubernetes提供了*Service*对象来实现这一点。
- en: The Service Object is very versatile, in that it can be used in many ways. Generally,
    it is used to provide an IP address to Kuberentes Objects like Pods.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: Service对象非常灵活，它可以以多种方式使用。通常，它用于为Kubernetes对象如Pod提供IP地址。
- en: The most common use case for a Service Object is to provide a single, stable,
    externally-accessible *Cluster IP* (also known as the *Service IP*) for a distributed
    service. When a request is made to this Cluster IP, the request will be proxied
    to one of the Pods running the service. In this use case, the Service Object is
    acting as a load balancer.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: Service对象最常见的用途是为分布式服务提供一个单一、稳定、外部可访问的*集群IP*（也称为*服务IP*）。当向此集群IP发出请求时，请求将被代理到运行该服务的某个Pod。在这种情况下，Service对象充当负载均衡器。
- en: However, that's not what we need for our Elasticsearch service. Instead of having
    a single cluster IP for the entire service, we want each Pod to have its own stable
    subdomain so that each Elasticsearch node can perform Node Discovery.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是我们为Elasticsearch服务所需要的东西。我们不想为整个服务使用单个集群IP，而是希望每个Pod都有自己的稳定子域，以便每个Elasticsearch节点都可以执行节点发现。
- en: 'For this use case, we want to use a special type of Service Object called** Headless
    Service**. As with other Kubernetes Objects, we can define a Headless Service
    using a manifest file. Create a new file at `manifests/elasticsearch/service.yaml` with
    the following content:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个用例，我们想要使用一种特殊类型的Service对象，称为**无头服务**。与其他Kubernetes对象一样，我们可以使用清单文件定义无头服务。在`manifests/elasticsearch/service.yaml`创建一个新文件，内容如下：
- en: '[PRE50]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let''s go through what some of the fields mean:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些字段代表什么：
- en: '`metadata.name`: Like other Kuberentes Objects, having a name allows us to
    identify the Service by name and not ID.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata.name`：与其他Kubernetes对象一样，有一个名称允许我们通过名称而不是ID来识别服务。'
- en: '`spec.selector`: This specifies the Pods that should be managed by the Service
    Controller. Specifically for Services, this defines the selector to select all
    the Pods that constitute a service.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec.selector`：这指定了应由服务控制器管理的Pod。对于服务而言，这定义了选择器，用于选择构成服务的所有Pod。'
- en: '`spec.clusterIP`: This specifies the Cluster IP for the Service. Here, we set
    it to `None` to indicate that we want a Headless Service.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec.clusterIP`：这指定了服务的集群IP。在这里，我们将它设置为`None`，表示我们想要一个无头服务。'
- en: '`spec.ports`: A mapping of how requests are mapped from a port to the container''s
    port.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec.ports`：请求从端口映射到容器端口的映射。'
- en: 'Let''s deploy this Service into our Kubernetes cluster:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此服务部署到我们的Kubernetes集群中：
- en: We don't need to actually run the Pods before we define a Service. A Service
    will frequently evaluate its selector to find new Pods that satisfy the selector.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义服务之前，实际上不需要运行Pod。服务通常会评估其选择器以找到满足选择器的新Pod。
- en: '[PRE51]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can run `kubectl get service` to see a list of running services:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行`kubectl get service`来查看正在运行的服务列表：
- en: '[PRE52]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Linking StatefulSet to a service
  id: totrans-531
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将StatefulSet链接到服务
- en: 'First, let''s remove our existing `elasticsearch` Deployment Object:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们删除现有的`elasticsearch`Deployment对象：
- en: '[PRE53]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, the final step is to create our StatefulSet, which provides each Pod with
    a unique identity, and link it to the Service, which gives each Pod a subdomain.
    We do this by specifying the name of the Service as the `spec.serviceName` property
    in our StatefulSet manifest file:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最后一步是创建我们的StatefulSet，它为每个Pod提供唯一的标识，并将其链接到服务，为每个Pod提供子域名。我们通过在StatefulSet清单文件中指定服务的名称作为`spec.serviceName`属性来完成此操作：
- en: '[PRE54]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, the Service linked to the StatefulSet will get a domain with the following
    structure:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，与StatefulSet关联的服务将获得以下结构的域名：
- en: '[PRE55]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Our Service's name is `elasticsearch`. By default, Kubernetes will use the `default` namespace,
    and `cluster.local` as the Cluster Domain. Therefore, the Service Domain for our
    Headless Service is `elasticsearch.default.svc.cluster.local`.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 我们服务的名称是`elasticsearch`。默认情况下，Kubernetes将使用`default`命名空间和`cluster.local`作为集群域名。因此，我们的无头服务的服务域名为`elasticsearch.default.svc.cluster.local`。
- en: 'Each Pod within the Headless Service will have its own subdomain, which has
    the following structure:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: Headless Service中的每个Pod都将有自己的子域名，其结构如下：
- en: '[PRE56]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Or if we expand this out:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们展开它：
- en: '[PRE57]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Therefore, our three replicas would have the subdomains:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的三个副本将具有以下子域名：
- en: '[PRE58]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Updating Zen Discovery configuration
  id: totrans-545
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新Zen Discovery配置
- en: 'We can now combine these subdomains into a comma-separated list, and use it
    as the value for the `discovery.zen.ping.unicast.hosts` environment variable we
    are passing into the Elasticsearch containers. Update the `manifests/elasticsearch/stateful-set.yaml` file
    to read the following:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这些子域名组合成一个以逗号分隔的列表，并将其用作传递给Elasticsearch容器的`discovery.zen.ping.unicast.hosts`环境变量的值。更新`manifests/elasticsearch/stateful-set.yaml`文件，内容如下：
- en: '[PRE59]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The final `stateful-set.yaml` should read as follows:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的`stateful-set.yaml`应如下所示：
- en: '[PRE60]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, we can add this StatefulSet to our cluster by running `kubectl apply`:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过运行`kubectl apply`将此StatefulSet添加到我们的集群中：
- en: '[PRE61]'
  id: totrans-551
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We can check that the StatefulSet is deployed by running `kubectl get statefulset`:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行`kubectl get statefulset`来检查StatefulSet是否已部署：
- en: '[PRE62]'
  id: totrans-553
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We should also check that the Pods are deployed and running:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该检查Pod是否已部署并正在运行：
- en: '[PRE63]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Note how each Pod now has a name with the structure `<statefulset-name>-<ordinal>`.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 注意现在每个Pod都有一个具有以下结构的名称`<statefulset-name>-<ordinal>`。
- en: 'Now, let''s `curl` port `9200` of each Pod and see if the Elasticsearch Nodes
    have discovered each other and have collectively formed a single cluster. We will
    be using the `-o` flag of `kubectl get pods` to extract the IP address of each
    Pod. The `-o` flag allows you to specify custom formats for your output. For example,
    you can get a table of Pod names and IPs:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们`curl`每个Pod的端口号`9200`，看看Elasticsearch节点是否已经相互发现，并共同形成一个单一集群。我们将使用`kubectl
    get pods`的`-o`标志来提取每个Pod的IP地址。`-o`标志允许你指定自定义的输出格式。例如，你可以获取Pod名称和IP地址的表格：
- en: '[PRE64]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We will run the following command to get the Cluster ID of the Elasticsearch
    node running on Pod `elasticsearch-0`:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行以下命令以获取运行在Pod`elasticsearch-0`上的Elasticsearch节点的集群ID：
- en: '[PRE65]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '`kubectl get pod elasticsearch-0 -o=jsonpath=''{.status.podIP}''` returns the
    IP address of the Pod. This is then used to `curl` the port `9200` of this IP;
    the `-s` flag silences the progress information that cURL normally prints to `stdout`.
    Lastly, the JSON returned from Elasticsearch is parsed by the `jq` tool which
    extracts the `cluster_uuid` field from the JSON object.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl get pod elasticsearch-0 -o=jsonpath=''{.status.podIP}''`返回Pod的IP地址。然后使用该IP地址的`curl`命令访问端口号`9200`；`-s`标志静默cURL通常打印到`stdout`的进度信息。最后，使用`jq`工具解析来自Elasticsearch的JSON，从中提取`cluster_uuid`字段。'
- en: 'The end result gives a Elasticsearch Cluster ID of `eeDC2IJeRN6TOBr227CStA`.
    Repeat the same step for the other Pods to confirm that they''ve successfully
    performed Node Discovery and are part of the same Elasticsearch Cluster:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果给出一个Elasticsearch集群ID为`eeDC2IJeRN6TOBr227CStA`。对其他Pod重复相同的步骤以确认它们已成功执行节点发现，并且是同一Elasticsearch集群的一部分：
- en: '[PRE66]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Perfect! Another way to confirm this is to send a `GET /cluster/state` request
    to any one of the Elasticsearch nodes:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！另一种确认方法是向任意一个Elasticsearch节点发送`GET /cluster/state`请求：
- en: '[PRE67]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Validating Zen Discovery
  id: totrans-566
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证Zen发现
- en: Once all ES nodes have been discovered, most API operations are propagated from
    one ES node to another in a peer-to-peer manner. To test this, let's repeat what
    we did previously and add a document to one Elasticsearch node and test whether
    you can access this newly indexed document from a different Elasticsearch node.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有ES节点都被发现，大多数API操作都以对等的方式从一个ES节点传播到另一个节点。为了测试这一点，让我们重复之前所做的操作，并向一个Elasticsearch节点添加一个文档，并测试是否可以从不同的Elasticsearch节点访问这个新索引的文档。
- en: 'First, let''s index a new document on the Elasticsearch node running inside
    the `elasticsearch-0` Pod:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在`elasticsearch-0` Pod内部运行的Elasticsearch节点上索引一个新的文档：
- en: '[PRE68]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, let''s try to retrieve this document from another Elasticsearch node (for
    example, the one running inside Pod `elasticsearch-1`):'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试从另一个Elasticsearch节点（例如，运行在Pod`elasticsearch-1`内部的节点）检索这个文档：
- en: '[PRE69]'
  id: totrans-571
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Try repeating the same command for `elasticsearch-0` and `elasticsearch-2` and
    confirm that you get the same result.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试对`elasticsearch-0`和`elasticsearch-2`重复相同的命令，并确认你得到相同的结果。
- en: Amazing! We've now successfully deployed our Elasticsearch service in a distributed
    manner inside our Kubernetes cluster!
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们现在已经成功地在我们的Kubernetes集群内部以分布式方式部署了我们的Elasticsearch服务！
- en: Deploying on cloud provider
  id: totrans-574
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云服务提供商上部署
- en: So far, we've deployed everything locally so that you can experiment freely
    without costs. But for us to make our service available to the wider internet,
    we need to deploy our cluster remotely, with a cloud provider.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在本地部署了一切，这样你可以免费自由地实验。但为了使我们的服务对更广泛的互联网可用，我们需要将我们的集群远程部署，使用云服务提供商。
- en: DigitalOcean supports running Kubernetes clusters, and so we will sign in to
    our DigitalOcean dashboard and create a new cluster.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean支持运行Kubernetes集群，因此我们将登录到我们的DigitalOcean仪表板并创建一个新的集群。
- en: Creating a new remote cluster
  id: totrans-577
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个新的远程集群
- en: 'After signing into your DigitalOcean account, click on the Kubernetes tab on
    your dashboard. You should be greeted with the message Get started with Kubernetes
    on DigitalOcean. Click on the Create a Cluster button and you will be shown a
    screen similar to how you configured your droplet:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 登录到你的DigitalOcean账户后，点击仪表板上的Kubernetes标签。你应该会看到“在DigitalOcean上开始使用Kubernetes”的消息。点击创建集群按钮，你将看到一个类似于你配置droplet的屏幕：
- en: '![](img/c6b273ac-87cf-4bc0-9e9a-33a9dd3b42dc.png)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c6b273ac-87cf-4bc0-9e9a-33a9dd3b42dc.png)'
- en: 'Make sure you select at least three Nodes, where each node has at least 4 GB
    of RAM. Then, click Create Cluster. You''ll be brought back to the main Kubernetes
    tab, where you can see that the cluster is being provisioned:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你选择至少三个节点，每个节点至少有4 GB的RAM。然后点击创建集群。你将被带回到主Kubernetes标签页，在那里你可以看到集群正在被配置：
- en: '![](img/0dc405da-9af2-4c4a-bb4e-cd9bed20c444.png)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dc405da-9af2-4c4a-bb4e-cd9bed20c444.png)'
- en: 'Click on the cluster and you''ll be brought to the Overview section for the
    cluster:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 点击集群，您将被带到集群的概览部分：
- en: '![](img/b6c41811-0166-4695-b74e-3b9fd128978c.png)'
  id: totrans-583
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6c41811-0166-4695-b74e-3b9fd128978c.png)'
- en: 'Click on the Download Config button to download the configuration required
    to connect with our newly-created cluster on DigitalOcean. When you open it up,
    you should see something similar to this:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下载配置按钮以下载连接到我们在DigitalOcean上创建的新集群所需的配置。当您打开它时，您应该会看到类似以下内容：
- en: '[PRE70]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Let''s examine the fields to understand why they''re there:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查字段以了解它们为什么存在：
- en: '`apiVersion`, `kind`: These fields have the same meaning as before'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apiVersion`、`kind`：这些字段与之前具有相同的意义'
- en: '`clusters`: Define different clusters to be managed by `kubectl`, including
    the cluster''s server''s hostname, and certificates required to verify the identity
    of the server'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters`：定义由`kubectl`管理的不同集群，包括集群服务器的hostname以及验证服务器身份所需的证书'
- en: '`users`: Defines user credentials that are used to connect to a cluster; this
    may be certificates and keys, or simple usernames and passwords. You can use the
    same user to connect to multiple clusters, although normally you''d create a separate
    user for each cluster.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`users`：定义用于连接到集群的用户凭据；这可能包括证书和密钥，或者简单的用户名和密码。您可以使用相同的用户连接到多个集群，尽管通常您会为每个集群创建一个单独的用户。'
- en: '`context`: A grouping of clusters, users, and namespaces.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context`：集群、用户和命名空间的分组。'
- en: It will take a few minutes for the nodes to initialize; in the meantime, let's
    see how we can configure `kubectl` to interact with our new remote cluster.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 节点初始化需要几分钟时间；在此期间，让我们看看如何配置`kubectl`以与我们的新远程集群交互。
- en: Switching contexts
  id: totrans-592
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切换上下文
- en: When using `kubectl`, a context is a grouping of clusters, user credentials,
    and namespaces. `kubectl` uses information stored in these contexts to communicate
    with any cluster.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`kubectl`时，上下文是集群、用户凭据和命名空间的分组。`kubectl`使用存储在这些上下文中的信息与任何集群通信。
- en: 'When we set up our local cluster using Minikube, it creates a default `minikube` context
    for us. We can confirm this by running `kubectl config current-context`:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用Minikube设置本地集群时，它会为我们创建一个默认的`minikube`上下文。我们可以通过运行`kubectl config current-context`来确认这一点：
- en: '[PRE71]'
  id: totrans-595
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '`kubectl` gets its configuration from the file specified by the `KUBECONFIG` environment
    variable. This was set in our `.profile` file to `$HOME/.kube/config`. If we look
    inside it, we will see that it is very similar to the config we downloaded from
    DigitalOcean:'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl`从由`KUBECONFIG`环境变量指定的文件中获取其配置。这已在我们的`.profile`文件中设置为`$HOME/.kube/config`。如果我们查看它，我们将看到它与从DigitalOcean下载的配置非常相似：'
- en: '[PRE72]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `~/.kube/config` file records the IP address of the cluster's master API
    server, the credentials for our client to interact with it, and grouped the cluster
    information and user credentials together in the context object.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '`~/.kube/config`文件记录了集群主API服务器的IP地址，我们客户端与之交互的凭据，并将集群信息和用户凭据一起分组在上下文对象中。'
- en: For `kubectl` to interact with our new DigitalOcean Hobnob cluster, we must
    update the `KUBECONFIG` environment variable to include our new configuration
    file.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使`kubectl`与我们的新DigitalOcean Hobnob集群交互，我们必须更新`KUBECONFIG`环境变量以包含我们的新配置文件。
- en: 'First, copy the configuration file from DigitalOcean to a new file:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将配置文件从DigitalOcean复制到一个新文件：
- en: '[PRE73]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now, edit your `~/.profile` file and update the `KUBECONFIG` environment variable
    to include the new configuration file:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，编辑您的`~/.profile`文件，并更新`KUBECONFIG`环境变量以包含新的配置文件：
- en: '[PRE74]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Save and source the file to make it apply to the current shell:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 保存并源文件以使其应用于当前shell：
- en: '[PRE75]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now, when we run `kubectl config view`, we will see that configuration from
    both of our files has merged together:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们运行`kubectl config view`时，我们将看到来自我们两个文件的配置已合并在一起：
- en: '[PRE76]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now, to make `kubectl` interact with our DigitalOcean cluster instead of our
    local cluster, all we have to do is change the context:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使`kubectl`与我们的DigitalOcean集群而不是本地集群交互，我们只需更改上下文：
- en: '[PRE77]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, when we run `kubectl cluster-info`, we get information about the remote
    cluster instead of the local one:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们运行`kubectl cluster-info`时，我们得到有关远程集群而不是本地集群的信息：
- en: '[PRE78]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Configuring nodes for Elasticsearch
  id: totrans-612
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Elasticsearch节点
- en: 'As mentioned in the official Elasticsearch Guide ([https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults)),
    we must configure the node running Elasticsearch in a certain way when deploying
    on production. For instance:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 如官方Elasticsearch指南（[https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults)）中所述，在生产环境中部署时，我们必须以某种方式配置运行Elasticsearch的节点。例如：
- en: 'By default, Elasticsearch uses a `mmapfs` directory to store its indices. However,
    most systems set a limit of `65530` on mmap counts, which means Elasticsearch
    may run out of memory for its indices. If we do not change this setting, you''ll
    encounter the following error when trying to run Elasticsearch:'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，Elasticsearch使用`mmapfs`目录来存储其索引。然而，大多数系统对mmap计数设置了`65530`的上限，这意味着Elasticsearch可能会因为索引而耗尽内存。如果我们不更改此设置，在尝试运行Elasticsearch时，你将遇到以下错误：
- en: '[PRE79]'
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Therefore, we should change the `vm.max_map_count` kernel setting to at least `262144`.
    This can be done temporarily by running `sysctl -w vm.max_map_count=262144`, or
    permanently by adding it to a new file at `/etc/sysctl.d/elasticsearch.conf`.
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，我们应该将`vm.max_map_count`内核设置至少改为`262144`。这可以通过运行`sysctl -w vm.max_map_count=262144`临时实现，或者通过将其添加到`/etc/sysctl.d/elasticsearch.conf`的新文件中永久实现。
- en: UNIX systems impose an upper limit on the number of open files, or more specifically,
    the number of file descriptors. If you go over that limit, the process which is
    trying to open a new file will encounter the error `Too many open files`.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UNIX系统对打开的文件数量设定了一个上限，或者更具体地说，是对文件描述符的数量设定了上限。如果你超过了这个限制，试图打开新文件的进程将会遇到错误“打开文件过多”。
- en: There's a global limit for the kernel, which is stored at `/proc/sys/fs/file-max`;
    on most systems, this is a large number like `2424348`. There's also a hard and
    soft limit per user; hard limits can only be raised by the root, while soft limits
    can be changed by the user, but never go above the hard limit. You can check the
    soft limit on file descriptors by running `ulimit -Sn`; on most systems, this
    defaults to `1024`. You can check the hard limit by running `ulimit -Hn`; the
    hard limit on my machine is `1048576`, for example.
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内核有一个全局限制，存储在`/proc/sys/fs/file-max`中；在大多数系统中，这是一个很大的数字，如`2424348`。每个用户也有硬限制和软限制；硬限制只能由root提升，而软限制可以被用户更改，但永远不会超过硬限制。你可以通过运行`ulimit
    -Sn`来检查文件描述符的软限制；在大多数系统中，默认值为`1024`。你可以通过运行`ulimit -Hn`来检查硬限制；例如，在我的机器上，硬限制是`1048576`。
- en: Elasticsearch recommends that we change the soft and hard limit to at least `65536`.
    This can be done by running `ulimit -n 65536` as `root`.
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Elasticsearch建议我们将软限制和硬限制至少设置为`65536`。这可以通过以`root`身份运行`ulimit -n 65536`来实现。
- en: We need to make these changes for every node in our cluster. But first, let's
    return to our DigitalOcean dashboard to see if our nodes have been created successfully.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为集群中的每个节点进行这些更改。但首先，让我们回到我们的DigitalOcean仪表板，看看我们的节点是否已成功创建。
- en: Running commands on multiple servers
  id: totrans-621
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在多台服务器上运行命令
- en: 'When on your DigitalOcean dashboard, click into your cluster and go to the
    Nodes tab. Here, you should see that the nodes in your cluster have successfully
    been provisioned:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在DigitalOcean仪表板上时，点击你的集群并转到节点标签页。在这里，你应该看到你的集群中的节点已经成功配置：
- en: '![](img/30628f97-6df1-4e10-ac6d-7b0f2377a2f2.png)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/30628f97-6df1-4e10-ac6d-7b0f2377a2f2.png)'
- en: 'We can confirm this from the command line by running `kubectl get nodes`:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行`kubectl get nodes`从命令行确认这一点：
- en: '[PRE80]'
  id: totrans-625
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Because our current context is set to `do-nyc1-hobnob`, it will get the nodes
    on our remote cluster, and not the local cluster.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的当前上下文设置为`do-nyc1-hobnob`，它将获取远程集群中的节点，而不是本地集群。
- en: 'Now that the nodes are ready, how do we go about updating the Elasticsearch-specific
    settings mentioned previously? The simplest way is to SSH into each server and
    run the following three sets of commands:'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 现在节点已经准备好了，我们如何更新之前提到的Elasticsearch特定设置呢？最简单的方法是通过SSH进入每个服务器并运行以下三组命令：
- en: '[PRE81]'
  id: totrans-628
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: However, this becomes unmanageable once we have a large number of servers. Instead,
    we can use a tool called `pssh`.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦我们拥有大量服务器，这就会变得难以管理。相反，我们可以使用一个名为`pssh`的工具。
- en: Using pssh
  id: totrans-630
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pssh
- en: Tools such as `pssh` (parallel **ssh**, [https://github.com/robinbowes/pssh](https://github.com/robinbowes/pssh)), `pdsh` ([https://github.com/chaos/pdsh](https://github.com/chaos/pdsh)),
    or `clusterssh` ([https://github.com/duncs/clusterssh](https://github.com/duncs/clusterssh))
    allow you to issue commands simultaneously to multiple servers at once. Out of
    all of them, `pssh` is the easiest to install.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如 `pssh`（并行 **ssh**，[https://github.com/robinbowes/pssh](https://github.com/robinbowes/pssh)）、`pdsh`（[https://github.com/chaos/pdsh](https://github.com/chaos/pdsh)）或
    `clusterssh`（[https://github.com/duncs/clusterssh](https://github.com/duncs/clusterssh)）允许您同时向多个服务器发送命令。在所有这些工具中，`pssh`
    的安装最为简单。
- en: '`pssh` is listed in the APT registry, so we can simply update the registry
    cache and install it:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '`pssh` 已列在 APT 注册表中，因此我们可以简单地更新注册表缓存并安装它：'
- en: '[PRE82]'
  id: totrans-633
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: This will actually install `pssh` under the name `parallel-ssh`; this was done
    to avoid conflict with the `putty` package.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这将使用 `parallel-ssh` 的名称安装 `pssh`；这样做是为了避免与 `putty` 软件包发生冲突。
- en: 'We can now use `kubectl get nodes` to programmatically get the IPs of all nodes
    in the cluster, and pass it to `parallel-ssh`:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `kubectl get nodes` 命令来程序化地获取集群中所有节点的 IP 地址，并将其传递给 `parallel-ssh`：
- en: '[PRE83]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We are setting the `ssh` parameter `StrictHostKeyChecking` to `no` to temporarily
    disable `ssh` checking the authenticity of the nodes. This is insecure but offers
    convenience; otherwise, you'll have to add each node's key to the `~/.ssh/known_hosts` file.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `ssh` 参数 `StrictHostKeyChecking` 设置为 `no` 以暂时禁用 `ssh` 对节点真实性的检查。这虽然不安全但提供了便利；否则，您必须将每个节点的密钥添加到
    `~/.ssh/known_hosts` 文件中。
- en: Using init containers
  id: totrans-638
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 init containers
- en: Using `pssh` is acceptable, but it's an extra command we need to remember. Ideally,
    this configuration should be recorded inside `stateful-set.yaml`, so that the
    commands only run on nodes that have our Elasticsearch StatefulSet deployed there.
    Kuberentes provides a special type of Container called Init Containers, which
    allows us to do just that.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pssh` 是可接受的，但它是一个我们需要记住的额外命令。理想情况下，此配置应记录在 `stateful-set.yaml` 文件中，这样命令只会在已部署我们的
    Elasticsearch StatefulSet 的节点上运行。Kubernetes 提供了一种特殊的容器类型，称为 Init Containers，它允许我们做到这一点。
- en: Init Containers are special Containers that run and exit before your "normal" *app
    Containers* are initiated. When multiple Init Containers are specified, they run
    in a sequential order. Also, if the previous Init Container exits with a non-zero
    exit status, then the next Init Container is not ran and the whole Pod fails.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: Init Containers 是特殊的容器，在您的“正常” *app Containers* 启动之前运行和退出。当指定多个 Init Containers
    时，它们按顺序运行。此外，如果上一个 Init Container 以非零退出状态退出，则下一个 Init Container 不会运行，整个 Pod 失败。
- en: 'This allows you to use Init Containers to:'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许您使用 Init Containers 来：
- en: Poll for the readiness of other services. For instance, if your service X depends
    on another service Y, you can use an Init Container to poll service Y, and this
    exits only when service Y responds correctly. After the Init Container exits,
    the app container can begin its initialization steps.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查其他服务的就绪状态。例如，如果您的服务 X 依赖于另一个服务 Y，您可以使用 Init Container 来检查服务 Y，并且只有当服务 Y 正确响应时才会退出。Init
    Container 退出后，应用容器可以开始其初始化步骤。
- en: Update configurations on the node running the Pod.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新运行 Pod 的节点上的配置。
- en: Therefore, we can define Init Containers inside `stateful-set.yaml`, which will
    update the configurations on nodes running our Elasticsearch StatefulSet.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以在 `stateful-set.yaml` 文件中定义 Init Containers，这将更新运行我们的 Elasticsearch StatefulSet
    的节点上的配置。
- en: 'Inside `stateful-set.yaml`, under `spec.template.spec`, add a new field called `initContainers` with
    the following settings:'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `stateful-set.yaml` 文件中，在 `spec.template.spec` 下添加一个名为 `initContainers` 的新字段，并设置以下参数：
- en: '[PRE84]'
  id: totrans-646
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: We are using the `busybox` Docker image. `busybox` is an image that "combines
    tiny versions of many common UNIX utilities into a single small executable". Essentially,
    it is an extremely lightweight (<5 MB) image that allows you to run many of the
    utility commands you'd expect from the GNU operating system.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 `busybox` Docker 镜像。`busybox` 是一个将许多常见的 UNIX 工具的微小版本组合成一个单一可执行文件的形象。本质上，它是一个极轻量级（<5
    MB）的镜像，允许您运行许多您期望从 GNU 操作系统获得的实用命令。
- en: 'The final `stateful-set.yaml` file should look like this:'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的 `stateful-set.yaml` 文件应如下所示：
- en: '[PRE85]'
  id: totrans-649
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: This configures our nodes in the same way as `pssh`, but with the added benefit
    of configuration-as-code, since it's now part of our `stateful-set.yaml`.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置以与 `pssh` 相同的方式配置我们的节点，但增加了配置为代码的好处，因为现在它已成为我们 `stateful-set.yaml` 的一部分。
- en: Running the Elasticsearch service
  id: totrans-651
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行 Elasticsearch 服务
- en: With our `stateful-set.yaml` ready, it's time to deploy our Service and StatefulSet
    onto our remote cloud cluster.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 `stateful-set.yaml` 准备就绪，现在是时候将我们的服务和有状态集部署到我们的远程云集群上了。
- en: 'At the moment, our remote cluster is not running anything apart from the Kubernetes
    Master Components:'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的远程集群除了 Kubernetes 主组件外没有运行任何其他东西：
- en: '[PRE86]'
  id: totrans-654
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The Kubernetes Master Components are automatically deployed when we create a
    new cluster using DigitalOcean.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 DigitalOcean 创建新集群时，Kubernetes 主组件会自动部署。
- en: 'To deploy our Service and StatefulSet, we will use `kubectl apply`:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署我们的服务和有状态集，我们将使用 `kubectl apply`：
- en: '[PRE87]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Give it a minute or so, and run `kubectl get all` again. You should see that
    the Pods, StatefulSet, and our headless Service are running successfully!
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 等待一分钟或更长时间，然后再次运行 `kubectl get all`。你应该会看到 Pods、有状态集和我们的无头服务正在成功运行！
- en: '[PRE88]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Validating Zen Discovery on the remote cluster
  id: totrans-660
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在远程集群上验证 Zen Discovery
- en: Let's validate that all three Elasticsearch nodes has been successfully added
    to the Elasticsearch cluster once more. We can do this by sending a `GET` request
    to `/_cluster/state?pretty` and checking the output.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次验证三个 Elasticsearch 节点是否已成功添加到 Elasticsearch 集群中。我们可以通过向 `/_cluster/state?pretty`
    发送一个 `GET` 请求并检查输出来完成此操作。
- en: But since we want to keep the database service internal, we haven't exposed
    it to an external-reachable URL, so the only way to validate this is to SSH into
    one of the VPS and query Elasticsearch using its private IP.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于我们希望将数据库服务内部化，我们没有将其暴露给外部可访问的 URL，因此验证的唯一方法是 SSH 连接到其中一个 VPS，并使用其私有 IP 查询
    Elasticsearch。
- en: However, `kubectl` provides a more convenient alternative. `kubectl` has a `port-forward` command,
    which forwards requests going into a port on `localhost` to a port on one of the
    Pods. We can use this feature to send requests from our local machine to each
    Elasticsearch instance.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`kubectl` 提供了一个更方便的替代方案。`kubectl` 有一个 `port-forward` 命令，它将进入 `localhost`
    上端口的请求转发到 Pods 上的一个端口。我们可以使用这个功能将来自我们本地机器的请求发送到每个 Elasticsearch 实例。
- en: 'Let''s suppose that we have three Pods running Elasticsearch:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们运行了三个运行 Elasticsearch 的 Pods：
- en: '[PRE89]'
  id: totrans-665
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'We can set up port forward on `elasticsearch-0` by running the following:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下命令在 `elasticsearch-0` 上设置端口转发：
- en: '[PRE90]'
  id: totrans-667
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Now, on a separate terminal, send a `GET` request to `http://localhost:9200/_cluster/state?pretty`:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在另一个终端中，向 `http://localhost:9200/_cluster/state?pretty` 发送一个 `GET` 请求：
- en: '[PRE91]'
  id: totrans-669
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: As you can see, the `node` field contains three objects, representing each of
    our Elasticsearch instances. They are all part of the cluster, with a `cluster_uuid` value
    of `ZF1t_X_XT0q5SPANvzE4Nw`. Try port forwarding to the other Pods, and confirm
    that the `cluster_uuid` for those nodes are the same.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`node` 字段包含三个对象，代表我们每个 Elasticsearch 实例。它们都是集群的一部分，具有 `cluster_uuid` 值为
    `ZF1t_X_XT0q5SPANvzE4Nw`。尝试将端口转发到其他 Pods，并确认这些节点的 `cluster_uuid` 是否相同。
- en: If everything worked, we have now successfully deployed the same Elasticsearch
    service on DigitalOcean!
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，我们现在已经在 DigitalOcean 上成功部署了相同的 Elasticsearch 服务！
- en: Persisting data
  id: totrans-672
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化数据
- en: However, we're not finished yet! Right now, if all of our Elasticsearch containers
    fail, the data stored inside them would be lost.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有完成！目前，如果所有的 Elasticsearch 容器都失败了，存储在其中的数据将会丢失。
- en: This is because containers are *ephemeral*, meaning that any file changes inside
    the container, be it addition or deletion, only persist for as long as the container
    persists; once the container is gone, the changes are gone.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为容器是 **ephemeral** 的，意味着容器内部任何文件的变化（无论是添加还是删除），都只会持续到容器存在的时间；一旦容器消失，这些变化也随之消失。
- en: This is fine for stateless applications, but our Elasticsearch service's primary
    purpose is to hold state. Therefore, similar to how we persist data using Volumes
    in Docker, we need to do the same with Kubernetes.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于无状态应用来说是可以的，但我们的 Elasticsearch 服务的主要目的是存储状态。因此，类似于我们在 Docker 中使用卷持久化数据的方式，我们同样需要在
    Kubernetes 中这样做。
- en: Introducing Kubernetes Volumes
  id: totrans-676
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Kubernetes Volumes
- en: Like Docker, Kubernetes has an API Object that's also called Volume, but there
    are several differences between the two.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Docker 一样，Kubernetes 也有一个名为 Volume 的 API 对象，但两者之间有几个区别。
- en: With both Docker and Kubernetes, the storage solution that backs a Volume can
    be a directory on the host machine, or it can be a part of a cloud solution like
    AWS.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker 和 Kubernetes，存储卷背后的存储解决方案可以是主机上的目录，也可以是云解决方案（如 AWS）的一部分。
- en: And for both Docker and Kubernetes, a Volume is an abstraction for a piece of
    storage that can be attached or mounted. The difference is which resource it is
    mounted to.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Docker和Kubernetes来说，卷是对存储抽象的一种，可以是附加或挂载的。区别在于它挂载到哪个资源上。
- en: With Docker Volumes, the storage is mounted on to a directory inside the container.
    Any changes made to the contents of this directory would be accessible by both
    the host machine and the container.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Docker卷，存储被挂载到容器内部的目录上。对目录内容所做的任何更改都可以由主机机器和容器访问。
- en: With Kubernetes Volumes, the storage is mapped to a directory inside a Pod.
    Containers within the same Pod has access to the Pod's Volume. This allows containers
    inside the same Pod to share information easily.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes卷，存储被映射到Pod内部的目录。同一Pod内的容器可以访问Pod的卷。这允许同一Pod内的容器轻松共享信息。
- en: Defining Volumes
  id: totrans-682
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义卷
- en: Volumes are created by specifying information about the Volume in the `.spec.volumes` field
    inside a Pod manifest file. The following manifest snippet will create a Volume
    of type `hostPath`, using the parameters defined in the `path` and `type` properties.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 卷是通过在Pod清单文件中的`.spec.volumes`字段中指定有关卷的信息来创建的。以下清单片段将创建一个类型为`hostPath`的卷，使用在`path`和`type`属性中定义的参数。
- en: '`hostPath` is the Volume type most similar to a Docker Volume, where the Volume
    exists as a directory from the host node''s filesystem:'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostPath`是与Docker卷最相似的卷类型，其中卷作为主机节点文件系统中的一个目录存在：'
- en: '[PRE92]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: This Volume will now be available to all containers within the Pod. However,
    the Volume is not automatically mounted onto each container. This is done by design
    because not all containers may need to use the Volume; it allows the configuration
    to be explicit rather than implicit.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 此卷现在将可供Pod内的所有容器使用。然而，卷不会自动挂载到每个容器上。这是出于设计考虑，因为并非所有容器都需要使用卷；它允许配置是明确的而不是隐含的。
- en: 'To mount the Volume to a container, specify the `volumeMounts` option in the
    container''s specification:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 将卷挂载到容器中，请在容器的规范中指定`volumeMounts`选项：
- en: '[PRE93]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: The `mountPath` specifies the directory inside the container where the Volume
    should be mounted at.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '`mountPath`指定了卷应该挂载到容器内部的目录。'
- en: 'To run this Pod, you first need to create a `/data` directory on your host
    machine and change its ownership to having a `UID` and `GID` of `1000`:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此Pod，您首先需要在您的宿主机上创建一个`/data`目录，并将其所有权更改为具有`UID`和`GID`为`1000`：
- en: '[PRE94]'
  id: totrans-691
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Now, when we run this Pod, you should be able to query it on `<pod-ip>:9200` and
    see the content written to the `/data` directory:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们运行这个Pod时，你应该能够在`<pod-ip>:9200`上查询它，并看到写入到`/data`目录的内容：
- en: '[PRE95]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Problems with manually-managed Volumes
  id: totrans-694
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动管理卷的问题
- en: 'While you can use Volumes to persists data for individual Pods, this won''t
    work for our StatefulSet. This is because each of the replica Elasticsearch nodes
    will try to write to the same files at the same time; only one will succeed, the
    others will fail. If you tried, the following hanged state is what you''ll encounter:'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以使用卷来持久化单个Pod的数据，但这对于我们的StatefulSet不起作用。这是因为每个副本Elasticsearch节点都会尝试同时写入相同的文件；只有一个会成功，其他都会失败。如果您尝试这样做，您将遇到以下挂起状态：
- en: '[PRE96]'
  id: totrans-696
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'If we use `kubectl logs` to inspect one of the failing Pods, you''ll see the
    following error message:'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`kubectl logs`检查其中一个失败的Pod，您将看到以下错误信息：
- en: '[PRE97]'
  id: totrans-698
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Basically, before an Elasticsearch instance is writing to the database files,
    it creates a `node.lock` file. Before other instances try to write to the same
    files, it will detect this `node.lock` file and abort.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，在Elasticsearch实例开始写入数据库文件之前，它会创建一个`node.lock`文件。在其他实例尝试写入相同的文件之前，它将检测到这个`node.lock`文件并中止。
- en: Apart from this issue, attaching Volumes directly to Pods is not good for another
    reason—Volumes persist data at the Pod-level, but Pods can get rescheduled to
    other Nodes. When this happens, the "old" Pod is destroyed, along with its associated
    Volume, and a new Pod is deployed on a different Node with a blank Volume.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个问题之外，直接将卷附加到Pod上还有一个不好的原因——卷在Pod级别持久化数据，但Pod可能会被重新调度到其他节点。当这种情况发生时，"旧"的Pod及其关联的卷将被销毁，并在不同的节点上部署一个新的Pod，其卷为空。
- en: Finally, scaling storage this way is also difficult—if the Pod requires more
    storage, you'll have to destroy the Pod (so it doesn't write anything to the Volume,
    create a new Volume, copy contents from the old Volume to the new, and then restart
    the Pod).
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以这种方式扩展存储也很困难——如果Pod需要更多存储，你必须销毁Pod（以确保它不对卷进行写入，创建一个新的卷，将旧卷的内容复制到新卷，然后重启Pod）。
- en: Introducing PersistentVolume (PV)
  id: totrans-702
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍持久卷（PV）
- en: To tackle these issues, Kubernetes provides the PersistentVolume (PV) object.
    PersistentVolume is a variation of the Volume Object, but the storage capability
    is associated with the entire cluster, and not with any particular Pod.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，Kubernetes提供了持久卷（PV）对象。持久卷是卷对象的变体，但存储能力与整个集群相关联，而不是与任何特定的Pod相关联。
- en: Consuming PVs with PersistentVolumeClaim (PVC)
  id: totrans-704
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用持久卷声明（PVC）消耗PV
- en: When an administrator wants a Pod to use storage provided by a PV, the administrator
    would create a new **PersistentVolumeClaim** (**PVC**) object and assign that
    PVC Object to the Pod. A PVC object is simply a request for a suitable PV to be
    bound to the PVC (and thus the Pod).
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 当管理员希望Pod使用由PV提供的存储时，管理员会创建一个新的**持久卷声明（PersistentVolumeClaim**）对象，并将该PVC对象分配给Pod。PVC对象简单来说就是请求将一个合适的PV绑定到PVC（以及Pod）。
- en: After the PVC has been registered with the Master Control Plane, the Master
    Control Plane would search for a PV that satisfies the criteria laid out in the
    PVC, and bind the two together. For instance, if the PVC requests a PV with at
    least 5 GB of storage space, the Master Control Plane will only bind that PVC
    with PVs which have at least 5 GB of space.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: PVC在主控制平面注册后，主控制平面会寻找满足PVC中规定的标准的PV，并将两者绑定在一起。例如，如果PVC请求至少有5 GB存储空间的PV，主控制平面只会将具有至少5
    GB空间的PV绑定到该PVC。
- en: After the PVC has been bound to the PV, the Pod would be able to read and write
    to the storage media backing the PV.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: PVC绑定到PV后，Pod将能够读写PV背后的存储介质。
- en: A PVC-to-PV binding is a one-to-one mapping; this means when a Pod is rescheduled,
    the same PV would be associated with the Pod.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: PVC到PV的绑定是一对一的映射；这意味着当Pod重新调度时，相同的PV将与Pod相关联。
- en: Deleting a PersistentVolumeClaim
  id: totrans-709
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除持久卷声明
- en: When a Pod no longer needs to use the PersistentVolume, the PVC can simply be
    deleted. When this happens, what happens to the data stored inside the storage
    media depends on the PersistentVolume's Reclaim Poli*cy*.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个Pod不再需要使用持久卷（PersistentVolume）时，可以直接删除PVC。当这种情况发生时，存储介质内部存储的数据将取决于持久卷的回收策略（Reclaim
    Policy）。
- en: 'If the Reclaim Policy is set to:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回收策略设置为：
- en: Retain, the PV is retained—the PVC is simply released/unbounded from the PV.
    The data in the storage media is retained.
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留，PV被保留——PVC只是从PV释放/解绑。存储介质中的数据被保留。
- en: Delete, it deletes both the PV and the data in the storage media.
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除，它将删除PV和存储介质中的数据。
- en: Deleting a PersistentVolume
  id: totrans-714
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除持久卷
- en: When you no longer need a PV, you can delete it. But because the actually data
    is stored externally, the data will remain in the storage media.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不再需要PV时，你可以删除它。但由于实际数据存储在外部，数据将保留在存储介质中。
- en: Problems with manually provisioning PersistentVolume
  id: totrans-716
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动配置持久卷的问题
- en: 'Whil a PersistentVolume decouples storage from individual Pods, it still lacks
    the automation that we''ve come to expect from Kubernetes, because the cluster
    administrator (you) must manually interact with their cloud provider to provision
    new storage spaces, and then create a PersistentVolume to represent them in Kubernetes:'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然**持久卷**将存储与单个Pod解耦，但它仍然缺乏我们从Kubernetes期望的自动化，因为集群管理员（你）必须手动与云提供商交互以配置新的存储空间，然后创建表示它们的Kubernetes中的**持久卷**：
- en: '![](img/8b4d4ff5-c69d-437c-9c3f-f996836024c1.png)'
  id: totrans-718
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8b4d4ff5-c69d-437c-9c3f-f996836024c1.png)'
- en: Furthermore, a PVC to PV binding is a one-to-one mapping; this means we must
    take care when creating our PVs. For instance, let's suppose we have 2 PVCs—one
    requesting 10 GB and the other 40 GB. If we register two PVs, each of size 25GB,
    then only the 10 GB PVC would succeed, even though there is enough storage space
    for both PVCs.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PVC到PV的绑定是一对一的映射；这意味着我们在创建PV时必须小心。例如，假设我们有两个PVC，一个请求10 GB，另一个请求40 GB。如果我们注册两个大小为25GB的PV，那么只有10
    GB的PVC会成功，尽管为两个PVC都有足够的存储空间。
- en: Dynamic volume provisioning with StorageClass
  id: totrans-720
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用StorageClass进行动态卷配置
- en: To resolve these issues, Kubernetes provides another API Object called `StorageClass`.
    With `StorageClass`, Kubernetes is able to interact with the cloud provider directly.
    This allows Kubernetes to provision new storage volumes, and create `PersistentVolumes`
    automatically.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，Kubernetes 提供了另一个 API 对象，称为 `StorageClass`。通过 `StorageClass`，Kubernetes
    能够直接与云提供商交互。这使得 Kubernetes 能够提供新的存储卷，并自动创建 `PersistentVolumes`。
- en: Basically, a `PersistentVolume` is a representation of a piece of storage, whereas
    `StorageClass` is a specification of *how* to create `PersistentVolumes` *dynamically*.
    `StorageClass` abstracts the manual processes into a set of fields you can specify
    inside a manifest file.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，`PersistentVolume` 是存储的一部分表示，而 `StorageClass` 是动态创建 `PersistentVolumes`
    的 *方式* 的规范。`StorageClass` 将手动过程抽象成可以在清单文件中指定的字段集。
- en: Defining a StorageClass
  id: totrans-723
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 StorageClass
- en: 'For example, if you want to create a `StorageClass` that will create Amazon
    EBS Volume of type General Purpose SSD (`gp2`), you''d define a StorageClass manifest
    like so:'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想创建一个将创建 Amazon EBS 通用型 SSD (`gp2`) 卷的 `StorageClass`，你可以定义一个如下所示的 `StorageClass`
    清单：
- en: '[PRE98]'
  id: totrans-725
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Here''s what each field means (required fields are marked with an asterik (`*`):'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是每个字段的意义（带星号 `*` 的字段为必填项）：
- en: '`apiVersion`: The `StorageClass` object is provided in the `storage.k8s.io` API
    group.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apiVersion`: `StorageClass` 对象位于 `storage.k8s.io` API 组中。'
- en: '`*provisioner`: The name of a *provisioner* that would prepare new storage
    spaces on-demand. For instance, if a Pod requests 10 GB of block storage from
    the `standard` StorageClass, then the `kubernetes.io/aws-ebs` provisioner will
    interact directly with AWS to create a new storage volume of at least 10 GB in
    size.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*provisioner`: 一个 *提供者* 的名称，该提供者将根据需求准备新的存储空间。例如，如果一个 Pod 从 `standard` StorageClass
    请求 10 GB 的块存储，那么 `kubernetes.io/aws-ebs` 提供者将直接与 AWS 交互，创建至少 10 GB 大小的新的存储卷。'
- en: '`*parameters`: The parameters that are passed to the provisioner so it knows
    how to provision the storage. Valid parameters depends on the provisioner. For
    example, both `kubernetes.io/aws-ebs` and `kubernetes.io/gce-pd` support the `type` parameter.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*parameters`: 传递给提供者的参数，以便它知道如何提供存储。有效的参数取决于提供者。例如，`kubernetes.io/aws-ebs`
    和 `kubernetes.io/gce-pd` 都支持 `type` 参数。'
- en: '`*reclaimPolicy`: As with `PersistentVolumes,` the Reclaim Policy determines
    whether the data written to the storage media is retained or deleted. This can
    be either `Delete` or `Retain`, but it defaults to `Delete`.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*reclaimPolicy`: 与 `PersistentVolumes` 类似，回收策略决定了写入存储介质的 数据是保留还是删除。这可以是 `Delete`
    或 `Retain`，但默认为 `Delete`。'
- en: There are many types of provisioners available. Amazon EBS provisions *Block
    storage* on AWS, but there are other types of storage, namely file and object
    storage. We will be using block storage here because it provides the lowest latency,
    and is suitable for use with our Elasticsearch database.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的提供者类型有很多。Amazon EBS 在 AWS 上提供 *块存储*，但还有其他类型的存储，即文件和对象存储。在这里我们将使用块存储，因为它提供了最低的延迟，并且适合与我们的
    Elasticsearch 数据库一起使用。
- en: Using the csi-digitalocean provisioner
  id: totrans-732
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 csi-digitalocean 提供者
- en: DigitalOcean provides its own provisioner called CSI-DigitalOcean ([https://github.com/digitalocean/csi-digitalocean](https://github.com/digitalocean/csi-digitalocean)).
    To use it, simply follow the instructions in the `README.md` file. Essentially,
    you have go to the DigitalOcean dashboard, generate a token, use that to generate
    a Secret Kubernetes Object, and then deploy the StorageClass manifest file found
    at [https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-latest-stable.yaml](https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-latest-stable.yaml).
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: DigitalOcean 提供了自己的提供者，称为 CSI-DigitalOcean ([https://github.com/digitalocean/csi-digitalocean](https://github.com/digitalocean/csi-digitalocean))。要使用它，只需遵循
    `README.md` 文件中的说明。基本上，你需要进入 DigitalOcean 控制台，生成一个令牌，使用该令牌生成一个 Secret Kubernetes
    对象，然后部署位于 [https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-latest-stable.yaml](https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-latest-stable.yaml)
    的 StorageClass 清单文件。
- en: 'However, because we are using the DigitalOcean Kubernetes platform, our Secret
    and the `csi-digitaloceanstorage` class is already configured for us, so we don''t
    actually need to do anything! You can check both the Secret and StorageClass using `kubectl
    get`:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，因为我们使用的是 DigitalOcean Kubernetes 平台，我们的 Secret 和 `csi-digitaloceanstorage`
    类已经为我们配置好了，所以我们实际上不需要做任何事情！你可以使用 `kubectl get` 命令来检查 Secret 和 StorageClass：
- en: '[PRE99]'
  id: totrans-735
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Note down the name of the StorageClass (`do-block-storage` here).
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 记下存储类（此处为 `do-block-storage`）的名称。
- en: Provisioning PersistentVolume to StatefulSet
  id: totrans-737
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 PersistentVolume 挂载到有状态集
- en: 'We now need to update our `stateful-set.yaml` file to use the `do-block-storage` StorageClass.
    Under the StatefulSet spec (`.spec`), add a new field called `volumeClaimTemplates` with
    the following value:'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要更新我们的 `stateful-set.yaml` 文件以使用 `do-block-storage` 存储类。在有状态集规范（`.spec`）下，添加一个名为
    `volumeClaimTemplates` 的新字段，其值为以下内容：
- en: '[PRE100]'
  id: totrans-739
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: This will use the `do-block-storage` class to dynamically provision 2 GB `PersistentVolumeClaim`
    Objects for any containers which mount it. The PVC is given the name `data` as
    a reference.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用 `do-block-storage` 类为任何挂载它的容器动态配置 2 GB 的 `PersistentVolumeClaim` 对象。PVC
    被命名为 `data` 作为参考。
- en: 'To mount it to a container, add a `volumeMounts` property under the `spec` property
    of the container spec:'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其挂载到容器中，请在容器规范属性的 `spec` 属性下添加一个 `volumeMounts` 属性：
- en: '[PRE101]'
  id: totrans-742
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Elasticsearch writes its data to `/usr/share/elasticsearch/data`, so that's
    the data we want to persist.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 将其数据写入 `/usr/share/elasticsearch/data`，因此这是我们想要持久化的数据。
- en: Configuring permissions on a bind-mounted directory
  id: totrans-744
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置绑定挂载目录的权限
- en: By default, Elasticsearch runs inside the Docker container as the user `elasticsearch`,
    with both a `UID` and `GID` of `1000`. Therefore, we must ensure that the data
    directory (`/usr/share/elasticsearch/data`) and all its content is going to be owned
    by this the `elasticsearch` user so that Elasticsearch can write to them.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Elasticsearch 以 `elasticsearch` 用户身份在 Docker 容器中运行，具有 `UID` 和 `GID` 都为
    `1000`。因此，我们必须确保数据目录（`/usr/share/elasticsearch/data`）及其所有内容将由这个 `elasticsearch`
    用户拥有，以便 Elasticsearch 可以写入它们。
- en: When Kubernetes bind-mounted the `PersistentVolume` to our `/usr/share/elasticsearch/data`,
    it was done using the `root` user. This means that the `/usr/share/elasticsearch/data` directory
    is no longer owned by the `elasticsearch` user.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Kubernetes 将 `PersistentVolume` 绑定到我们的 `/usr/share/elasticsearch/data` 时，这是使用
    `root` 用户完成的。这意味着 `/usr/share/elasticsearch/data` 目录不再由 `elasticsearch` 用户拥有。
- en: Therefore, to complete our deployment of Elasticsearch, we need to use an Init
    Container to fix our permissions. This can be done by running `chown -R 1000:1000
    /usr/share/elasticsearch/data` on the node as `root`.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了完成我们的 Elasticsearch 部署，我们需要使用初始化容器来修复我们的权限。这可以通过在节点上以 `root` 身份运行 `chown
    -R 1000:1000 /usr/share/elasticsearch/data` 来完成。
- en: 'Add the following entry to the `initContainers` array inside `stateful-set.yaml`:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `stateful-set.yaml` 内部的 `initContainers` 数组中添加以下条目：
- en: '[PRE102]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'This basically mounts the `PersistentVolume` and updates its owner before the
    app Container starts initializing, so that the correct permissions would already
    be set when the app container executes. To summarize, your final `elasticsearch/service.yaml` should
    look like this:'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是在应用程序容器开始初始化之前挂载 `PersistentVolume` 并更新其所有者，以确保应用程序容器执行时权限已经正确设置。总结一下，您的最终
    `elasticsearch/service.yaml` 应该看起来像这样：
- en: '[PRE103]'
  id: totrans-751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'And your final `elasticsearch/stateful-set.yaml` should look like this:'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 并且您的最终 `elasticsearch/stateful-set.yaml` 应该看起来像这样：
- en: '[PRE104]'
  id: totrans-753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Delete your existing Services, StatefulSets, and Pods and try deploying them
    from scratch:'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 删除您现有的服务、有状态集和 Pod，并尝试从头开始部署它们：
- en: '[PRE105]'
  id: totrans-755
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Visualizing Kubernetes Objects using the Web UI Dashboard
  id: totrans-756
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Web UI 仪表板可视化 Kubernetes 对象
- en: You've been introduced to *a lot* of Kubernetes in this chapter—Namespaces,
    Nodes, Pods, Deployments, ReplicaSet, StatefulSet, DaemonSet, Services, Volumes,
    PersistentVolumes, and StorageClasses. So, let's take a mini-breather before we
    continue.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经接触到了很多 Kubernetes——命名空间、节点、Pod、部署、副本集、有状态集、守护进程集、服务、卷、持久卷和存储类。所以，在我们继续之前，让我们稍作休息。
- en: So far, we've been using `kubectl` for everything. While `kubectl` is great,
    sometimes, visual tools can help. The Kubernetes project provides a convenient
    Web UI Dashboard that allows you to visualize all Kubernetes Objects easily.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用 `kubectl`。虽然 `kubectl` 很棒，但有时可视化工具可以有所帮助。Kubernetes 项目提供了一个方便的
    Web UI 仪表板，允许您轻松地可视化所有 Kubernetes 对象。
- en: The Kubernetes Web UI Dashboard is different from the DigitalOcean Dashboard.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Web UI 仪表板与 DigitalOcean 仪表板不同。
- en: Both `kubectl` and the Web UI Dashboard make calls to the `kube-apiserver`,
    but the former is a command-line tool, whereas the latter provides a web interface.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl` 和 Web UI 仪表板都会调用 `kube-apiserver`，但前者是命令行工具，而后者提供了一个网络界面。'
- en: 'By default, the Web UI Dashboard is not deployed automatically. We''d normally
    need to run the following to get an instance of the Dashboard running on our cluster:'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Web UI 仪表板不会自动部署。我们通常需要运行以下命令来在我们的集群上运行仪表板实例：
- en: '[PRE106]'
  id: totrans-762
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: However, both DigitalOcean and Minikube deploy this Dashboard feature by default,
    so we don't need to deploy anything.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DigitalOcean 和 Minikube 默认都部署了此仪表板功能，因此我们不需要部署任何内容。
- en: Launching the Web UI Dashboard locally
  id: totrans-764
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地启动 Web UI 仪表板
- en: 'To launch the Web UI Dashboard for your local cluster, run `minikube dashboard`.
    This will open a new tab on your web browser with an Overview screen like the
    following:'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动本地集群的 Web UI 仪表板，请运行 `minikube dashboard`。这将在新标签页的浏览器中打开一个类似于以下概述屏幕：
- en: '![](img/77b2e134-9ad8-4da4-b156-31fc7c23c35a.png)'
  id: totrans-766
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/77b2e134-9ad8-4da4-b156-31fc7c23c35a.png)'
- en: 'You can use the menu on the left to navigate and view other Kubernetes Objects
    currently running in our cluster:'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用左侧菜单进行导航并查看我们集群中当前运行的其他 Kubernetes 对象：
- en: '![](img/9d1dcb8b-1096-4beb-8124-f0feb93737f3.png)'
  id: totrans-768
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9d1dcb8b-1096-4beb-8124-f0feb93737f3.png)'
- en: Launching the Web UI Dashboard on a remote cluster
  id: totrans-769
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在远程集群上启动 Web UI 仪表板
- en: To access the Web UI Dashboard deployed on the remote cluster, the easier method
    is to use kubectl proxy to access the remote cluster's Kubernetes API. Simply
    run `kubectl proxy`, and the Web UI Dashboard should be available at [http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/](http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/).
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问远程集群上部署的 Web UI 仪表板，更简单的方法是使用 `kubectl proxy` 访问远程集群的 Kubernetes API。只需运行
    `kubectl proxy`，Web UI 仪表板应该可在 [http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/](http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/)
    上访问。
- en: We will continue using `kubectl` for the rest of this chapter, but feel free
    to switch to the Web UI Dashboard to get a more intuitive view of the cluster.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的剩余部分继续使用 `kubectl`，但你可以自由切换到 Web UI 仪表板以获得对集群更直观的视图。
- en: Deploying the backend API
  id: totrans-772
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署后端 API
- en: We've deployed Elasticsearch, so let's carry on with the rest of the deployment—of
    our backend API and our frontend application.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已部署了 Elasticsearch，因此让我们继续部署剩余的内容——我们的后端 API 和前端应用程序。
- en: The `elasticsearch` Docker image used in the deployment was available publicly.
    However, our backend API Docker image is not available anywhere, and thus our
    remote Kubernetes cluster won't be able to pull and deploy it.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 部署中使用的 `elasticsearch` Docker 镜像在公共领域可用。然而，我们的后端 API Docker 镜像在任何地方都不可用，因此我们的远程
    Kubernetes 集群将无法拉取和部署它。
- en: Therefore, we need to build our Docker images and make it available on a Docker
    registry. If we don't mind our image being downloaded by others, we can publish
    it on a public registry like Docker Hub. If we want to control access to our image,
    we need to deploy it on a private registry.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要构建我们的 Docker 镜像并在 Docker 仓库中使其可用。如果我们不介意我们的镜像被其他人下载，我们可以在公共仓库如 Docker
    Hub 上发布它。如果我们想控制对镜像的访问，我们需要在私有仓库上部署它。
- en: For simplicity's sake, we will simply publish our images publicly on Docker
    Hub.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将在 Docker Hub 上公开发布我们的镜像。
- en: Publishing our image to Docker Hub
  id: totrans-777
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将我们的镜像发布到 Docker Hub
- en: First, go to [https://hub.docker.com/](https://hub.docker.com/) and create an
    account with Docker Hub. Make sure to verify your email.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，前往 [https://hub.docker.com/](https://hub.docker.com/) 并在 Docker Hub 上创建一个账户。确保验证你的电子邮件。
- en: 'Then, click on Create | create Repository at the top navigation. Give the repository
    a unique name and press Create. You can set the repository to Public or Private
    as per your own preferences (at the time of writing this book, Docker Hub provides
    one free private repository):'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点击顶部导航中的“创建 | 创建仓库”。为仓库提供一个唯一的名称并按创建。你可以根据自己的偏好设置仓库为公共或私有（在撰写本书时，Docker Hub
    提供一个免费的私有仓库）：
- en: '![](img/6031fa93-6807-4158-8e27-de32a7e36ce0.png)'
  id: totrans-780
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6031fa93-6807-4158-8e27-de32a7e36ce0.png)'
- en: The repository can be identified using `<namespace>/<repository-name>`, where
    the namespace is simply your Docker Hub username. You can find it on Docker Hub
    via the URL `hub.docker.com/r/<namespace>/<repository-name>/`.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库可以使用 `<namespace>/<repository-name>` 来识别，其中命名空间只是你的 Docker Hub 用户名。你可以在 Docker
    Hub 上通过 URL `hub.docker.com/r/<namespace>/<repository-name>/` 找到它。
- en: If you have an organization, the namespace may be the name of the organization.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个组织，命名空间可能是组织的名称。
- en: 'Next, return to your terminal and login using your Docker Hub credentials.
    For example, my Docker Hub username is `d4nyll`, so I would run the following:'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，返回你的终端并使用你的Docker Hub凭据登录。例如，我的Docker Hub用户名是 `d4nyll`，所以我将运行以下命令：
- en: '[PRE107]'
  id: totrans-784
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Enter your password when prompted, and you should see a message informing you
    of your `Login Succeeded`. Next, build the image (if you haven''t already):'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示输入密码时，你应该会看到一个消息通知你 `Login Succeeded`。接下来，如果你还没有做的话，构建镜像：
- en: '[PRE108]'
  id: totrans-786
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Then, tag the local image with the full repository name on Docker Hub, as well
    as a tag that''ll appear on Docker Hub to distinguish between different versions
    of your image. The `docker tag` command you should run will have the following
    structure:'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用Docker Hub上的完整仓库名称以及一个将在Docker Hub上出现的标签来标记本地镜像，以区分不同版本的镜像。你应该运行的 `docker
    tag` 命令结构如下：
- en: '[PRE109]'
  id: totrans-788
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'In my example, I would run the following:'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的例子中，我会运行以下命令：
- en: '[PRE110]'
  id: totrans-790
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Lastly, push the image onto Docker Hub:'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将镜像推送到Docker Hub：
- en: '[PRE111]'
  id: totrans-792
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Confirm it has been successfully pushed by going to `https://hub.docker.com/r/<namespace>/<repository-name>/tags/`.
    You should see the tagged image appear there.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问 `https://hub.docker.com/r/<namespace>/<repository-name>/tags/` 确认它已被成功推送。你应该在那里看到标记的镜像。
- en: Creating a Deployment
  id: totrans-794
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建部署
- en: Since our backend API is a stateless application, we don't need to deploy a
    StatefulSet like we did with Elasticsearch. We can simply use a simpler Kubernetes
    Object that we've encountered already—Deployment.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的后端API是无状态应用程序，我们不需要像Elasticsearch那样部署一个有状态集。我们可以简单地使用我们已经遇到过的更简单的Kubernetes对象——部署。
- en: 'Create a new manifest at `manifests/backend/deployment.yaml` with the following
    content:'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `manifests/backend/deployment.yaml` 创建一个新的清单，内容如下：
- en: '[PRE112]'
  id: totrans-797
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: For the `.spec.template.spec.containers[].env` field, add in the same environment
    variables that we passed in to our Docker image from the previous chapter (the
    ones we stored inside our `.env` file). However, for the `ELASTICSEARCH_PORT` variable,
    hard-code it to `"9200"`, and for `ELASTICSEARCH_HOSTNAME`, use the value `"http://elasticsearch"`.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `.spec.template.spec.containers[].env` 字段，添加与我们在上一章中传递给Docker镜像相同的环境变量（我们存储在
    `.env` 文件中的那些）。然而，对于 `ELASTICSEARCH_PORT` 变量，将其硬编码为 `"9200"`，对于 `ELASTICSEARCH_HOSTNAME`，使用值
    `"http://elasticsearch"`。
- en: Discovering Services using kube-dns/CoreDNS
  id: totrans-799
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 kube-dns/CoreDNS 发现服务
- en: While Kubernetes Components constitutes the essential parts of the Kubernetes
    platform, there are also *add-ons*, which extend the core functionalities. They
    are optional, but some are highly recommended and are often included by default.
    In fact, the Web UI Dashboard is an example of an add-on.
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Kubernetes 组件构成了 Kubernetes 平台的基本部分，但也有扩展核心功能的 *附加组件*。它们是可选的，但其中一些强烈推荐，并且通常默认包含。实际上，Web
    UI仪表板就是一个附加组件的例子。
- en: Another such add-on is `kube-dns`, a DNS server which is used by Pods to resolve
    hostnames.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个这样的附加组件是 `kube-dns`，这是一个DNS服务器，Pod用于解析主机名。
- en: '*CoreDNS* is an alternative DNS server which reached **General Availability**
    (**GA**) status in Kubernetes 1.11, replacing the existing `kube-dns` addon as
    the default. For our purposes, they achieve the same results.'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '*CoreDNS* 是一个替代DNS服务器，在Kubernetes 1.11中达到 **GA**（通用可用性）状态，取代了现有的 `kube-dns`
    附加组件作为默认。对于我们的目的，它们达到相同的结果。'
- en: This DNS server watches the Kubernetes API for new Services. When a new Service
    is created, a DNS record is created that would route the name `<service-name>.<service-namespace>` to
    the Service's Cluster IP. Or, in the case of a Headless Service (without a cluster
    IP), a list of IPs of the Pods that constitutes the Headless Service.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 此DNS服务器监视Kubernetes API以查找新的服务。当创建新的服务时，会创建一个DNS记录，将名称 `<service-name>.<service-namespace>`
    路由到服务的集群IP。或者，在无头服务（没有集群IP）的情况下，是无头服务构成Pod的IP列表。
- en: This is why we can use `"http://elasticsearch"` as the value of the `ELASTICSEARCH_HOSTNAME` environment
    variable, because the DNS server will resolve it, even if the Service changes
    its IP.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们可以将 `"http://elasticsearch"` 作为 `ELASTICSEARCH_HOSTNAME` 环境变量的值，因为DNS服务器会解析它，即使服务更改了其IP。
- en: Running Our backend Deployment
  id: totrans-805
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行我们的后端部署
- en: 'With our Deployment manifest ready, let''s deploy it onto our remote cluster.
    You should be familiar with the drill by now—simply run `kubectl apply`:'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的部署清单准备好了，现在让我们将其部署到我们的远程集群。你现在应该熟悉这个过程了——只需运行 `kubectl apply`：
- en: '[PRE113]'
  id: totrans-807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Check the status of the Deployment using `kubectl get all`:'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl get all` 检查部署的状态：
- en: '[PRE114]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'You can also check the logs for the backend Pods. If you get back a message
    saying the server is listening on port `8080`, the deployment was successful:'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以检查后端 Pod 的日志。如果您收到一条消息说服务器正在监听端口 `8080`，则部署成功：
- en: '[PRE115]'
  id: totrans-811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: Creating a backend Service
  id: totrans-812
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建后端服务
- en: Next, we should deploy a Service that sits in front of the backend Pods. As
    a recap, every `backend` Pod inside the `backend` Deployment will have its own
    IP address, but these addresses can change as Pods are destroyed and created.
    Having a Service that sits in front of these Pods allow other parts of the application
    to access these backend Pods in a consistent manner.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该部署一个位于后端 Pod 前面的服务。作为回顾，`backend` 部署内部的每个 `backend` Pod 都将有自己的 IP 地址，但这些地址可能会随着
    Pod 的销毁和创建而改变。在 Pod 前面有一个服务可以允许应用程序的其他部分以一致的方式访问这些后端 Pod。
- en: 'Create a new manifest file at `./manifests/backend/service.yaml` with the following
    content:'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `./manifests/backend/service.yaml` 创建一个新的清单文件，内容如下：
- en: '[PRE116]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'And deploy it using `kubectl apply`:'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl apply` 部署它：
- en: '[PRE117]'
  id: totrans-817
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Our `backend` Service is now reachable through its Cluster IP (`10.32.187.38`, in
    our example). However, that is a private IP address, accessible only within the
    cluster. We want our API to be available externally – to the wider internet. To
    do this, we need to look at one final Kubernetes Object—Ingress.
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `backend` 服务现在可以通过其集群 IP (`10.32.187.38`，在我们的示例中）访问。然而，这是一个私有 IP 地址，只能在集群内部访问。我们希望我们的
    API 对外部——更广泛的互联网——可用。为此，我们需要查看一个最终的 Kubernetes 对象——Ingress。
- en: Exposing services through Ingress
  id: totrans-819
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 Ingress 暴露服务
- en: An Ingress is a Kubernetes Object that sits at the edge of the cluster and manages
    external access to Services inside the cluster.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 是一个位于集群边缘的 Kubernetes 对象，用于管理集群内部服务的外部访问。
- en: The Ingress holds a set of rules that takes inbound requests as parameters and
    routes them to the relevant Service. It can be used for routing, load balancing,
    terminate SSL, and more.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 包含一组规则，这些规则将传入请求作为参数，并将它们路由到相关的服务。它可以用于路由、负载均衡、终止 SSL 等。
- en: Deploying the NGINX Ingress Controller
  id: totrans-822
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 NGINX Ingress 控制器
- en: An Ingress Object requires a Controller to enact it. Unlike other Kubernetes
    controllers, which are part of the `kube-controller-manager` binary, the Ingress
    controller is not. Apart from the GCE/Google Kubernetes Engine, the Ingress controller
    needs to be deployed separately as a Pod.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 对象需要一个控制器来执行它。与其他 Kubernetes 控制器不同，这些控制器是 `kube-controller-manager`
    二进制文件的一部分，而 Ingress 控制器不是。除了 GCE/Google Kubernetes Engine 之外，Ingress 控制器需要作为 Pod
    独立部署。
- en: 'The most popular Ingress controller is the NGINX controller ([https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx)),
    which is officially supported by Kubernetes and NGINX. Deploy it by running `kubectl
    apply`:'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的 Ingress 控制器是 NGINX 控制器 ([https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx))，它由
    Kubernetes 和 NGINX 正式支持。通过运行 `kubectl apply` 来部署它：
- en: '[PRE118]'
  id: totrans-825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'The `mandatory.yaml` file contains a Deployment manifest that deploys the NGINX
    Ingress controller as a Pod with the label `app: ingress-nginx`.'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: '`mandatory.yaml` 文件包含一个部署清单，该清单将 NGINX Ingress 控制器作为带有标签 `app: ingress-nginx`
    的 Pod 部署。'
- en: 'The `cloud-generic.yaml` file contains a Service manifest of type `LoadBalancer`,
    with a label selector for the label `app: ingress-nginx`. When deployed, this
    will interact with the DigitalOcean API to spin up an L4 network load balancer
    (note that this load balaner is *outside* our Kubernetes cluster):'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: '`cloud-generic.yaml` 文件包含一个类型为 `LoadBalancer` 的服务清单，用于选择标签 `app: ingress-nginx`。当部署时，这将与
    DigitalOcean API 交互，启动一个 L4 网络负载均衡器（注意，这个负载均衡器是**外部**我们的 Kubernetes 集群）：'
- en: '![](img/dfe922fc-ec13-44db-b9f3-5357ae2ddaf0.png)'
  id: totrans-828
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dfe922fc-ec13-44db-b9f3-5357ae2ddaf0.png)'
- en: 'The L4 load balancer will provide an external IP address for our end users
    to hit. The Kubernetes service controller will automatically populate the L4 load
    balancer with entries for our Pods, and set up health checks and firewalls. The
    end result is that any requests that hits the L4 load balancer will be forwarded
    to Pods that matches the Service''s selector, which, in our case, is the Ingress
    controller Pod:'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: L4 负载均衡器将为我们的最终用户提供一个外部 IP 地址。Kubernetes 服务控制器将自动将我们的 Pod 条目填充到 L4 负载均衡器中，并设置健康检查和防火墙。最终结果是，任何击中
    L4 负载均衡器的请求都将转发到匹配服务选择器的 Pod，在我们的例子中是 Ingress 控制器 Pod：
- en: '![](img/6c12f1ca-0d08-448d-92b3-6b1fb5f087a3.png)'
  id: totrans-830
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6c12f1ca-0d08-448d-92b3-6b1fb5f087a3.png)'
- en: 'When the request reaches the Ingress controller Pod, it can then examine the
    host and path of the request, and proxy the request to the relevant Service:'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 当请求到达 Ingress 控制器 Pod 时，它可以检查请求的主机和路径，并将请求代理到相关的服务：
- en: '![](img/9a1f616c-c407-4cd1-8c34-1e9d6673ea2b.png)'
  id: totrans-832
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9a1f616c-c407-4cd1-8c34-1e9d6673ea2b.png)'
- en: 'Give it a minute or two, and then check that the controller is created successfully
    by running `kubectl get pods`, specifying `ingress-nginx` as the namespace:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 等一两分钟，然后通过运行 `kubectl get pods` 并指定 `ingress-nginx` 作为命名空间来检查控制器是否成功创建：
- en: '[PRE119]'
  id: totrans-834
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: If you see a Pod named `nginx-ingress-controller-XXX` with the status `Running`,
    you're ready to go!
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到一个名为 `nginx-ingress-controller-XXX` 的 Pod 状态为 `Running`，你就准备就绪了！
- en: Deploying the Ingress resource
  id: totrans-836
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Ingress 资源
- en: 'Now that our Ingress controller is running, we are ready to deploy our Ingress
    resource. Create a new manifest file at `./manifests/backend/ingress.yaml` with
    the following content:'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的 Ingress 控制器正在运行，我们已经准备好部署我们的 Ingress 资源。在 `./manifests/backend/ingress.yaml`
    创建一个新的清单文件，内容如下：
- en: '[PRE120]'
  id: totrans-838
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: The important parts lies at `.spec.rules`. This is a list of rules that checks
    the request's host and path, and if it matches, proxies the request to a specified
    Service.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的部分在于 `.spec.rules`。这是一个检查请求的主机和路径的规则列表，如果匹配，则将请求代理到指定的服务。
- en: In our example, we are matching any requests for the domain `api.hobnob.social` to
    our `backend` service, on port `8080`; likewise, we'll also forward requests for
    the host `docs.hobnob.social` to our `backend` Service, but on the `8100` port
    instead.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将域名 `api.hobnob.social` 的任何请求匹配到我们的 `backend` 服务，端口为 `8080`；同样，我们也将主机
    `docs.hobnob.social` 的请求转发到我们的 `backend` 服务，但端口为 `8100`。
- en: 'Now, deploy it with `kubectl apply`, and then wait for the address of the L4
    load balancer to appear in the `kubectl describe output`:'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用 `kubectl apply` 部署它，然后等待 L4 负载均衡器的地址出现在 `kubectl describe` 输出中：
- en: '[PRE121]'
  id: totrans-842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: This means any requests with the hosts `api.hobnob.social` and `docs.hobnob.social` can
    now reach our distributed service!
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着任何带有主机 `api.hobnob.social` 和 `docs.hobnob.social` 的请求现在都可以到达我们的分布式服务！
- en: Updating DNS records
  id: totrans-844
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新 DNS 记录
- en: 'Now that the `api.hobnob.social` and `docs.hobnob.social` domains can both
    be accessed through the load balancer, it''s time to update our DNS records to
    point those subdomains to the load balancer''s external IP address:'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于 `api.hobnob.social` 和 `docs.hobnob.social` 域名都可以通过负载均衡器访问，是时候更新我们的 DNS
    记录，将这些子域名指向负载均衡器的公网 IP 地址了：
- en: '![](img/4a8987e9-3be9-4c40-901d-0a611c729ccb.png)'
  id: totrans-846
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4a8987e9-3be9-4c40-901d-0a611c729ccb.png)'
- en: After the DNS records have been propagated, go to a browser and try `docs.hobnob.social`.
    You should be able to see the Swagger UI documentation!
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DNS 记录传播之后，打开浏览器并尝试访问 `docs.hobnob.social`。你应该能看到 Swagger UI 文档！
- en: Summary
  id: totrans-848
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have successfully deployed our Elasticsearch instance and
    backend API on Kubernetes. We have learned the roles of each Component and the
    types of Objects each manages.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经成功地在 Kubernetes 上部署了我们的 Elasticsearch 实例和后端 API。我们学习了每个组件的角色以及每个组件管理的对象类型。
- en: You've come a long way since we started! To finish it off, let's see if you
    can use what you've learned to deploy the frontend application on Kubernetes on
    your own.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们开始到现在，你已经走了很长的路！为了完成它，让我们看看你是否能利用你所学的知识自己部署前端应用程序到 Kubernetes 上。
