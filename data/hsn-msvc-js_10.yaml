- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Monitoring Microservices
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控微服务
- en: '**Microservices** have become a core architectural approach for building scalable
    and flexible applications, but ensuring their health and performance is just as
    important as their functionality. Without proper visibility, identifying issues
    in such a distributed system can be like trying to find a needle in a haystack.
    Think of monitoring and logging as placing cameras and sensors in different parts
    of a bustling city, where each microservice is a shop. These tools help you observe
    how the system is functioning, capture key events, and detect any unusual behavior.
    By establishing robust logging and monitoring practices, you can quickly pinpoint
    problems and keep your microservices running smoothly.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**微服务**已成为构建可扩展和灵活应用程序的核心架构方法，但确保它们的健康和性能与它们的功能一样重要。没有适当的可见性，在如此分布式的系统中识别问题就像在海绵中寻找针一样。想象一下，将监控和日志记录视为在繁忙城市的不同部分放置摄像头和传感器，其中每个微服务都是一个商店。这些工具帮助您观察系统如何运行，捕捉关键事件，并检测任何异常行为。通过建立强大的日志和监控实践，您可以快速定位问题，并确保微服务平稳运行。'
- en: 'This chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Importance of observability
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可观察性的重要性
- en: Introduction to logging
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录简介
- en: Centralized logging with the **Elasticsearch, Logstash, and Kibana** (**ELK**)
    stack
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Elasticsearch、Logstash和Kibana**（**ELK**）堆栈进行集中式日志记录
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow along with this chapter, we need to have an IDE installed (we prefer
    Visual Studio Code), Postman, Docker, and a browser of your choice.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章内容，我们需要安装一个IDE（我们更喜欢Visual Studio Code），Postman，Docker以及您选择的浏览器。
- en: It is preferable to download our repository from [https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10)
    to easily follow our code snippets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您从[https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10)下载我们的存储库，以便轻松跟随我们的代码片段。
- en: Importance of observability
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可观察性的重要性
- en: 'In the world of software, particularly microservices, **observability** is
    crucial. It allows us to gain deep insights into how our system functions by analyzing
    its outputs. Observability is an important concept in monitoring and understanding
    systems. It refers to the ability to gain insight into the internal workings of
    a system by examining its outputs. Let’s try to understand the building blocks
    of it:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件的世界里，尤其是在微服务领域，**可观察性**至关重要。它通过分析其输出，使我们能够深入了解系统的工作方式。可观察性是监控和理解系统的重要概念。它指的是通过检查系统的输出而获得系统内部工作情况洞察的能力。让我们尝试理解其构建块：
- en: '**Logs**: Logs are detailed records of events that happen within a system.
    They provide a history of what has occurred, including errors, warnings, and informational
    messages. Logs can help in identifying and diagnosing issues by showing a step-by-step
    account of system activities.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志**：日志是系统内部发生事件的详细记录。它们提供了发生事件的记录，包括错误、警告和信息性消息。日志可以通过显示系统活动的逐步记录来帮助识别和诊断问题。'
- en: '**Metrics**: Metrics are numerical values that represent the performance and
    behavior of a system. They can include data such as CPU usage, memory consumption,
    request rates, and error rates. Metrics provide a quantitative measure of the
    system’s health and performance.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标**：指标是表示系统性能和行为数值。它们可以包括CPU使用率、内存消耗、请求速率和错误率等数据。指标提供了系统健康和性能的定量度量。'
- en: '**Alerts**: Alerts are notifications that are triggered when metrics reach
    certain thresholds. They are used to inform administrators or operators about
    potential problems or abnormal behavior in real time, allowing for quick responses
    to issues.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报**：当指标达到特定阈值时，会触发警报。它们用于实时通知管理员或操作员潜在问题或异常行为，以便快速响应问题。'
- en: '**Traces**: Traces provide a detailed view of the flow of requests through
    a system. They show how requests move from one component to another, highlighting
    the interactions and dependencies between different parts of the system. Traces
    help in understanding the path of a request and identifying bottlenecks or points
    of failure.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪**：跟踪提供了对系统请求流程的详细视图。它们显示了请求如何从一个组件移动到另一个组件，突出了系统不同部分之间的交互和依赖关系。跟踪有助于理解请求的路径，并识别瓶颈或故障点。'
- en: Observability helps in understanding what is happening inside a system by using
    logs, metrics, alerts, and traces. Logs give detailed records of events, metrics
    provide numerical data on performance, alerts notify of potential problems, and
    traces show the flow of requests. Together, these outputs offer a comprehensive
    view of a system’s state, aiding in monitoring, troubleshooting, and optimizing
    performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性通过使用日志、指标、警报和跟踪来帮助理解系统内部正在发生的事情。日志提供了事件的详细记录，指标提供了性能的数值数据，警报通知潜在问题，而跟踪显示了请求的流程。这些输出共同提供了一个系统的全面视图，有助于监控、故障排除和优化性能。
- en: Now that we’ve covered the concept, let’s dive into the world of logging in
    microservices.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了概念，让我们深入探讨微服务日志记录的世界。
- en: Introduction to logging
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录简介
- en: Have you ever driven a car with a broken dashboard? The speedometer might be
    stuck, the fuel gauge unreliable, and warning lights might flicker mysteriously.
    Without clear information about how the engine is running, it’s difficult to diagnose
    problems or ensure a safe journey.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有开过仪表盘损坏的汽车？速度表可能卡住，燃油表可能不可靠，警告灯可能神秘地闪烁。如果没有关于发动机运行状况的明确信息，就很难诊断问题或确保安全行驶。
- en: In the world of software, particularly complex systems built with microservices,
    logging plays a similar role. **Logs** are detailed records of events and activities
    within a system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件的世界里，尤其是在使用微服务构建的复杂系统中，日志记录扮演着类似的角色。**日志**是系统内部事件和活动的详细记录。
- en: When building your microservices, just thinking about business implementations
    is not enough. Microservices are, by nature, complex, with many independent services
    interacting. Logging helps understand individual service behavior and pinpoint
    issues within a specific service. When things go wrong, logs provide the audit
    trail to diagnose and fix problems. They help identify errors, dropped requests,
    or performance bottlenecks. Every microservice application should have a proper
    logging mechanism.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在构建微服务时，仅仅考虑业务实现是不够的。微服务本质上很复杂，有许多独立的服务相互交互。日志记录有助于理解单个服务的表现并定位特定服务中的问题。当事情出错时，日志提供了审计跟踪以诊断和修复问题。它们有助于识别错误、丢失的请求或性能瓶颈。每个微服务应用程序都应该有一个适当的日志记录机制。
- en: Logging microservices is essential for diagnostics, but it comes with challenges
    such as handling high volumes of distributed logs across different machines and
    languages, making it harder to aggregate and interpret them. Additionally, missing
    key details and ensuring sensitive information in logs is securely stored add
    complexity to managing logs effectively.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录微服务对于诊断至关重要，但它也带来了挑战，例如处理不同机器和语言之间的分布式日志的大量数据，这使得聚合和解释它们变得更加困难。此外，遗漏关键细节和确保日志中的敏感信息安全存储增加了有效管理日志的复杂性。
- en: By understanding these challenges, we can implement effective logging strategies
    to keep our microservices teams talking and our systems running smoothly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这些挑战，我们可以实施有效的日志记录策略，让我们的微服务团队保持沟通，并确保系统平稳运行。
- en: Logging levels and node libraries
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志级别和节点库
- en: Before practical examples, we need to understand some basics related to logging
    and one of them is **log levels**. Different log levels are used to categorize
    the severity or importance of log messages.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际示例之前，我们需要了解一些与日志记录相关的基础知识，其中之一就是**日志级别**。不同的日志级别用于对日志消息的严重性或重要性进行分类。
- en: '**Error logs** capture critical issues that need immediate attention, such
    as crashes or system failures, while **warning logs** highlight potential problems
    that may need investigation. **Info logs** track general system operations, **debug
    logs** provide detailed diagnostic information, and **trace logs** offer the most
    granular level of logging for tracking execution flow.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**错误日志**捕获需要立即关注的严重问题，例如崩溃或系统故障，而**警告日志**则突出可能需要调查的潜在问题。**信息日志**跟踪一般系统操作，**调试日志**提供详细的诊断信息，而**跟踪日志**提供最细粒度的日志记录，用于跟踪执行流程。'
- en: Of course, you don’t need to implement logging algorithms from scratch. One
    of the beauties of Node.Js is it provides a cool collection of libraries for us
    to use. We have different popular log libraries to integrate and use when we build
    our microservices. You can use `winston`, `pino`, `morgan` (log middleware for
    Express.js), `bunyan`, `log4js`, and so on when logging your microservices. We
    will integrate `winston` and `morgan` as a logging library for the current chapter
    but it is up to you to select one of them.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您不需要从头开始实现日志算法。Node.js 的一个优点是它为我们提供了一系列酷炫的库供我们使用。当我们构建我们的微服务时，我们有不同的流行日志库可以集成和使用。当您记录微服务时，您可以使用
    `winston`、`pino`、`morgan`（Express.js 的日志中间件）、`bunyan`、`log4js` 等等。我们将在本章中集成 `winston`
    和 `morgan` 作为日志库，但选择其中一个取决于您。
- en: Log formats
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志格式
- en: 'In Node.js microservices, logging formats can be categorized into unstructured
    logging, structured logging, and semi-structured logging. Here is an explanation
    of each:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Node.js 微服务中，日志格式可以分为非结构化日志、结构化日志和半结构化日志。以下是每种日志的解释：
- en: '**Unstructured logging**: Unstructured logging involves writing plain text
    log messages. This format is straightforward but can be harder to parse and analyze
    programmatically. Here is an example showing unstructured logging:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非结构化日志**：非结构化日志涉及编写纯文本日志消息。这种格式简单直接，但可能更难通过程序解析和分析。以下是一个展示非结构化日志的示例：'
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`.csv`, `.xml`, or other formats as well, but the most used format is JSON.
    This approach makes it easier to search, filter, and analyze logs programmatically.
    Here is an example showing structured logging:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.csv`、`.xml` 或其他格式，但最常用的格式是 JSON。这种方法使得通过程序搜索、过滤和分析日志变得更加容易。以下是一个展示结构化日志的示例：'
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Semi-structured logging**: It combines elements of both unstructured and
    structured logging. It often involves a consistent pattern or delimiter within
    plain text logs, making them somewhat easier to parse than completely unstructured
    logs but not as robust as fully structured logs.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半结构化日志**：它结合了非结构化和结构化日志的元素。它通常涉及在纯文本日志中的一致模式或分隔符，这使得它们比完全非结构化的日志更容易解析，但不如完全结构化的日志健壮。'
- en: We explored the importance of logging in microservices, and its challenges,
    and discussed the different log levels, popular Node.js logging libraries, and
    how to choose the right logging format for your microservices. Now, let’s cover
    best practices for logging.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了日志在微服务中的重要性，以及其挑战，并讨论了不同的日志级别、流行的 Node.js 日志库以及如何为您的微服务选择正确的日志格式。现在，让我们来看看日志的最佳实践。
- en: Best practices for logging
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志的最佳实践
- en: 'Effective logging can help you understand system behavior, diagnose issues,
    and monitor performance. Here are some essential best practices for logging in
    Node.js microservices:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的日志可以帮助您理解系统行为、诊断问题和监控性能。以下是 Node.js 微服务日志的一些基本最佳实践：
- en: '**Use a structured logging format**: Ensure logs are structured (e.g., JSON),
    making them easily parsed and searchable by log management tools. This facilitates
    more efficient log analysis and filtering.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用结构化日志格式**：确保日志是结构化的（例如，JSON），以便日志管理工具可以轻松解析和搜索。这有助于更有效地进行日志分析和过滤。'
- en: '**Include contextual information**: Enrich logs with context such as timestamps,
    service names, correlation IDs, and user information, enabling better tracing
    and correlation across microservices.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包含上下文信息**：通过时间戳、服务名称、关联 ID 和用户信息等上下文信息丰富日志，以便在微服务之间进行更好的跟踪和关联。'
- en: '**Log at appropriate levels**: Apply suitable log levels (error, warn, info,
    debug, trace) to categorize log messages based on severity, which helps in filtering
    logs for relevance and troubleshooting.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在适当的级别记录日志**：应用合适的日志级别（错误、警告、信息、调试、跟踪），根据严重性对日志消息进行分类，这有助于根据相关性过滤日志和进行故障排除。'
- en: '**Avoid logging sensitive information**: Ensure sensitive, data such as passwords
    and personal details, are redacted or masked before logging to maintain security
    and compliance.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免记录敏感信息**：在记录之前，确保敏感数据（如密码和个人详细信息）被删除或屏蔽，以保持安全和合规性。'
- en: '**Centralize logs**: Aggregate logs from all microservices in a centralized
    location using tools such as the ELK stack or cloud-based logging services for
    streamlined monitoring, analysis, and alerting.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集中日志**：使用如 ELK 堆栈或基于云的日志服务之类的工具，在集中位置聚合所有微服务的日志，以实现简化的监控、分析和警报。'
- en: These practices will help you ensure that your logging is efficient, secure,
    and scalable, making it easier to monitor system behavior, diagnose issues, and
    maintain overall performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实践将帮助您确保您的日志记录既高效又安全，可扩展，从而更容易监控系统行为，诊断问题，并维护整体性能。
- en: Implementing logging in your microservices
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的微服务中实现日志记录
- en: It is really simple to implement logging thanks to the packages of Node.js.
    In this section, we will use `winston` and `morgan` to demonstrate the usage of
    logging in microservices. Let’s integrate log support into the `Account` microservice
    we developed before. To follow this chapter, go to our GitHub repository and download
    the source code and `Ch10` using your favorite IDE. We plan to integrate monitoring
    functionality into our microservice, which we implemented in [*Chapter 9*](B09148_09.xhtml#_idTextAnchor147).
    You can just copy the `Ch09` folder and start to work on it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Node.js的包，实现日志记录实际上非常简单。在本节中，我们将使用`winston`和`morgan`来演示在微服务中日志记录的使用。让我们将日志支持集成到我们之前开发的`Account`微服务中。为了跟随本章内容，请访问我们的GitHub仓库，并使用您喜欢的IDE下载源代码和`Ch10`。我们计划将监控功能集成到我们在[*第9章*](B09148_09.xhtml#_idTextAnchor147)中实现的微服务中。您只需复制`Ch09`文件夹并开始工作即可。
- en: 'To install the `winston` and `morgan` libraries on the account microservice,
    run the following command from the `accountservice` folder:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要在账户微服务中安装`winston`和`morgan`库，请从`accountservice`文件夹运行以下命令：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, our `package.json` file should contain appropriate versions to use the
    libraries. Let’s first try to use `winston` for logging. Create a file called
    `logger.js` under the `src/log` folder with the following content:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的`package.json`文件应该包含适当的版本以使用这些库。让我们首先尝试使用`winston`进行日志记录。在`src/log`文件夹下创建一个名为`logger.js`的文件，并包含以下内容：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code defines a `winston` logger in Node.js for an application named `account-microservice`.
    Let’s break down the code step by step:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了Node.js中名为`account-microservice`的应用程序的`winston`日志记录器。让我们一步一步地分解代码：
- en: '`const winston = require(''winston'');`: This line imports the `winston` library,
    which is a popular logging framework for Node.js.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`const winston = require(''winston'');`: 这行代码导入了`winston`库，这是一个流行的Node.js日志框架。'
- en: '`const logger = winston.createLogger({...});`: This line creates a new `winston`
    logger instance and stores it in the logger constant. The curly braces (`{}`)
    contain configuration options for the logger.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`const logger = winston.createLogger({...});`: 这行代码创建了一个新的`winston`日志记录器实例，并将其存储在`logger`常量中。大括号（`{}`）包含日志记录器的配置选项。'
- en: '`level: process.env.LOG_LEVEL || ''info''`: This sets the minimum severity
    level of logs that will be captured. It checks the `LOG_LEVEL` environment variable
    first. If that’s not set, it defaults to the `''info''` level. Levels such as
    `''error''`, `''warn''`, `''info''`, `''debug''`, and so on exist, with `''error''`
    being the most severe.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`level: process.env.LOG_LEVEL || ''info''`: 这设置了将被捕获的日志的最小严重级别。它首先检查`LOG_LEVEL`环境变量。如果没有设置，则默认为`''info''`级别。存在诸如`''error''`、`''warn''`、`''info''`、`''debug''`等级别，其中`''error''`是最严重的。'
- en: '`defaultMeta`: This defines additional information that will be attached to
    every log message. Here, it includes the service name (`account-microservice`)
    and build information (version and `nodeVersion`).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`defaultMeta`: 这定义了将被附加到每个日志消息的附加信息。这里，它包括服务名称（`account-microservice`）和构建信息（版本和`nodeVersion`）。'
- en: '`transports`: This configures where the log messages will be sent. Here, it’s
    an array defining three transports:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transports`: 这配置了日志消息将被发送到何处。这里，它是一个定义了三个传输的数组：'
- en: '`winston.transports.Console`: This sends logs to the console (usually your
    terminal)'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winston.transports.Console`: 这将日志发送到控制台（通常是您的终端）'
- en: '`format: winston.format.combine(...)`: This defines how the log message will
    be formatted when sent to the console. It combines two formatters:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format: winston.format.combine(...)`: 这定义了当日志消息发送到控制台时将如何格式化。它组合了两个格式化程序：'
- en: '`winston.format.colorize()`: This adds color to the console output for better
    readability.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winston.format.colorize()`: 这为控制台输出添加颜色，以便更好地阅读。'
- en: '`winston.format.simple()`: This formats the message in a simple text format.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winston.format.simple()`: 这以简单的文本格式格式化消息。'
- en: '`winston.transports.File({ filename: ''combined.log'' })`: This sends all logs
    (based on the level setting) to a file named `combined.log`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winston.transports.File({ filename: ''combined.log'' })`: 这将所有日志（基于级别设置）发送到名为`combined.log`的文件中。'
- en: '`format: winston.format.combine(...)`: Similar to the console, it combines
    formatters:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format: winston.format.combine(...)`: 与控制台类似，它组合了格式化程序：'
- en: '`winston.format.json()`: This formats the message as a JSON object for easier
    parsing by machines.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winston.format.json()`: 这会将消息格式化为JSON对象，以便机器更容易解析。'
- en: '`winston.format.timestamp()`: This adds a timestamp to each log message.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winston.format.timestamp()`: 这会给每个日志消息添加时间戳。'
- en: '`winston.transports.File({ filename: ''error.log'', level: ''error'' })`: This
    sends only error-level logs to a separate file named `error.log`. It uses the
    same formatters (`json` and `timestamp`).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`winston.transports.File({ filename: ''error.log'', level: ''error'' })`: 这只会将错误级别的日志发送到名为`error.log`的单独文件。它使用相同的格式化程序（`json`和`timestamp`）。'
- en: '`module.exports =[];:` This line makes the created logger (`logger`) available
    for import and use in other parts of your application.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module.exports =[];:` 这行代码使得创建的日志记录器（`logger`）可以在应用的其它部分导入和使用。'
- en: In summary, this code sets up a comprehensive logging system for our application.
    It logs messages to both the console and files, with different formatting and
    filtering based on severity level. This allows us to easily monitor application
    behavior, debug issues, and analyze logs for further insights.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这段代码为我们的应用设置了一个全面的日志系统。它将消息记录到控制台和文件中，根据严重程度进行不同的格式化和过滤。这使我们能够轻松监控应用行为，调试问题，并分析日志以获得更深入的见解。
- en: 'Let’s integrate logging into `accountController` and see the result. Here is
    a simplified version:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将日志集成到`accountController`中，看看结果。这里是一个简化的版本：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When you call the endpoint that is responsible for delivering the `getAccountById`
    method, you will get a log message in your terminal and a `combined.log` file.
    We also integrated logging in `index.js` of our application to see whether everything
    is OK with the application running:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用负责执行`getAccountById`方法的端点时，你将在终端看到一个日志消息和一个`combined.log`文件。我们还把日志集成到我们应用的`index.js`中，以查看应用运行是否一切正常：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you have any errors, you’ll get the error message in your terminal and it
    will automatically be added to the `error.log` file.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何错误，你将在终端看到错误消息，并且它将自动添加到`error.log`文件中。
- en: In Node.js, particularly when using Express.js for building web applications,
    the `morgan` package is a popular tool for streamlining HTTP request logging.
    It automates the process of capturing and recording information about incoming
    requests to your application.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Node.js中，尤其是在使用Express.js构建Web应用时，`morgan`包是一个流行的工具，用于简化HTTP请求日志记录。它自动捕获并记录有关应用接收到的请求的信息。
- en: 'Here’s why you may use it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是为什么你可能需要使用它的原因：
- en: '`morgan` eliminates this by automatically capturing data such as the request
    method, URL, status code, response time, and more. This saves development time
    and ensures consistent logging.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`morgan`通过自动捕获诸如请求方法、URL、状态码、响应时间等信息来消除这一点。这节省了开发时间并确保了日志记录的一致性。'
- en: '`morgan` provides valuable insights into how your application handles requests.
    This can be crucial for debugging purposes, helping you identify potential issues
    or performance bottlenecks within your application’s request processing.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`morgan`提供了关于你的应用如何处理请求的宝贵见解。这对于调试目的至关重要，可以帮助你识别应用请求处理中的潜在问题或性能瓶颈。'
- en: '**Monitoring application traffic**: By reviewing the logs, you can gain a better
    understanding of your application’s traffic patterns. This can be useful for monitoring
    overall application health, identifying usage trends, and making informed decisions
    about scaling or resource allocation.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控应用流量**：通过审查日志，你可以更好地了解应用流量模式。这对于监控整体应用健康、识别使用趋势以及就扩展或资源分配做出明智决策非常有用。'
- en: '`combined`, `common`, and `dev`) that cater to different levels of detail.
    You can also create custom formats to capture specific data points relevant to
    your application’s needs.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`combined`, `common`, 和 `dev`)，它们针对不同级别的细节。你也可以创建自定义格式来捕获与你的应用需求相关的特定数据点。
- en: 'We’ve already installed the `morgan` package and it is time to use it. We usually
    use it as middleware and here is how to implement your own `morgan` middleware.
    Create a new file called `morganmiddleware.j`s under the `src/middlewares` folder.
    Copy and paste the following inside it:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经安装了`morgan`包，现在是时候使用它了。我们通常将其用作中间件，以下是如何实现自己的`morgan`中间件的步骤。在`src/middlewares`文件夹下创建一个名为`morganmiddleware.js`的新文件。将以下内容复制粘贴到其中：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code defines a custom middleware function for logging HTTP requests in
    JSON format using the `morgan` library. The code defines a logging mechanism that
    uses the `morgan` middleware to log HTTP requests in a Node.js application. It
    integrates logging with both a `combined.log` file and a Logstash server for external
    log management.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个用于使用 `morgan` 库以 JSON 格式记录 HTTP 请求的自定义中间件函数。该代码定义了一个使用 `morgan` 中间件在
    Node.js 应用程序中记录 HTTP 请求的日志机制。它将日志记录与 `combined.log` 文件和 Logstash 服务器集成，以进行外部日志管理。
- en: '`morganFormat` is a custom format that logs details such as the HTTP method,
    URL, status code, and response time for each request. These logs are then handled
    by a custom `messageHandler` function.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`morganFormat` 是一个自定义格式，它记录每个请求的 HTTP 方法、URL、状态码和响应时间等详细信息。然后，这些日志由自定义的 `messageHandler`
    函数处理。'
- en: In the `messageHandler`, the incoming log message is parsed from a JSON string
    into an object. The parsed log is then sent to Logstash using the `logger.info`
    function, which is imported from the `logger-logstash` module. At the same time,
    the original log message is also written to a local file named `combined.log`.
    This is done by creating a write stream to the file using Node.js’s `fs` module,
    which appends each new log to the file.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `messageHandler` 中，传入的日志消息被解析为一个 JSON 字符串到对象。然后，解析后的日志通过 `logger.info` 函数发送到
    Logstash，该函数是从 `logger-logstash` 模块导入的。同时，原始的日志消息也被写入一个名为 `combined.log` 的本地文件。这是通过使用
    Node.js 的 `fs` 模块创建一个写入流到文件来完成的，它将每个新的日志追加到文件中。
- en: Finally, the custom `morganMiddleware` is created using the `morgan` function,
    with the logging stream directed to `messageHandler`. This middleware is then
    exported to be used in other parts of the application for logging purposes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用 `morgan` 函数创建了自定义的 `morganMiddleware`，日志流被定向到 `messageHandler`。然后，这个中间件被导出，以便在其他应用程序部分用于日志记录目的。
- en: This setup ensures that HTTP request logs are recorded both locally in a file
    and sent to an external Logstash service for further processing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置确保 HTTP 请求日志既记录在本地文件中，也发送到外部 Logstash 服务进行进一步处理。
- en: 'We’re done with middleware functionality and now it is time to apply it. Open
    `app.js`, which is where we have configured our middleware flow and make the following
    changes:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了中间件功能，现在是时候应用它了。打开 `app.js`，这是我们配置中间件流程的地方，并进行以下更改：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Before everything else in the middleware flow, we need to use `morganMiddleware`
    and now you can just remove the previous logging functions we did via `winston`.
    Run the application and call any endpoint you want. Before running the account
    microservice, make sure that Docker is running with the appropriate `docker-compose`
    file. Don’t forget to run both `docker-compose` files (`accountservice/docker-compose.yml`
    and `accountservice/elk-stack/docker-compose.yml`).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间件流程中的所有其他内容之前，我们需要使用 `morganMiddleware`，现在你可以通过 `winston` 移除之前所做的日志记录函数。运行应用程序并调用你想要的任何端点。在运行账户微服务之前，确保
    Docker 正在运行，并使用适当的 `docker-compose` 文件。别忘了运行两个 `docker-compose` 文件（`accountservice/docker-compose.yml`
    和 `accountservice/elk-stack/docker-compose.yml`）。
- en: 'Here is the terminal output for logging:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是日志记录的终端输出：
- en: '![Figure 10.1: Terminal output for logging](img/B09148_10_001.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1：日志的终端输出](img/B09148_10_001.jpg)'
- en: 'Figure 10.1: Terminal output for logging'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：日志的终端输出
- en: Check the `combined.log` file and terminal window to see the logs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 `combined.log` 文件和终端窗口以查看日志。
- en: In the next section, we will cover centralized logging.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍集中式日志记录。
- en: Centralized logging with Elasticsearch, Logstash, and Kibana (ELK) stack
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Elasticsearch、Logstash 和 Kibana (ELK) 堆栈进行集中式日志记录
- en: 'In a microservices architecture, where applications are broken down into independent,
    loosely coupled services, **centralized logging** becomes crucial for effective
    monitoring and troubleshooting. We have a lot of reasons to use it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务架构中，应用程序被分解为独立、松散耦合的服务，**集中式日志记录**对于有效的监控和故障排除变得至关重要。我们有充分的理由使用它：
- en: '**Spread out logs**: Normally, logs would be all over the place, on each individual
    mini-app. Imagine hunting for a problem that jumps between them – like looking
    for a lost sock in a messy house!'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分散的日志**：通常，日志会散布在各个微型应用程序中。想象一下在它们之间寻找问题——就像在杂乱的房子里找一只丢失的袜子一样！'
- en: '**See everything at once**: Centralized logging brings all the logs together
    in one spot, like putting all your socks in a basket. This way, you can easily
    see how everything is working and if any parts are causing trouble.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一次性查看所有内容**：集中式日志记录将所有日志集中在一个地方，就像把所有的袜子都放在篮子里一样。这样，你可以轻松地看到一切是如何工作的，以及是否有任何部分引起麻烦。'
- en: '**Fixing problems faster**: With all the logs in one place, it’s like having
    a super magnifying glass to find issues. You can search through the logs quickly
    to see what went wrong, saving you time and frustration.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快地解决问题**：所有日志都在一个地方，就像有一个超级放大镜来查找问题。您可以快速搜索日志以查看发生了什么，节省您的时间和挫败感。'
- en: '**Keeping an eye on things**: Centralized logging often works with monitoring
    tools, like having a dashboard for your socks. This lets you see how well everything
    is performing and identify any slow spots.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注细节**：集中式日志记录通常与监控工具一起工作，就像有一个袜子的仪表板。这使您能够看到一切的表现如何，并识别任何缓慢的区域。'
- en: '**Log care made easy**: Having everything in one place makes taking care of
    the logs much simpler. It’s like having a dedicated sock drawer! Tools can be
    used to keep things organized, get rid of old logs, and follow any rules you need
    to follow.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志管理变得简单**：将所有内容集中在一起使得管理日志变得更加简单。就像有一个专门的袜子抽屉！可以使用工具来保持事物有序，删除旧日志，并遵循您需要遵循的任何规则。'
- en: By using centralized logging, you get a powerful tool to watch over your microservices,
    fix problems faster, and keep everything running smoothly.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用集中式日志记录，您将获得一个强大的工具来监控您的微服务，更快地解决问题，并保持一切运行顺畅。
- en: We have many different options to implement centralized logging when building
    microservices and one of them is the ELK stack.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建微服务时，我们有多种不同的选项来实现集中式日志记录，其中之一就是ELK堆栈。
- en: 'The **ELK stack** is a powerful suite of tools used for centralized logging,
    real-time search, and data analysis. Here’s a brief overview of each component:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**ELK堆栈**是一套强大的工具，用于集中式日志记录、实时搜索和数据分析。以下是每个组件的简要概述：'
- en: '**Elasticsearch**: This is a distributed search and analytics engine. We use
    it to store, search, and analyze large volumes of data quickly and in near real
    time. Elasticsearch is built on Apache Lucene and provides a RESTful interface
    for interacting with your data.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elasticsearch**：这是一个分布式搜索和分析引擎。我们使用它来快速存储、搜索和分析大量数据，几乎实时。Elasticsearch建立在Apache
    Lucene之上，并为与您的数据交互提供了一个RESTful接口。'
- en: '**Logstash**: This is a server-side data processing pipeline that ingests data
    from multiple sources simultaneously, transforms it, and then sends it to your
    chosen *stash*, such as Elasticsearch. It can handle a variety of data formats
    and provides a rich set of plugins to perform different transformations and enrichments.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Logstash**：这是一个服务器端数据处理管道，可以同时从多个来源摄取数据，对其进行转换，然后将它发送到您选择的*stash*，例如Elasticsearch。它可以处理各种数据格式，并提供丰富的插件来执行不同的转换和增强。'
- en: '**Kibana**: This is a data visualization and exploration tool used for analyzing
    and visualizing the data stored in Elasticsearch. It provides a user-friendly
    interface for creating dashboards and performing advanced data analysis.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kibana**：这是一个用于分析并可视化存储在Elasticsearch中的数据的数据可视化和探索工具。它提供了一个用户友好的界面，用于创建仪表板和执行高级数据分析。'
- en: But how do they work together? Well, Logstash collects and processes the log
    data from various sources (e.g., server logs, application logs, network logs)
    and forwards it to Elasticsearch. Elasticsearch indexes and stores the data, making
    it searchable in near real time. Kibana connects to Elasticsearch and provides
    the tools necessary to query, visualize, and analyze the data, allowing users
    to create custom dashboards and reports.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 但它们是如何协同工作的呢？好吧，Logstash从各种来源（例如，服务器日志、应用程序日志、网络日志）收集和处理日志数据，并将其转发到Elasticsearch。Elasticsearch索引并存储数据，使其几乎实时可搜索。Kibana连接到Elasticsearch，并提供查询、可视化和分析数据的工具，使用户能够创建自定义仪表板和报告。
- en: 'There are multiple benefits of using the ELK stack:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ELK堆栈有多个好处：
- en: '**Scalability**: The ELK stack can scale horizontally, allowing you to handle
    large volumes of log data.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：ELK堆栈可以水平扩展，允许您处理大量日志数据。'
- en: '**Real-time insights**: Elasticsearch’s real-time search capabilities provide
    instant insights into your data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时洞察**：Elasticsearch的实时搜索功能提供了对数据的即时洞察。'
- en: '**Flexibility**: Logstash’s ability to ingest data from various sources and
    formats makes it highly flexible.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**：Logstash从各种来源和格式摄取数据的能力使其非常灵活。'
- en: '**Visualization**: Kibana’s rich visualization options enable you to create
    interactive dashboards for monitoring and analysis.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化**：Kibana丰富的可视化选项使您能够创建用于监控和分析的交互式仪表板。'
- en: '**Open source**: The ELK stack is open source, with a large community and a
    wealth of plugins and extensions.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源**：ELK堆栈是开源的，拥有庞大的社区和丰富的插件和扩展。'
- en: As always, we prefer to install tools via Docker and it applies to the ELK stack
    also. Go to the `Ch10/accountservice/elk-stack` folder and run the `docker-compose.yml`
    file using the `docker-compose up -d` command. We will not dive into the details
    of `docker-compose` because we did it in our previous chapters. Simply, we install
    Elasticsearch, Logstash, and Kibana in the given `docker-compose.yml` file.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们更喜欢通过 Docker 安装工具，这也适用于 ELK 堆栈。转到 `Ch10/accountservice/elk-stack` 文件夹，并使用
    `docker-compose up -d` 命令运行 `docker-compose.yml` 文件。我们不会深入探讨 `docker-compose`
    的细节，因为我们已经在之前的章节中介绍过了。简单来说，我们在给定的 `docker-compose.yml` 文件中安装了 Elasticsearch、Logstash
    和 Kibana。
- en: A brief introduction to Logstash
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Logstash 的简要介绍
- en: 'We can collect and transform logs using Logstash. We are able to get input
    from multiple different sources such as logs generated by other applications,
    plain text, or networks. For log ingestion, we have different approaches that
    we can follow:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Logstash 收集和转换日志。我们能够从多个不同的来源获取输入，例如其他应用程序生成的日志、纯文本或网络。对于日志摄取，我们有不同的方法可以遵循：
- en: '**Direct transport**: We can configure our application to directly send data
    to Elasticsearch. Yes, that is an option but not a preferable way of ingesting
    logs.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接传输**: 我们可以将我们的应用程序配置为直接向 Elasticsearch 发送数据。是的，这是一个选项，但不是一种理想的日志摄取方式。'
- en: '**Write logs to the file**: As we implement in our microservices, it is preferable
    to implement such types of logging because other applications, such as Logstash,
    as a separate process, will be able to read, parse, and forward the data to Elasticsearch.
    It requires more configuration but it is the more robust and preferable way of
    doing logging for production.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将日志写入文件**: 正如我们在微服务中实现的那样，实现此类类型的日志记录是更可取的，因为其他应用程序，如作为独立进程的 Logstash，将能够读取、解析并将数据转发到
    Elasticsearch。这需要更多的配置，但它是更健壮且更可取的生产日志记录方式。'
- en: 'Logstash configuration is typically written in a configuration file (e.g.,
    `logstash.conf`). This file consists of three main sections: `input`, `filter`,
    and `output`. Each section defines different aspects of the data processing pipeline.
    Here’s a breakdown of each section and an example configuration:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash 配置通常写入配置文件（例如，`logstash.conf`）。此文件由三个主要部分组成：`input`、`filter` 和 `output`。每个部分定义了数据处理管道的不同方面。以下是每个部分的分解和示例配置：
- en: The `input` section defines where Logstash should collect data from. This could
    be from files, syslog, **Transmission Control Protocol** (**TCP**)/ **User Datagram
    Protocol** (**UDP**) ports, or various other sources.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input` 部分定义了 Logstash 应从何处收集数据。这可能是文件、syslog、**传输控制协议**（**TCP**）/ **用户数据报协议**（**UDP**）端口或其他各种来源。'
- en: The `filter` section is used to process and transform the data. Filters can
    parse, enrich, and modify the log data. Common filters include `grok` for pattern
    matching, `mutate` for modifying fields, and `date` for parsing date/time information.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter` 部分用于处理和转换数据。过滤器可以解析、丰富和修改日志数据。常见的过滤器包括用于模式匹配的 `grok`、用于修改字段的 `mutate`
    和用于解析日期/时间信息的 `date`。'
- en: The `output` section specifies where the processed data should be sent. This
    could be Elasticsearch, a file, a message queue, or another destination.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output` 部分指定了处理后的数据应发送到何处。这可能是 Elasticsearch、文件、消息队列或其他目的地。'
- en: 'To see the detailed explanation in action, simply open the `logstash.conf`
    file:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看详细说明的实际操作，只需打开 `logstash.conf` 文件：
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s dive into the details of the given configuration:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解给定配置的细节：
- en: '`tcp { port => 5000 }`: This section defines an `input` plugin that listens
    for data coming in over a TCP socket on port `5000`. Any logs or events sent to
    this port will be ingested by Logstash.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tcp { port => 5000 }`: 此部分定义了一个 `input` 插件，它监听端口 `5000` 上通过 TCP 套接字传入的数据。任何发送到该端口的日志或事件都将被
    Logstash 摄取。'
- en: '`json { source => "message" }`: This `filter` plugin parses the incoming data,
    assuming it’s in JSON format, and extracts the value from the field named `message`.
    This field is likely where the actual log content resides. By parsing it as JSON,
    Logstash can understand the structure of the data and make it easier to work with
    in subsequent processing steps.*   `elasticsearch { hosts => ["elasticsearch:9200"],
    index => "app-%{+YYYY.MM.dd}" }`: This `output` plugin sends the processed data
    to Elasticsearch, a search and analytics engine optimized for handling large volumes
    of log data. The host’s option specifies the location of the Elasticsearch instance
    (presumably running on a machine named `elasticsearch` with the default port `9200`).*   The
    `index` option defines a dynamic index naming pattern. Each day’s logs will be
    stored in a separate index named `app-YYYY.MM.dd` (where `YYYY` represents the
    year, `MM` the month, and `dd` the day). This pattern helps in efficient log management
    and allows you to easily search for logs from specific dates.*   `stdout { }`:
    This `output` plugin simply prints the processed data to the console (standard
    output) for debugging or monitoring purposes. The empty curly braces (`{}`) indicate
    the default configuration for the standard output.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`json { source => "message" }`：此 `filter` 插件解析传入的数据，假设其格式为 JSON，并从名为 `message`
    的字段中提取值。这个字段很可能是实际日志内容所在的位置。通过将其解析为 JSON，Logstash 可以理解数据的结构，并在后续处理步骤中使其更容易处理。*   `elasticsearch
    { hosts => ["elasticsearch:9200"], index => "app-%{+YYYY.MM.dd}" }`：此 `output`
    插件将处理后的数据发送到 Elasticsearch，这是一个针对处理大量日志数据进行了优化的搜索和分析引擎。主机选项指定了 Elasticsearch 实例的位置（可能运行在名为
    `elasticsearch` 的机器上，默认端口为 `9200`）。*   `index` 选项定义了一个动态索引命名模式。每天的日志将存储在名为 `app-YYYY.MM.dd`
    的单独索引中（其中 `YYYY` 代表年份，`MM` 代表月份，`dd` 代表日期）。这种模式有助于高效的日志管理，并允许你轻松地搜索特定日期的日志。*   `stdout
    { }`：此 `output` 插件简单地将处理后的数据打印到控制台（标准输出），用于调试或监控目的。空的大括号 (`{}`) 表示标准输出的默认配置。'
- en: This Logstash configuration ingests data from TCP source, parses JSON-formatted
    logs, and then sends them to Elasticsearch for storage and analysis. Daily indexes
    are created for organized log management. The `stdout` plugin provides a way to
    view the processed data during development or troubleshooting.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此 Logstash 配置从 TCP 源摄取数据，解析 JSON 格式的日志，然后将它们发送到 Elasticsearch 进行存储和分析。为有序日志管理创建每日索引。`stdout`
    插件提供了一种在开发或故障排除期间查看处理数据的方法。
- en: 'Let’s integrate logging into our account microservice. Create a new file called
    `logger-logstash.js` under the `accountmicroservice/src/log` folder with the following
    content:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将日志集成到我们的账户微服务中。在 `accountmicroservice/src/log` 文件夹下创建一个名为 `logger-logstash.js`
    的新文件，内容如下：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have already talked about `winston` configuration and the only new thing
    here is `logstashTransport`. We added two `transports` channel: one for the terminal’s
    console and the other one for sending logs to `logstash`. To use the given file
    with `morgan`, just change `morganmiddleware.js`’s logger to the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 `winston` 配置，这里唯一的新内容是 `logstashTransport`。我们添加了两个 `transports` 通道：一个用于终端的控制台，另一个用于将日志发送到
    `logstash`。要使用给定的文件与 `morgan` 一起使用，只需将 `morganmiddleware.js` 的记录器更改为以下内容：
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, run our application and go to `http://localhost:5601`; this is our Kibana.
    From the left menu, find **Management** | **Dev tools**. Click on the **Execute**
    button and you will see the total value with your logs (*Figure 10**.2*)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行我们的应用程序并访问 `http://localhost:5601`；这是我们使用的 Kibana。从左侧菜单中，找到 **管理** | **开发工具**。点击
    **执行** 按钮，你将看到日志的总值（*图 10.2*.2*）
- en: '![Figure 10.2: Getting logs from Kibana](img/B09148_10_002.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2：从 Kibana 获取日志](img/B09148_10_002.jpg)'
- en: 'Figure 10.2: Getting logs from Kibana'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：从 Kibana 获取日志
- en: Now, our logs are flowing to the ELK stack. You can think of Elasticsearch as
    a search and analytics engine with a data warehouse capability.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的日志正在流入 ELK 堆栈。你可以将 Elasticsearch 视为一个具有数据仓库功能的搜索和分析引擎。
- en: A brief introduction to Elasticsearch
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对 Elasticsearch 的简要介绍
- en: '**Elasticsearch** is a powerhouse search engine built for speed and scalability.
    At its core, it’s a distributed system designed to store, search, and analyze
    large volumes of data in near real time. It is document-oriented and uses JSON.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**Elasticsearch** 是一个为速度和可扩展性而构建的强大搜索引擎。在其核心，它是一个分布式系统，旨在几乎实时地存储、搜索和分析大量数据。它是面向文档的，并使用
    JSON。'
- en: 'Let’s dive deeper into the key attributes of Elasticsearch:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解 Elasticsearch 的关键属性：
- en: '**Distributed**: Elasticsearch can store data across multiple nodes (servers)
    in a cluster. This distribution allows for the following:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：Elasticsearch 可以在集群中的多个节点（服务器）上存储数据。这种分布允许以下操作：'
- en: '**Fault tolerance**: If one node fails, other nodes can handle the requests,
    keeping your search service operational.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：如果一个节点失败，其他节点可以处理请求，保持您的搜索服务正常运行。'
- en: '**Horizontal scaling**: You can easily add more nodes to the cluster as your
    data volume or search traffic grows.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**横向扩展**：随着您的数据量或搜索流量的增长，您可以轻松地向集群添加更多节点。'
- en: '**Scalable**: As mentioned previously, Elasticsearch excels at horizontal scaling.
    You can add more nodes to the cluster to handle increasing data and search demands.
    This scalability makes it suitable for large datasets and high search volumes.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：如前所述，Elasticsearch 在横向扩展方面表现出色。您可以向集群添加更多节点来处理不断增长的数据和搜索需求。这种可扩展性使其适用于大型数据集和高搜索量。'
- en: '**Search and analytics**: Elasticsearch specializes in full-text search, which
    analyzes the entire text content of your documents. This allows you to search
    for keywords, phrases, and even concepts within your data. It also provides powerful
    analytics capabilities. You can aggregate data, identify trends, and gain insights
    from your search results.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索和分析**：Elasticsearch 专注于全文搜索，它分析您文档的整个文本内容。这使得您能够在数据中搜索关键词、短语，甚至概念。它还提供了强大的分析功能。您可以聚合数据，识别趋势，并从搜索结果中获得洞察。'
- en: '**Flexible search**: Elasticsearch offers a wide range of query options. You
    can search for specific terms, filter results based on various criteria, and perform
    complex aggregations. This flexibility allows you to tailor your searches to your
    specific needs and uncover valuable information from your data.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的搜索**：Elasticsearch 提供了广泛的查询选项。您可以搜索特定术语，根据各种标准过滤结果，并执行复杂的聚合。这种灵活性允许您根据特定需求定制搜索，并从数据中挖掘有价值的信息。'
- en: '**Search speed**: Due to its distributed architecture and efficient indexing
    techniques, Elasticsearch delivers fast search results. This is crucial for applications
    where users expect an immediate response to their queries.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索速度**：由于其分布式架构和高效的索引技术，Elasticsearch 提供了快速的搜索结果。这对于用户期望对其查询立即做出响应的应用程序至关重要。'
- en: In this section, we provided a brief overview of Elasticsearch, focusing on
    the core attributes that make it a powerful tool for search and data analysis.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要概述了 Elasticsearch，重点关注使其成为搜索和数据分析强大工具的核心属性。
- en: A brief introduction to Kibana
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对 Kibana 的简要介绍
- en: '**Kibana** is the last item in our ELK stack. It is the visualization layer
    that complements the data storage and search muscle of Elasticsearch. It’s an
    open source platform that acts as a window into your Elasticsearch data, allowing
    you to explore, analyze, and understand it with clear visualizations.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kibana** 是我们 ELK 堆栈中的最后一项。它是数据存储和搜索功能的可视化层，补充了 Elasticsearch 的数据存储和搜索能力。它是一个开源平台，充当您
    Elasticsearch 数据的窗口，让您能够通过清晰的可视化来探索、分析和理解它。'
- en: 'Kibana has the following interesting possibilities:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 有以下有趣的潜力：
- en: '**Visualization powerhouse**: Kibana lets you create interactive dashboards
    with various charts, graphs, and maps. This visual representation transforms raw
    data into easily digestible insights.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化强大**：Kibana 允许您创建包含各种图表、图形和地图的交互式仪表板。这种视觉表示将原始数据转化为易于消化的洞察。'
- en: '**Data exploration**: Kibana provides tools to explore, search, and filter
    your data within Elasticsearch. You can drill down into specific details and uncover
    hidden patterns.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据探索**：Kibana 提供了在 Elasticsearch 中探索、搜索和过滤数据的工具。您可以深入了解特定细节，并揭示隐藏的模式。'
- en: '**Sharing insights**: Created dashboards can be shared with others, fostering
    collaboration and promoting data-driven decision-making.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享洞察**：创建的仪表板可以与他人共享，促进协作并推动数据驱动的决策。'
- en: 'There are several compelling reasons to choose Kibana for microservices:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 Kibana 用于微服务有以下几个令人信服的理由：
- en: '**Effortless integration**: As part of the ELK Stack (Elasticsearch, Logstash,
    and Kibana), Kibana integrates seamlessly with Elasticsearch. This tight integration
    streamlines the process of visualizing data stored within Elasticsearch.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无缝集成**：作为 ELK 堆栈（Elasticsearch、Logstash 和 Kibana）的一部分，Kibana 与 Elasticsearch
    无缝集成。这种紧密集成简化了在 Elasticsearch 中可视化存储数据的流程。'
- en: '**Real-time insights**: Kibana allows you to visualize data in near real-time,
    providing valuable insights as your data streams in. This is crucial for applications
    requiring immediate response to changes.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时洞察**：Kibana 允许您几乎实时地可视化数据，在数据流进来时提供有价值的见解。这对于需要立即对变化做出响应的应用程序至关重要。'
- en: '**Customization options**: Kibana offers a wide range of visualizations and
    customization options. You can tailor dashboards to fit your specific needs and
    effectively communicate insights to your audience.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义选项**：Kibana 提供了广泛的可视化自定义选项。您可以根据自己的特定需求定制仪表板，并有效地向您的受众传达见解。'
- en: '**Open source and free**: Being open source, Kibana is free to use and offers
    a vibrant community for support and development.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源和免费**：作为开源软件，Kibana 可以免费使用，并提供了一个充满活力的社区以支持和发展。'
- en: 'Microservices architectures involve multiple, independent services working
    together. Kibana shines in this environment for several reasons:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构涉及多个独立服务协同工作。Kibana 在此环境中表现出色，原因有以下几点：
- en: '**Monitoring performance**: Visualize key metrics from your microservices on
    Kibana dashboards to monitor their health and performance. This helps identify
    bottlenecks and ensure smooth operation.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控性能**：在 Kibana 仪表板上可视化微服务的关键指标，以监控其健康和性能。这有助于识别瓶颈并确保平稳运行。'
- en: '**Log analysis**: Centralize and analyze logs from all your microservices within
    Kibana. This unified view simplifies troubleshooting issues and pinpointing errors
    across the system.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志分析**：在 Kibana 中集中和分析来自所有微服务的日志。这种统一的视图简化了故障排除和定位系统中的错误。'
- en: '**Application insights**: Gain insights into how users interact with your microservices
    by visualizing usage patterns and trends within Kibana. This data can guide development
    efforts and improve user experience.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用洞察**：通过在 Kibana 中可视化使用模式和趋势，了解用户如何与您的微服务互动。这些数据可以指导开发工作并改善用户体验。'
- en: Learning about the ELK stack, diving into details of Elasticsearch querying
    and Kibana-related topics, such as custom dashboards, and working with metrics
    are beyond the scope of this book and that is why we finish our chapter only with
    a simple introduction to them.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 学习 ELK 堆栈，深入了解 Elasticsearch 查询和与 Kibana 相关的主题，例如自定义仪表板，以及与指标一起工作，这些都超出了本书的范围，这就是为什么我们只通过一个简单的介绍来结束本章。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter delved into the crucial aspects of monitoring and logging in microservices
    architecture, emphasizing the importance of observability in maintaining the health
    and performance of distributed systems. We began by explaining how observability
    provides deep insights into system behavior through key components such as logs,
    metrics, alerts, and traces.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了微服务架构中监控和日志记录的关键方面，强调了可观察性在维护分布式系统健康和性能中的重要性。我们首先解释了可观察性如何通过日志、指标、警报和跟踪等关键组件提供对系统行为的深入洞察。
- en: We then shifted focus to the importance of logging in microservices, which is
    essential for capturing detailed records of system events, identifying performance
    bottlenecks, and diagnosing issues in real time. We explored different log levels—error,
    warning, info, debug, and trace—and discussed how they help categorize log messages
    based on severity, making troubleshooting more efficient. Additionally, the chapter
    covered popular logging libraries in Node.js such as `winston` and `morgan`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将重点转向微服务中日志记录的重要性，这对于捕获系统事件的详细记录、识别性能瓶颈和实时诊断问题至关重要。我们探讨了不同的日志级别——错误、警告、信息、调试和跟踪——并讨论了它们如何根据严重程度对日志消息进行分类，从而提高故障排除效率。此外，本章还涵盖了
    Node.js 中流行的日志库，如 `winston` 和 `morgan`。
- en: Following the theoretical foundation, we demonstrated how to implement logging
    in a real-world scenario by integrating `winston` and `morgan` into the account
    microservice.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在理论基础上，我们通过将 `winston` 和 `morgan` 集成到账户微服务中，展示了如何在现实场景中实现日志记录。
- en: The chapter then moved on to centralized logging, introducing the powerful ELK
    stack. We explained how Logstash collects and processes log data, Elasticsearch
    stores and indexes the data for real-time search, and Kibana visualizes the information
    through interactive dashboards. By integrating these tools, we established a centralized
    logging system that simplifies log collection, analysis, and visualization.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本章接着转向集中式日志记录，介绍了强大的 ELK 堆栈。我们解释了 Logstash 如何收集和处理日志数据，Elasticsearch 如何存储和索引数据以进行实时搜索，以及
    Kibana 如何通过交互式仪表板可视化信息。通过整合这些工具，我们建立了一个集中式日志系统，简化了日志收集、分析和可视化。
- en: In the next chapter, we will explore how to manage multiple microservices effectively
    using popular microservices architecture elements. You’ll learn about using an
    API gateway, which acts as a single entry point to manage requests and direct
    them to the correct services, as well as organizing data and actions within your
    system through CQRS and Event Sourcing, two important methods that help handle
    complex data flows. By the end, you’ll have a clear understanding of building
    and connecting services in a way that’s efficient and easy to maintain.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何有效地使用流行的微服务架构元素来管理多个微服务。你将了解如何使用API网关，它作为一个单一入口点来管理请求并将它们引导到正确的服务，以及通过CQRS和事件溯源这两种重要方法来组织系统内的数据和操作，这些方法有助于处理复杂的数据流。到结束时，你将清楚地理解以高效且易于维护的方式构建和连接服务的方法。
