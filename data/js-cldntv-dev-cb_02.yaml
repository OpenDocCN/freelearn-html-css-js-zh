- en: Applying the Event Sourcing and CQRS Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用事件源和CQRS模式
- en: 'In this chapter, the following recipes will be covered:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下示例：
- en: Creating a data lake
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据湖
- en: Applying the event-first variant of the Event Sourcing pattern
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用事件优先的事件源模式变体
- en: Creating a micro event store
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建微事件存储
- en: Applying the database-first variant of the Event Sourcing pattern with DynamoDB
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DynamoDB应用数据库优先的事件源模式变体
- en: Applying the database-first variant of the Event Sourcing pattern with Cognito
    datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Cognito数据集应用数据库优先的事件源模式变体
- en: Creating a materialized view in DynamoDB
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DynamoDB中创建物化视图
- en: Creating a materialized view in S3
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在S3中创建物化视图
- en: Creating a materialized view in Elasticsearch
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Elasticsearch中创建物化视图
- en: Creating a materialized view in a Cognito dataset
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Cognito数据集中创建物化视图
- en: Replaying events
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重放事件
- en: Indexing the data lake
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引数据湖
- en: Implementing bi-directional synchronization
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现双向同步
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Cloud-native is autonomous. It empowers self-sufficient, full-stack teams to
    rapidly perform lean experiments and continuously deliver innovation with confidence.
    The operative word here is *confidence*. We leverage fully managed cloud services,
    such as function-as-a-service, cloud-native databases, and event streaming to
    decrease the risk of running these advanced technologies. However, at this rapid
    pace of change, we cannot completely eliminate the potential for human error.
    To remain stable despite the pace of change, cloud-native systems are composed
    of bounded, isolated, and autonomous services that are separated by bulkheads
    to minimize the blast radius when any given service experiences a failure. Each
    service is completely self-sufficient and stands on its own, even when related
    services are unavailable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生是自治的。它赋予自给自足的全栈团队能够快速进行精益实验，并自信地持续交付创新。这里的关键词是*自信*。我们利用完全托管的云服务，如函数即服务、云原生数据库和事件流，以降低运行这些先进技术的风险。然而，在这种快速变化的速度下，我们无法完全消除人为错误的可能性。为了在变化的速度下保持稳定，云原生系统由边界明确、隔离和自治的服务组成，这些服务通过防波堤隔开，以最小化任何给定服务出现故障时的破坏范围。每个服务都是完全自给自足的，即使相关服务不可用，也能独立运行。
- en: Following reactive principles, these autonomous services leverage event streaming
    for all inter-service communication. Event streaming turns the database inside
    out by replicating data across services in the form of materialized views stored
    in cloud-native databases. This cloud-native data forms a bulkhead between services
    and effectively turns the cloud into the database to maximize responsiveness,
    resilience, and elasticity. The **Event Sourcing** and **Command Query Responsibility
    Segregation** (**CQRS**) patterns are fundamental to creating autonomous services.
    This chapter contains recipes that demonstrate how to use fully managed, serverless
    cloud services to apply these patterns.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循响应式原则，这些自治服务利用事件流进行所有服务间通信。事件流通过在云原生数据库中存储以物化视图形式复制的跨服务数据，将数据库内部结构颠倒。这种云原生数据在服务之间形成一道防波堤，有效地将云转变为数据库，以最大化响应性、弹性和容错性。**事件源**和**命令查询责任分离**（**CQRS**）模式对于创建自治服务至关重要。本章包含了一些示例，展示了如何使用完全托管、无服务器的云服务来应用这些模式。
- en: Creating a data lake
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据湖
- en: One of the major benefits of the Event Sourcing pattern is that it results in
    an audit trail of all the state changes within a system. This store of events
    can also be leveraged to replay events to repair broken services and seed new
    components. A cloud-native event stream, such as **AWS Kinesis**, only stores
    events for a short period of time, ranging from 24 hours to 7 days. An **event
    stream** can be thought of as a temporary or temporal event store that is used
    for normal, near real-time operation. In the *Creating a micro event store* recipe,
    we will discuss how to create specialized event stores that are dedicated to a
    single service. In this recipe, we will create a data lake in S3\. A **data lake**
    is a perpetual event store that collects and stores all events in their raw format
    in perpetuity with complete fidelity and high durability to support auditing and
    replay.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 事件溯源模式的主要好处之一是它记录了系统内所有状态变化的历史记录。这些事件存储库也可以用来重放事件以修复损坏的服务和初始化新组件。例如，**AWS Kinesis**这样的云原生事件流，仅存储事件很短的时间，从24小时到7天不等。**事件流**可以被视为一个临时或时间性的事件存储，用于常规的、接近实时的操作。在*创建一个微事件存储*配方中，我们将讨论如何创建专门针对单个服务的事件存储。在这个配方中，我们将在S3中创建一个数据湖。**数据湖**是一个永久性的事件存储，以原始格式永久收集和存储所有事件，具有完整的保真度和高度的耐用性，以支持审计和重放。
- en: Getting ready
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before starting this recipe, you will need an **AWS Kinesis Stream**, such as
    the one created in the *Creating an event stream* recipe.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始此配方之前，您需要一个**AWS Kinesis Stream**，例如在*创建事件流*配方中创建的那个。
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Navigate to the `cncb-data-lake-s3` directory with `cd cncb-data-lake-s3`.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cd cncb-data-lake-s3`命令导航到`cncb-data-lake-s3`目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查以下内容的`serverless.yml`文件：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查以下内容的`handler.js`文件：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Install the dependencies with `npm install`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`npm install`安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`npm test -- -s $MY_STAGE`运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`.serverless`目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署堆栈：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Review the stack, data lake bucket, and Firehose delivery stream in the AWS
    Console.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS控制台中检查堆栈、数据湖存储桶和Firehose传输流。
- en: 'Publish an event from a separate Terminal with the following commands:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从单独的终端发布事件：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Allow the Firehose buffer time to process and then review the data lake contents
    created in the S3 bucket.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 允许Firehose缓冲时间处理，然后检查S3存储桶中创建的数据湖内容。
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成以下命令后，请移除堆栈：`npm run rm:lcl -- -s $MY_STAGE`。
- en: Remove the data lake stack after you have worked through all the other recipes.
    This will allow you to watch the data lake accumulating all the other events.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完所有其他配方后，请移除数据湖堆栈。这将允许您观察数据湖累积所有其他事件。
- en: How it works...
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The most important characteristic of a data lake is that it stores data in perpetuity.
    The only way to really meet this requirement is to use object storage, such as
    AWS S3\. S3 provides 11 nines of durability. Said another way, S3 provides 99.999999999%
    durability of objects over a given year. It is also fully managed and provides
    life cycle management features to age objects into cold storage. Note that the
    bucket is defined with the `DeletionPolicy` set to `Retain`. This highlights that
    even if the stack is deleted, we still want to ensure that we are not inappropriately
    deleting this valuable data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖最重要的特征是它永久存储数据。真正满足这一要求的方法是使用对象存储，例如AWS S3。S3提供了11个9的耐用性。换句话说，S3在给定一年内提供了99.999999999%的对象耐用性。它也是完全管理的，并提供生命周期管理功能，将对象老化到冷存储。请注意，存储桶使用`DeletionPolicy`设置为`Retain`定义，这表明即使堆栈被删除，我们仍然想确保我们不会不当地删除这些宝贵的数据。
- en: We are using Kinesis Firehose because it performs the heavy lifting of writing
    the events to the bucket. It provides a buffer based on the time and size, compression,
    encryption, and error handling. To simplify this recipe, I did not use compression
    or encryption, but it is recommended that you use these features.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Kinesis Firehose是因为它执行将事件写入存储桶的重型工作。它基于时间和大小提供缓冲，压缩，加密和错误处理。为了简化此配方，我没有使用压缩或加密，但建议您使用这些功能。
- en: This recipe defines one delivery stream, because in this cookbook, our stream
    topology consists of only one stream with `${cf:cncb-event-stream-${opt:stage}.streamArn}`.
    In practice, your topology will consist of multiple streams, and you will define
    one Firehose delivery stream per Kinesis stream to ensure that the data lake is
    capturing all events. We set `prefix` to `${cf:cncb-event-stream-${opt:stage}.streamName}/`
    so that we can easily distinguish the events in the data lake by their stream.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方定义了一个交付流，因为在这本烹饪书中，我们的流拓扑只包含一个流，即 `${cf:cncb-event-stream-${opt:stage}.streamArn}`。在实践中，您的拓扑将包含多个流，并且您将为每个
    Kinesis 流定义一个 Firehose 交付流，以确保数据湖能够捕获所有事件。我们将 `prefix` 设置为 `${cf:cncb-event-stream-${opt:stage}.streamName}/`，这样我们就可以通过流轻松区分数据湖中的事件。
- en: Another important characteristic of a data lake is that the data is stored in
    its raw format, without modification. To this end, the `transformer` function
    adorns all available metadata about the specific Kinesis stream and Firehose delivery
    stream, to ensure that all available information is collected. In the *Replaying
    events* recipe, we will see how this metadata can be leveraged. Also, note that
    `transformer` adds the end-of-line character (`\n`) to facilitate future processing
    of the data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖的另一个重要特征是数据以原始格式存储，未经修改。为此，`transformer` 函数装饰了有关特定 Kinesis 流和 Firehose 交付流的全部可用元数据，以确保收集所有可用信息。在
    *重放事件* 配方中，我们将看到如何利用这些元数据。此外，请注意，`transformer` 添加了换行符 (`\n`) 以便于未来数据处理。
- en: Applying the event-first variant of the Event Sourcing pattern
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用事件溯源模式的事件首先变体
- en: Event sourcing is a key pattern for designing eventually consistent cloud-native
    systems. Upstream services produce events as their state changes, and downstream
    services consume these events and produce their own events as needed. This results
    in a chain of events whereby services collaborate to produce a business process
    that results in an eventual consistency solution. Each step in this chain must
    be implemented as an atomic unit of work. Cloud-native systems do not support
    distributed transactions, because they do not scale horizontally in a cost-effective
    manner. Therefore, each step must update one, and only one, system. If multiple
    systems must be updated, then each is updated in a series of steps. In this recipe,
    we leverage the event-first variant of the Event Sourcing pattern where the atomic
    unit of work is writing to the event stream. The ultimate persistence of the data
    is delegated to downstream components.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 事件溯源是设计最终一致云原生系统的一个关键模式。上游服务在其状态变化时产生事件，下游服务消费这些事件并根据需要产生自己的事件。这导致一系列事件，其中服务协作以产生一个业务流程，该流程最终实现一致性解决方案。链中的每一步都必须作为一个原子工作单元实现。云原生系统不支持分布式事务，因为它们无法以经济高效的方式水平扩展。因此，每一步必须更新一个，并且只有一个系统。如果必须更新多个系统，则每个系统都按一系列步骤更新。在这个配方中，我们利用事件溯源模式的事件首先变体，其中原子工作单元是写入事件流。数据的最终持久性委托给下游组件。
- en: How to do it...
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Navigate to the `cncb-event-first` directory with `cd cncb-event-first`.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-event-first` 命令导航到 `cncb-event-first` 目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查包含以下内容的 `serverless.yml` 文件：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查名为 `handler.js` 的文件，其内容如下：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Install the dependencies with `npm install`.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署堆栈：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Review the stack and function in the AWS Console.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中检查堆栈和函数。
- en: 'Invoke the `submit` function with the following command:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令调用 `submit` 函数：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Take a look at the logs:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看日志：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成使用 `npm run rm:lcl -- -s $MY_STAGE` 后，请删除堆栈。
- en: How it works...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we implement a command function called `submit` that would be
    part of a Backend For Frontend service. Following the Event Sourcing pattern,
    we make this command atomic by only writing to a single resource. In some scenarios,
    such as initiating a long-lived business process or tracking user clicks, we only
    need to **fire-and-forget**. In these cases, the event-first variant is most appropriate.
    The command just needs to execute quickly and leave as little to chance as possible.
    We write the event to the highly available, fully-managed cloud-native event stream
    and trust that the downstream services will eventually consume the event.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们实现了一个名为 `submit` 的命令函数，它将是后端前端服务的一部分。遵循事件溯源模式，我们通过只写入单个资源来使这个命令原子化。在某些场景中，例如启动长期业务流程或跟踪用户点击，我们只需要
    **fire-and-forget**。在这些情况下，事件优先变体最为合适。命令只需要快速执行，并尽可能减少偶然性。我们将事件写入高度可用、完全管理的云原生事件流，并相信下游服务最终会消费该事件。
- en: The logic wraps the domain object in the standard event format, as discussed
    in the *Creating an event stream and publishing an event* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml), *Getting
    Started with Cloud-Native*. The event `type` is specified, the domain object ID
    is used as the `partitionKey`, and useful `tags` are adorned. Finally, the event
    is written to the stream specified by the `STREAM_NAME` environment variable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑将域对象包装在标准事件格式中，正如在 [第1章](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml) 的 *创建事件流和发布事件*
    菜谱中讨论的那样，*开始使用云原生*。指定了事件 `type`，使用域对象 ID 作为 `partitionKey`，并装饰了有用的 `tags`。最后，将事件写入由
    `STREAM_NAME` 环境变量指定的流。
- en: Creating a micro event store
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个微事件存储库
- en: In the *Creating a data lake* recipe, we will discuss how the Event Sourcing
    pattern provides the system with an audit trail of all the state-change events
    in the system. An event stream essentially provides a temporal event store that
    feeds downstream event processors in near real-time. The data lake provides a
    high durability, perpetual event store that is the official source of record.
    However, we have a need for a middle ground. Individual stream processors need
    the ability to source specific events that support their processing requirement.
    In this recipe, we will implement a micro event store in **AWS DynamoDB** that
    is owned by and tailored to the needs of a specific service.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *创建数据湖* 菜谱中，我们将讨论事件溯源模式如何为系统提供所有状态变更事件的审计跟踪。事件流本质上提供了一个近似实时的事件存储，为下游的事件处理器提供数据。数据湖提供了一个高度耐用的永久事件存储，是官方记录的来源。然而，我们有一个中间需求。单个流处理器需要能够获取支持其处理需求的具体事件。在这个菜谱中，我们将实现一个由
    **AWS DynamoDB** 拥有并针对特定服务需求定制的微事件存储库。
- en: How to do it...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Create the project from the following template:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Navigate to the `cncb-micro-event-store` directory with `cd cncb-micro-event-store`.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-micro-event-store` 命令进入 `cncb-micro-event-store` 目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下内容的 `serverless.yml` 文件：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下内容的 `handler.js` 文件：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Install the dependencies with `npm install`.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看在 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署堆栈：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Review the stack, functions, and table in the AWS Console.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中查看堆栈、函数和表。
- en: 'Publish an event from a separate Terminal with the following commands:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从单独的终端发布事件：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Take a look at the logs for the `listener` function:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看监听函数的日志：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Take a look at the logs for the `trigger` function:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看触发函数的日志：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用 `npm run rm:lcl -- -s $MY_STAGE` 删除堆栈。
- en: How it works...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: When implementing a stream processor function, we often need more information
    than is available in the current event object. It is a best practice when publishing
    events to include all the relevant data that is available in the publishing context
    so that each event represents a micro snapshot of the system at the time of publishing.
    When this data is not enough, we need to retrieve more data; however, in cloud-native
    systems, we strive to eliminate all synchronous inter-service communication because
    it reduces the *autonomy* of the services. Instead, we create a micro event store
    that is tailored to the needs of the specific service.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现流处理器函数时，我们通常需要比当前事件对象中可用的信息更多。在发布事件时包含所有在发布上下文中可用的相关数据是一种最佳实践，这样每个事件就代表系统在发布时的一个微快照。当这些数据不足时，我们需要检索更多数据；然而，在云原生系统中，我们努力消除所有同步的跨服务通信，因为这会降低服务的*自主性*。相反，我们创建一个针对特定服务需求的微事件存储。
- en: First, we implement a `listener` function and `filter` for the desired events
    from the stream. Each event is stored in a DynamoDB table. You can store the entire
    event or just the information that is needed. When storing these events, we need
    to collate related events by carefully defining the `HASH` and `RANGE` keys. For
    example, we might want to collate all events for a specific domain object ID or
    all events from a specific user ID. In this example, we use `event.partitionKey`
    as the hash key, but you can calculate the hash key from any of the available
    data. For the range key, we need a value that is unique within the hash key. The
    `event.id` is a good choice if it is implemented with a V1 UUID because they are
    time-based. The Kinesis sequence number is another good choice. The `event.timestamp`
    is another alternative, but there could be a potential that events are created
    at the exact same time within a hash key.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们实现一个用于从流中获取所需事件的`listener`函数和`filter`。每个事件都存储在DynamoDB表中。你可以存储整个事件或仅存储所需的信息。在存储这些事件时，我们需要通过仔细定义`HASH`和`RANGE`键来整理相关事件。例如，我们可能希望整理特定域对象ID的所有事件或特定用户ID的所有事件。在这个例子中，我们使用`event.partitionKey`作为哈希键，但你也可以从任何可用数据中计算哈希键。对于范围键，我们需要一个在哈希键内唯一的值。如果`event.id`实现了基于V1
    UUID，那么它是一个不错的选择，因为它们是基于时间的。Kinesis序列号也是一个不错的选择。`event.timestamp`是另一个替代方案，但可能存在在哈希键内事件创建时间完全相同的情况。
- en: The `trigger` function, which is attached to the DynamoDB stream, takes over
    after the `listener` has saved an event. The trigger calls `getMicroEventStore`
    to retrieve the micro event store based on the hash key calculated for the current
    event. At this point, the stream processor has all the relevant data available
    in memory. The events in the micro event store are in historical order, based
    on the value used for the range key. The stream processor can use this data however
    it sees fit to implement its business logic.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与DynamoDB流关联的`trigger`函数在`listener`保存事件后接管。触发器调用`getMicroEventStore`来根据为当前事件计算的哈希键检索微事件存储。此时，流处理器在内存中拥有所有相关的数据。微事件存储中的事件按历史顺序排列，基于用于范围键的值。流处理器可以使用这些数据来实现其业务逻辑。
- en: Use the DynamoDB TTL feature to keep the micro event store from growing unbounded.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DynamoDB的TTL功能来防止微事件存储无限制地增长。
- en: Applying the database-first variant of the Event Sourcing pattern with DynamoDB
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在DynamoDB上应用事件溯源模式的数据库优先变体
- en: In the previous recipe, *Applying the event-first variant of the Event Sourcing
    pattern*, we discussed how the Event Sourcing pattern allows us to design eventually
    consistent systems that are composed of a chain of atomic steps. Distributed transactions
    are not supported in cloud-native systems, because they do not scale effectively.
    Therefore, each step must update one, and only one, system. In this recipe, we
    will leverage the **database-first** variant of the Event Sourcing pattern, where
    the atomic unit of work is writing to a single cloud-native database. A cloud-native
    database provides a **change data capture** mechanism that allows further logic
    to be atomically triggered that publishes an appropriate **domain event** to the
    event stream for further downstream processing. In this recipe, we will demonstrate
    implementing this pattern with **AWS DynamoDB** and **DynamoDB Streams**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个食谱中，*应用事件溯源模式的“事件优先”变体*，我们讨论了事件溯源模式如何使我们能够设计由一系列原子步骤组成的最终一致性的系统。在云原生系统中不支持分布式事务，因为它们无法有效地扩展。因此，每个步骤必须更新一个，并且只有一个系统。在本食谱中，我们将利用事件溯源模式的
    **数据库优先** 变体，其中原子工作单元是写入单个云原生数据库。云原生数据库提供了一种 **变更数据捕获** 机制，允许进一步逻辑原子触发，将适当的 **领域事件**
    发布到事件流以进行进一步的下层处理。在本食谱中，我们将使用 **AWS DynamoDB** 和 **DynamoDB Streams** 来演示实现此模式。
- en: How to do it...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Navigate to the `cncb-db-first-dynamodb` directory with `cd cncb-db-first-dynamodb`.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-db-first-dynamodb` 命令进入 `cncb-db-first-dynamodb` 目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查名为 `serverless.yml` 的文件，其内容如下：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查名为 `handler.js` 的文件，其内容如下：
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Install the dependencies with `npm install`.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 命令安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 命令运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署栈：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Review the stack, functions, and table in the AWS Console.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中检查栈、函数和表。
- en: 'Invoke the function with the following command:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令调用函数：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Take a look at the `command` function logs:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看命令函数的日志：
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Take a look at the trigger function logs:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看触发函数的日志：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Review the events collected in the data lake bucket.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据湖存储桶中收集的事件。
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成操作后，使用 `npm run rm:lcl -- -s $MY_STAGE` 命令删除栈。
- en: How it works...
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we implement a `command` function that would be part of a Backend
    For Frontend service. Following the Event Sourcing pattern, we make this command
    atomic by only writing to a single resource. In many scenarios, such as the authoring
    of data, we need to write data and make sure it's immediately available for reading.
    In these cases, the *database-first* variant is most appropriate. The command
    just needs to execute quickly and leave as little to chance as possible. We write
    the **domain object** to the highly available, fully-managed cloud-native database
    and trust that the database's **change data capture** mechanism will handle the
    next step.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们实现了一个 `command` 函数，该函数将是后端前端服务的一部分。遵循事件溯源模式，我们通过只写入单个资源来使此命令原子化。在许多场景中，例如数据的编写，我们需要写入数据并确保它立即可供读取。在这些情况下，*数据库优先*
    变体最为合适。命令只需快速执行，并尽可能减少偶然性。我们将 **领域对象** 写入高可用、完全管理的云原生数据库，并相信数据库的 **变更数据捕获** 机制将处理下一步。
- en: In this recipe, the database is DynamoDB and the change data capture mechanism
    is DynamoDB Streams. The `trigger` function is a stream processor that is consuming
    events from the specified DynamoDB stream. We enable the stream by adding the
    `StreamSpecification` to the definition of the table.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，数据库是 DynamoDB，变更数据捕获机制是 DynamoDB Streams。`trigger` 函数是一个流处理器，它从指定的 DynamoDB
    流中消费事件。我们通过将 `StreamSpecification` 添加到表的定义中启用流。
- en: The stream processor logic wraps the domain object in the standard event format,
    as discussed in the *Creating an event stream and publishing an event* recipe
    in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml), *Getting Started with
    Cloud-Native*. The `record.eventID` generated by DynamoDB is reused as the domain
    event ID, the database trigger's `record.eventName` is translated into the domain
    event type, the domain object ID is used as `partitionKey`, and useful `tags`
    are adorned. The `old` and `new` values of the domain object are included in the
    event so that downstream services can calculate a delta however they see fit.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理器逻辑将领域对象包装在标准事件格式中，如 [第 1 章](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml)
    中 *Getting Started with Cloud-Native* 的 *创建事件流和发布事件* 菜谱所述。DynamoDB 生成的 `record.eventID`
    被重用作领域事件 ID，数据库触发器的 `record.eventName` 被转换为领域事件类型，领域对象 ID 用作 `partitionKey`，并添加了有用的
    `tags`。领域对象的 `old` 和 `new` 值包含在事件中，以便下游服务可以按其认为合适的方式计算增量。
- en: Finally, the event is written to the stream specified by the `STREAM_NAME` environment
    variable. Note that the trigger function is similar to the *event-first* variant.
    It just needs to execute quickly and leave as little to chance as possible. We
    write the event to a single resource, the highly available, fully-managed cloud-native
    event stream, and trust that the downstream services will eventually consume the
    event.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，事件被写入由 `STREAM_NAME` 环境变量指定的流中。请注意，触发函数类似于 *event-first* 变体。它只需要快速执行，并尽可能减少偶然性。我们将事件写入单个资源，即高度可用、完全管理的云原生事件流，并相信下游服务最终会消费该事件。
- en: Applying the database-first variant of the Event Sourcing pattern with Cognito
    datasets
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Cognito 数据集应用事件源模式的数据库优先变体
- en: In the *Applying the event-first variant of the Event Sourcing pattern* recipe,
    we discussed how the Event Sourcing pattern allows us to design eventually consistent
    systems that are composed of a chain of atomic steps. Distributed transactions
    are not supported in cloud-native systems, because they do not scale effectively.
    Therefore, each step must update one, and only one, system. In this recipe, we
    leverage the *database-first* variant of the Event Sourcing pattern, where the
    atomic unit of work is writing to a single cloud-native database. A cloud-native
    database provides a change data capture mechanism that allows further logic to
    be atomically triggered that publishes an appropriate domain event to the event
    stream for further downstream processing. In the recipe, we demonstrate an **offline-first**
    implementation of this pattern with **AWS Cognito datasets**.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *应用事件源模式的 event-first 变体* 菜谱中，我们讨论了事件源模式如何使我们能够设计最终一致的系统，这些系统由一系列原子步骤组成。云原生系统中不支持分布式事务，因为它们无法有效扩展。因此，每个步骤必须更新一个，并且只有一个系统。在这个菜谱中，我们利用事件源模式的
    *database-first* 变体，其中原子工作单元是写入单个云原生数据库。云原生数据库提供了一种更改数据捕获机制，允许进一步逻辑原子触发，将适当的领域事件发布到事件流以供进一步下游处理。在菜谱中，我们使用
    **AWS Cognito 数据集**演示了此模式的 **离线优先** 实现。
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Navigate to the `cncb-db-first-cognito` directory with `cd cncb-db-first-cognito`.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-db-first-cognito` 命令导航到 `cncb-db-first-cognito` 目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查名为 `serverless.yml` 的文件，内容如下：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 AWS 控制台中名为 `handler.js` 的文件，内容如下：
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Install the dependencies with `npm install`.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署栈：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Review the stack, function, and identity pool in the AWS Console.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 AWS 控制台中的栈、函数和身份池。
- en: Update the file named `./index.html` with the  `identityPoolId` from the previous
    output.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新名为 `./index.html` 的文件，并使用之前输出的 `identityPoolId`。
- en: 'Open the file named `./index.html` in a browser, enter a `name` and `description`,
    and press `Save` and then `Synchronize`, as shown in the following screenshot:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中打开名为 `./index.html` 的文件，输入 `name` 和 `description`，然后点击 `Save` 和 `Synchronize`，如图所示：
- en: '![](img/9b194c69-5f43-45c1-95cc-69277df5207e.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b194c69-5f43-45c1-95cc-69277df5207e.png)'
- en: 'Take a look at the logs:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看日志：
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用 `npm run rm:lcl -- -s $MY_STAGE` 删除栈。
- en: How it works...
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we are implementing an offline-first solution where the ReactJS
    client application stores all changes in local storage and then synchronizes the
    data to a Cognito dataset in the cloud when connectivity is available. This scenario
    is very common in mobile applications where the mobile application may not always
    be connected. An AWS Cognito dataset is associated with a specific user in an
    **AWS Identity Pool**. In this recipe, the identity pool supports unauthenticated
    users. Anonymous access is another common characteristic of mobile applications.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们正在实现一个离线优先的解决方案，其中 ReactJS 客户端应用程序将所有更改存储在本地存储中，然后在可连接时将数据同步到云中的 Cognito
    数据集。这种场景在移动应用程序中非常常见，因为移动应用程序可能并不总是连接。AWS Cognito 数据集与 **AWS Identity Pool** 中的特定用户相关联。在这个菜谱中，身份池支持未认证的用户。匿名访问是移动应用程序的另一个常见特征。
- en: The bare bones ReactJS application is implemented in the `./index.html` file.
    It contains a form for the user to enter data. The `Save` button saves the form's
    data to local storage via the Cognito SDK. The `Synchronize` button uses the SDK
    to send the local data to the cloud. In a typical application, this synchronization
    would be triggered behind the scenes by events in the normal flow of the application,
    such as on save, on load, and before exit. In the *Creating a materialized view
    in a Cognito Dataset* recipe, we show how synchronizing will also retrieve data
    from the cloud.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 纯粹的 ReactJS 应用程序在 `./index.html` 文件中实现。它包含一个用户输入数据的表单。`保存` 按钮通过 Cognito SDK
    将表单数据保存到本地存储。`同步` 按钮使用 SDK 将本地数据发送到云。在典型应用程序中，这种同步通常在应用程序的正常流程中由事件触发，例如在保存、加载和退出之前。在
    *在 Cognito 数据集中创建物化视图* 菜谱中，我们展示了同步也会从云中检索数据。
- en: Cognito's change data capture feature is implemented via **AWS Kinesis**. Therefore,
    we create a Kinesis stream called `CognitoStream` that is dedicated to our Cognito
    datasets. The `trigger` function is a stream processor that is consuming sync
    records from this stream. The stream processor's `recordToSync` step extracts
    the domain object from each sync record, where it is stored as a JSON string.
    The `toEvent` step wraps the domain object in the standard event format, as discussed
    in the *Creating an event stream and publishing an event* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. Finally, the event is written to the stream
    specified by the `STREAM_NAME` environment variable. Note that the trigger function
    is similar to the event-first variant. It just needs to execute quickly and leave
    as little to chance as possible. We write the event to a single resource, the
    highly available, fully managed cloud-native event stream, and trust that the
    downstream services will eventually consume the event.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Cognito 的更改数据捕获功能是通过 **AWS Kinesis** 实现的。因此，我们创建了一个名为 `CognitoStream` 的 Kinesis
    流，专门用于我们的 Cognito 数据集。`trigger` 函数是一个流处理器，它从该流中消费同步记录。流处理器的 `recordToSync` 步骤从每个同步记录中提取域对象，该对象以
    JSON 字符串的形式存储。`toEvent` 步骤将域对象包装在标准事件格式中，正如在 [第 1 章](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml)
    中“创建事件流并发布事件”菜谱所讨论的那样。最后，事件被写入由 `STREAM_NAME` 环境变量指定的流。请注意，触发函数类似于事件优先的变体。它只需要快速执行，并尽可能减少偶然性。我们将事件写入单个资源，即高度可用、完全管理的云原生事件流，并相信下游服务最终会消费该事件。
- en: Creating a materialized view in DynamoDB
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 DynamoDB 中创建物化视图
- en: The **Command Query Responsibility Segregation** (**CQRS**) pattern is critical
    for designing cloud-native systems that are composed of bounded, isolated, and
    autonomous services with appropriate bulkheads to limit the blast radius when
    a service experiences an outage. These bulkheads are implemented by creating materialized
    views in downstream services.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**命令查询责任分离**（**CQRS**）模式对于设计由边界、隔离和自主服务组成，并具有适当防波堤以限制服务出现故障时的破坏半径的云原生系统至关重要。这些防波堤通过在下游服务中创建物化视图来实现。'
- en: Upstream services are responsible for the commands that write data using the
    Event Sourcing pattern. Downstream services take responsibility for their own
    queries by creating materialized views that are specifically tailored to their
    needs. This **replication** of data increases scalability, reduces latency, and
    allows services to be completely **autonomous** and function even when upstream
    source services are unavailable. In this recipe, we will implement a materialized
    view in **AWS DynamoDB**.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 上游服务负责使用事件溯源模式写入数据的命令。下游服务通过创建专门针对其需求的物化视图来承担其自身的查询责任。这种 **复制** 数据增加了可伸缩性，减少了延迟，并允许服务在完全
    **自主** 的情况下运行，即使上游源服务不可用。在这个配方中，我们将实现一个 AWS DynamoDB 中的物化视图。
- en: How to do it...
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Navigate to the `cncb-materialized-view-dynamodb` directory with `cd cncb-materialized-view-dynamodb`.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-materialized-view-dynamodb` 命令导航到 `cncb-materialized-view-dynamodb`
    目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `serverless.yml` 的文件，其内容如下：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `handler.js` 的文件，其内容如下：
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Install the dependencies with `npm install`.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署堆栈：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Review the stack, functions, and table in the AWS Console.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中查看堆栈、函数和表。
- en: 'Publish an event from a separate Terminal with the following commands:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从另一个终端使用以下命令发布事件：
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Take a look at the `listener` function logs:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看日志中的 `listener` 函数：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Invoke the `query` command:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `query` 命令：
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后使用 `npm run rm:lcl -- -s $MY_STAGE` 删除堆栈。
- en: How it works...
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we implemented a `listener` function that consumes upstream
    events and populates a materialized view that is used by a **Backend For Frontend** (**BFF**)
    service. This function is a *stream processor,* such as the one we discussed in
    the *Creating a stream processor* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. The function performs a `filter` for the
    desired events and then transforms the data in a `map` step to the desired materialized
    view. The materialized view is optimized to support the requirements of the query
    needed by the BFF. Only the minimum necessary data is stored and the optimal database
    type is used. In this recipe, the database type is DynamoDB. DynamoDB is a good
    choice for a materialized view when the data changes frequently.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们实现了一个 `listener` 函数，该函数消费上游事件并填充一个由 **Backend For Frontend** （**BFF**）
    服务使用的物化视图。这个函数是一个 *流处理器*，就像我们在 [第 1 章](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml)
    *云原生入门* 中讨论的 *创建流处理器* 配方中提到的那样。该函数对所需事件执行 `filter` 操作，然后在 `map` 步骤中将数据转换到所需的物化视图中。物化视图已优化以支持
    BFF 所需的查询要求。只存储必要的数据，并使用最优的数据库类型。在这个配方中，数据库类型是 DynamoDB。当数据频繁变化时，DynamoDB 是物化视图的一个好选择。
- en: Note that the `asOf` timestamp is included in the record. In an eventually consistent
    system, it is important to provide the user with the `asOf` value so that he or
    she can access the latency of the data. Finally, the data is stored in the highly
    available, fully managed, cloud-native database.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意记录中包含了 `asOf` 时间戳。在最终一致性的系统中，向用户提供 `asOf` 值非常重要，这样他们就可以访问数据的延迟。最后，数据存储在高度可用、完全管理的云原生数据库中。
- en: Creating a materialized view in S3
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 S3 中创建物化视图
- en: In the *Creating a materialized view in* *DynamoDB* recipe, we discussed how
    the CQRS pattern allows us to design services that are bounded, isolated, and
    autonomous. This allows services to operate, even when their upstream dependencies
    are unavailable, because we have eliminated all synchronous inter-service communication
    in favor of replicating and caching the required data locally in dedicated materialized
    views. In this recipe, we will implement a materialized view in AWS S3.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *在 DynamoDB 中创建物化视图* 的配方中，我们讨论了 CQRS 模式如何使我们能够设计边界明确、隔离和自主的服务。这使得服务即使在它们的上游依赖不可用的情况下也能运行，因为我们已经消除了所有同步的跨服务通信，转而将所需数据在专用的物化视图中本地复制和缓存。在这个配方中，我们将在
    AWS S3 中实现一个物化视图。
- en: How to do it...
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Navigate to the `cncb-materialized-view-s3` directory with `cd cncb-materialized-view-s3`.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-materialized-view-s3` 命令导航到 `cncb-materialized-view-s3` 目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `serverless.yml` 的文件，其内容如下：
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `handler.js` 的文件，其内容如下：
- en: '[PRE39]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Install the dependencies with `npm install`.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看在 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署堆栈：
- en: '[PRE40]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Review the stack, function, and bucket from the AWS Console.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS控制台中查看堆栈、函数和桶。
- en: 'Publish an event from a separate Terminal with the following commands:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从单独的终端发布事件：
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Take a look at the logs:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看日志：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Invoke the following command, after updating the `bucket-suffix`, to get the
    data from S3:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更新 `bucket-suffix` 之后，调用以下命令从S3获取数据：
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Use the console to delete the objects from the bucket before removing the stack.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在删除堆栈之前，使用控制台删除桶中的对象。
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用 `npm run rm:lcl -- -s $MY_STAGE` 删除堆栈。
- en: How it works...
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we implement a `listener` function that consumes upstream events
    and populates a materialized view that is used by a Backend For Frontend service.
    This function is a stream processor, such as the one we discussed in the *Creating
    a stream processor* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. The function performs a `filter` for the
    desired events and then transforms the data in a `map` step to the desired materialized
    view. The materialized view is optimized to support the requirements of the query
    needed by the BFF. Only the minimum necessary data is stored, and the optimal
    database type is used.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们实现了一个 `listener` 函数，该函数消费上游事件并填充一个由Backend For Frontend服务使用的物化视图。这个函数是一个流处理器，就像我们在第1章
    *Getting Started with Cloud-Native* 中讨论的 *创建流处理器* 菜谱中提到的那样。该函数对所需事件执行 `filter`
    操作，然后在 `map` 步骤中将数据转换到所需的物化视图中。物化视图已优化以支持BFF所需的查询需求。只存储必要的数据，并使用最优的数据库类型。
- en: In this recipe, the database type is S3\. S3 is a good choice for a materialized
    view when the data changes infrequently, and it can be cached in the CDN. Note
    that the `asOf` timestamp is included in the record so that the user can access
    the latency of the data.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，数据库类型是S3。当数据变化不频繁时，S3是物化视图的一个好选择，并且它可以缓存在CDN中。请注意，记录中包含了 `asOf` 时间戳，以便用户可以访问数据的延迟。
- en: Creating a materialized view in Elasticsearch
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Elasticsearch中创建物化视图
- en: In the *Creating a materialized view in DynamoDB* recipe, we discussed how the
    CQRS pattern allows us to design services that are bounded, isolated, and autonomous.
    This allows services to operate, even when their upstream dependencies are unavailable,
    because we have eliminated all synchronous inter-service communication in favor
    of replicating and caching the required data locally in dedicated materialized
    views. In this recipe, we will implement a materialized view in AWS Elasticsearch.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *在DynamoDB中创建物化视图* 菜谱中，我们讨论了CQRS模式如何使我们能够设计出有界、隔离和自主的服务。这使得服务即使在它们的上游依赖不可用时也能运行，因为我们已经消除了所有同步的跨服务通信，转而将所需数据在专用的物化视图中本地复制和缓存。在这个菜谱中，我们将实现一个AWS
    Elasticsearch中的物化视图。
- en: How to do it...
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE44]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Navigate to the `cncb-materialized-view-es` directory with `cd cncb-materialized-view-es`.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-materialized-view-es` 命令导航到 `cncb-materialized-view-es` 目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `serverless.yml` 的文件，其内容如下：
- en: '[PRE45]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `handler.js` 的文件，其内容如下：
- en: '[PRE46]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Install the dependencies with `npm install`.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看在 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署堆栈：
- en: Deploying an Elasticsearch domain can take upwards of 1-20 minutes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 部署Elasticsearch域可能需要1-20分钟以上。
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Review the stack, function, and Elasticsearch domain in the AWS Console.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS控制台中查看堆栈、函数和Elasticsearch域。
- en: 'Publish an event from a separate Terminal with the following commands:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从单独的终端发布事件：
- en: '[PRE48]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Take a look at the `listener` function logs:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 `listener` 函数的日志：
- en: '[PRE49]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Invoke the `search` command:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `search` 命令：
- en: '[PRE50]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后使用 `npm run rm:lcl -- -s $MY_STAGE` 删除栈。
- en: How it works...
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we implement a `listener` function that consumes upstream events
    and populates a materialized view that is used by a Backend For Frontend service.
    This function is a stream processor, such as the one we discussed in the *Creating
    a stream processor* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. The function performs a `filter` for the
    desired events and then transforms the data in a `map` step to the desired materialized
    view. The materialized view is optimized to support the requirements of the query
    needed by the BFF. Only the minimum necessary data is stored, and the optimal
    database type is used. In this recipe, the database type is Elasticsearch. Elasticsearch
    is a good choice for a materialized view when the data must be searched and filtered.
    Note that the `asOf` timestamp is included in the record so that the user can
    access the latency of the data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们实现了一个 `listener` 函数，该函数消费上游事件并填充一个由 Backend For Frontend 服务使用的物化视图。这个函数是一个流处理器，就像我们在第
    1 章 *Getting Started with Cloud-Native* 中讨论的 *创建流处理器* 食谱中提到的那样。该函数对所需事件执行 `filter`
    操作，然后在 `map` 步骤中将数据转换到所需的物化视图中。物化视图已优化以支持 BFF 所需的查询要求。只存储必要的数据，并使用最优的数据库类型。在本食谱中，数据库类型是
    Elasticsearch。当数据必须被搜索和过滤时，Elasticsearch 是物化视图的一个好选择。请注意，记录中包含了 `asOf` 时间戳，以便用户可以访问数据的延迟。
- en: Creating a materialized view in a Cognito dataset
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Cognito 数据集中创建物化视图
- en: In the *Creating a materialized view in DynamoDB* recipe, we discussed how the
    CQRS pattern allows us to design services that are bounded, isolated, and autonomous.
    This allows services to operate, even when their upstream dependencies are unavailable,
    because we have eliminated all synchronous inter-service communication in favor
    of replicating and caching the required data locally in materialized views. In
    this recipe, we will implement an offline-first materialized view in an AWS Cognito
    dataset.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *在 DynamoDB 中创建物化视图* 食谱中，我们讨论了 CQRS 模式如何使我们能够设计边界明确、隔离和自主的服务。这允许服务在它们的上游依赖不可用时运行，因为我们已经消除了所有同步的跨服务通信，转而将所需数据在本地物化视图中复制和缓存。在本食谱中，我们将在
    AWS Cognito 数据集中实现一个离线优先的物化视图。
- en: How to do it...
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE51]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Navigate to the `cncb-materialized-view-cognito` directory with `cd cncb-materialized-view-cognito`.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-materialized-view-cognito` 命令导航到 `cncb-materialized-view-cognito`
    目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查以下内容的 `serverless.yml` 文件：
- en: '[PRE52]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查以下内容的 `handler.js` 文件：
- en: '[PRE53]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Install the dependencies with `npm install`.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署栈：
- en: '[PRE54]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Review the stack, function, and identity pool in the AWS Console.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中检查栈、函数和身份池。
- en: Update the file named `index.html` file with the `identityPoolId` from previous
    output.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前一个输出的 `identityPoolId` 更新名为 `index.html` 的文件。
- en: Open the file named `index.html` in a browser and copy the identity ID for use
    in the next step.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中打开名为 `index.html` 的文件并复制身份 ID 以用于下一步。
- en: 'Publish an event from a separate Terminal with the following commands:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在单独的终端中发布事件：
- en: '[PRE55]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Take a look at the logs:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看日志：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Open the file named `index.html` in a browser and press the Synchronize button
    to retrieve the data from the materialized view:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中打开名为 `index.html` 的文件并按同步按钮从物化视图中检索数据：
- en: '![](img/16110567-c727-48a5-83cc-3657233b5f86.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/16110567-c727-48a5-83cc-3657233b5f86.png)'
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后使用 `npm run rm:lcl -- -s $MY_STAGE` 删除栈。
- en: How it works...
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we implement a `listener` function that consumes upstream events
    and populates a materialized view that is used by a Backend For Frontend service.
    This function is a stream processor, such as the one we discussed in the *Creating
    a stream processor* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. The function performs a `filter` for the
    desired events and then transforms the data in a `map` step to the desired materialized
    view. The materialized view is optimized to support the requirements of the query
    needed by the BFF. Only the minimum necessary data is stored, and the optimal
    database type is used.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们实现了一个`listener`函数，该函数消费上游事件并填充一个由Backend For Frontend服务使用的物化视图。这个函数是一个流处理器，就像我们在第1章中讨论的*创建流处理器*示例中提到的。该函数对所需事件执行`filter`操作，然后在`map`步骤中将数据转换到所需的物化视图中。物化视图已优化以支持BFF所需的查询需求。只存储必要的数据，并使用最佳数据库类型。
- en: In this recipe, the database type is a Cognito dataset. A Cognito dataset is
    a good choice for a materialized view when network availability is intermittent,
    and thus an offline-first approach is needed to synchronize data to a user's devices.
    The data must also be specific to a user so that it can be targeted to the user
    based on the user's `identityId`.  Due to the intermittent nature of connectivity,
    the `asOf` timestamp is included in the record so that the user can access the
    latency of the data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，数据库类型是Cognito数据集。当网络可用性间歇性时，Cognito数据集是物化视图的一个好选择，因此需要采用离线优先的方法将数据同步到用户的设备。数据还必须针对特定用户，以便可以根据用户的`identityId`定位到用户。由于连接的间歇性，记录中包含`asOf`时间戳，以便用户可以访问数据的延迟。
- en: Replaying events
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回放事件
- en: One of the advantages of the Event Sourcing and data lake patterns is that they
    allow us to replay events when necessary to repair broken services and seed new
    services, and even new versions of a service. In this recipe, we will implement
    a utility that reads selected events from the data lake and applies them to a
    specified Lambda function.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 事件源和数据湖模式的优势之一是它们允许我们在必要时回放事件以修复损坏的服务和初始化新的服务，甚至服务的新版本。在这个示例中，我们将实现一个实用程序，该实用程序从数据湖中读取选定的事件并将它们应用到指定的Lambda函数。
- en: Getting ready
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before starting this recipe, you will need the data lake that was created in
    the *Creating a data lake* recipe in this chapter. The data lake should contain
    events that were generated by working through the other recipes in this chapter.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始此示例之前，您需要本章中*创建数据湖*示例中创建的数据湖。数据湖应包含本章其他示例生成的事件。
- en: How to do it...
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE57]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Navigate to the `cncb-replaying-events` directory with `cd cncb-replaying-events`.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令导航到`cncb-replaying-events`目录：`cd cncb-replaying-events`。
- en: 'Review the file named `./lib/replay.js` with the following content:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为`./lib/replay.js`的文件，其内容如下：
- en: '[PRE58]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Install the dependencies with `npm install`.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装依赖项：`npm install`。
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令部署堆栈：`npm run dp:lcl -- -s $MY_STAGE`。
- en: 'Replay events with the following command:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令回放事件：
- en: '[PRE59]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Take a look at the logs:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看日志：
- en: '[PRE60]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用以下命令删除堆栈：`npm run rm:lcl -- -s $MY_STAGE`。
- en: How it works...
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we implement a **Command-Line Interface** (**CLI**) program
    that reads events from the data lake S3 bucket and sends them to a specific AWS
    Lambda function. When replaying events, we do not re-publish the events because
    this would broadcast the events to all subscribers. Instead, we want to replay
    events to a specific function to either repair the specific service or seed a
    new service.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们实现了一个**命令行界面**（CLI）程序，该程序从数据湖S3存储桶读取事件并将它们发送到特定的AWS Lambda函数。在回放事件时，我们不重新发布事件，因为这会将事件广播给所有订阅者。相反，我们希望将事件回放到特定的函数中，以修复特定的服务或初始化新的服务。
- en: When executing the program, we provide the name of the data lake `bucket` and
    the specific path `prefix` as arguments. The `prefix` allows us to replay only
    a portion of the events, such as a specific month, day, or hour. The program uses
    functional reactive programming with the `Highland.js` library. We use a `generator`
    function to page through the objects in the bucket and `push` each object down the
    stream. **Backpressure** is a major advantage of this programming approach, as
    we will discuss in [Chapter 8](5c400ff6-91da-4782-9369-549622d4a0d1.xhtml), *Designing
    for Failure*. If we retrieved all the data from the bucket in a loop, as we would
    in the imperative programming style, then we would likely run out of memory and/or
    overwhelm the Lambda function and receive throttling errors.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行程序时，我们提供数据湖 `bucket` 的名称和特定的路径 `prefix` 作为参数。`prefix` 允许我们仅回放部分事件，例如特定月份、日期或小时。程序使用
    `Highland.js` 库进行功能反应式编程。我们使用 `generator` 函数遍历桶中的对象并将每个对象`推入`流中。**背压**是这种编程方法的主要优势，我们将在第
    8 章 [设计故障](5c400ff6-91da-4782-9369-549622d4a0d1.xhtml)中讨论。如果我们像在命令式编程风格中那样在循环中检索桶中的所有数据，那么我们可能会耗尽内存和/或压倒
    Lambda 函数并收到节流错误。
- en: Instead, we pull data through the stream. When downstream steps are ready for
    more work they pull the `next` piece of data. This triggers the generator function
    to paginate data from S3 when the program is ready for more data.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们通过流式传输提取数据。当下游步骤准备好进行更多工作时会提取`下一个`数据块。这触发了生成函数在程序准备好更多数据时从 S3 分页数据。
- en: When storing events in the data lake bucket, Kinesis Firehose buffers the events
    until a maximum amount of time is reached or a maximum file size is reached. This
    buffering maximizes the write performance when saving the events. When transforming
    the data for these files, we delimited the events with an EOL character. Therefore,
    when we `get` a specific file, we leverage the Highland.js `split` function to
    stream each row in the file one at a time. The split function also supports **backpressure**.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当在数据湖存储桶中存储事件时，Kinesis Firehose 会将事件缓冲，直到达到最大时间量或最大文件大小。这种缓冲最大化了保存事件时的写入性能。在转换这些文件的数据时，我们使用
    EOL 字符分隔事件。因此，当我们`获取`一个特定文件时，我们利用 Highland.js 的 `split` 函数一次流式传输文件中的每一行。split
    函数还支持**背压**。
- en: For each event, we `invoke` the `function` specified in the command-line arguments.
    These functions are designed to listen for events from a Kinesis stream. Therefore,
    we must wrap each event in the Kinesis input format that these functions are expecting.
    This is one reason why we included the Kinesis metadata when saving the events
    to the data lake in the *Creating a data lake* recipe. To maximize throughput,
    we invoke the Lambda *asynchronously* with the `Event` InvocationType, provided
    that the payload size is within the limits. Otherwise, we invoke the Lambda *synchronously*
    with the `RequestReponse` InvocationType. We also leverage the Lambda `DryRun`
    feature so that we can see what events might be replayed before actually effecting
    the change.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个事件，我们根据命令行参数指定的`函数`进行`调用`。这些函数旨在监听来自 Kinesis 流的事件。因此，我们必须将每个事件包装在这些函数期望的
    Kinesis 输入格式中。这也是为什么我们在 *创建数据湖* 配方中将事件保存到数据湖时包含了 Kinesis 元数据的原因之一。为了最大化吞吐量，只要有效负载大小在限制范围内，我们就以
    `Event` InvocationType 异步调用 Lambda。否则，我们以 `RequestReponse` InvocationType 同步调用
    Lambda。我们还利用 Lambda 的 `DryRun` 功能，以便在实际上更改之前查看可能被回放的事件。
- en: Indexing the data lake
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖索引
- en: A data lake is a crucial design pattern for providing cloud-native systems with
    an audit trail of all the events in a system and for supporting the ability to
    replay events. In the *Creating a data lake* recipe, we implemented the S3 component
    of the data lake that provides high durability. However, a data lake is only useful
    if we can find the relevant data. In this recipe, we will index all the events
    in Elasticsearch so that we can search events for troubleshooting and business
    analytics.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是为云原生系统提供系统内所有事件的审计跟踪并支持事件回放能力的关键设计模式。在 *创建数据湖* 的配方中，我们实现了提供高持久性的数据湖的 S3
    组件。然而，如果我们可以找到相关数据，数据湖才有用。在这个配方中，我们将索引 Elasticsearch 中的所有事件，以便我们可以搜索事件进行故障排除和业务分析。
- en: How to do it...
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE61]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Navigate to the `cncb-data-lake-es` directory with `cd cncb-data-lake-es`.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-data-lake-es` 命令导航到 `cncb-data-lake-es` 目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `serverless.yml` 的文件，其内容如下：
- en: '[PRE62]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `handler.js` 的文件，其内容如下：
- en: '[PRE63]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Install the dependencies with `npm install`.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看在 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署栈：
- en: Deploying an Elasticsearch domain can take upwards of 20 minutes.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 Elasticsearch 域可能需要超过 20 分钟。
- en: '[PRE64]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Review the stack, function, and Elasticsearch domain in the AWS Console.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中查看栈、函数和 Elasticsearch 域。
- en: 'Publish an event from a separate Terminal with the following commands:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从单独的终端使用以下命令发布事件：
- en: '[PRE65]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Allow the Firehose buffer time to process, as the buffer interval is 60 seconds.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 允许 Firehose 缓冲时间处理，因为缓冲间隔是 60 秒。
- en: 'Take a look at the `transformer` function logs:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看一下 `transformer` 函数的日志：
- en: '[PRE66]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Open Kibana using the preceding `KibanaEndpoint` output with protocol `https`.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前面的 `KibanaEndpoint` 输出和 `https` 协议打开 Kibana。
- en: Select the `Management` menu, set the index pattern to `events-*`, and press
    the `Next step` button.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 `Management` 菜单，将索引模式设置为 `events-*`，然后按 `Next step` 按钮。
- en: Select `timestamp` as the `Time Filter field name`, and press `Create Index
    pattern`.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 `timestamp` 作为 `Time Filter field name`，然后按 `Create Index pattern`。
- en: Select the `Discover` menu to view the current events in the index.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 `Discover` 菜单以查看索引中的当前事件。
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后使用 `npm run rm:lcl -- -s $MY_STAGE` 删除栈。
- en: How it works...
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The data lake is a valuable source of information. Elasticsearch is uniquely
    suited for indexing this coarse-grained time series information. **Kibana** is
    the data visualization plugin for Elasticsearch. Kibana is a great tool for creating
    dashboards containing statistics about the events in the data lake and to perform
    ad hoc searches to troubleshoot system problems based on the contents of the events.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是一个宝贵的信息来源。Elasticsearch 特别适合索引这种粗粒度的时间序列信息。**Kibana** 是 Elasticsearch 的数据可视化插件。Kibana
    是一个创建包含数据湖中事件统计信息的仪表板和基于事件内容的即席搜索以排查系统问题的优秀工具。
- en: In this recipe, we are using Kinesis Firehose because it performs the heavy
    lifting of writing the events to Elasticsearch. It provides buffering based on
    time and size, hides the complexity of the Elasticsearch bulk index API, provides
    error handling, and supports index rotation. In the custom `elasticsearch` Serverless
    plugin, we create the index template that defines the `index_patterns` and the
    `timestamp` field used to affect the index rotation.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用 Kinesis Firehose，因为它承担了将事件写入 Elasticsearch 的繁重工作。它基于时间和大小提供缓冲，隐藏了
    Elasticsearch 批量索引 API 的复杂性，提供错误处理，并支持索引轮换。在自定义的 `elasticsearch` Serverless 插件中，我们创建索引模板，该模板定义了
    `index_patterns` 和用于影响索引轮换的 `timestamp` 字段。
- en: This recipe defines one delivery stream, because in this cookbook, our stream
    topology consists of only one stream with `${cf:cncb-event-stream-${opt:stage}.streamArn}`.
    In practice, your topology will consist of multiple streams and you will define
    one Firehose delivery stream per Kinesis stream to ensure that all events are
    indexed.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱定义了一个交付流，因为在本书中，我们的流拓扑只包含一个流，即 `${cf:cncb-event-stream-${opt:stage}.streamArn}`。在实际应用中，您的拓扑将包含多个流，并且您将为每个
    Kinesis 流定义一个 Firehose 交付流，以确保所有事件都被索引。
- en: Implementing bi-directional synchronization
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现双向同步
- en: Cloud-native systems are architectured to support the continuous evolution of
    the system. Upstream and downstream services are designed to be pluggable. New
    service implementations can be added without impacting related services. Furthermore,
    continuous deployment and delivery necessitate the ability to run multiple versions
    of a service side by side and synchronize data between the different versions.
    The old version is simply removed when the new version is complete and the feature
    is flipped on. In this recipe, we will enhance the *database-first* variant of
    the Event Sourcing pattern with the *latching* pattern to facilitate bi-directional
    synchronization without causing an infinite loop of events.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生系统被设计为支持系统的持续演进。上游和下游服务被设计为可插拔的。新的服务实现可以添加，而不会影响相关服务。此外，持续部署和交付需要能够并行运行服务的多个版本并同步不同版本之间的数据。当新版本完成并且功能开启后，简单地移除旧版本。在这个配方中，我们将使用
    **数据库优先** 的 Event Sourcing 模式的 **锁定** 模式来增强双向同步，而不会导致事件的无限循环。
- en: How to do it...
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create two projects from the following template:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建两个项目：
- en: '[PRE67]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Review the file named `serverless.yml` with the following content in each project:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看每个项目中名为 `serverless.yml` 的文件，其内容如下：
- en: '[PRE68]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `handler.js` 的文件，其内容如下：
- en: '[PRE69]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Navigate to the `cncb-1-bi-directional-sync` directory with`cd cncb-1-bi-directional-sync`.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-1-bi-directional-sync` 命令导航到 `cncb-1-bi-directional-sync` 目录。
- en: Install the dependencies with `npm install`.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 命令安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 命令运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看在 `.serverless` 目录中生成的内容。
- en: 'Deploy the stack:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署堆栈：
- en: '[PRE70]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Review the stack, functions, and table in the AWS Console.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中查看堆栈、函数和表。
- en: Navigate to the `cncb-2-bi-directional-sync` directory with `cd cncb-2-bi-directional-sync`.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-2-bi-directional-sync` 命令导航到 `cncb-2-bi-directional-sync` 目录。
- en: Repeat steps 5-9 for the `cncb-2-bi-directional-sync` project.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 `cncb-2-bi-directional-sync` 项目重复步骤 5-9。
- en: Navigate back to the `cncb-1-bi-directional-sync` directory with `cd cncb-1-bi-directional-sync`.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-1-bi-directional-sync` 命令导航回 `cncb-1-bi-directional-sync` 目录。
- en: 'Invoke the `command` function to save data to the first service:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `command` 函数将数据保存到第一个服务：
- en: '[PRE71]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Take a look at the logs for the `command` and `trigger` functions:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看日志以了解 `command` 和 `trigger` 函数：
- en: '[PRE72]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Navigate to the `cncb-2-bi-directional-sync` directory with `cd cncb-2-bi-directional-sync`
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-2-bi-directional-sync` 命令导航到 `cncb-2-bi-directional-sync` 目录。
- en: 'Take a look at the logs for the `listener` and `trigger` functions:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看日志以了解 `listener` 和 `trigger` 函数：
- en: '[PRE73]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Invoke the `query` function to retrieve the synchronized data to the second
    service:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `query` 函数从第二个服务检索同步数据：
- en: '[PRE74]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Remove both stacks once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用 `npm run rm:lcl -- -s $MY_STAGE` 命令移除两个堆栈。
- en: How it works...
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Cloud-native systems are architected to evolve. Over time, the functional requirements
    will change and the technology options will improve. However, some changes are
    not incremental and/or do not support an immediate switch from one implementation
    to another. In these cases, it is necessary to have multiple versions of the same
    functionality running simultaneously. If these services produce data, then it
    is necessary to synchronize data changes between the services. This bi-directional
    synchronization will produce an infinite messaging loop if an appropriate **latching**
    mechanism is not employed.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生系统被设计为可以演进的。随着时间的推移，功能需求将发生变化，技术选项也将得到改善。然而，一些变化不是增量性的，或者不支持立即从一个实现切换到另一个实现。在这些情况下，需要同时运行同一功能的多个版本。如果这些服务产生数据，那么需要在服务之间同步数据变化。如果没有采用适当的**锁定**机制，这种双向同步将产生无限的消息循环。
- en: This recipe builds on the database-first variant of the Event Sourcing pattern.
    A user of service one invokes the command function. The `command` function opens
    the `latch` by setting the latch on the domain object to `open`. The `trigger`
    function's `forLatchOpen` filter will only allow publishing an event when the
    latch is `open`, because the open latch indicates that the change originated in
    service one. The `listener` function's `forSourceNotSelf` filter in service one
    ignores the event because the `source` tag indicates that the event originates
    from service one. The `listener` function in service two closes the `latch` before
    saving the data by setting the latch on the domain object to `closed`. The `trigger`
    function in service two does not publish an event, because the `closed` latch
    indicates that the change did not originate in service two.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方基于事件源模式的数据库优先变体。服务一的用户调用命令函数。`命令`函数通过将域对象上的`latch`设置为`open`来打开`latch`。`触发`函数的`forLatchOpen`过滤器只允许在`latch`为`open`时发布事件，因为打开的`latch`表示更改起源于服务一。服务一中的`listener`函数的`forSourceNotSelf`过滤器忽略事件，因为`source`标签表示事件起源于服务一。服务二中的`listener`函数在保存数据之前关闭`latch`，通过将域对象上的`latch`设置为`closed`。服务二中的`触发`函数不发布事件，因为`closed`的`latch`表示更改并非起源于服务二。
- en: This same flow unfolds when the command originates in service two. You can add
    a third and fourth service and more, and all the services will remain in sync.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 当命令起源于服务二时，相同的流程会展开。你可以添加第三个、第四个服务以及更多，所有服务都将保持同步。
