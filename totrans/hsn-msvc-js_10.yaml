- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Microservices** have become a core architectural approach for building scalable
    and flexible applications, but ensuring their health and performance is just as
    important as their functionality. Without proper visibility, identifying issues
    in such a distributed system can be like trying to find a needle in a haystack.
    Think of monitoring and logging as placing cameras and sensors in different parts
    of a bustling city, where each microservice is a shop. These tools help you observe
    how the system is functioning, capture key events, and detect any unusual behavior.
    By establishing robust logging and monitoring practices, you can quickly pinpoint
    problems and keep your microservices running smoothly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Importance of observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized logging with the **Elasticsearch, Logstash, and Kibana** (**ELK**)
    stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with this chapter, we need to have an IDE installed (we prefer
    Visual Studio Code), Postman, Docker, and a browser of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: It is preferable to download our repository from [https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10)
    to easily follow our code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: Importance of observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the world of software, particularly microservices, **observability** is
    crucial. It allows us to gain deep insights into how our system functions by analyzing
    its outputs. Observability is an important concept in monitoring and understanding
    systems. It refers to the ability to gain insight into the internal workings of
    a system by examining its outputs. Let’s try to understand the building blocks
    of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logs**: Logs are detailed records of events that happen within a system.
    They provide a history of what has occurred, including errors, warnings, and informational
    messages. Logs can help in identifying and diagnosing issues by showing a step-by-step
    account of system activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics**: Metrics are numerical values that represent the performance and
    behavior of a system. They can include data such as CPU usage, memory consumption,
    request rates, and error rates. Metrics provide a quantitative measure of the
    system’s health and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerts**: Alerts are notifications that are triggered when metrics reach
    certain thresholds. They are used to inform administrators or operators about
    potential problems or abnormal behavior in real time, allowing for quick responses
    to issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traces**: Traces provide a detailed view of the flow of requests through
    a system. They show how requests move from one component to another, highlighting
    the interactions and dependencies between different parts of the system. Traces
    help in understanding the path of a request and identifying bottlenecks or points
    of failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability helps in understanding what is happening inside a system by using
    logs, metrics, alerts, and traces. Logs give detailed records of events, metrics
    provide numerical data on performance, alerts notify of potential problems, and
    traces show the flow of requests. Together, these outputs offer a comprehensive
    view of a system’s state, aiding in monitoring, troubleshooting, and optimizing
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the concept, let’s dive into the world of logging in
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever driven a car with a broken dashboard? The speedometer might be
    stuck, the fuel gauge unreliable, and warning lights might flicker mysteriously.
    Without clear information about how the engine is running, it’s difficult to diagnose
    problems or ensure a safe journey.
  prefs: []
  type: TYPE_NORMAL
- en: In the world of software, particularly complex systems built with microservices,
    logging plays a similar role. **Logs** are detailed records of events and activities
    within a system.
  prefs: []
  type: TYPE_NORMAL
- en: When building your microservices, just thinking about business implementations
    is not enough. Microservices are, by nature, complex, with many independent services
    interacting. Logging helps understand individual service behavior and pinpoint
    issues within a specific service. When things go wrong, logs provide the audit
    trail to diagnose and fix problems. They help identify errors, dropped requests,
    or performance bottlenecks. Every microservice application should have a proper
    logging mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Logging microservices is essential for diagnostics, but it comes with challenges
    such as handling high volumes of distributed logs across different machines and
    languages, making it harder to aggregate and interpret them. Additionally, missing
    key details and ensuring sensitive information in logs is securely stored add
    complexity to managing logs effectively.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding these challenges, we can implement effective logging strategies
    to keep our microservices teams talking and our systems running smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: Logging levels and node libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before practical examples, we need to understand some basics related to logging
    and one of them is **log levels**. Different log levels are used to categorize
    the severity or importance of log messages.
  prefs: []
  type: TYPE_NORMAL
- en: '**Error logs** capture critical issues that need immediate attention, such
    as crashes or system failures, while **warning logs** highlight potential problems
    that may need investigation. **Info logs** track general system operations, **debug
    logs** provide detailed diagnostic information, and **trace logs** offer the most
    granular level of logging for tracking execution flow.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you don’t need to implement logging algorithms from scratch. One
    of the beauties of Node.Js is it provides a cool collection of libraries for us
    to use. We have different popular log libraries to integrate and use when we build
    our microservices. You can use `winston`, `pino`, `morgan` (log middleware for
    Express.js), `bunyan`, `log4js`, and so on when logging your microservices. We
    will integrate `winston` and `morgan` as a logging library for the current chapter
    but it is up to you to select one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Log formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Node.js microservices, logging formats can be categorized into unstructured
    logging, structured logging, and semi-structured logging. Here is an explanation
    of each:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unstructured logging**: Unstructured logging involves writing plain text
    log messages. This format is straightforward but can be harder to parse and analyze
    programmatically. Here is an example showing unstructured logging:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`.csv`, `.xml`, or other formats as well, but the most used format is JSON.
    This approach makes it easier to search, filter, and analyze logs programmatically.
    Here is an example showing structured logging:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Semi-structured logging**: It combines elements of both unstructured and
    structured logging. It often involves a consistent pattern or delimiter within
    plain text logs, making them somewhat easier to parse than completely unstructured
    logs but not as robust as fully structured logs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explored the importance of logging in microservices, and its challenges,
    and discussed the different log levels, popular Node.js logging libraries, and
    how to choose the right logging format for your microservices. Now, let’s cover
    best practices for logging.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Effective logging can help you understand system behavior, diagnose issues,
    and monitor performance. Here are some essential best practices for logging in
    Node.js microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use a structured logging format**: Ensure logs are structured (e.g., JSON),
    making them easily parsed and searchable by log management tools. This facilitates
    more efficient log analysis and filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Include contextual information**: Enrich logs with context such as timestamps,
    service names, correlation IDs, and user information, enabling better tracing
    and correlation across microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log at appropriate levels**: Apply suitable log levels (error, warn, info,
    debug, trace) to categorize log messages based on severity, which helps in filtering
    logs for relevance and troubleshooting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoid logging sensitive information**: Ensure sensitive, data such as passwords
    and personal details, are redacted or masked before logging to maintain security
    and compliance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centralize logs**: Aggregate logs from all microservices in a centralized
    location using tools such as the ELK stack or cloud-based logging services for
    streamlined monitoring, analysis, and alerting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These practices will help you ensure that your logging is efficient, secure,
    and scalable, making it easier to monitor system behavior, diagnose issues, and
    maintain overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing logging in your microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is really simple to implement logging thanks to the packages of Node.js.
    In this section, we will use `winston` and `morgan` to demonstrate the usage of
    logging in microservices. Let’s integrate log support into the `Account` microservice
    we developed before. To follow this chapter, go to our GitHub repository and download
    the source code and `Ch10` using your favorite IDE. We plan to integrate monitoring
    functionality into our microservice, which we implemented in [*Chapter 9*](B09148_09.xhtml#_idTextAnchor147).
    You can just copy the `Ch09` folder and start to work on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the `winston` and `morgan` libraries on the account microservice,
    run the following command from the `accountservice` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our `package.json` file should contain appropriate versions to use the
    libraries. Let’s first try to use `winston` for logging. Create a file called
    `logger.js` under the `src/log` folder with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This code defines a `winston` logger in Node.js for an application named `account-microservice`.
    Let’s break down the code step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '`const winston = require(''winston'');`: This line imports the `winston` library,
    which is a popular logging framework for Node.js.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`const logger = winston.createLogger({...});`: This line creates a new `winston`
    logger instance and stores it in the logger constant. The curly braces (`{}`)
    contain configuration options for the logger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`level: process.env.LOG_LEVEL || ''info''`: This sets the minimum severity
    level of logs that will be captured. It checks the `LOG_LEVEL` environment variable
    first. If that’s not set, it defaults to the `''info''` level. Levels such as
    `''error''`, `''warn''`, `''info''`, `''debug''`, and so on exist, with `''error''`
    being the most severe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`defaultMeta`: This defines additional information that will be attached to
    every log message. Here, it includes the service name (`account-microservice`)
    and build information (version and `nodeVersion`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transports`: This configures where the log messages will be sent. Here, it’s
    an array defining three transports:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`winston.transports.Console`: This sends logs to the console (usually your
    terminal)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`format: winston.format.combine(...)`: This defines how the log message will
    be formatted when sent to the console. It combines two formatters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`winston.format.colorize()`: This adds color to the console output for better
    readability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`winston.format.simple()`: This formats the message in a simple text format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`winston.transports.File({ filename: ''combined.log'' })`: This sends all logs
    (based on the level setting) to a file named `combined.log`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`format: winston.format.combine(...)`: Similar to the console, it combines
    formatters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`winston.format.json()`: This formats the message as a JSON object for easier
    parsing by machines.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`winston.format.timestamp()`: This adds a timestamp to each log message.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`winston.transports.File({ filename: ''error.log'', level: ''error'' })`: This
    sends only error-level logs to a separate file named `error.log`. It uses the
    same formatters (`json` and `timestamp`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`module.exports =[];:` This line makes the created logger (`logger`) available
    for import and use in other parts of your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, this code sets up a comprehensive logging system for our application.
    It logs messages to both the console and files, with different formatting and
    filtering based on severity level. This allows us to easily monitor application
    behavior, debug issues, and analyze logs for further insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s integrate logging into `accountController` and see the result. Here is
    a simplified version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When you call the endpoint that is responsible for delivering the `getAccountById`
    method, you will get a log message in your terminal and a `combined.log` file.
    We also integrated logging in `index.js` of our application to see whether everything
    is OK with the application running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you have any errors, you’ll get the error message in your terminal and it
    will automatically be added to the `error.log` file.
  prefs: []
  type: TYPE_NORMAL
- en: In Node.js, particularly when using Express.js for building web applications,
    the `morgan` package is a popular tool for streamlining HTTP request logging.
    It automates the process of capturing and recording information about incoming
    requests to your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s why you may use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`morgan` eliminates this by automatically capturing data such as the request
    method, URL, status code, response time, and more. This saves development time
    and ensures consistent logging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`morgan` provides valuable insights into how your application handles requests.
    This can be crucial for debugging purposes, helping you identify potential issues
    or performance bottlenecks within your application’s request processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring application traffic**: By reviewing the logs, you can gain a better
    understanding of your application’s traffic patterns. This can be useful for monitoring
    overall application health, identifying usage trends, and making informed decisions
    about scaling or resource allocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combined`, `common`, and `dev`) that cater to different levels of detail.
    You can also create custom formats to capture specific data points relevant to
    your application’s needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ve already installed the `morgan` package and it is time to use it. We usually
    use it as middleware and here is how to implement your own `morgan` middleware.
    Create a new file called `morganmiddleware.j`s under the `src/middlewares` folder.
    Copy and paste the following inside it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a custom middleware function for logging HTTP requests in
    JSON format using the `morgan` library. The code defines a logging mechanism that
    uses the `morgan` middleware to log HTTP requests in a Node.js application. It
    integrates logging with both a `combined.log` file and a Logstash server for external
    log management.
  prefs: []
  type: TYPE_NORMAL
- en: '`morganFormat` is a custom format that logs details such as the HTTP method,
    URL, status code, and response time for each request. These logs are then handled
    by a custom `messageHandler` function.'
  prefs: []
  type: TYPE_NORMAL
- en: In the `messageHandler`, the incoming log message is parsed from a JSON string
    into an object. The parsed log is then sent to Logstash using the `logger.info`
    function, which is imported from the `logger-logstash` module. At the same time,
    the original log message is also written to a local file named `combined.log`.
    This is done by creating a write stream to the file using Node.js’s `fs` module,
    which appends each new log to the file.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the custom `morganMiddleware` is created using the `morgan` function,
    with the logging stream directed to `messageHandler`. This middleware is then
    exported to be used in other parts of the application for logging purposes.
  prefs: []
  type: TYPE_NORMAL
- en: This setup ensures that HTTP request logs are recorded both locally in a file
    and sent to an external Logstash service for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re done with middleware functionality and now it is time to apply it. Open
    `app.js`, which is where we have configured our middleware flow and make the following
    changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Before everything else in the middleware flow, we need to use `morganMiddleware`
    and now you can just remove the previous logging functions we did via `winston`.
    Run the application and call any endpoint you want. Before running the account
    microservice, make sure that Docker is running with the appropriate `docker-compose`
    file. Don’t forget to run both `docker-compose` files (`accountservice/docker-compose.yml`
    and `accountservice/elk-stack/docker-compose.yml`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the terminal output for logging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Terminal output for logging](img/B09148_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Terminal output for logging'
  prefs: []
  type: TYPE_NORMAL
- en: Check the `combined.log` file and terminal window to see the logs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover centralized logging.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized logging with Elasticsearch, Logstash, and Kibana (ELK) stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a microservices architecture, where applications are broken down into independent,
    loosely coupled services, **centralized logging** becomes crucial for effective
    monitoring and troubleshooting. We have a lot of reasons to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spread out logs**: Normally, logs would be all over the place, on each individual
    mini-app. Imagine hunting for a problem that jumps between them – like looking
    for a lost sock in a messy house!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**See everything at once**: Centralized logging brings all the logs together
    in one spot, like putting all your socks in a basket. This way, you can easily
    see how everything is working and if any parts are causing trouble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fixing problems faster**: With all the logs in one place, it’s like having
    a super magnifying glass to find issues. You can search through the logs quickly
    to see what went wrong, saving you time and frustration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keeping an eye on things**: Centralized logging often works with monitoring
    tools, like having a dashboard for your socks. This lets you see how well everything
    is performing and identify any slow spots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log care made easy**: Having everything in one place makes taking care of
    the logs much simpler. It’s like having a dedicated sock drawer! Tools can be
    used to keep things organized, get rid of old logs, and follow any rules you need
    to follow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using centralized logging, you get a powerful tool to watch over your microservices,
    fix problems faster, and keep everything running smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: We have many different options to implement centralized logging when building
    microservices and one of them is the ELK stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ELK stack** is a powerful suite of tools used for centralized logging,
    real-time search, and data analysis. Here’s a brief overview of each component:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elasticsearch**: This is a distributed search and analytics engine. We use
    it to store, search, and analyze large volumes of data quickly and in near real
    time. Elasticsearch is built on Apache Lucene and provides a RESTful interface
    for interacting with your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logstash**: This is a server-side data processing pipeline that ingests data
    from multiple sources simultaneously, transforms it, and then sends it to your
    chosen *stash*, such as Elasticsearch. It can handle a variety of data formats
    and provides a rich set of plugins to perform different transformations and enrichments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana**: This is a data visualization and exploration tool used for analyzing
    and visualizing the data stored in Elasticsearch. It provides a user-friendly
    interface for creating dashboards and performing advanced data analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But how do they work together? Well, Logstash collects and processes the log
    data from various sources (e.g., server logs, application logs, network logs)
    and forwards it to Elasticsearch. Elasticsearch indexes and stores the data, making
    it searchable in near real time. Kibana connects to Elasticsearch and provides
    the tools necessary to query, visualize, and analyze the data, allowing users
    to create custom dashboards and reports.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple benefits of using the ELK stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: The ELK stack can scale horizontally, allowing you to handle
    large volumes of log data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time insights**: Elasticsearch’s real-time search capabilities provide
    instant insights into your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Logstash’s ability to ingest data from various sources and
    formats makes it highly flexible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualization**: Kibana’s rich visualization options enable you to create
    interactive dashboards for monitoring and analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open source**: The ELK stack is open source, with a large community and a
    wealth of plugins and extensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As always, we prefer to install tools via Docker and it applies to the ELK stack
    also. Go to the `Ch10/accountservice/elk-stack` folder and run the `docker-compose.yml`
    file using the `docker-compose up -d` command. We will not dive into the details
    of `docker-compose` because we did it in our previous chapters. Simply, we install
    Elasticsearch, Logstash, and Kibana in the given `docker-compose.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to Logstash
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can collect and transform logs using Logstash. We are able to get input
    from multiple different sources such as logs generated by other applications,
    plain text, or networks. For log ingestion, we have different approaches that
    we can follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct transport**: We can configure our application to directly send data
    to Elasticsearch. Yes, that is an option but not a preferable way of ingesting
    logs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write logs to the file**: As we implement in our microservices, it is preferable
    to implement such types of logging because other applications, such as Logstash,
    as a separate process, will be able to read, parse, and forward the data to Elasticsearch.
    It requires more configuration but it is the more robust and preferable way of
    doing logging for production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logstash configuration is typically written in a configuration file (e.g.,
    `logstash.conf`). This file consists of three main sections: `input`, `filter`,
    and `output`. Each section defines different aspects of the data processing pipeline.
    Here’s a breakdown of each section and an example configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: The `input` section defines where Logstash should collect data from. This could
    be from files, syslog, **Transmission Control Protocol** (**TCP**)/ **User Datagram
    Protocol** (**UDP**) ports, or various other sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `filter` section is used to process and transform the data. Filters can
    parse, enrich, and modify the log data. Common filters include `grok` for pattern
    matching, `mutate` for modifying fields, and `date` for parsing date/time information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `output` section specifies where the processed data should be sent. This
    could be Elasticsearch, a file, a message queue, or another destination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To see the detailed explanation in action, simply open the `logstash.conf`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s dive into the details of the given configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tcp { port => 5000 }`: This section defines an `input` plugin that listens
    for data coming in over a TCP socket on port `5000`. Any logs or events sent to
    this port will be ingested by Logstash.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`json { source => "message" }`: This `filter` plugin parses the incoming data,
    assuming it’s in JSON format, and extracts the value from the field named `message`.
    This field is likely where the actual log content resides. By parsing it as JSON,
    Logstash can understand the structure of the data and make it easier to work with
    in subsequent processing steps.*   `elasticsearch { hosts => ["elasticsearch:9200"],
    index => "app-%{+YYYY.MM.dd}" }`: This `output` plugin sends the processed data
    to Elasticsearch, a search and analytics engine optimized for handling large volumes
    of log data. The host’s option specifies the location of the Elasticsearch instance
    (presumably running on a machine named `elasticsearch` with the default port `9200`).*   The
    `index` option defines a dynamic index naming pattern. Each day’s logs will be
    stored in a separate index named `app-YYYY.MM.dd` (where `YYYY` represents the
    year, `MM` the month, and `dd` the day). This pattern helps in efficient log management
    and allows you to easily search for logs from specific dates.*   `stdout { }`:
    This `output` plugin simply prints the processed data to the console (standard
    output) for debugging or monitoring purposes. The empty curly braces (`{}`) indicate
    the default configuration for the standard output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This Logstash configuration ingests data from TCP source, parses JSON-formatted
    logs, and then sends them to Elasticsearch for storage and analysis. Daily indexes
    are created for organized log management. The `stdout` plugin provides a way to
    view the processed data during development or troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s integrate logging into our account microservice. Create a new file called
    `logger-logstash.js` under the `accountmicroservice/src/log` folder with the following
    content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have already talked about `winston` configuration and the only new thing
    here is `logstashTransport`. We added two `transports` channel: one for the terminal’s
    console and the other one for sending logs to `logstash`. To use the given file
    with `morgan`, just change `morganmiddleware.js`’s logger to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, run our application and go to `http://localhost:5601`; this is our Kibana.
    From the left menu, find **Management** | **Dev tools**. Click on the **Execute**
    button and you will see the total value with your logs (*Figure 10**.2*)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2: Getting logs from Kibana](img/B09148_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Getting logs from Kibana'
  prefs: []
  type: TYPE_NORMAL
- en: Now, our logs are flowing to the ELK stack. You can think of Elasticsearch as
    a search and analytics engine with a data warehouse capability.
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to Elasticsearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Elasticsearch** is a powerhouse search engine built for speed and scalability.
    At its core, it’s a distributed system designed to store, search, and analyze
    large volumes of data in near real time. It is document-oriented and uses JSON.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive deeper into the key attributes of Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed**: Elasticsearch can store data across multiple nodes (servers)
    in a cluster. This distribution allows for the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: If one node fails, other nodes can handle the requests,
    keeping your search service operational.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal scaling**: You can easily add more nodes to the cluster as your
    data volume or search traffic grows.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalable**: As mentioned previously, Elasticsearch excels at horizontal scaling.
    You can add more nodes to the cluster to handle increasing data and search demands.
    This scalability makes it suitable for large datasets and high search volumes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search and analytics**: Elasticsearch specializes in full-text search, which
    analyzes the entire text content of your documents. This allows you to search
    for keywords, phrases, and even concepts within your data. It also provides powerful
    analytics capabilities. You can aggregate data, identify trends, and gain insights
    from your search results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible search**: Elasticsearch offers a wide range of query options. You
    can search for specific terms, filter results based on various criteria, and perform
    complex aggregations. This flexibility allows you to tailor your searches to your
    specific needs and uncover valuable information from your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search speed**: Due to its distributed architecture and efficient indexing
    techniques, Elasticsearch delivers fast search results. This is crucial for applications
    where users expect an immediate response to their queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we provided a brief overview of Elasticsearch, focusing on
    the core attributes that make it a powerful tool for search and data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to Kibana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kibana** is the last item in our ELK stack. It is the visualization layer
    that complements the data storage and search muscle of Elasticsearch. It’s an
    open source platform that acts as a window into your Elasticsearch data, allowing
    you to explore, analyze, and understand it with clear visualizations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kibana has the following interesting possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualization powerhouse**: Kibana lets you create interactive dashboards
    with various charts, graphs, and maps. This visual representation transforms raw
    data into easily digestible insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data exploration**: Kibana provides tools to explore, search, and filter
    your data within Elasticsearch. You can drill down into specific details and uncover
    hidden patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sharing insights**: Created dashboards can be shared with others, fostering
    collaboration and promoting data-driven decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several compelling reasons to choose Kibana for microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Effortless integration**: As part of the ELK Stack (Elasticsearch, Logstash,
    and Kibana), Kibana integrates seamlessly with Elasticsearch. This tight integration
    streamlines the process of visualizing data stored within Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time insights**: Kibana allows you to visualize data in near real-time,
    providing valuable insights as your data streams in. This is crucial for applications
    requiring immediate response to changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization options**: Kibana offers a wide range of visualizations and
    customization options. You can tailor dashboards to fit your specific needs and
    effectively communicate insights to your audience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open source and free**: Being open source, Kibana is free to use and offers
    a vibrant community for support and development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microservices architectures involve multiple, independent services working
    together. Kibana shines in this environment for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring performance**: Visualize key metrics from your microservices on
    Kibana dashboards to monitor their health and performance. This helps identify
    bottlenecks and ensure smooth operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log analysis**: Centralize and analyze logs from all your microservices within
    Kibana. This unified view simplifies troubleshooting issues and pinpointing errors
    across the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application insights**: Gain insights into how users interact with your microservices
    by visualizing usage patterns and trends within Kibana. This data can guide development
    efforts and improve user experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about the ELK stack, diving into details of Elasticsearch querying
    and Kibana-related topics, such as custom dashboards, and working with metrics
    are beyond the scope of this book and that is why we finish our chapter only with
    a simple introduction to them.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter delved into the crucial aspects of monitoring and logging in microservices
    architecture, emphasizing the importance of observability in maintaining the health
    and performance of distributed systems. We began by explaining how observability
    provides deep insights into system behavior through key components such as logs,
    metrics, alerts, and traces.
  prefs: []
  type: TYPE_NORMAL
- en: We then shifted focus to the importance of logging in microservices, which is
    essential for capturing detailed records of system events, identifying performance
    bottlenecks, and diagnosing issues in real time. We explored different log levels—error,
    warning, info, debug, and trace—and discussed how they help categorize log messages
    based on severity, making troubleshooting more efficient. Additionally, the chapter
    covered popular logging libraries in Node.js such as `winston` and `morgan`.
  prefs: []
  type: TYPE_NORMAL
- en: Following the theoretical foundation, we demonstrated how to implement logging
    in a real-world scenario by integrating `winston` and `morgan` into the account
    microservice.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter then moved on to centralized logging, introducing the powerful ELK
    stack. We explained how Logstash collects and processes log data, Elasticsearch
    stores and indexes the data for real-time search, and Kibana visualizes the information
    through interactive dashboards. By integrating these tools, we established a centralized
    logging system that simplifies log collection, analysis, and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how to manage multiple microservices effectively
    using popular microservices architecture elements. You’ll learn about using an
    API gateway, which acts as a single entry point to manage requests and direct
    them to the correct services, as well as organizing data and actions within your
    system through CQRS and Event Sourcing, two important methods that help handle
    complex data flows. By the end, you’ll have a clear understanding of building
    and connecting services in a way that’s efficient and easy to maintain.
  prefs: []
  type: TYPE_NORMAL
