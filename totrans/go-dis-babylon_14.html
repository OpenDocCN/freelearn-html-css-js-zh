<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-287"><a id="_idTextAnchor290"/>14</h1>
<h1 id="_idParaDest-288"><a id="_idTextAnchor291"/>Extended Topics, Extended</h1>
<p>This is a chapter about endings, but it is also a chapter about beginnings. Our journey together on this long haul may be approaching its destination, but this is just the beginning of your personal journey with Babylon.js. In this chapter, we abandon any pretense of linear or sequential progress, and instead, we will be bouncing between several disparate topics that will each provide individual jumping-off points to help you go the distance with Babylon.js.</p>
<p>When navigating unfamiliar streets, it can be useful to have a guide, someone who is knowledgeable about an area. Someone with deep practical experience, who knows how to guide visitors and new arrivals to the best places and sights. Our Space-Dispatcher has located several talented individuals to show us areas of Babylon.js that we didn’t get to see or learn about during our trip.</p>
<p>In this chapter, we’re going to visit two active construction sites in the metatropolies of BJS. At the first of those sites, we’ll learn about ongoing efforts to bring the simple elegance of Babylon.js out of the web and directly onto device hardware with Babylon Native. The second of those sites encompasses the exciting world (“metaverse”) of <strong class="bold">augmented reality</strong> (<strong class="bold">AR</strong>) and <strong class="bold">virtual reality</strong> (<strong class="bold">VR</strong>) in the form of <strong class="bold">WebXR</strong> – the new standard for web-based AR/VR applications.</p>
<p>After those stops, we’ll meet our first guide, BJS community member and serial helper of people on the forum, Andrei Stepanov, who will take us through the loading docks and into the Babylon.js Mall. He’ll show us glittering displays of the latest gadgets on a tour of how easy it is to use BJS with Content Management Systems and e-commerce platforms. Parting ways with Mr. Stepanov, we next visit a shiny new transport terminal as we go to meet our last guide, Erich Loftis.</p>
<p>Erich has been ranging out on a journey of his own for some time now, and he’s going to entertain and enlighten us with the story of his quest seeking the Holy Grail of photorealism in 3D graphics – <strong class="bold">Real-Time Ray (Path) Tracing</strong>. That’s just a preview of what’s to come<a id="_idIndexMarker1087"/> because it’s time to take a hard right and put on a hard hat as we pull<a id="_idIndexMarker1088"/> into our first construction site for AR and VR with <strong class="bold">WebXR</strong>.</p>
<p>There’s always more to learn in any given technical arena, and that applies double or more when the topic is rapidly changing. <strong class="bold">WebXR</strong> is the standard for developing web-based AR and VR, and it qualifies under the “double-or-more” policy with its rapidly evolving mix of standard and support. As we learn about <strong class="bold">WebXR</strong>, we’re not going to focus on every feature of the standard – that would be like trying to ice-skate up a hill during a heatwave. What we’re going to focus on are the features and capabilities of Babylon.js that allow you as the developer to write applications that make use of <strong class="bold">WebXR</strong> while lowering the risks involved in those changing standards and APIs.</p>
<p>Here are the topics that we’ll be covering in this chapter:</p>
<ul>
<li>AR and VR with <strong class="bold">WebXR</strong></li>
<li>A tour of the <strong class="bold">Babylon.js Native project</strong></li>
<li>Incorporating 3D content into a website</li>
<li>Tracing out a path to advanced rendering</li>
</ul>
<h1 id="_idParaDest-289"><a id="_idTextAnchor292"/>AR and VR with WebXR</h1>
<p>The inexorable march of Moore’s law has brought increasingly greater computing power into increasingly smaller <a id="_idIndexMarker1089"/>microchips at a steady rate for long enough that the casual consumer has a staggering amount of raw computational silicone<a id="_idIndexMarker1090"/> contained in their smartphones<a id="_idIndexMarker1091"/> and tablets. There’s enough processing throughput in the average smartphone now that it’s realistic to entertain scenarios such as AR and VR. </p>
<p>AR is a category of applications that encompasses a large variety of different use cases and scenarios. The common feature shared by these scenarios is that they make use of a device’s camera, location, orientation, and other sensors to emplace 3D content into a depiction of the real world. VR is very similar to AR, save that instead of the content being immersed in the user’s world (the real world), the user is immersed in the content (the virtual world). </p>
<p>Whether considering an AR and VR experience, it is important to keep in mind that both are more of a spectrum than a binary quality – there’s no rule that says something must use <em class="italic">X</em> percent of features to <a id="_idIndexMarker1092"/>be considered an AR or a VR app. That would be a silly piece of gatekeeping. </p>
<p class="callout-heading">Note</p>
<p class="callout">If you are looking for a great band name, Reality-Virtuality Spectrum/Continuum are both cool sounding ones! Read more about the Virtuality Spectrum at <a href="https://creatxr.com/the-virtuality-spectrum-understanding-ar-mr-vr-and-xr/">https://creatxr.com/the-virtuality-spectrum-understanding-ar-mr-vr-and-xr/</a>. </p>
<p>Consider this – an application may only support basic head tracking and stereoscopic views, but it is still a<a id="_idIndexMarker1093"/> VR application. Similarly, a simple application that draws a rabbit’s ears over a person’s image in a video feed could technically be considered an AR application. Most of the time when discussing AR and VR in <a id="_idIndexMarker1094"/>context of web development, it is assumed that the focus is on the VR side of things. Historically, that has been accurate, but it won’t always be the case. By examining some historical context, it will be clearer how this might have come to pass and when to expect that to change.</p>
<h2 id="_idParaDest-290"><a id="_idTextAnchor293"/>An Abridged History of AR/VR on the WWW</h2>
<p>In the wide world of<a id="_idIndexMarker1095"/> web development, there have been numerous <a id="_idIndexMarker1096"/>attempts to bring about a standardized set of APIs for VR content, such as the <strong class="bold">VRML</strong> standard. The last-but-one effort was called <strong class="bold">WebVR</strong>, which was aimed at <a id="_idIndexMarker1097"/>VR content with little to no consideration for AR – not out of neglect, but simply because AR didn’t exist in any commercially accessible form until relatively recently (let’s call it ca. 2015 or so). </p>
<p>By 2018, it had become clear that to make AR a commercially viable application, it needed to be able to run on the web. The problem is deceptively simple but deviously hard to solve. Consumers don’t want to have to install five separate apps to browse five separate furniture stores just to display selections of furniture in the prospective buyer’s living room, but they’re happy to go to a website that offers the same! Unfortunately, requirements for even basic AR involve accessing device and sensor data that normally isn’t available to the browser JavaScript sandbox, where performance can also sometimes be suboptimal. </p>
<p>The <strong class="bold">WebXR</strong> standard was<a id="_idIndexMarker1098"/> introduced in 2018 by an industry-wide consortium of hardware and software manufacturers. This standard encapsulates and abstracts many areas that were left out of the previous <strong class="bold">WebVR</strong> standard, such as object/body part tracking, unified controller interfaces that account for the many different inputs possible with AR/VR, and in general, everything needed to program a world-class experience. All the cool kids (Apple, Google, Meta/Facebook, Samsung, Microsoft, et al) are a part of this standards body, which means that developers and consumers alike should be able to benefit from an explosion of innovation in the commercial AR/VR space. Or at least that should have been the <a id="_idIndexMarker1099"/>case. Devices dedicated to AR, such as Microsoft’s HoloLens, as well as devices dedicated to VR, such as Oculus, have started to proliferate the consumer electronics market, but progress in general for supporting the <strong class="bold">WebXR</strong> standard has been stunted at best by the actions – or rather, a lack of action – from one of the most influential members of that consortium.</p>
<p>While most of the consortium members have been busy working to implement key <strong class="bold">WebXR</strong> features and standards, one of its members – Apple – has sat mostly on the sidelines. They have recently released their new iOS hardware-based application SDK known as <strong class="bold">ARKit</strong>, which is a <a id="_idIndexMarker1100"/>potential reason for Apple’s inaction on supporting <strong class="bold">WebXR</strong>. Allowing the hardware access that <strong class="bold">WebXR</strong> requires would effectively involve breaking the iron grip that <strong class="bold">WebKit</strong> has on web rendering on iOS. That’s unfortunate, because in the United States, iOS enjoys roughly 60 percent of the market share, meaning that most of the US market is inaccessible to companies, individuals, and organizations who want to develop and provide AR experiences and products on the web (for contrast, iOS holds less than 30 of the percent market share worldwide outside of the US. Android owns the bulk of the overseas market). The news doesn’t get too much better on the Apple front: as of summer 2022, it does not appear likely that Apple will release support for <strong class="bold">WebXR</strong> in its <strong class="bold">WebKit</strong> rendering engine at any point within the upcoming 6 to 12 months. </p>
<p class="callout-heading">Important Note</p>
<p class="callout">Pending anti-trust litigation and legislation debate is ongoing in numerous courts and legislatures around the world. It is possible that the outcome of some of these matters could result in Apple allowing alternative web engines (such as Chromium) to be used in iOS. All bets are off if that happens!</p>
<p>With all that depressing talk of <strong class="bold">WebXR</strong> not being supported on iOS, constantly shifting standards, and frequent breaking changes, what’s the silver lining? How is the glass half-full, and why would you <a id="_idIndexMarker1101"/>want to subject yourself to this type of software engineering misery? Let’s all say it together now: “Because Babylon.js’ Got You” with the <strong class="bold">WebXR</strong> Experience Helper – blunting sharp pains into dull aches.</p>
<h2 id="_idParaDest-291"><a id="_idTextAnchor294"/>Building Tomorrow, Today with the WebXR Experience Helper</h2>
<p>It’s a founding precept<a id="_idIndexMarker1102"/> of Babylon.js that backward compatibility is of paramount importance. Code written 10 years ago on BJS 1.0 still largely works in BJS 5.0, which is quite an achievement when talking about tech and the web! When dealing with something like <strong class="bold">WebXR</strong>, where features and APIs can come and go quickly though, does it even make sense to try and build a production application against such a moving target? </p>
<p class="callout-heading">Note</p>
<p class="callout">Recalling our previous discussion about rhetorical questions and their answers, you should already know the answer to that question to be “YES!”</p>
<p>The BJS <code>WebXRExperienceHelper</code> is a component that does exactly what it says it does on the box – that is, to help with <strong class="bold">WebXR</strong> implementation by setting up all of the necessary elements for an immersive session. The <strong class="bold">Default Experience</strong> provided is set up for a VR session along with basic features such as pointer tracking and teleportation while, of course, providing the ability to enable, attach, and use other features in collaboration with the <strong class="bold">FeatureManager</strong>. </p>
<p>The important concept to<a id="_idIndexMarker1103"/> understand about how the <strong class="bold">FeatureManager</strong> works is the process of enabling a given feature – at either a specific version, the “latest,” or “stable” version – and making it available to be attached to a Scene. Enabling a feature and attaching to the Scene is, along with their associated converse operations such as disabling and detaching, a two-step process for the application code. Two steps for the application, but hidden under the hood lies a whole host of sub-operations. Things such as browser feature detection, device capability enumeration, and more all occur during the feature enabling stage. The result of the enabling process leaves the <strong class="bold">WebXRSession</strong> with a new set of <strong class="bold">Observables</strong> related to the newly enabled feature(s). These Observables are now available to be used to attach those features to a given Scene. </p>
<p>The reason why this is an important concept is because while it isn’t necessary to use <code>WebXRExperienceHelper</code> or <code>FeatureManager</code>, those components provide your code with the critical ability to isolate itself from the effects of external changes. Production applications can make use of the latest VR/AR functionality available on a user’s device with confidence that they won’t suddenly break when the standard or a web browser’s support<a id="_idIndexMarker1104"/> for the standard changes. The abstractions provided allow developers to write, extend, and maintain applications that leverage cutting-edge browser capabilities while gracefully degrading functionality for devices that don’t. </p>
<p><strong class="bold">WebXR</strong> has some incredibly exciting features and capabilities available today in Chrome- and Mozilla-based browsers, though some might require users to “unhide” features via flags. The types and features of applications built using <strong class="bold">WebXR</strong> are just beginning to be explored, and the Babylon.js team intends to be there to help developers use them the entire way. Unfortunately, that’s all the time we’ve got for this construction site visit – there are other places to go and things to see, after all, and we have a schedule to keep!</p>
<h2 id="_idParaDest-292"><a id="_idTextAnchor295"/>Further Reading</h2>
<ul>
<li><em class="italic">WebXR Experience Helpers</em>: <a href="https://doc.babylonjs.com/divingDeeper/webXR/webXRExperienceHelpers">https://doc.babylonjs.com/divingDeeper/webXR/webXRExperienceHelpers</a></li>
<li><em class="italic">WebXR Features Manager</em>:  <a href="https://doc.babylonjs.com/divingDeeper/webXR/webXRFeaturesManager">https://doc.babylonjs.com/divingDeeper/webXR/webXRFeaturesManager</a></li>
<li>Demos and Playgrounds: <a href="https://doc.babylonjs.com/divingDeeper/webXR/webXRDemos">https://doc.babylonjs.com/divingDeeper/webXR/webXRDemos</a></li>
</ul>
<p>Our next visit will be to the grounds of a sprawling new technology campus in the Babylon.js “Metatropolis.” This campus is the home of the <strong class="bold">Babylon Native</strong> project – an impressive, ambitious, and particularly complex undertaking. Among other areas of study, Native offers one potential solution to the problems posed around iOS support for <strong class="bold">WebXR</strong>. Let’s learn more about Native and what that solution looks like as part of our campus tour of the Babylon Native ecosystem.</p>
<h1 id="_idParaDest-293"><a id="_idTextAnchor296"/>A Tour of the Babylon.js Native Project</h1>
<p>Babylon.js is primarily used as part of a web application, but that’s not the only place where it can add value. Sometimes, an application needs to target multiple platforms with the same code base. Other<a id="_idIndexMarker1105"/> times, an existing device application wants to be able to easily add 3D rendering activities that are secondary to the application’s purpose (for example, in a scientific simulation, the renderer is simply drawing the output of the simulation onto the screen). Specific requirements might include the need for AR capabilities on platforms that include iOS. </p>
<p>In each of those scenarios (and more that aren’t listed), there is a place for Babylon.js to add value to an application. What’s commonly referred to as “Babylon Native” in the singular, proper sense is actually a collection of technologies that apply to a specific range of scenarios. Every scenario is different and should have a solution tailored to the specific needs of the situation, and the set of technologies that comprise Babylon Native allows you as the developer to pick and choose where and when to apply them. One way to understand the technologies is to show them along a spectrum with a fully native app at one end and a fully web-native app at the other:</p>
<div><div><img alt="Figure 14.1 – Spectrum of application types. Source: https://github.com/BabylonJS/BabylonNative/blob/master/Documentation/WhenToUseBabylonNative.md" height="158" src="img/Figure_14.01_B17266.jpg" width="1557"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Spectrum of application types. Source: <a href="https://github.com/BabylonJS/BabylonNative/blob/master/Documentation/WhenToUseBabylonNative.md">https://github.com/BabylonJS/BabylonNative/blob/master/Documentation/WhenToUseBabylonNative.md</a></p>
<p>The preceding diagram (taken from the BJS Native docs, linked in the caption) is one method of depicting the Native Collective that shows the relative scale of how close to the native device hardware a particular component or framework lies. </p>
<p>In his blog post about <a id="_idIndexMarker1106"/>the technical underpinnings of BJS Native at <a href="https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb">https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb</a>, Sergio explains how the Babylon Native parts fit from a different perspective:</p>
<div><div><img alt="Figure 14.2 – A layered diagram of how Babylon Native works in the absence of WebGL. Diagram source: https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb" height="498" src="img/Figure_14.02_B17266.jpg" width="775"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – A layered diagram of how Babylon Native works in the absence of WebGL. Diagram source: <a href="https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb">https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb</a></p>
<p>Whether using <strong class="bold">Babylon React Native</strong> or simply <strong class="bold">Babylon Native</strong>, the preceding diagram shows how the unifying abstraction layer of <strong class="bold">Babylon Native</strong> covers the ugly and sometimes chaotic<a id="_idIndexMarker1107"/> mess of talking to various hardware components, such as the BGFX cross-platform graphics driver with ARCore and ARKit for other device sensor and input API abstractions. Having these concepts in mind, we can now consider a few potential usage scenarios where it makes sense to take a good look at the options presented by Babylon Native. </p>
<h2 id="_idParaDest-294"><a id="_idTextAnchor297"/>Choosing Babylon Native</h2>
<p>The decision on <a id="_idIndexMarker1108"/>whether Babylon Native is a good fit for a given project can be complex. The docs for Native have an entire page devoted to a questionnaire to help you determine what approaches are worth the most research – and what aren’t – and while helpful, they can be better understood via a contrived scenario.</p>
<p>If your application is based on <strong class="bold">React Native</strong>, there is a light integration option and a full integration option. The light option is to use a <strong class="bold">WebView</strong> to host the WebGL context and canvas. This has the advantage of being able to take advantage of the <strong class="bold">Just-In-Time</strong> (<strong class="bold">JIT</strong>) compilation of JavaScript, meaning JS code will tend to be faster than when not using a WebView for some platforms. The full integration option is to use <strong class="bold">Babylon React Native</strong>. Here’s what we might imagine the app this looks like.</p>
<h2 id="_idParaDest-295"><a id="_idTextAnchor298"/>The Evolution of a Babylon Native App</h2>
<p>The LARP’in app is an app for <strong class="bold">Live Action Role Players</strong> – people who like to take the table out of tabletop <a id="_idIndexMarker1109"/>games and act out the gameplay themselves using the app to coordinate events, chat, and so on, with all the different luxuries that people have come to expect from a modern Web Application. The “Player App” is built using React and has enjoyed a steady run of releases, enhancing and extending the site’s functionality. The app’s creators want to allow event schedulers to be able to manage events offline (because event spaces sometimes don’t have reception) so they’ve added PWA capabilities, making everyone happy. </p>
<p>Then one day, some LARP’in LARPers were playing <em class="italic">Pokémon Go</em> when they had the realization that while LARPing is cool, what’s even cooler is LARPing… with AR! Players would be able to see visualizations of their spells cast, detect traps with skill rolls, and walk around exploring a fantasy world brought to life. Their existing LARP tools consist of some home-brewed Bluetooth-connected devices embedded into items (for example, a sword) that register hits and similar game-management tasks by lighting up or beeping, but that’s the extent of it. Many of the members have iOS devices, while others are on Android, and there are even a few odd souls clinging to heavily tweaked versions of Windows Mobile (bless their souls). In 2021, the group won first prize at a cosplay competition, which came with enough funds to allow the group to purchase a set of <strong class="bold">HoloLens</strong> headsets along with an <strong class="bold">Oculus VR</strong> device for a member whose health problems prevented them from attending events in person. The AR-enhanced Player App would need to be able to talk to these devices to be useful as well as utilize existing functionality within the Player App (for example, displaying the player’s inventory). Finally, the group has developed a custom C# desktop application they appropriately call the “GM App” to connect to these BT devices and to act as a game’s referee (often called a <strong class="bold">GM</strong> or <strong class="bold">Game Master</strong>). The app’s <a id="_idIndexMarker1110"/>maintainers have the wonderful opportunity here to evolve the app toward their vision in valuable and discrete steps:</p>
<ol>
<li>Bring the app over into a <strong class="bold">React Native</strong> application that otherwise behaves exactly as it currently does. </li>
<li>Add basic rendering capabilities with Babylon in a <strong class="bold">WebView</strong>. This will allow the team to release the same functionality with the same code base as the web app. </li>
</ol>
<p>Build local mesh connectivity between BT and WiFi devices that feeds data into <strong class="bold">React Native </strong>app. </p>
<ol>
<li value="3">Integrate a <strong class="bold">Babylon Native</strong> rendering of a pure 3D scene in the C# application to show GMs different views of the action (picture a sword fight where the swords have sensors embedded in them, with the scene depicting the state of the swords as relayed by sensors).</li>
<li>Transition rendering responsibilities from a <strong class="bold">WebView</strong> to <strong class="bold">Babylon React Native</strong>. Use Babylon.js with <strong class="bold">WebXR</strong> to leverage device capabilities to render scenes onto a live image stream or to a VR set in a remote location.</li>
<li>Enjoy LARPing!</li>
</ol>
<p>This example isn’t intended to <a id="_idIndexMarker1111"/>be comprehensive or exhaustive, but it does cover a decent range of potential use cases by implication. When embarking on a Native project, it is worth considering whether the same goals might be accomplished more easily using a different framework such as Unity or Unreal. It is also important to keep in mind that the current (summer 2022) state of the project at the time of writing is still immature, and thus there are limitations and gaps in supported functionality. Check the links in the next section to get the latest information on what is supported and what isn’t in Babylon Native. </p>
<h2 id="_idParaDest-296"><a id="_idTextAnchor299"/>Further Reading</h2>
<p>As the project is rapidly evolving, so too is the documentation. Here are some places to start reading more about<a id="_idIndexMarker1112"/> Babylon Native and Babylon React Native at the <a id="_idIndexMarker1113"/>following links:</p>
<ul>
<li><a href="https://www.babylonjs.com/native/">https://www.babylonjs.com/native/</a></li>
<li><a href="https://www.babylonjs.com/reactnative/">https://www.babylonjs.com/reactnative/</a></li>
</ul>
<p>Although it was short, our overview of the Babylon Native campus has covered the more important guideposts and signs that mark the various trails throughout the area. As a collection of technologies, Babylon Native is all about fitting the right set of tools to the right situation. Web apps that already use React or apps using React Native are the most stable and advanced implementations currently available, but Babylon Native is the path to follow if you’re looking to build an AR app that runs on iOS. Each of those approaches has its benefits and drawbacks, some potentially quite significant. The good news is that regardless of which approach is chosen, the code you write that interacts with Babylon.js doesn’t need to change for multiplatform targeting scenarios. </p>
<p>Moving on, we’ve got business to attend to with our first guide, Andrei Stepanov. Andrei has been working with Babylon.js and <strong class="bold">Content Management Systems</strong> (<strong class="bold">CMSs</strong>) for a long time now, so he’s the <a id="_idIndexMarker1114"/>perfect person to give us a quick tour of how BJS can be used in e-commerce and CMS business scenarios.</p>
<h1 id="_idParaDest-297"><a id="_idTextAnchor300"/>Incorporating 3D Content into a Website</h1>
<p>When it comes to <a id="_idIndexMarker1115"/>understanding how to make Babylon.js work in real-world, customer-centric business scenarios, there aren’t many people more knowledgeable about the topic than Andrei, who posts to the BJS community forums under the name of “Labris.” As a senior 3D developer at MetaDojo (<a href="https://metadojo.io">https://metadojo.io</a>), he satisfies and delights clients with 3D experiences built to spec. Not content with just talking about how to build and create with Babylon.js, Andrei is also the creator of <a id="_idIndexMarker1116"/>the <strong class="bold">BabylonPress</strong> site (<a href="https://babylonpress.org">https://babylonpress.org</a>), which serves as a showcase of different examples and patterns that use BJS in conjunction with the <strong class="bold">WordPress</strong> CMS. </p>
<h2 id="_idParaDest-298"><a id="_idTextAnchor301"/>Babylon.js and CMS</h2>
<p>Babylon.js lets us build <a id="_idIndexMarker1117"/>very complex JS 3D applications from scratch. At the<a id="_idIndexMarker1118"/> same time, there are a lot of cases when we need to integrate Babylon.js into an already existing website with CMS – an application that enables users to create, edit, publish and store digital content – or just to some HTML template.</p>
<p>There are numerous ways to do this, on different levels. They will depend on specific needs, especially on the “3D User Experience,” which you need to provide. Since the number and variety of different<a id="_idIndexMarker1119"/> CMS wouldn’t allow us the luxury of describing all possible solutions in this space, I will explain in the next few subsections just some of the most common solutions and approaches.</p>
<h3>The Babylon Viewer</h3>
<p>Babylon.js has an official <a id="_idIndexMarker1120"/>extension, Babylon Viewer, which may simplify a lot of time for integration. It even has its own HTML tags, <code>&lt;babylon&gt;&lt;/babylon&gt;</code>, between which you define all needed parameters. </p>
<p>To display a 3D model in a prepared environment – with already tuned lights, shadows, reflections, and so on – you just need to add a script reference to the viewer like so:</p>
<pre class="source-code">&#13;
&lt;script&#13;
  src="img/babylon.viewer.js"&gt;&#13;
&lt;/script&gt;</pre>
<p>Then, add a <code>&lt;babylon&gt;</code> tag and set the model attribute to point to a <code>.gltf</code> or <code>.glb</code> file:</p>
<pre class="source-code">&#13;
&lt;babylon model="model.gltf"&gt;&lt;/babylon&gt;</pre>
<p>Besides the <code>.gtlf</code> and <code>.glb</code> formats, <code>.babylon</code>, <code>.obj</code>, and <code>.stl</code> formats. Its simplicity <a id="_idIndexMarker1121"/>allows easy integration of Babylon Viewer into any CMS and makes it an ideal choice for cases where you need to display a lot of different 3D models (e-commerce, game websites, and 3D artist blogs) in a user-editable CMS. More information about different Babylon Viewer configurations is available here: <a href="https://doc.babylonjs.com/extensions/babylonViewer/configuringViewer">https://doc.babylonjs.com/extensions/babylonViewer/configuringViewer</a>. </p>
<h3>Babylon Viewer 3D WordPress Plugin</h3>
<p>Built on the base of <strong class="bold">Babylon Viewer</strong>, there also<a id="_idIndexMarker1122"/> exists a community extension: the <strong class="bold">Babylon Viewer 3D Wordpress plugin</strong>. This allows you to <a id="_idIndexMarker1123"/>display 3D models and 3D scenes with the help of a <strong class="bold">Shortcode</strong>:</p>
<pre class="source-code">&#13;
[babylon]model.gltf[/babylon]</pre>
<p>You can use the 3D Viewer in <code>README</code> file at its home on GitHub at <a href="https://github.com/eldinor/babylon-wordpress-plugin">https://github.com/eldinor/babylon-wordpress-plugin</a>.</p>
<h3>Kiosk Mode and Iframes</h3>
<p>With regards to iframe implementations, it is<a id="_idIndexMarker1126"/> worth mentioning that the <strong class="bold">Babylon Sandbox</strong> (<a href="https://sandbox.babylonjs.com/">https://sandbox.babylonjs.com/</a>) has a special “kiosk” mode that <a id="_idIndexMarker1127"/>allows you to use its functionality with any 3D model in appropriate format. As an example, have a look at this beautiful example (a 3D model of an<a id="_idIndexMarker1128"/> ancient mosquito in amber) of <strong class="bold">GLTF</strong> transparency in the <strong class="bold">Khronos Group</strong> article: <a href="https://www.khronos.org/news/press/new-gltf-extensions-raise-the-bar-on-3d-asset-visual-realism">https://www.khronos.org/news/press/new-gltf-extensions-raise-the-bar-on-3d-asset-visual-realism</a>. </p>
<p>The different query string elements embedded within the URL allow the content creator or manager to define the source 3D file and all other parameters, such as camera position, auto-rotation behavior, the skybox, and environment texture.</p>
<p>To use “kiosk mode,” define the URL according to the following table. The first parameter starts with <code>?</code> after <a href="https://sandbox.babylonjs.com/">https://sandbox.babylonjs.com/</a>; all others start with <code>&amp;</code> before the parameter. Also note that since Babylon.js is an open source project, you can create and host your own version of the Sandbox!</p>
<div><div><img alt="Table 14.1 – Table of parameters for iframes for the BJS Sandbox&#13;&#10;" height="534" src="img/Table_14.01_B17266.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 14.1 – Table of parameters for iframes for the BJS Sandbox</p>
<p>At the end, you’ll get something like this – quite a long HTML link:</p>
<p><a href="https://sandbox.babylonjs.com/?kiosk=true&amp;assetUrl=https://raw.githubusercontent.com/wallabyway/gltf-presskit-transparency/main/docs/MosquitoInAmber_withRefraction.glb&amp;cameraPosition=-0.14,0.005,0.03&amp;autoRotate=true&amp;skybox=true&amp;environment=https://assets.babylonjs.com/environments/studio.env">https://sandbox.babylonjs.com/?kiosk=true&amp;assetUrl=https://raw.githubusercontent.com/wallabyway/gltf-presskit-transparency/main/docs/MosquitoInAmber_withRefraction.glb&amp;cameraPosition=-0.14,0.005,0.03&amp;autoRotate=true&amp;skybox=true&amp;environment=https://assets.babylonjs.com/environments/studio.env</a></p>
<h3>The BJS Playground and Iframes</h3>
<p>Another option that is especially useful for displaying scenes directly from Babylon Playground is a <a id="_idIndexMarker1129"/>special HTML template. Just add <code>frame.xhtml</code> before <a id="_idIndexMarker1130"/>the Playground URL and it will show the render area in full screen, but with a bottom toolbar showing FPS, reload and edit buttons.</p>
<p>Here is an example: <a href="https://www.babylonjs-playground.com/frame.xhtml%236F0LKI%232">https://www.babylonjs-playground.com/frame.xhtml#6F0LKI#2</a>.</p>
<p>To show only the render area, use <code>full.xhtml</code> as the prefix. More info about Playground URL formats is available here: <a href="https://doc.babylonjs.com/toolsAndResources/tools/playground#playground-url-formats">https://doc.babylonjs.com/toolsAndResources/tools/playground#playground-url-formats</a>. The result of this option is that you can then use that URL as the source for an iframe image element – see <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe">https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe</a> for how to define an iframe element.</p>
<h3>Babylon.js within a CMS</h3>
<p>Finally, if you are looking for more <a id="_idIndexMarker1131"/>close integration between Babylon.js and a<a id="_idIndexMarker1132"/> CMS, you would need to take into consideration these universal steps. Make sure that you have the following:</p>
<ul>
<li>The Babylon.js scripts are loaded properly. Depending on the CMS, you can also load Babylon.js conditionally if there is 3D content to be displayed on the web page.</li>
<li>The CMS supports the uploading of 3D files (most modern CMSs have a limited set of allowed file extensions).</li>
<li>A proper canvas element to display. It makes sense to assign a unique ID to each Babylon canvas (for example, with the help of a post ID or other CMS variable).</li>
<li>Canvas and BJS Engine elements properly hooked up to respond to resizes.</li>
</ul>
<p>Here, the complexity and the <a id="_idIndexMarker1133"/>scale of applications depends only on your creativity. Server-side languages can preprocess any needed data before delivering it to a JS client, allowing us to build a truly 3D CMS, where all user experiences and interactions happen in 3D space. </p>
<p>Babylon.js is not just another JavaScript framework to use for two-dimensional websites; it is one of the key components required to build multi-user 3D worlds and metaverses, at least with the current meaning of this term. </p>
<p>There’s a big difference <a id="_idIndexMarker1134"/>between loading a 3D model onto a single web page and bringing potentially lots of 3D models onto lots of different web pages. Managing the content and change processes is of utmost importance, but with the Guidance of Andrei, you’ll be ready to face those challenges and more. Now, what does 3D content in an e-commerce or CMS app have to do with a roughly 50-year-old technique for photo-realistic renders? Why, Babylon.js, of course! It’s time to continue our tour as we transition from the highly practical to the highly experimental side of 3D programming.</p>
<h1 id="_idParaDest-299"><a id="_idTextAnchor302"/>Tracing out a Path to Advanced Rendering</h1>
<p>Our last stop on our <em class="italic">Extended Topics, Extended</em> tour is with musician, engineer, and graphics wizard Erich Loftis. He’s going to guide us with the story of his journey to achieving <strong class="bold">Real-Time Path Tracing</strong> (<strong class="bold">RTPT</strong>) with <a id="_idIndexMarker1135"/>Babylon.js. RTPT – also <a id="_idIndexMarker1136"/>referred to as <strong class="bold">Ray Tracing</strong> or just <strong class="bold">RT</strong> – is a rendering technique built on top of Path Tracing that companies such as Nvidia and AMD are only just beginning to make available in AAA commercial titles, and only in select ways. Through the retelling of Erich’s journey, the reason why the technique has been so difficult to accomplish in real-time games and simulations will hopefully become abundantly clear. </p>
<h2 id="_idParaDest-300"><a id="_idTextAnchor303"/>Ray Tracing and its History by Erich Loftis</h2>
<p>RT is a technique for <a id="_idIndexMarker1137"/>rendering realistic images and effects on a computer. It follows the laws of optics and models how physical light rays behave in the real world. Therefore, RT can produce truly photo-realistic images. <strong class="bold">RT</strong> is <em class="italic">the</em> standard for photo-realistic offline rendering. Because it has found its way into real-time applications and games (where Rasterization was the undisputed king), it’s important to have at least a basic understanding of how it all works under the hood. </p>
<p>By leveraging the awesome <strong class="bold">Babylon.js</strong> engine, we can use this understanding to make our own <strong class="bold">Physically Based Renderer</strong> that runs right inside the browser. This is important because it opens the door to experiencing photo-realistic graphics<a id="_idIndexMarker1138"/> on any platform or device, even your cellphone. Of course, the journey to get to this point wasn’t exactly the most straightforward or easy, but as you’ll see from the demos and examples, the effort is totally worth it!</p>
<h3>My Own RT Journey</h3>
<p>It was this dream of experiencing <strong class="bold">RT</strong> on all devices that led me to create a Path Tracing Renderer for <strong class="bold">three.js,</strong> starting back in 2015. For the past 7+ years, I have been slowly but steadily researching, building, refining, and optimizing a browser-based renderer that not only produces high-quality, photo-realistic images but also aims to do so at 30–60 frames a second! Please check out my ongoing project on <strong class="bold">GitHub</strong>, where you can try dozens of clickable demos: <a href="https://github.com/erichlof/THREE.js-PathTracing-Renderer">https://github.com/erichlof/THREE.js-PathTracing-Renderer</a>.</p>
<p>A while ago, back in 2020, a Babylon.js developer reached out to me and asked if I could possibly make a similar renderer for the BJS engine. I must state here that I have primarily worked with <strong class="bold">three.js</strong> all these <a id="_idIndexMarker1139"/>years, but I have always admired and been impressed by the amazing <strong class="bold">Babylon.js</strong> library. When I agreed to do the port of my ray/path tracing system from three.js to BJS, I was equally impressed with the BJS forum community. They are so friendly and helpful and are just awesome folks! I couldn’t have gotten our BJS renderer up and running without their help and support. So, before we dive in, a quick shout out to you all – thank you, BJS community!</p>
<h3>RT or Rasterization?</h3>
<p>What does it take to get interactive, real-time RT working inside of BJS and a browser? Firstly, let’s take a quick look at the two main techniques for rendering 3D graphics. Once we see how that works, we’ll also see why we would want to try this RT route with BJS. </p>
<p>When it comes to displaying 3D graphics on a 2D screen, there are two main approaches: <strong class="bold">Rasterization</strong> and <strong class="bold">RT</strong>. In a nutshell, Rasterization works by first taking the <strong class="bold">scene geometry</strong>, in the <a id="_idIndexMarker1140"/>form of 3D vertices, and then projecting those to the screen in the form of many flat<a id="_idIndexMarker1141"/> 2D triangles. Any pixels on the device’s display that happen to occupy a screen triangle’s area are sent to the pixel shader (also known as <a id="_idIndexMarker1142"/>a fragment shader). When <a id="_idIndexMarker1143"/>the fragment shader is run on a pixel, its final display color is computed. All of these colored pixels make up the final image that we see on our devices.</p>
<p>In contrast, RT renders images by addressing each pixel on the display first. For every pixel on the screen, a geometric <strong class="bold">Ray</strong> is constructed that starts at the camera’s position.  Pointing from the camera, this <strong class="bold">Camera Ray</strong> then <a id="_idIndexMarker1144"/>shoots out toward its pixel wherever it lies on the view plane (usually your screen). After piercing through the target pixel, the Camera Ray continues out into the 3D scene. It will then model how a physical light ray in the real world would interact with its environment.</p>
<p>Only now at this point in the pipeline do we consider the scene geometry. Each of the camera pixel rays is tested for intersection against every 3D shape in the scene. Wherever the ray hits a surface, the color and lighting at that location are recorded and a “bounce” ray is then spawned and sent in a new direction. This direction is dictated by the material properties of the hit surface location. Further, the bounce ray must check the entire scene geometry (every 3D shape or triangle) for any intersections just as its parent ray did, thus repeating the whole process again and again for however long you are willing to wait for it to complete. After a pixel’s camera ray and its spawned bounce rays finish interacting with the scene, the ray tracer reports back the final color for that pixel. Just like Rasterization, we end up with a screen full of colored pixels but with a totally different path taken to arrive at these results!</p>
<p>Both rendering approaches have trade-offs in terms of realism and speed. <strong class="bold">Rasterization</strong> (comprising 99% of all 3D graphics) has full GPU hardware support, so it is very fast and efficient. There’s a drawback, though. As soon as the GPU is done projecting and rasterizing the scene’s triangles to the 2D screen, the surrounding 3D scene information is lost. To retrieve this lost global scene information, sophisticated techniques such as light mapping, shadow mapping, reflection probes, and others must be used. In other words, a lot of graphics knowledge and extra effort is required to get close to RT-quality visuals. </p>
<p>RT, on the other hand, automatically produces the ultimate in realistic graphics, right out of the box! Lighting effects that are difficult if not impossible with Rasterization just naturally fall out of the RT algorithm. However, as of 2022, RT is not widely supported by most GPU hardware. All CPUs <a id="_idIndexMarker1145"/>can run RT programs, but CPUs aren’t designed to be <a id="_idIndexMarker1146"/>massively parallel. Therefore, traditional CPU-based software RT is very slow in comparison to hardware-accelerated Rasterization on the GPU. Even if the RT software is moved inside a shader that runs entirely on the GPU (as our project here will do), several RT algorithm optimizations must be made in that shader, and/or a decent acceleration structure such as a <strong class="bold">Bounding Volume Hierarchy</strong> (<strong class="bold">BVH</strong>) is required if we <a id="_idIndexMarker1147"/>can have any hope of RT at interactive frame rates.</p>
<h3>Taking the RT Route</h3>
<p>So, knowing most of these trade-offs in <a id="_idIndexMarker1148"/>advance (and some not until I was years-deep into the project – ha!), I decided to go the RT route. I’ll now fast-forward to when I started implementing RT with Babylon.js as the host engine. I’ll give an overview of the necessary setup, as well as a few code snippets to show some of the implementation details. Let’s jump right in!</p>
<p>Since we are now following the RT approach, we must find <a id="_idIndexMarker1149"/>a way to construct a viewing ray from the camera through to each and every pixel on the screen. A common method for gaining access to the screen pixels is <a id="_idIndexMarker1150"/>to create a <strong class="bold">Full-Screen Post-Process Effect</strong>, or just <strong class="bold">Post-Process</strong> for short (as you learned in <a href="B17266_10_Final_AM.xhtml#_idTextAnchor207"><em class="italic">Chapter 10</em></a>, <em class="italic">Improving the Environment with Lighting and Materials</em>). Since the post-process is a common operation, BJS has a really handy library wrapper that takes care of all the <strong class="bold">WebGL</strong> boilerplate code and post-process setup for us. In BJS, this helper is called an <strong class="bold">EffectWrapper</strong>. Here’s an example<a id="_idIndexMarker1151"/> of a typical post-process creation:</p>
<pre class="source-code">&#13;
const { Effect, RenderTargetTexture, Constants } = BABYLON;&#13;
const store =&#13;
  Effect.ShadersStore["screenCopyFragmentShader"];&#13;
const screenCopyEffect = new EffectWrapper({&#13;
     engine: engine,&#13;
     fragmentShader: store,&#13;
     uniformNames: [],&#13;
     samplerNames: ["pathTracedImageBuffer"],&#13;
     name: "screenCopyEffectWrapper"&#13;
});</pre>
<p>Now, here is where the setup gets a little tricky, not because of <code>pathTracingEffect</code>), we ray trace <a id="_idIndexMarker1153"/>on all pixels and save their color results by using a <strong class="bold">Render Target Texture</strong> (<strong class="bold">RTT</strong>):</p>
<pre class="source-code">&#13;
const pathTracingRenderTarget =&#13;
    new RenderTargetTexture("pathTracingRenderTarget",&#13;
     {width, height}, pathTracingScene, false, false, &#13;
     Constants.TEXTURETYPE_FLOAT, false,&#13;
     Constants.TEXTURE_NEAREST_SAMPLINGMODE,&#13;
     false, false, false, Constants.TEXTUREFORMAT_RGBA);</pre>
<p>This large <code>screenCopyEffect</code>) and then fed back through to the first post-process (<code>pathTracingEffect</code>) on the next animation frame. Now, our GPU ray tracer can use its previous result (its own pixel color history) to blend with the fresh new pixel color results that it is currently calculating from RT. In other words, it keeps blending and mixing with itself again and again. Over a couple of hundred frames or so, this ping-pong feedback process will quickly produce very smooth anti-aliased results that seem to magically converge right before our eyes! The last piece of the rendering setup puzzle is a final monitor output post-process (named <code>screenOutputEffect</code>). Its job is to <a id="_idIndexMarker1155"/>perform <strong class="bold">noise filtering</strong>, then <strong class="bold">Tone Mapping</strong> (which you<a id="_idIndexMarker1156"/> learned about in <em class="italic">Tone Mapping and Basic Post-Processing </em>section of<em class="italic"> </em><a href="B17266_10_Final_AM.xhtml#_idTextAnchor207"><em class="italic">Chapter 10</em></a><em class="italic">, Improving the Environment with Lighting and Materials</em>), and then finally some <strong class="bold">gamma correction</strong> (also in <em class="italic">Tone Mapping and Basic Post-Processing </em>section of <a href="B17266_10_Final_AM.xhtml#_idTextAnchor207"><em class="italic">Chapter 10</em></a><em class="italic">, Improving the Environment with Lighting and Materials</em>) to <a id="_idIndexMarker1157"/>produce more pleasing color output on digital monitors and screens.</p>
<p>All in all, we need a total of three post-processing effects:</p>
<ul>
<li><code>pathTracingEffect</code>: This performs all of the RT calculations on every single pixel. It will take whatever pixel history given to it by the following <code>screenCopyEffect</code> to use for blending with itself. It outputs to <strong class="bold">RenderTargetTexture (RTT)</strong>, which is finally fed to the following post-process.</li>
<li><code>screenCopyEffect</code>: This takes that supplied RTT output from the preceding post-process and copies/saves it to its own RTT. It then sends this saved copy back through to the preceding <code>pathTracingEffect</code> to use for blending with itself.</li>
<li><code>screenOutputEffect</code>: This post-process is responsible for the screen’s final color output. It takes the preceding <code>pathTracingEffect</code> <strong class="bold">RTT</strong> (which holds all the refined, <strong class="bold">ping-pong</strong> blended, ray-traced pixel results so far), applies its <a id="_idIndexMarker1158"/>special filters and pixel color adjustments, and then directly outputs to your screen.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">The first two effects make up the <strong class="bold">ping-pong buffers</strong>, or feedback loop.</p>
<p>Now that we have our custom system set up for progressively refining our ray-traced images over time and can correctly display the final pixel color output, we just need to do one more thing – the actual RT! Let’s switch gears for a moment and briefly discuss the similarities and differences <a id="_idIndexMarker1159"/>between RT and <strong class="bold">Path Tracing</strong> (<strong class="bold">PT</strong>), and what our ray tracers/path tracers will need in order to do their magic in the browser.</p>
<h3>The Path to PT</h3>
<p>To best understand how <a id="_idIndexMarker1160"/>RT and PT are related, let’s follow a brief timeline/lineage of RT discoveries and techniques in CG history. In 1968, Arthur Appel invented Ray Casting, a groundbreaking technique in which mathematical rays are shot out from the camera through every pixel. Whatever these camera rays hit first out in the 3D scene determines what we see in our image. Then, in 1979, Turner Whitted invented RT, which relies on Appel’s earlier 1968 Ray Casting technique but does it many times recursively while following the laws of optics, in order to capture physically accurate reflections and refractions from specular surfaces (mirrors, glass, and so on). Then, in 1986, James Kajiya invented PT, the ultimate evolution of RT. Building from all the previous RT techniques, Kajiya added Monte Carlo integration (random sampling and averaging) to randomly sample material BRDFs (diffuse surfaces in particular), in order to capture physical light <a id="_idIndexMarker1161"/>effects such as caustics and inter-reflected diffuse surface “bounce lighting.” PT gets its name from tracing (random sampling) all the possible paths that light rays might take as they interact with different types of materials in the scene, and then gathering all of these light paths’ contributions to produce a ground-truth, photo-realistic image.</p>
<p>Looking at this potted RT/PT history, hopefully you can see how PT is related to, evolved from, and improves upon RT (and Ray Casting before that). Since I wanted the ultimate in realistic graphics, I chose the more <a id="_idIndexMarker1162"/>sophisticated <strong class="bold">Monte Carlo PT</strong> method (1986 Kajiya-style), which captures light effects that are impossible with Rasterization and even older-style RT. And thanks to our hard work on setting up the progressively refining post-process effects system, our randomly sampled <strong class="bold">Monte Carlo</strong> PT results for all pixels can be correctly averaged and refined over time into a <strong class="bold">ground-truth image</strong>. This <a id="_idIndexMarker1163"/>basically means photo-realistic rendering in your browser!</p>
<h3>PT in the Browser</h3>
<p>Now, let’s discuss scene geometry and what PT requires in terms of how the scene is defined. We have two <a id="_idIndexMarker1164"/>options for telling the PT fragment shader what is in the scene. The first and easiest option is to simply write a GLSL function in the fragment shader itself that defines the entire scene’s geometry as part of the shader. All objects/shapes are hardcoded and listed one after the other. This is fine if the number of shapes/objects in your scene does not exceed 20 or so, but as soon as you get into the hundreds of objects or, worse yet, use a typical model with thousands of triangles (with each triangle being tested by every ray!), our path tracer would grind to a halt. To speed things up tremendously and keep our PT interactive, we need to use an acceleration structure, such<a id="_idIndexMarker1165"/> as a <strong class="bold">BVH</strong>. A <strong class="bold">BVH</strong> is basically just a binary tree of bounding boxes that tightly surrounds the triangular model(s). When testing for intersection, rays can skip large portions of the model if they miss some of the larger bounding boxes. To see how a BVH is built, check out my custom BVH builder code at <a href="https://github.com/erichlof/Babylon.js-PathTracing-Renderer/blob/main/js/BVH_Fast_Builder.js">https://github.com/erichlof/Babylon.js-PathTracing-Renderer/blob/main/js/BVH_Fast_Builder.js</a>. Recall that the path tracer (inside the fragment shader) must have access to the entire scene, and since we can’t fit most large scenes containing thousands of triangles into shader <strong class="bold">uniforms</strong> (there is a hard limit on most graphics cards), we tightly pack the BVH and all its bounding boxes into a data texture. This BVH texture will give our GPU path tracer quick and easy access to the entire optimized scene geometry (via simple texture lookups).</p>
<p>Next, all ray tracers and <a id="_idIndexMarker1166"/>path tracers require a shape intersection library to have ray intersection testing with a wide variety of primitive shapes, such as spheres, boxes, and triangles. Historically, when RT was just coming into existence, computers were only fast enough to intersect rays with simple mathematical shapes. Examples of these shapes include spheres, cylinders, cones, and planes, and they all belong to a class of shapes known as quadrics. The <a id="_idIndexMarker1167"/>solution for where a ray intersects these quadric shapes is handled by simply solving the quadratic equation for that shape. That’s why, when you look at more historical ray-traced images, the scenes only contain checkered planes and spheres (or other quadrics) of different sizes and materials. In these early years of RT, the math for intersecting rays with more complex triangle geometry (like what we use today) was well understood, but it would take many years for computers to get fast enough to be able to handle testing rays with an entire polygonal 3D model with thousands of triangles. Over the last 7 years, I have collected almost every routine I could find for determining the intersection of rays against various shapes. Here’s a link to my <code>PathTracingCommon.js</code> file, which contains all of these intersection routines: <a href="https://github.com/erichlof/Babylon.js-PathTracing-Renderer/blob/main/js/PathTracingCommon.js">https://github.com/erichlof/Babylon.js-PathTracing-Renderer/blob/main/js/PathTracingCommon.js</a>. Equally important and also included in this library file are the functions that handle <strong class="bold">Monte Carlo PT</strong>-style random sampling of different light source types (point, spot, directional, area, and HDRI) and material types (BRDFs from the<em class="italic"> Tone Mapping and Basic Post-Processing of </em><a href="B17266_10_Final_AM.xhtml#_idTextAnchor207"><em class="italic">Chapter 10</em></a><em class="italic">, Improving the Environment with Lighting and Materials</em>) that rays might interact with in any given scene. </p>
<h3>Further Reading</h3>
<p>Well, unfortunately, there isn’t enough space in this more general, overview-style article to go into detail about my <strong class="bold">GLSL</strong> PT shader code (where all the RT/PT algorithms happen). However, if you want to see some nice examples of RT/PT in GLSL (where I have learned from too), check out <a id="_idIndexMarker1168"/>a couple of these shaders on <strong class="bold">Shadertoy</strong>:</p>
<ul>
<li><a href="https://www.shadertoy.com/view/Xtt3DB">https://www.shadertoy.com/view/Xtt3DB</a></li>
<li><a href="https://www.shadertoy.com/view/XsSSWW">https://www.shadertoy.com/view/XsSSWW</a></li>
<li><a href="https://www.shadertoy.com/view/XdcfRr">https://www.shadertoy.com/view/XdcfRr</a></li>
<li><a href="https://www.shadertoy.com/view/tddSz4">https://www.shadertoy.com/view/tddSz4</a></li>
</ul>
<p>And if you would like to go much deeper into the theory and practice of RT and PT, I can think of no better resource than <strong class="bold">Scratchapixel</strong>. This <a id="_idIndexMarker1169"/>amazing website contains everything you need to know about Rasterization, RT, PT, and graphics in general: <a href="https://www.scratchapixel.com/">https://www.scratchapixel.com/</a>.</p>
<p>Lastly, to see all of the pieces of this article come together, check out the Babylon.js <strong class="bold">PathTracing</strong> Renderer: <a href="https://github.com/erichlof/Babylon.js-PathTracing-Renderer">https://github.com/erichlof/Babylon.js-PathTracing-Renderer</a></p>
<p>This is our ongoing project, which has several clickable demos that showcase different areas of PT. As with the Space-Truckers OSS project, this BJS <strong class="bold">PathTracing</strong> Renderer project is open for Pull Requests. If you start getting into this fascinating world of RT and PT, we would love to see your contributions! A word of warning though – once you start down the road of RT and PT, it can be hard to stop!</p>
<p>Happy rendering!</p>
<h1 id="_idParaDest-301"><a id="_idTextAnchor304"/>Summary</h1>
<p>We’ve seen a lot of new things on our trip through the BJS <em class="italic">Metatropolis</em>. We’ve heard of new wonders under construction but ready for business, such as <strong class="bold">VR</strong> and <strong class="bold">AR</strong> with <strong class="bold">WebXR</strong>. To help developers make use of these wonders, we learned about how Babylon.js offers the <strong class="bold">WebXRExperienceHelper</strong>. Working in conjunction with the <strong class="bold">FeaturesManager</strong>, it allows developers to code with confidence against a rapidly evolving and changing standard. </p>
<p>Babylon.js is a project that places backward compatibility as one of its cornerstone principles, and so as hardware improves – or more products open up their hardware to <strong class="bold">WebXR</strong> APIs – capabilities will “light up” as browser vendors add support. While it would be great to include iOS (and <strong class="bold">WebKit</strong>) in the supported application list today for <strong class="bold">WebXR</strong>, and while we can lament for a world that could have been, applications using Babylon.js will be ready to best take advantage when that day finally does arrive.</p>
<p>Until that happens, developers and designers have several potential approaches that will ideally allow the greatest code reuse and lowest friction to implement and maintain. The <strong class="bold">Babylon.js Native</strong> project is a collection of tools and techniques that people working on cross-platform or Native projects can leverage to gain maximum productivity and effectiveness. These tools fall into a spectrum going from full-on bare-metal BJS Native to the “vanilla” BJS that we’ve come to know and love. In between, <strong class="bold">Babylon React Native</strong> provides a way for developers already using React and React Native to incorporate BJS into their applications, while toward the other end of the spectrum, the hosting of a <strong class="bold">WebGL</strong> context in a <strong class="bold">WebView</strong> provides another avenue for potential native device application integration in arbitrary software apps.</p>
<p>Babylon.js is more than about making games such as Space-Truckers. As a general 3D application development platform, BJS gives us access to entire universes of possibilities, waiting to be unlocked by curious explorers. Perhaps one of those curious explorers will be you! Every coin has a flip side, and the flip side of having so many possibilities is that it’s very difficult to give a good account of the more interesting ones in the same context as the rest of our journey with Space-Truckers. That is where our two guides come into play. As long-time explorers into some of these other provinces of BJS, Andrei Stepanov and Erich Loftis have much to share with the community. </p>
<p>Through his <strong class="bold">Babylon Viewer 3D WordPress Plugin</strong> and his extensive and detailed example site, <a href="http://babylonpress.org">babylonpress.org</a>, which shows off the viewer, Andrei has opened our eyes to how easy it can be to use <strong class="bold">shortcodes</strong> to include 3D models as a content editor once the proper script references have been injected into the CMS page. By telling us of his journey into PT/RT, Erich Loftis has, in turn, opened our eyes to the innovative history of graphics rendering technologies and how they’re used in the world of computer graphics. </p>
<p>Each of them has given us their unique insights and approaches to their respective topics and helped to guide us to the Terminal Destination for this book. Although this is the end of one journey, it is just the beginning of another. Unlike this book though, the path for this new journey – your journey – isn’t captured or written out anywhere, nor is there any pre-determination on what route that path will take. Where this path takes and what it entails is entirely up to you, but wherever that destination lies, whether shrouded in mist or lit up with a beacon, you’re not alone. The BJS community is there to assist, support, and, of course, guide folks. The BJS forums at <a href="https://forum.babylonjs.com">https://forum.babylonjs.com</a> are the best place to go to ask questions, meet folks like Erich and Andrei, and learn from other community members. </p>
<p>Good luck on your journey – the world of web-based 3D and the BJS community awaits!</p>
</div>
</div></body></html>