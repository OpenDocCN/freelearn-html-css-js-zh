- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Load testing microservices with Vegeta
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load testing microservices with Gatling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building auto-scaling clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A significant advantage of using microservices over a monolith architecture
    is that microservices can be separately scaled to meet the unique traffic demands
    they serve. A service that must do work for every single request will have very
    different scaling needs than a service that only needs to perform work for specific
    kinds of request.
  prefs: []
  type: TYPE_NORMAL
- en: Because microservices encapsulate ownership over a single-domain entity, they
    can be load tested independently. They can also be configured to scale automatically
    based on demand. In this chapter, we'll discuss load testing using two different
    load testing tools and set up auto-scaling groups in AWS that can scale on demand.
    Finally, we'll discuss strategies for capacity-planning.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing microservices with Vegeta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load testing is an important part of predicting how your service is going to
    behave over time. When we are performing load testing, we shouldn't just ask simple
    questions, such as "*How many requests per second is our system capable of serving?*"
    Instead, we should try to understand how our whole system performs under various
    load conditions. In order to answer this question, we need to understand the infrastructure
    that makes up our system and the dependencies that a particular service has.
  prefs: []
  type: TYPE_NORMAL
- en: For example, is the service behind a load-balancer? How about a CDN? What other
    caching mechanisms are used? All of these questions and more can be answered by
    our systems having good observability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vegeta** is an open source load testing utility designed to test HTTP services
    with a constant request rate. It''s a versatile tool that can be used as a command-line
    utility or a library. In this recipe, we''ll focus on using the command-line utility.
    Vegeta allows you to specify targets as URLs in a separate file—optionally with
    custom headers and request bodies—that can be used as an input to the command-line
    tool. The command-line tool can then attack the targets in the file, with various
    options to control the request rate and duration, as well as other variables.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll be using Vegeta to test the message-service we've been
    working with in previous chapters. We'll test a simple request path that includes
    creating a new message and retrieving a list of messages.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll modify our message-service and add a new endpoint that allows us to
    query all messages for a particular user. This introduces the notion of an inbox,
    so we''ll modify our `MessageRepository` class to add a new in-memory map of usernames
    to lists of messages, as shown in the following code. Note that in a production
    system, we''d choose a more durable and flexible store, but this will suffice
    for demonstration purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify `MessageController` to add the endpoint itself:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll need a mock socialgraph service, so create the following Ruby script
    in a file called `socialgraph.rb` and run it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Install `vegeta`. If you''re on Mac OS X and have HomeBrew installed, you can
    just use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can launch an attach with `vegeta`, we''ll need to create a `targets`
    file. The first request we''ll make will create a message with the specified request
    body. The second request will get a list of messages by user ID. Create a file
    called `message-request-body.json`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create another file called `targets.txt`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With both our message-service and our mock socialgraph service running, we''re
    ready to load test these two services using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Experiment with different duration values and request rates to see how the behavior
    of the system changes. If you increase the rate to 1,000, what happens? Depending
    on hardware and other factors, it's possible that the single-threaded Ruby mock
    service will be overwhelmed and trip the circuit breaker we added to the message-service.
    This should change certain details, such as the success rate, so it's an important
    observation to make. What would happen if you load tested the mock Ruby service
    separately?
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we load tested the message-service, which depends on the socialgraph
    service. Both services were running locally, which was necessary for demonstration
    purposes and gives us some insight into how the two systems behave. In a production
    system, it's vital to load test your services in production so that you include
    all of the infrastructure involved in serving requests (load balancers, caches,
    and so on). In a production system, you can also monitor dashboards and look for
    changes to how your system behaves under load conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing microservices with Gatling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gatling is an open source load testing tool that allows users to script custom
    scenarios using a *Scala-based DSL*. Scenarios can go beyond simple straight path
    testing and involve multiple steps, even simulating user behavior, such as pauses
    and making decisions about how to proceed based on output in the test. Gatling
    can be used to automate the load testing of microservices or even browser-based
    web applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous recipe, we used Vegeta to send a constant request rate to our
    message-service. Our request path created a new message and then retrieved all
    messages for a user. This method had the advantage of being able to test the response
    time of retrieving all messages for a user as the list of messages grew. Vegeta
    excels at this type of testing, but because it is fed attack targets from a static
    file, you cannot use Vegeta to build dynamic request paths based on the responses
    from previous requests.
  prefs: []
  type: TYPE_NORMAL
- en: Because Gatling uses a DSL to script load testing scenarios, it's possible to
    make a request, capture some element of the response, and use that output to make
    decisions about future requests. In this recipe, we'll use Gatling to script a
    load testing scenario that involves creating a message and then retrieving that
    specific message by its ID. This is a very different kind of test than what we
    did in the previous recipe, so it's a good opportunity to demonstrate the differences
    between Vegeta and Gatling.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s check the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download `gatling` for your platform. Gatling is distributed as a ZIP bundle
    and is available for download at [https://gatling.io/download/](https://gatling.io/download/).
    Unzip the bundle into the directory of your choice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Simulations for `gatling` are placed by default in the `user-files/simulations`
    directory. Create a new subdirectory called `messageservice` and a new file called
    `BasicSimulation.scala`. This is the file that contains the code that describes
    your scenario. In our scenario, we''ll use the Gatling DSL to script a POST request
    to the create message endpoint followed by a GET request to the message endpoint,
    as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the same mock Ruby service we used in the previous recipe and run it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the Ruby mock service as well as our message-service. From the Gatling
    directory, launch Gatling by running `bin/gatling.sh`. You''ll be prompted to
    select a simulation to run. Choose `messageservice.BasicSimulation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output will show some statistics about the results from the load test. Requests
    will be bucketed into under 800 ms, between 800 ms and 1,200 ms, and over 1,200
    ms. A link to an HTML file will be displayed. Open it in a browser to see charts
    and other useful visualizations about your load test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we've seen in this recipe, Gatling offers a lot of flexibility in running
    load tests. With some clever scripting using the DSL, it's possible to more closely
    simulate production traffic by parsing log files and generating requests, making
    dynamic decisions based on latency, responses, or other elements of requests.
    Both Gatling and Vegeta are great load testing tools that you can use to explore
    how your systems operate under various load conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Building auto-scaling clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the advent of virtualization and the move to cloud-based infrastructure,
    applications can exist on elastic infrastructure designed to grow and shrink based
    on anticipated or measured traffic patterns. If your application experiences peak
    periods, you shouldn't have to provision full capacity during non-peak periods,
    wasting compute resources and money. From virtualization to containers and container
    schedulers, it's more and more common to have dynamic infrastructure that changes
    to accommodate the needs of your system.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices are a natural fit for auto-scaling. Because we can scale separate
    parts of a system separately, it's easier to measure the scaling needs of a specific
    service and its dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to create auto-scaling clusters. In the next chapter, we'll
    talk about container orchestration tools, but without skipping ahead, auto-scaling
    clusters can also be created in any cloud provider. In this recipe, we'll cover
    creating auto-scaling compute clusters using *Amazon Web Services*, particularly
    Amazon EC2 Auto Scaling. We'll create a cluster with multiple EC2 instances running
    our message-service behind an **Application Load Balancer** (**ALB**). We'll configure
    out cluster to automatically add instances based on CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s check the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This recipe requires an AWS account. If you do not already have an AWS account,
    create one at [https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/](https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/) and
    create a set of access keys at [https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html](https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html).
    Install the `aws cli` utility. If you''re on OS X and have HomeBrew installed,
    this can be done with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the `aws` command-line utility, entering the access key you created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a launch configuration. Launch configurations are templates used by
    auto-scaling groups when creating new instances. In this case, we''ve chosen an
    Amazon AMI and `t2.nano` as our EC2 instance type (see [https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/) for
    more details), as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the actual auto-scaling group. Auto-scaling groups have configurable
    maximum and minimum sizes that specify how much the auto-scaling group can shrink
    or grow based on demand. In this case, we''ll create an auto-scaling group with
    a minimum of `1` instance and a maximum of `5` instances, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We want the instances in our auto-scaling group to be accessible behind a load
    balancer, so we''ll create that now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to automatically scale our auto-scaling group, we need to define a
    metric. Clusters can be scaled based on memory, CPU utilization, or request rate.
    In this case, we''re going to configure our scaling policy to use CPU utilization.
    If CPU utilization hits a 20% average, our auto-scaling group will create more
    instances. Create a file called `config.json`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Attach the scaling policy to our auto-scaling group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Our auto-scaling group is now configured to grow when CPU utilization exceeds
    a 20% average. Launch configurations can also include bootstrapping steps for
    installing and configuring your service—typically with some kind of configuration-management
    tool, such as **Chef** or **Puppet**—or it can be configured to pull a Docker
    image from a private Docker repository.
  prefs: []
  type: TYPE_NORMAL
