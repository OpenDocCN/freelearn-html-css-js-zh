<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Storing Data in Elasticsearch</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we developed the bulk of our Create User feature by following a TDD process and writing all our E2E test cases first. The last piece of the puzzle is to actually persist the user data into a database.</p>
<p>In this chapter, we will install and run <strong>ElasticSearch</strong> on our local development machine, and use it as our database. Then, we will implement our last remaining step definition, using it to drive the development of our application code. Specifically, we will cover the following:</p>
<ul>
<li>Installing Java and Elasticsearch</li>
<li>Understanding Elasticsearch concepts, such as <strong>indices</strong>, <strong>types</strong>, and <strong>documents</strong></li>
<li>Using the<span> </span>Elasticsearch<span> </span>JavaScript client to complete our create user endpoint</li>
<li>Writing a Bash script to run our E2E tests with a single command</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Elasticsearch</h1>
                </header>
            
            <article>
                
<p>So, what is Elasticsearch? First and foremost, Elasticsearch should not be viewed as a single, <span>one-dimensional </span>tool. Rather, it's a suite of tools that consists of a<span> </span><strong>distributed database</strong>, a<span> </span><strong>full-text search engine</strong>, and also an<span> </span><strong>analytics engine</strong>. We will focus on the "database" part in this chapter, dealing with the "distributed" and "full-text search" parts later.</p>
<p>At its core, Elasticsearch is a high-level abstraction layer for <span><strong>Apache Lucene</strong>, a full-text search engine. Lucene is arguably the most powerful full-text search engine around; it is used by <strong>Apache Solr</strong>, another search platform similar to Elasticsearch. However, Lucene is very complex and the barrier to entry is high; thus Elasticsearch abstracts that complexity away into a </span>RESTful API.</p>
<p class="mce-root"/>
<p>Instead of using Java to interact with Lucene directly, we can instead send HTTP requests to the API. Furthermore, Elasticsearch also provides many language-specific clients that abstract the API further into nicely-packaged objects and methods. We will be making use of Elasticsearch's JavaScript client to interact with our database.</p>
<div class="packt_tip"><span>You can find the </span><span>documentation</span><span> for the most current JavaScript client at </span><a href="https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference.html">https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference.html</a><span>.<br/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elasticsearch versus other distributed document store</h1>
                </header>
            
            <article>
                
<div>
<p>For simple document storage, you'd typically pick a general-purpose database like MongoDB, and store <strong>normalized</strong> data.</p>
<div class="packt_infobox">Normalization is the process of reducing data redundancy and improving data integrity by ensuring components of the data structure are atomic elements.<br/>
<br/>
Denormalization is the process of introducing data redundancy for other benefits, such as performance.</div>
</div>
<p><span>However, searching on </span><span>normalized data is extremely inefficient. Therefore, t</span><span>o perform a full-text search, you would usually <strong>denormalize</strong> the data and replicate it onto more specialized database such as Elasticsearch.</span></p>
<p>Therefore, in most setups, you would have to run two different databases. <span>However, in this book, we will use Elasticsearch for both data storage and search, for the following reasons:</span></p>
<ul>
<li><span>The page count of the book is limited</span></li>
<li><span>Tooling around syncing MongoDB with Elasticsearch is not mature</span></li>
<li><span>Our data requirements are very basic, so it won't make much difference</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Java and Elasticsearch</h1>
                </header>
            
            <article>
                
<p>First, let's install Elasticsearch and its dependencies. Apache Lucene and Elasticsearch are both written in Java, and so we must first install Java.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Java</h1>
                </header>
            
            <article>
                
<p>When you install Java, it usually means one of two things: you are installing the<span> </span><strong>Java Runtime Environment</strong><span> </span>(<strong>JRE</strong>) or the<span> </span><strong>Java Development Kit</strong><span> </span>(<strong>JDK</strong>). The JRE provides the runtime that allows you to<span> </span><em>run</em><span> </span>Java programs, whereas the JDK contains the JRE, as well as other tools, that allow you to<span> </span><em>develop</em><span> </span>in Java.</p>
<p>We are going to install the JDK here, but to complicate things further, there are different implementations of the JDK—OpenJDK, Oracle Java, IBM Java—and the one we will be using is the<span> </span><kbd>default-jdk</kbd> APT package, which comes with our Ubuntu installation:</p>
<pre><strong>$ sudo apt update</strong><br/><strong>$ sudo apt install default-jdk</strong></pre>
<p><span>Next, we need to set </span><span>a system-wide environment variable</span><span> so that other programs using Java (for example, Elasticsearch) know where to find it. Run the following command to get a list of Java installations:</span></p>
<pre><strong>$ sudo update-alternatives --config java</strong><br/><strong>There is only one alternative in link group java (providing /usr/bin/java): /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java</strong><br/><strong>Nothing to configure.</strong></pre>
<p>For my machine, there's only a single Java installation, located at<span> </span><kbd>/usr/lib/jvm/java-8-openjdk-amd64/</kbd>. However, if you have multiple versions of Java on your machine, you'll be prompted to select the one you prefer:</p>
<pre><strong>$ sudo update-alternatives --config java</strong><br/><strong>There are 2 choices for the alternative java (providing /usr/bin/java).</strong><br/><br/><strong>  Selection Path Priority Status</strong><br/><strong>------------------------------------------------------------</strong><br/><strong>* 0 /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1101 auto mode</strong><br/><strong>  1 /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1101 manual mode</strong><br/><strong>  2 /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java 1081 manual mode</strong><br/><br/><strong>Press &lt;enter&gt; to keep the current choice[*], or type selection number:</strong> </pre>
<p>Next, o<span>pen</span><span> </span><kbd>/etc/environment</kbd><span> and add the path to the <kbd>JAVA_HOME</kbd> environment variable:</span></p>
<pre>JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"</pre>
<p class="mce-root"/>
<p><kbd>JAVA_HOME</kbd><span> </span>will be set for any user on login; to apply the changes now, we need to source the file:</p>
<pre><strong>$ . /etc/environment</strong><br/><strong>$ echo $JAVA_HOME</strong><br/><strong>/usr/lib/jvm/java-8-openjdk-amd64</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing and starting Elasticsearch</h1>
                </header>
            
            <article>
                
<p>Go to <a href="https://www.elastic.co/downloads/elasticsearch">elastic.co/downloads/elasticsearch</a><span> </span>and download the latest Elasticsearch version for your machine. For Ubuntu, we can download the official<span> </span><kbd>.deb</kbd><span> </span>package and install using<span> </span><kbd>dpkg</kbd>:</p>
<pre><strong>$ sudo dpkg -i elasticsearch-6.3.2.deb</strong></pre>
<div class="packt_infobox">Your version of Elasticsearch might be different from the one here. That's fine.</div>
<p>Next, we need to configure Elasticsearch to use the Java version we just installed. We have already done this for the entire system, but Elasticsearch also has its own configuration file for specifying the path to the Java binaries. Open up the<span> </span><kbd>/etc/default/elasticsearch</kbd><span> </span><span>file</span><span> and add an entry for the</span><span> </span><kbd>JAVA_HOME</kbd><span> </span><span>variable, just as you did before:</span></p>
<pre># Elasticsearch Java path<br/>JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</pre>
<p><span>Now, we can start Elasticsearch! Elasticsearch is installed as a service, so we can use <kbd>systemctl</kbd> to start and stop it:</span></p>
<pre><strong>sudo systemctl start elasticsearch.service</strong><br/><strong>sudo systemctl stop elasticsearch.service</strong></pre>
<p>To simplify development, we can make Elasticsearch start whenever the system is rebooted by enabling it:</p>
<pre><strong>sudo systemctl daemon-reload</strong><br/><strong>sudo systemctl enable elasticsearch.service</strong></pre>
<p class="mce-root"/>
<p>Now, we can c<span>heck that Elasticsearch is running using <kbd>systemctl</kbd>:</span></p>
<pre><strong>$ sudo systemctl start elasticsearch.service</strong><br/><strong>$ sudo systemctl status elasticsearch.service</strong><br/><strong>● elasticsearch.service - Elasticsearch</strong><br/><strong> Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled; vendo</strong><br/><strong> Active: active (running) since Wed 2017-12-27 17:52:06 GMT; 4s ago</strong><br/><strong> Docs: http://www.elastic.co</strong><br/><strong> Main PID: 20699 (java)</strong><br/><strong> Tasks: 42 (limit: 4915)</strong><br/><strong> Memory: 1.1G</strong><br/><strong> CPU: 12.431s</strong><br/><strong> CGroup: /system.slice/elasticsearch.service</strong><br/><strong> └─20699 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Xms1g -Xmx1g -XX:</strong><br/><br/><strong>Dec 27 17:52:06 nucleolus systemd[1]: Started Elasticsearch.</strong></pre>
<p><span>Alternatively, a more direct approach would simply be to send a</span><span> query to the Elasticsearch API on its default port of <kbd>9200</kbd>:</span></p>
<pre><strong>$ curl 'http://localhost:9200/?pretty'</strong><br/><strong>{</strong><br/><strong>  "name" : "6pAE96Q",</strong><br/><strong>  "cluster_name" : "elasticsearch",</strong><br/><strong>  "cluster_uuid" : "n6vLxwydTmeN4H6rX0tqlA",</strong><br/><strong>  "version" : {</strong><br/><strong>    "number" : "6.3.2",</strong><br/><strong>    "build_date" : "2018-07-20T05:20:23.451332Z",</strong><br/><strong>    "lucene_version" : "7.3.1"</strong><br/><strong>  },</strong><br/><strong>  "tagline" : "You Know, for Search"</strong><br/><br/><strong>}</strong></pre>
<p>We get a reply, which means Elasticsearch is running on your machine!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding key concepts in Elasticsearch</h1>
                </header>
            
            <article>
                
<p>We will be sending queries to Elasticsearch very shortly, but it helps if we understand a few basic concepts.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elasticsearch is a JSON document store</h1>
                </header>
            
            <article>
                
<p>As you might have noticed from the response body of our API call, <span>Elasticsearch stores data </span>in<span> </span><strong>JavaScript Object Notation</strong> (<strong>JSON</strong>) format. This allows developers to store objects with more complex (often nested) structures when compared to <strong>relational databases</strong> that impose a flat structure with <strong>rows</strong> and <strong>tables</strong>.</p>
<p>That's not to say document databases are better than relational databases, or vice versa; they are different and their suitability depends on their use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Document vs. relationship data storage</h1>
                </header>
            
            <article>
                
<p>For example, your application may be a school directory, storing information about schools, users (including teachers, staff, parents, and students), exams, classrooms, classes, and their relations with each other. Given that the data structure can be kept relatively flat (that is, mostly simple key-value entries), a relational database would be most suitable.</p>
<p>On the other hand, if you're building a social network, and want to store a user's settings, a document database may be more suitable. This is because the settings may be quite complex, such as the one shown here:</p>
<pre>{<br/>  "profile": {<br/>    "firstName": "",<br/>    "lastName": "",<br/>    "avatar": "",<br/>    "cover": "",<br/>    "color": "#fedcab"<br/>  },<br/>  "active": true,<br/>  "notifications": {<br/>    "email": {<br/>      "disable": false,<br/>      "like": true,<br/>      "comment": true,<br/>      "follow": true<br/>    },<br/>    "app": {<br/>      "disable": false,<br/>      "like": true,<br/>      "comment": true,<br/>      "follow": true }<br/>    }};</pre>
<p>With a relational database, you'll have to establish naming conventions for the columns (such as <kbd>settings.notification.app.follow</kbd>) in order to retain hierarchical information. However, to use the settings, you'll have to manually reconstruct the object before you can work with it. You'll need to do this each time the entry is retrieved.</p>
<p><span>Storing this user information as a document allows you to store objects as they are, retaining their structure, and retrieve them as they are, without having to do extra work.</span></p>
<div class="packt_infobox">Several relational databases have started allowing users to store documents as values. For example, starting with MySQL 5.7, you can store schema-less documents.<br/>
<br/>
However, if your intention is to structure your data in a non-relational way, you'd be better off using a NoSQL database from the start. I'd recommend storing documents in a traditional relational database only when you have existing data and you are adding a new data structure on top of it.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding indices, types, documents, and versions</h1>
                </header>
            
            <article>
                
<p>In Elasticsearch, every document is uniquely identified by four attributes: its <strong>index</strong>, <strong>type</strong>, <strong>ID</strong>, and <strong>version</strong>.</p>
<p><span>Related documents should be stored under the same index. Although not equivalent, an index is analogous to a database in a relational database. For example, all documents used in our user directory API may be stored in the <kbd>directory</kbd></span><span> </span><span>index, or since our</span><span> </span>platform<span> is called Hobnob, we may also name our index</span><span> </span><kbd>hobnob</kbd><span>.</span></p>
<p>Documents stored within an index must belong to a certain type. For our user directory API, you may have documents that belong to the<span> </span><kbd>person</kbd><span> </span>and<span> </span><kbd>company</kbd><span> </span>types. <span>Although not equivalent, type is analogous to a table in a relational database.</span></p>
<p>Each document must also have an ID and version. Whenever a document is modified in any way, its version increments by a certain amount (usually<span> </span><kbd>1</kbd>).</p>
<div class="packt_infobox">Elasticsearch does not store older versions of the document. The version counter is there to allow us to perform <strong>concurrent updates</strong> and <strong>optimistic locking</strong> (more on these techniques later).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Querying Elasticsearch from E2E tests</h1>
                </header>
            
            <article>
                
<p>We now have all the required knowledge in Elasticsearch to implement our last undefined step definition, which reads from the database to see if our user document has been indexed correctly. We will be using the JavaScript client, which is merely a wrapper around the REST API, with a one-to-one mapping to its endpoints. So first, let's install it:</p>
<pre><strong>$ yarn add elasticsearch</strong></pre>
<p><span>Next, import the package into our <kbd>spec/cucumber/steps/index.js</kbd> file and create an instance of</span><span> </span><kbd>elasticsearch.Client</kbd><span>:</span></p>
<pre>const client = new elasticsearch.Client({<br/>  host: `${process.env.ELASTICSEARCH_PROTOCOL}://${process.env.ELASTICSEARCH_HOSTNAME}:${process.env.ELASTICSEARCH_PORT}`,<br/>});</pre>
<p>By default, Elasticsearch runs on port <kbd>9200</kbd>. However, to avoid hard-coded values, we have explicitly passed in an options object, specifying the <kbd>host</kbd> option, which takes its value from the environment variables. <span>To make this work, add these </span>environment variables to our<span> </span><kbd>.env</kbd> and<span> </span><kbd>.env.example</kbd><span> </span>files:</p>
<pre>ELASTICSEARCH_PROTOCOL=http<br/>ELASTICSEARCH_HOSTNAME=localhost<br/>ELASTICSEARCH_PORT=9200</pre>
<div class="packt_infobox">For a full list of options that the<span> </span><kbd>elasticsearch.Client</kbd><span> </span>constructor function accepts, check out <a href="https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/configuration.html">elastic.co/guide/en/elasticsearch/client/javascript-api/current/configuration.html</a>.</div>
<p>As specified in our Cucumber test scenario, we require the <span class="packt_screen">Create User</span> endpoint to return a string, which we store in <kbd>this.responsePayload</kbd>. This should be the ID of the user. Therefore, if we can find the user document again using this ID, it means the document is in the database and we have completed our feature.</p>
<p class="mce-root"/>
<p>To find the document by ID, we can use the<span> </span><kbd>get</kbd><span> method from the Elasticsearch client, which will get a typed JSON document from the index based on its ID. All of the methods in the Elasticsearch client are asynchronous—if we provide a callback, it will invoke the callback; otherwise, it will return a promise.</span></p>
<p>The result from Elasticsearch would have the following structure:</p>
<pre>{ _index: &lt;index&gt;,<br/>  _type: &lt;type&gt;,<br/>  _id: &lt;id&gt;,<br/>  _version: &lt;version&gt;,<br/>  found: true,<br/>  _source: &lt;document&gt; }</pre>
<p>The <kbd>_source</kbd> property contains the actual document. To make sure it is the same as the one we sent in the request, w<span>e can use the</span><span> </span><kbd>deepEqual</kbd><span> </span><span>method from Node's</span><span> </span><kbd>assert</kbd><span> </span><span>module to compare the <kbd>_source</kbd> document with <kbd>this.requestPayload</kbd>.</span></p>
<p>Given this information, try to implement the final step definition yourself, and check back here for the answer:</p>
<pre>Then(/^the payload object should be added to the database, grouped under the "([a-zA-Z]+)" type$/, function (type, callback) {<br/>  client.get({<br/>    index: 'hobnob',<br/>    type,<br/>    id: this.responsePayload,<br/>  }).then((result) =&gt; {<br/>    assert.deepEqual(result._source, this.requestPayload);<br/>    callback();<br/>  }).catch(callback);<br/>});</pre>
<div class="packt_infobox">ESLint may complain that <kbd>_source</kbd> violates the <kbd>no-underscore-dangle</kbd> rule. Traditionally, underscores in an identifier are used to indicate that the variable or method should be "private", but since there're no truly private variables in JavaScript, this convention is highly controversial.<br/>
<br/>
Here, however, we are using the Elasticsearch client and this is their convention. Therefore, we should add a rule to the project-level <kbd>.eslintrc</kbd> file to disable this rule.</div>
<p>Run the tests again, and there should be no undefined step definitions anymore. But, it still fails because we haven't implemented the actual success scenario in our <kbd>src/index.js</kbd> yet. So, let's get down to it!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Indexing documents to Elasticsearch</h1>
                </header>
            
            <article>
                
<p><span>In <kbd>src/index.js</kbd>, import the Elasticsearch library and initiate a client as we did before; then, in the request handler for</span><span> </span><kbd>POST /users</kbd><span>,</span><span> </span><span>use the Elasticsearch JavaScript client's </span><kbd>index</kbd><span> method</span><span> </span><span>to add the payload object into the Elasticsearch index</span><span>:</span></p>
<pre>import elasticsearch from 'elasticsearch';<br/>const client = new elasticsearch.Client({<br/>  host: `${process.env.ELASTICSEARCH_PROTOCOL}://${process.env.ELASTICSEARCH_HOSTNAME}:${process.env.ELASTICSEARCH_PORT}`,<br/>});<br/>...<br/><br/>app.post('/users', (req, res, next) =&gt; {<br/>  ...<br/>  client.index({<br/>    index: 'hobnob',<br/>    type: 'user',<br/>    body: req.body<br/>  })<br/>}</pre>
<p>The<span> </span><kbd>index</kbd><span> </span>method returns a promise, which should resolve <span>to something similar to this:</span></p>
<pre>{ _index: 'hobnob',<br/>  _type: 'users',<br/>  _id: 'AV7HyAlRmIBlG9P7rgWY',<br/>  _version: 1,<br/>  result: 'created',<br/>  _shards: { total: 2, successful: 1, failed: 0 },<br/>  created: true }</pre>
<p>The only useful and relevant piece of information we can return to the client is the newly auto-generated<span> </span><kbd>_id</kbd><span> </span>field. Therefore, we should extract that information and make the function return a promise, which resolves to only the<span> </span><kbd>_id</kbd><span> </span>field value. As a last resort, return a<span> </span><kbd>500 Internal Server</kbd><span> </span>error to indicate to the client that their request is valid, but our server is experiencing some issues:</p>
<pre>client.index({<br/>  index: 'hobnob',<br/>  type: 'user',<br/>  body: req.body,<br/>}).then((result) =&gt; {<br/>  res.status(201);<br/>  res.set('Content-Type', 'text/plain');<br/>  res.send(result._id);<br/>}).catch(() =&gt; {<br/>  res.status(500);<br/>  res.set('Content-Type', 'application/json');<br/>  res.json({ message: 'Internal Server Error' });<br/>});</pre>
<p>Now, our E2E tests should all pass again!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleaning up after our tests</h1>
                </header>
            
            <article>
                
<p>When we run our tests, it'll index user documents into our local development database. Over many runs, our database will be filled with a large number of test user documents. Ideally, we want all our tests to be self-contained. This means with each test run, we should reset the state of the database back to the state before the test was run. To achieve this, we must make two further changes to our test code:</p>
<ul>
<li>Delete the test user after we have made the necessary assertions</li>
<li>Run the tests on a test database; in the case of Elasticsearch, we can simply use a different index for our tests</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting our test user</h1>
                </header>
            
            <article>
                
<p>First, add a new entry to the list of features in the Cucumber specification:</p>
<pre>...<br/>And the payload of the response should be a string<br/>And the payload object should be added to the database, grouped under the "user" type<br/><strong>And the newly-created user should be deleted</strong></pre>
<p>Next, define the corresponding step definition for this step. But first, we are going to modify the step definition that indexed the document, and change it to persist the document type in the context:</p>
<pre>Then(/^the payload object should be added to the database, grouped under the "([a-zA-Z]+)" type$/, function (type, callback) {<br/>  <strong>this.type = type;</strong><br/>  client.get({<br/>    index: 'hobnob'<br/>    type: type,<br/>    id: this.responsePayload<br/>  })<br/>  ...<br/>});</pre>
<p>Then, add a new step definition that uses<span> </span><kbd>client.delete</kbd><span> to delete a document by ID:</span></p>
<pre>Then('the newly-created user should be deleted', function () {<br/>  client.delete({<br/>    index: 'hobnob',<br/>    type: this.type,<br/>    id: this.responsePayload,<br/>  });<br/>});</pre>
<p>The result of the<span> </span><kbd>delete</kbd><span> </span>method looks something like this:</p>
<pre>{ _index: 'hobnob',<br/>  _type: 'user',<br/>  _id: 'N2hWu2ABiAD9b15yOZTt',<br/>  _version: 2,<br/>  result: 'deleted',<br/>  _shards: { total: 2, successful: 1, failed: 0 },<br/>  _seq_no: 4,<br/>  _primary_term: 2 }</pre>
<p>A successful operation will have its<span> </span><kbd>result</kbd><span> </span>property set to<span> </span><kbd>'deleted'</kbd>; therefore, we can use it to assert whether the step was successful or not. Update the step definition to the following:</p>
<pre>Then('the newly-created user should be deleted', function (callback) {<br/>  client.delete({<br/>    index: 'hobnob',<br/>    type: this.type,<br/>    id: this.responsePayload,<br/>  }).then(function (res) {<br/>    assert.equal(res.result, 'deleted');<br/>    callback();<br/>  }).catch(callback);<br/>});</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Run the tests and make sure they pass. We've now implemented our happy path/success scenario, so it's a good time to commit our changes:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Implement Create User success scenario"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving our testing experience</h1>
                </header>
            
            <article>
                
<p><span>Although we are now cleaning up after ourselves, using the same index for both testing and development is not ideal. Instead, we should use one index for development, and another for testing.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running tests in a test database</h1>
                </header>
            
            <article>
                
<p>For our project, let's use the index name <kbd>hobnob</kbd> for development, and <kbd>test</kbd> for testing. Instead of hard-coding the index name into our code, we can use an environment variable to set it dynamically. Therefore, in both our application and test code, replace <em>all</em> instances of<span> </span><kbd>index: 'hobnob'</kbd><span> </span>with<span> </span><kbd>index: process.env.ELASTICSEARCH_INDEX</kbd>.</p>
<p>Currently, we are using the <kbd>dotenv-cli</kbd> package to load our environment variables. As it turns out, the package also provides an <kbd>-e</kbd> flag that allows us to load multiple files. This means we can store default environment variables in our <kbd>.env</kbd> file, and create a new <kbd>test.env</kbd> to store testing-specific environment variables, which will override the defaults.</p>
<p>Therefore, add the following line to our <kbd>.env</kbd> file:</p>
<pre>ELASTICSEARCH_INDEX=hobnob</pre>
<p>Then, create two new files—<kbd>test.env</kbd> and <kbd>test.env.example</kbd>—and add the following line:</p>
<pre>ELASTICSEARCH_INDEX=test</pre>
<p>Lastly, update our <kbd>test</kbd> script to load the test environment before the default:</p>
<pre>"test:e2e": "dotenv -e test.env -e .env cucumber-js -- spec/cucumber/features --require-module @babel/register --require spec/cucumber/steps",</pre>
<p>Stop the API server and restart it with the following command:</p>
<pre><strong>$ npx dotenv -e test.env yarn run watch</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Run our E2E tests again, and they should all pass. The only difference now is that the tests are not affecting our development index at all!</p>
<p>Lastly, just to tidy things up, let's move all our environment files into a new directory called <kbd>envs</kbd> and update our <kbd>.gitignore</kbd> to ignore all files with the <kbd>.env</kbd> extension:</p>
<pre><strong>$ mkdir envs &amp;&amp; mv -t envs .env .env.example test.env test.env.example</strong><br/><strong>$ sed -i 's/^.env$/*.env/g' .gitignore</strong></pre>
<p>Of course, you also need to update your <kbd>serve</kbd> and <kbd>test</kbd> scripts:</p>
<pre>"serve": "yarn run build &amp;&amp; <strong>dotenv -e envs/.env</strong> node dist/index.js",<br/>"test:e2e": "<strong>dotenv -e envs/test.env -e envs/.env</strong> cucumber-js -- spec/cucumber/features --require-module @babel/register --require spec/cucumber/steps",</pre>
<p>Run the tests again and make sure they pass. Once you're happy, commit these changes to Git:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Use test index for E2E tests"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Separating development and testing servers</h1>
                </header>
            
            <article>
                
<p><span>Good job. Using a test database is certainly a step forward, but our testing workflow is still disjointed. At the moment, t</span><span>o run our tests, we need to stop our development API server, set the environment variables, and then restart it. Similarly, once the tests are finished, we need to stop and restart it again with the development environment.</span></p>
<p>Ideally, we should run two separate instances of the API server—one for development, one for testing—each binding to its own port. This way, there's no need to stop and restart our server just to run tests.</p>
<p>To achieve this, simply override the <kbd>SERVER_PORT</kbd> environment variable for our test environment by adding the following line to <kbd>envs/test.env</kbd> and <kbd>envs/test.env.example</kbd>:</p>
<pre>SERVER_PORT=8888</pre>
<p>Now, we can run <kbd>yarn run watch</kbd> to run our development API server, and <em>at the same time</em>, run <kbd>npx dotenv -e envs/test.env yarn run watch</kbd> to run our testing API server. We no longer need to stop and restart!</p>
<p>Although this is a minor change, let's still commit it to our repository:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Run test API server on different port"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making a standalone E2E test script</h1>
                </header>
            
            <article>
                
<p>But, we're not done yet! We can definitely improve our testing workflow even further. At the moment, to run our E2E test we have to ensure the following:</p>
<ul>
<li>An Elasticsearch instance is running</li>
<li>We use <kbd>dotenv-cli</kbd> to load our test environment and then run our API server</li>
</ul>
<p>While we could simply note down these instructions in a <kbd>README.md</kbd> file, it'll provide a better developer experience if we provide a single command to run, which will automatically load up Elasticsearch, set the right environment, run our API server, run our tests, and tear everything down once it's done.</p>
<p>This seems too much logic to fit into one line of npm script; instead, we can write a shell script, which allows us to specify this logic inside a file. We will use a <strong>Bash</strong> as the shell language, as it is the most popular and widely-supported shell.</p>
<div class="packt_tip">For Windows users, make sure you've installed the <em>Windows Subsystem for Linux</em> (WSL), which allows you to run GNU/Linux tools and Bash scripts natively on your Windows machine. You can find detailed instructions at <a href="https://docs.microsoft.com/en-us/windows/wsl/">docs.microsoft.com/en-us/windows/wsl/</a>.</div>
<p>Let's begin by creating a new directory called<span> </span><kbd>scripts</kbd><span>,</span> adding a new file inside it called<span> </span><kbd>e2e.test.sh</kbd>, and setting its file permission so it's executable:</p>
<pre><strong>$ mkdir scripts &amp;&amp; touch scripts/e2e.test.sh &amp;&amp; chmod +x scripts/e2e.test.sh</strong></pre>
<p>Then, update our <kbd>test:e2e</kbd> npm script to execute the shell script instead of running the <kbd>cucumber-js</kbd> command directly:</p>
<pre>"test:e2e": "dotenv -e envs/test.env -e envs/.env ./scripts/e2e.test.sh",</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The shebang interpreter directive</h1>
                </header>
            
            <article>
                
<p>The first line of a shell script is always the<span> </span><strong>shebang interpreter directive</strong>; it basically tells our shell which interpreter it should use to parse and run the instructions contained in this script file.</p>
<div class="packt_infobox">It's called a<span> </span><em>shebang</em><span> </span>interpreter directive because it starts with a <strong>shebang</strong>, which is simply a sequence of two characters: a hash sign (<kbd>#</kbd>) followed by an exclamation mark (!).</div>
<p>Some scripts might be written in Perl, or Python, or a different flavor of the shell; however, our script will be written for the Bash shell, so we should set the directive to the location of the<span> </span><kbd>bash</kbd><span> </span>executable, which we can derive from running <kbd>/usr/bin/env bash</kbd>. Therefore, add the following shebang as the first line in our <kbd>e2e.test.sh</kbd> file:</p>
<pre>#!/usr/bin/env bash</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensuring Elasticsearch is running</h1>
                </header>
            
            <article>
                
<p>Our API server depends on an active instance of Elasticsearch. Therefore, before we start our API server, let's make sure our Elasticsearch service is active. Add the following check under the shebang line:</p>
<pre>RETRY_INTERVAL=${RETRY_INTERVAL:-0.2}<br/>if ! systemctl --quiet is-active elasticsearch.service; then<br/>  sudo systemctl start elasticsearch.service<br/>  # Wait until Elasticsearch is ready to respond<br/>  until curl --silent $ELASTICSEARCH_HOSTNAME:$ELASTICSEARCH_PORT -w "" -o /dev/null; do<br/>    sleep $RETRY_INTERVAL<br/>  done<br/>fi</pre>
<p>First, we use the <kbd>is-active</kbd> command of <kbd>systemctl</kbd> to check whether the Elasticsearch service is active; the command will exit with a <kbd>0</kbd> if it is active, and a non-zero value if not.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Generally, when a process successfully executes, it will exit with a status of zero (</span><kbd>0</kbd><span>); otherwise, it will exit with a non-zero status code. </span>Inside an <kbd>if</kbd> block, the exit codes have special meaning—a <kbd>0</kbd> exit code means <kbd>true</kbd>, and a non-zero exit code means <kbd>false</kbd>.</p>
<p>This means that if the service is not active, we'd use the <kbd>start</kbd> command of <kbd>systemctl</kbd> to start it. However, Elasticsearch takes time to initiate before it can respond to requests. Therefore, we are polling its endpoint with <kbd>curl</kbd>, and blocking downstream execution until Elasticsearch is ready.</p>
<div class="packt_tip">If you're curious what the flags mean on the commands, you can get detailed documentation on them by using the <kbd>man</kbd> command. Try running <kbd>man systemctl</kbd>, <kbd>man curl</kbd>, and even <kbd>man man</kbd>! Some commands also support a <kbd>-h</kbd> or <kbd>--help</kbd> flag, which contains less information but is usually easier to digest.</div>
<p>We will retry the endpoint every 0.2 seconds. This is set in the <kbd>RETRY_INTERVAL</kbd> environment variable. The <kbd>${RETRY_INTERVAL:-0.2}</kbd> syntax means we should only use the <kbd>0.2</kbd> value if the environment variable is not already set; in other words, the <kbd>0.2</kbd> value should be used as a default.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the test API server in the background</h1>
                </header>
            
            <article>
                
<p>Next, before we can run our tests, we must run our API server. However, the API server and the tests need to run at the same time, but there can only be one foreground process group attached to the terminal. We want this to be our test, so we can interact with it if required (for example, to stop the test). Therefore, we need to run our API server as a background process.</p>
<p>In Bash (and other shells that support <strong>job control</strong>), we can run a command as a background process by appending a single ampersand (<kbd>&amp;</kbd>) after the command. Therefore, add the following lines after our Elasticsearch initiation block:</p>
<pre># Run our API server as a background process<br/><strong>yarn run serve &amp;<br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Checking our API server is ready</h1>
                </header>
            
            <article>
                
<p>Next, we need to run our tests. But, if we do it immediately after we execute<span> </span><kbd>yarn run serve &amp;</kbd>, it will not work:</p>
<pre># This won't work!<br/><strong>yarn run serve &amp;</strong><br/><strong>yarn run test:e2e</strong></pre>
<p>This is because the tests are run before our API server is ready to handle the requests. Therefore, just like we did with the Elasticsearch service, we must wait for our API server to be ready before running our tests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checking API status using netstat/ss</h1>
                </header>
            
            <article>
                
<p>But, how do we know when the API is ready? We could send a request to one of the API's endpoints and see if it returns a result. However, this couples our script with the implementation of the API. A better way would be to check whether the API is actively listening to the server port. We can do this using the <kbd>netstat</kbd> utility, or its replacement, <kbd>ss</kbd> (which stands for <strong>s</strong>ocket <strong>s</strong>tatistics). Both commands are used to display network-related information such as open connections and socket ports:</p>
<pre><strong>$ netstat -lnt</strong><br/><strong>$ ss -lnt</strong></pre>
<p>For both commands, the <kbd>-l</kbd> flag will limit the results to only listening sockets, the <kbd>-n</kbd> flag will display all hosts and ports as numeric values (for instance, it'll output <kbd>127.0.0.1:631</kbd> instead of <kbd>127.0.0.1:ipp</kbd>), and the <kbd>-t</kbd> flag will filter out non-TCP sockets. The end result is an output that looks like this:</p>
<pre><strong>$ netstat -lnt</strong><br/><strong>Proto Recv-Q Send-Q Local Address     Foreign Address   State</strong><br/><strong>tcp        0      0 127.0.0.1:5939    0.0.0.0:*         LISTEN</strong><br/><strong>tcp        0      0 127.0.0.1:53      0.0.0.0:*         LISTEN</strong><br/><strong>tcp6       0      0 ::1:631           :::*              LISTEN</strong><br/><strong>tcp6       0      0 :::8888           :::*              LISTEN</strong><br/><strong>tcp6       0      0 :::3128           :::*              LISTEN</strong></pre>
<p>To check whether a specific port is listening, we can simply run <kbd>grep</kbd> on the output (for instance, <kbd>ss -lnt | grep -q 8888</kbd>) If <kbd>grep</kbd> finds a result, it will exit with a status code of <kbd>0</kbd>; if no matches are found, it will exit with a non-zero code. We can use this feature of grep to poll <kbd>ss</kbd> at regular intervals until the port is bound.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Add the following block below our <kbd>yarn run serve &amp;</kbd> command:</p>
<pre>RETRY_INTERVAL=0.2<br/>until ss -lnt | grep -q :$SERVER_PORT; do<br/>  sleep $RETRY_INTERVAL<br/>done</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleaning up the background process</h1>
                </header>
            
            <article>
                
<p>We need to make a few last changes to our test script before we can run our tests. At the moment, we are running our API server in the background. However, when our script exits, the API will still keep running; this means we will get the <kbd>listen EADDRINUSE :::8888</kbd> error the next time we run the E2E tests.</p>
<p>Therefore, we need to kill that background process before the test script exits. This can be done with the <kbd>kill</kbd> command. Add the following line at the end of the test script:</p>
<pre># Terminate all processes within the same process group by sending a SIGTERM signal<br/>kill -15 0</pre>
<p><strong>Process ID</strong> (<strong>PID</strong>)<span> </span><kbd>0</kbd><span> </span>(zero) is a special PID that represents all member processes within the same process group as the process that raised the signal. Therefore, our previous command sends a<span> </span><kbd>SIGTERM</kbd><span> </span>signal (which has a numeric code of<span> </span><kbd>15</kbd>) to all processes within the same process group.</p>
<p>And, just to make sure no other process has bound to the same port as our API server, let's add a check at the beginning of our Bash script that'll exit immediately if the port is unavailable:</p>
<pre># Make sure the port is not already bound<br/>if ss -lnt | grep -q :$SERVER_PORT; then<br/>  echo "Another process is already listening to port $SERVER_PORT"<br/>  exit 1;<br/>fi</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running our tests</h1>
                </header>
            
            <article>
                
<p>Finally, we are able to run our tests! Add the <kbd>cucumber-js</kbd> command just prior to the <kbd>kill</kbd> command:</p>
<pre><strong>npx cucumber-js spec/cucumber/features --require-module @babel/register --require spec/cucumber/steps</strong></pre>
<p> </p>
<p>Your final <kbd>scripts/e2e.test.sh</kbd> script should look like this (comments removed):</p>
<pre>#!/usr/bin/env bash<br/>if ss -lnt | grep -q :$SERVER_PORT; then<br/>  echo "Another process is already listening to port $SERVER_PORT"<br/>  exit 1;<br/>fi<br/>RETRY_INTERVAL=${RETRY_INTERVAL:-0.2}<br/>if ! systemctl is-active --quiet elasticsearch.service; then<br/>  sudo systemctl start elasticsearch.service<br/>  until curl --silent $ELASTICSEARCH_HOSTNAME:$ELASTICSEARCH_PORT -w "" -o /dev/null; do<br/>    sleep $RETRY_INTERVAL<br/>  done<br/>fi<br/>yarn run serve &amp;<br/>until ss -lnt | grep -q :$SERVER_PORT; do<br/>  sleep $RETRY_INTERVAL<br/>done<br/>npx cucumber-js spec/cucumber/features --require-module @babel/register --require spec/cucumber/steps<br/>kill -15 0</pre>
<p>Just to double-check, run the E2E tests to make sure they still pass. Of course, commit these changes to Git:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Make standalone E2E test script"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we continued our work on the Create User endpoint. Specifically, we implemented the success scenario by persisting data into Elasticsearch. Then, we refactored our testing workflow by creating a Bash script that automatically loads up all dependencies before running our tests.</p>
<p>In the next chapter, we will refactor our code further, by breaking it down into smaller units, and covering them with unit and integration tests, written using <span>Mocha, Chai, and Sinon</span>. We will also continue to implement the rest of the endpoints, making sure we follow good API design principles.</p>


            </article>

            
        </section>
    </body></html>