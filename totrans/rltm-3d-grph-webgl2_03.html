<html><head></head><body>
        

                            
                    <h1 class="header-title">Lights</h1>
                
            
            
                
<p>In the previous chapter, we covered WebGL's rendering pipeline, defining geometries, passing data to the GPU, drawing types, and leveraging AJAX to asynchronously load external assets. Although we briefly covered shaders and their role in creating a WebGL application, we will go into more detail in this chapter and leverage the vertex and fragment shaders to create a lighting model for our scene.</p>
<p>Shaders allow us to define a mathematical model that governs how our scene is lit. To learn how to implement shaders, we will study different algorithms and see examples of their application. A basic knowledge of linear algebra will be really useful to help you understand the contents of this chapter. We will use a JavaScript library that handles most of the vector and matrix operations, so you do not need to worry about the mathematical operations. Nonetheless, your overall success depends on a strong conceptual understanding of the linear algebra operations that we will discuss.</p>
<p>In this chapter, we will:</p>
<ul>
<li>Learn about light sources, normals, and materials.</li>
<li>Learn the difference between shading and lighting.</li>
<li>Use the Goraud and Phong shading methods.</li>
<li>Use the Lambertian and Phong lighting models.</li>
<li>Define and use uniforms, attributes, and varyings.</li>
<li>Work with ESSL, the shading language for WebGL.</li>
<li>Discuss relevant WebGL API methods that relate to shaders.</li>
<li>Continue our analysis of WebGL as a state machine and describe the attributes relevant to shaders that can be set and retrieved from the state machine.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Lights, Normals, and Materials</h1>
                
            
            
                
<p>In the real world, we see objects because they reflect light. The illumination of an object depends on its position relative to the light source, surface orientation, and its material composition. In this chapter, we will learn how to combine these three elements in WebGL to model different illumination schemes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/09bf202d-a455-4a83-ab7d-c86e13ce91e1.png" style="width:39.42em;height:24.83em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Positional Versus Directional Lights</h1>
                
            
            
                
<div><p>Light sources can be <strong>positional</strong> or <strong>directional</strong>. A light source is called positional when its location will affect how the scene is lit. For instance, a lamp inside a room is a positional light source. Objects far from the lamp will receive very little light and may even appear obscure. In contrast, directional lights are lights that produce the same luminous result, regardless of their position. For example, the light from the sun will illuminate all objects in a terrestrial scene, regardless of their distance from the sun. This is because the sun is so far away that all light rays are considered parallel when they intersect the surface of an object. Directional lighting assumes that the light is coming uniformly from one direction:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9fa02bc7-d762-4a9a-a955-8e34745c481e.png"/></p>
</div>
<p>A positional light is modeled by a point in space, while a directional light is modeled with a vector that indicates its direction. It is common to use a normalized vector for this purpose, given that this simplifies mathematical operations. Also, it is generally the case that computing directional lighting is actually simpler and less computationally expensive than positional lighting.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Normals</h1>
                
            
            
                
<p><strong>Normals</strong> are vectors that are perpendicular to the surface we want to illuminate. Normals represent the orientation of the surface and are therefore critical to modeling the interaction between a light source and the object. Given that each vertex has an associated normal vector, we can use the cross product to calculate normals.</p>
<p>Cross-Product<strong><br/>
<br/></strong> By definition, the cross-product of vectors <kbd>A</kbd> and <kbd>B</kbd> will be a vector perpendicular to both vectors <kbd>A</kbd> and <kbd>B</kbd>.</p>
<p>Let's break this down. If we have the triangle conformed by vertices <kbd>p0</kbd>, <kbd>p1</kbd>, and <kbd>p2</kbd>, we can define the <kbd>v1</kbd> vector as <kbd>p1 - p0</kbd> and the <kbd>v2</kbd> vector as <kbd>p2 - p0</kbd>. The normal is then obtained by calculating the <kbd>v1 x v2</kbd> cross-product. Graphically, this procedure looks something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/cba4ae51-6cea-43a5-aa0e-9f6745103d12.png" style="width:28.00em;height:14.92em;"/></p>
<p>We then repeat the same calculation for each vertex on each triangle. But, what about the vertices that are shared by more than one triangle? Each shared vertex normal will receive a contribution from each of the triangles in which the vertex appears.</p>
<p>For example, say that the <kbd>p1</kbd> vertex is shared by the <kbd>#1</kbd> and <kbd>#2</kbd> triangles, and that we have already calculated the normals for the vertices of the <kbd>#1</kbd> triangle. Then, we need to update the <kbd>p1</kbd> normal by adding up the calculated normal for <kbd>p1</kbd> on the <kbd>#2</kbd> triangle. This is a <strong>vector sum</strong>. Graphically, this looks similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/7e742957-704e-4e2a-870a-71261fdb16c0.png" style="width:32.50em;height:17.00em;"/></p>
<p>Similar to lights, normals are generally normalized to facilitate mathematical operations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Materials</h1>
                
            
            
                
<p>In WebGL, the material of an object can be modeled by several parameters, including its color and texture. Material colors are usually modeled as triplets in the RGB (red, green, blue) space. Textures, on the other hand, correspond to images that are mapped onto the surface of the object. This process is usually called <strong>texture mapping</strong>. We will cover texture mapping in later chapters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Lights, Normals, and Materials in the Pipeline</h1>
                
            
            
                
<p>In <a href="d2019a49-9e84-448c-8799-e296187476d1.xhtml">Chapter 2</a>, <em>Rendering</em>, we discussed that WebGL buffers, attributes, and uniforms are used as input variables to the shaders, and that varyings are used to pass information between the vertex shader and the fragment shader. Let's revisit the pipeline and see where lights, normals, and materials fit in:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8e2b6c60-e7ac-4094-b712-f3785f985095.png" style="width:41.25em;height:22.17em;"/></p>
<p>Normals are defined on a vertex-per-vertex basis; therefore, normals are modeled as a VBO and are mapped using an attribute, as shown in the preceding diagram. Note that attributes cannot be directly passed to the fragment shader. To pass information from the vertex shader to the fragment shader, we must use varyings.</p>
<p>Lights and materials are passed as uniforms. Uniforms are available to both the vertex shader and the fragment shader. This gives us a lot of flexibility to calculate our lighting model, because we can calculate how the light is reflected on a vertex-by-vertex basis (vertex shader) or on a fragment-per-fragment basis (fragment shader).</p>
<p>Program<br/>
<br/>
Remember that the vertex shader and fragment shader together are referred to as a <strong>program</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Parallelism and the Difference Between Attributes and Uniforms</h1>
                
            
            
                
<p>There is an important distinction to make between attributes and uniforms. When a draw call is invoked (using <kbd>drawArrays</kbd> or <kbd>drawElements</kbd>), the GPU will launch several copies of the vertex shader in parallel. Each copy will receive a different set of attributes. These attributes are drawn from the VBOs that are mapped onto the respective attributes.</p>
<p>On the other hand, all of the copies of the vertex shaders will receive the same uniforms – hence the name: uniform. In other words, uniforms can be seen as constants <em>per draw call</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3903b1a6-0105-4377-bc3c-c3fcfb5e7fdc.png" style="width:38.75em;height:25.75em;"/></p>
<p>Once lights, normals, and materials are passed to the program, the next step is to determine which <em>shading</em> and <em>lighting</em> models we will implement. Let's investigate what this involves.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Shading Methods and Light-Reflection Models</h1>
                
            
            
                
<p>Although the terms <em>shading</em> and <em>lighting</em> are often ambiguously interchanged, they refer to two different concepts.</p>
<p>Shading refers to the type of <em>interpolation</em> that is performed to obtain the final color for every fragment in the scene. Later, we will explain how the type of shading determines where the final color is calculated – in the vertex shader or in the fragment shader.</p>
<p>Once the shading model is established, the lighting model determines <em>how</em> the normals, materials, and lights need to be combined to produce the final color. Since the equations for lighting models use the physical principles of light reflection, lighting models are also referred to as <em>reflection models</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Shading/Interpolation Methods</h1>
                
            
            
                
<p>In this section, we will analyze two basic types of interpolation methods: <strong>Goraud</strong> and <strong>Phong</strong> shading.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Goraud Interpolation</h1>
                
            
            
                
<p>The <strong>Goraud</strong> interpolation method calculates the final color in the <em>vertex shader</em>. The vertex normals are used to perform this calculation. Then, using a varying variable, the final color for the vertex is passed to the fragment shader. Due to the automatic interpolation of varyings provided by the rendering pipeline, each fragment will have a color that is the result of interpolating the colors of the enclosing triangle for each fragment.</p>
<p>Varying Interpolation<br/>
<br/>
The interpolation of varyings is automatic in the rendering pipeline. No programming is required.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Phong Interpolation</h1>
                
            
            
                
<p>The <strong>Phong</strong> method calculates the final color in the <em>fragment shader</em>. To do so, each vertex normal is passed from the vertex shader to the fragment shader using a varying. Because of the interpolation mechanism of varyings included in the pipeline, each fragment will have its own normal. Fragment normals are then used to calculate the final color in the fragment shader.</p>
<p>The following diagram summarizes the two interpolation models:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/aebd4849-7f90-4747-942e-67d2bcc45f4c.png"/></p>
<p>The shading method does not specify how the final color for each fragment is calculated. It only specifies <em>where</em> (vertex or fragment shader) and the <em>type of interpolation</em> (vertex colors or vertex normals) to be used.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Goraud Versus Phong Shading</h1>
                
            
            
                
<p>We now understand that Goraud shading performs the calculations inside the vertex shader and leverages the built-in rendering pipeline's interpolation. Phong shading, on the other hand, performs all of the calculations inside the fragment shader – that is, per fragment (or pixel). With these two details in mind, can you guess some of the advantages and disadvantages of these two shading techniques?</p>
<p>Goraud shading is considered to be faster since the performed calculations are computed per vertex, whereas Phong shading is calculated per fragment. The speed in performance does come at the cost of accurate or more realistic interpolation. This is most noticeable in cases where a light's intensity does not linearly degrade between two vertices. Later in this chapter, we will cover these two techniques in more detail.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Light-Reflection Models</h1>
                
            
            
                
<p>As we mentioned previously, the lighting model is independent from the shading/interpolation model. The shading model only determines where the final color is calculated. Now, it’s time to talk about how to perform such calculations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Lambertian Reflection Model</h1>
                
            
            
                
<p><strong>Lambertian reflections</strong> are commonly used in computer graphics as a model for <em>diffuse reflections</em>, which are the kinds of reflections where an incident light ray is reflected in many angles instead of just <em>one</em> angle, as is the case for <em>specular reflections:</em></p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9fefc2f3-a738-4958-aad9-e68189f583da.png" style="width:35.08em;height:18.92em;"/></p>
<p>This lighting model is based on the <strong>cosine emission law</strong>, or <strong>Lambert's emission law</strong>. It is named after Johann Heinrich Lambert, from his <em>Photometria</em>, published in 1760.</p>
<p>The Lambertian reflection is usually calculated as the dot product between the surface normal (vertex or fragment normal, depending on the interpolation method used) and the negative of the light-direction vector. Then, the number is multiplied by the material and light source colors.</p>
<p>Light-Direction Vector<strong><br/>
<br/></strong> The light-direction vector is the vector that starts on the surface and ends on the light source position. It is essentially the vector that maps the light's position to the surface of the geometry.</p>
<div><p class="CDPAlignCenter CDPAlign"><img src="img/d0d62439-0e80-4e28-a731-bfd297772d14.png" style="width:39.67em;height:20.75em;"/></p>
<p>Where:</p>
<pre class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/ff76cba5-be2c-4c85-8bc8-314ca66678fe.png" style="width:9.67em;height:1.33em;"/></pre>
<p><kbd><img class="fm-editor-equation" src="img/393da706-dd4c-4ab4-a7e0-10f4652a34e3.png" style="width:1.17em;height:1.33em;"/></kbd> is the final diffuse color, <kbd><img class="fm-editor-equation" src="img/bd8a12b0-4188-47aa-8911-018975269e5a.png" style="width:1.58em;height:1.58em;"/></kbd> is the light diffuse color, and <kbd><img class="fm-editor-equation" src="img/425ac9cb-4da1-4c64-9644-b00c460f945a.png" style="width:1.58em;height:1.58em;"/></kbd> is the material diffuse color.</p>
<p>That being said, we'd derive the final diffuse color with the following:</p>
<pre class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/5b5a8c64-269e-41c3-b28e-99ccc67412e4.png" style="width:12.83em;height:1.42em;"/></pre>
<p>If <kbd><em>L</em></kbd> and <kbd><em>N</em></kbd> are normalized, then:</p>
<pre class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/5257c985-b356-40f1-a122-1e4a2fb70066.png" style="width:9.25em;height:1.25em;"/></pre>
<pre class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/5257c985-b356-40f1-a122-1e4a2fb70066.png" style="width:9.25em;height:1.25em;"/><br/><img class="fm-editor-equation" src="img/6c4d5a0c-bbc9-4a02-8b52-f6d84174b119.png" style="width:9.33em;height:1.42em;"/></pre></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Phong Reflection Model</h1>
                
            
            
                
<p>The Phong reflection model describes the way a surface reflects the light as the sum of three types of reflection: ambient, diffuse, and specular. It was developed by Bui Tuong Phong, who published it in his 1973 PhD dissertation:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c6b4bb7f-75de-48f4-890a-920f4d158a3b.png"/></p>
<p>Let's cover these concepts individually.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Ambient</h1>
                
            
            
                
<p>The <strong>ambient</strong> term accounts for the scattered light present in the scene. This term is independent from any light source and is the same for all fragments.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Diffuse</h1>
                
            
            
                
<p>The <strong>diffuse</strong> term corresponds to diffuse reflections. A Lambertian model is typically used for this component.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Specular</h1>
                
            
            
                
<p>The <strong>specular</strong> term provides mirror-like reflections. Conceptually, the specular reflection reaches its maximum when we look at the object at an angle that is equal to the reflected light-direction vector.</p>
<p>The specular term is modeled by the dot product of two vectors, namely, the eye vector and the reflected light-direction vector. The eye vector originates in the fragment and terminates in the view position (camera). The reflected light-direction vector is obtained by reflecting the light-direction vector upon the surface normal vector. When this dot product equals <kbd>1</kbd> (by working with normalized vectors), our camera will capture the maximum specular reflection.</p>
<p>The dot product is then exponentiated by a number that represents the shininess of the surface. After that, the result is multiplied by the light and material specular components:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/fb521d70-d2d3-4d53-a76d-7be0d17b45c6.png" style="width:39.92em;height:19.58em;"/></p>
<p>Where:</p>
<pre class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/43935c46-91eb-4335-a9bd-bc37bc608c1a.png" style="width:13.42em;height:1.83em;"/></pre>
<p><img class="fm-editor-equation" src="img/48dc2c47-c0a4-483b-bb6c-191e544bf077.png" style="width:1.67em;height:1.50em;"/> is the final specular color, <kbd><img class="fm-editor-equation" src="img/fbc93053-92d3-45dc-91f5-8cc0d34ceeda.png" style="width:1.58em;height:1.58em;"/></kbd> is the light specular color, <kbd><img class="fm-editor-equation" src="img/186f032b-3881-4e8b-adeb-a3950ba8b607.png" style="width:1.58em;height:1.58em;"/></kbd> is the material specular color, and <em><kbd>n</kbd> </em>is the shininess factor.</p>
<p>That being said, we'd derive the final specular color with the following:</p>
<pre class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/81b56a8f-719f-4b2a-a344-f15f0fe7a635.png" style="width:13.33em;height:1.83em;"/></pre>
<p>If <kbd><em>R</em></kbd> and <kbd><em>E</em></kbd> are normalized, then:</p>
<pre class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/deefec73-eded-4ea2-8b01-bd78a7c14155.png" style="width:9.25em;height:1.33em;"/><br/><img class="fm-editor-equation" src="img/320e0d99-9db2-4fe5-b020-172c55c681a1.png" style="width:11.17em;height:1.58em;"/></pre>
<p>It's important to note that the specular reflection reaches its maximum when <kbd><em>R</em></kbd> and <kbd><em>E</em></kbd> have the same direction.</p>
<p>Once we have the ambient, diffuse, and specular terms, we add them to find the final color of the fragment, which provides us with the Phong reflection model.</p>
<p>Now, it’s time to learn about the language that will allow us to implement the shading and lighting strategies inside the vertex and fragment shaders. This language is called ESSL.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">OpenGL ES Shading Language (ESSL)</h1>
                
            
            
                
<p>The OpenGL ES Shading Language (ESSL) is the language we'll use to write our shaders. Its syntax and semantics are very similar to C/C++. However, it has types and built-in functions that make it easier to manipulate vectors and matrices. In this section, we will cover the basics of ESSL so that we can start using it right away.</p>
<p>GLSL and ESSL<strong><br/>
<br/></strong> It’s quite common for developers to refer to the shading language used in WebGL as GLSL. However, it is technically ESSL. WebGl2 is built on the OpenGL ES 3.0 spec and therefore uses ESSL, which is a subset of GLSL (the shading language for OpenGL).<br/>
<br/>
This section summarizes the official GLSL ES specifications. You can find the complete reference at <a href="https://www.khronos.org/registry/OpenGL/specs/es/3.0/GLSL_ES_Specification_3.00.pdf">https://www.khronos.org/registry/OpenGL/specs/es/3.0/GLSL_ES_Specification_3.00.pdf</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Storage Qualifier</h1>
                
            
            
                
<p>Variable declarations may have a storage qualifier specified in front of the type:</p>
<ul>
<li><kbd>attribute</kbd>: Data pulled from buffers that serve as the link between a vertex shader and a WebGL application for per-vertex data. This storage qualifier is only legal inside the vertex shader.</li>
<li><kbd>uniform</kbd>: Value does not change across the object being processed. Uniforms form the link between a shader and a WebGL application. Uniforms are legal in both the vertex and fragment shaders. If a uniform is shared by the vertex and fragment shader, the respective declarations must match. Uniform values stay the same for all vertices of a single draw call.</li>
<li><kbd>varying</kbd>: This is the link between a vertex shader and a fragment shader for interpolated data. By definition, varyings must be shared by the vertex shader and fragment shader. The declaration of varyings needs to match between the vertex and fragment shaders.</li>
<li><kbd>const</kbd>: A compile-time constant, or a function parameter that is read-only. They can be used anywhere in the code of an ESSL program.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Types</h1>
                
            
            
                
<p>Here is a non-exhaustive list of the most common ESSL types:</p>
<ul>
<li><kbd>void</kbd>: For functions that do not return a value or for an empty parameter list</li>
<li><kbd>bool</kbd>: A conditional type, taking on values of true or false</li>
<li><kbd>int</kbd>: A signed integer</li>
<li><kbd>float</kbd>: A single floating-point scalar</li>
<li><kbd>vec2</kbd>: A two-component floating-point vector</li>
<li><kbd>vec3</kbd>: A three-component floating-point vector</li>
<li><kbd>vec4</kbd>: A four-component floating-point vector</li>
<li><kbd>bvec2</kbd>: A two-component Boolean vector</li>
<li><kbd>bvec3</kbd>: A three-component Boolean vector</li>
<li><kbd>bvec4</kbd>: A four-component Boolean vector</li>
<li><kbd>ivec2</kbd>: A two-component integer vector</li>
<li><kbd>ivec3</kbd>: A three-component integer vector</li>
<li><kbd>ivec4</kbd>: A four-component integer vector</li>
<li><kbd>mat2</kbd>: A 2×2 floating-point matrix</li>
<li><kbd>mat3</kbd>: A 3×3 floating-point matrix</li>
<li><kbd>mat4</kbd>: A 4×4 floating-point matrix</li>
<li><kbd>sampler2D</kbd>: A handle for accessing a 2D texture</li>
<li><kbd>sampler3D</kbd>:  A handle for accessing a 3D texture</li>
<li><kbd>samplerCube</kbd>: A handle for accessing a cube-mapped texture</li>
<li><kbd>struct</kbd>: Used to declare custom data structures based on standard types</li>
</ul>
<p>ESSL<br/> <strong><br/></strong> There are many other types and features that the OpenGL ES 3.0 shading language provides. Here is a useful guide that covers many of its core features: <a href="https://www.khronos.org/files/opengles3-quick-reference-card.pdf">https://www.khronos.org/files/opengles3-quick-reference-card.pdf</a>.<a href="https://www.khronos.org/files/opengles3-quick-reference-card.pdf"/></p>
<p>An input variable will have one of the qualifiers followed by one type. For example, we will declare our <kbd>uLightColor</kbd> variable as follows:</p>
<div><pre>uniform vec4 uLightColor;</pre></div>
<p>This means that the <kbd>uLightColor</kbd> variable is a <kbd>uniform</kbd> vector with four components.</p>
<p>The GLSL and ESSL Naming Convention<strong><br/>
<br/></strong> Convention dictates that we prefix shader variables with their type. This makes for clear and readable shader code. For example, for a given color uniform, you would name the variable <kbd>uLightColor</kbd>. For a position varying, <kbd>vNormal</kbd>. For a normal attribute, <kbd>aVertexNormal</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Vector Components</h1>
                
            
            
                
<p>We can refer to each one of the components of an ESSL vector by its index. For example, <kbd>uLightColor[3]</kbd> will refer to the fourth element of the vector (zero-based vectors). However, we can also refer to each component by a letter, as demonstrated in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%;border-color: #000000">
<tbody>
<tr>
<td class="CDPAlignLeft CDPAlign"><kbd>{ x, y, z, w }</kbd></td>
<td class="CDPAlignLeft CDPAlign">Useful when accessing vectors that represent points or vectors.</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign"><kbd>{ r, g, b, a }</kbd></td>
<td class="CDPAlignLeft CDPAlign">Useful when accessing vectors that represent colors.</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign">
<p class="mce-root"><kbd>{ s, t, p, q }</kbd></p>
</td>
<td class="CDPAlignLeft CDPAlign">Useful when accessing vectors that represent texture coordinates.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For example, if we want to set the <em>alpha channel</em> (fourth component) of our <kbd>uLightColor</kbd> variable to <kbd>1.0</kbd>, we can do so by writing in any of the following formats:</p>
<div><pre>uLightColor[3] = 1.0;<br/>uLightColor.w = 1.0;<br/>uLightColor.a = 1.0;<br/>uLightColor.q = 1.0;</pre></div>
<p>In all these of cases, we are referring to the same fourth component. However, given that <kbd>uLightColor</kbd> represents a color, it makes more sense to use the <kbd>r</kbd>, <kbd>g</kbd>, <kbd>b</kbd>, <kbd>a</kbd> notation.</p>
<p>It’s also possible to use the vector component notation to refer to subsets inside a vector. For example <em>(taken from GLSL ES specification)</em>:</p>
<div><pre>vec4 v4;<br/><br/>v4.rgba;  // is a vec4 and the same as just using v4<br/>v4.rgb;   // is a vec3<br/>v4.b;     // is a float<br/>v4.xy;    // is a vec2<br/>v4.xgba;  // is illegal - the component names do not come from the same set</pre></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Operators and Functions</h1>
                
            
            
                
<p>One of the major advantages of GLSL and ESSL are the powerful built-in mathematical operators. ESSL provides many useful operators and functions that simplify vector and matrix operations. According to the specifications, the arithmetic binary operators add (<kbd>+</kbd>), subtract (<kbd>-</kbd>), multiply (<kbd>*</kbd>), and divide (<kbd>/</kbd>) operate on integer and floating-point typed expressions, including vectors and matrices. The two operands must be the same type, or one can be a scalar float and the other a float vector or matrix, or one can be a scalar integer and the other an integer vector. Additionally, for multiply (<kbd>*</kbd>), one can be a vector and the other a matrix with the same dimensional size as the vector. These result in the same fundamental types (integer or float) as the expressions they operate on. If one operand is a scalar and the other is a vector or a matrix, the scalar is applied component-wise to the vector or the matrix, with the final result being of the same type as the vector or the matrix. It's important to note that dividing by zero does not cause an exception, but it does result in an unspecified value. Let's see a few examples of these operations:</p>
<ul>
<li><kbd>-x</kbd>: The negative of the <kbd>x</kbd> vector. It produces the same vector in the exact opposite direction.</li>
<li><kbd>x + y</kbd>: Sum of the <kbd>x</kbd> and <kbd>y</kbd> vectors. Both vectors need to have the same number of components.</li>
<li><kbd>x - y</kbd>: Subtraction of the <kbd>x</kbd> and <kbd>y</kbd> vectors. Both vectors need to have the same number of components.</li>
<li><kbd>x * y</kbd>: If <kbd>x</kbd> and <kbd>y</kbd> are both vectors, this operator yields a component-wise multiplication. Multiplication applied to two matrices returns a linear algebraic matrix multiplication, not a component-wise multiplication.</li>
<li><kbd>matrixCompMult(matX, matY)</kbd>: Component-wise multiplication of matrices. They need to have the same dimensions (<kbd>mat2</kbd>, <kbd>mat3</kbd>, or <kbd>mat4</kbd>).</li>
<li><kbd>x / y</kbd>: The division operator behaves similarly to the multiplication operator.</li>
<li><kbd>dot(x, y)</kbd>: Returns the dot product (scalar) of two vectors. They need to have the same dimensions.</li>
<li><kbd>cross(vecX, vecY)</kbd>: Returns the cross product (vector) of two vectors. They must both be <kbd>vec3</kbd>.</li>
<li><kbd>normalize(x)</kbd>: Returns a vector in the same direction but with a length of <kbd>1</kbd>.</li>
<li><kbd>reflect(t, n)</kbd>: Reflects the <kbd>t</kbd><em> </em>vector along the <kbd>n</kbd> vector.</li>
</ul>
<p>Shaders offer many more functions, including trigonometric and exponential functions. We will refer to them as needed in the development of different lighting models.</p>
<p>Let's see a quick example of the shader's ESSL code for a scene with the following properties:</p>
<ul>
<li><strong>Goraud shading</strong>: We will interpolate vertex colors to obtain fragment colors. Therefore, we need one <kbd>varying</kbd> to pass the vertex color information from the vertex shader to the fragment shader.</li>
<li><strong>Lambertian reflection model</strong>: We account for the diffuse interaction between one light source and our scene. This means that we will use uniforms to define the light properties that is, the material properties. We will follow <em>Lambert's Emission Law </em>to calculate the final color for every vertex.</li>
</ul>
<p>First, let's dissect what the attributes, uniforms, and varyings will be.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Vertex Attributes</h1>
                
            
            
                
<p>We will start by defining two attributes in the vertex shader. Every vertex will have the following code:</p>
<div><pre>in vec3 aVertexPosition;<br/>in vec3 aVertexNormal;</pre></div>
<div><strong>Attributes</strong><br/>
<br/>
Remember that attributes are only available to use inside the vertex shader.</div>
<p>If you're curious as to why <kbd>in</kbd> is used instead of the <kbd>attribute</kbd> qualifier, we will cover this shortly. Right after the <kbd>in</kbd> keyword, we find the type of the variable. In this case, it is <kbd>vec3</kbd>, as each vertex position is determined by three elements (<kbd>x</kbd>, <kbd>y</kbd>, <kbd>z</kbd>). Similarly, the normals are also determined by three elements (<kbd>x</kbd>, <kbd>y</kbd>, <kbd>z</kbd>). Please note that a position is a <em>point</em> in three-dimensional space that tells us where the vertex is, while a normal is a <em>vector</em> that gives us information about the orientation of the surface that passes along that vertex.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Uniforms</h1>
                
            
            
                
<p>Uniforms are available to both the vertex shader and the fragment shader. While attributes differ every time the vertex shader is invoked, uniforms are constant throughout a rendering cycle – that is, during the <kbd>drawArrays</kbd> or <kbd>drawElements</kbd> WebGL call.</p>
<p>Parallel Processing<br/>
<br/>
We process vertices in parallel; therefore, each copy/thread of the vertex shader processes a different vertex.</p>
<p>We can use uniforms to pass along information about lights (such as diffuse color and direction), and materials (diffuse color):</p>
<div><pre>uniform vec3 uLightDirection;  // incoming light source direction<br/>uniform vec4 uLightDiffuse;    // light diffuse component<br/>uniform vec4 uMaterialDiffuse; // material diffuse color</pre></div>
<p>Again, the <kbd>uniform</kbd> keyword tells us that these variables are uniforms, and the <kbd>vec3</kbd> and <kbd>vec4</kbd> ESSL types tell us that these variables have three or four components. For the colors, these components are the red, blue, green, and alpha channels (RGBA), and for the light direction, these components are the <kbd>x</kbd>, <kbd>y</kbd>, and <kbd>z</kbd> coordinates that define the vector in which the light source is directed in the scene.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Varyings</h1>
                
            
            
                
<p>As described earlier, varyings allow for the vertex shader to pass information to the fragment shader. For example, if we want to carry the vertex color from the vertex shader to the fragment shader, we would first update our vertex shader:</p>
<div><pre>#version 300 es<br/><br/>out vec4 vVertexColor;<br/> <br/>void main(void) {<br/>  vVertexColor = vec4(1.0, 1.0, 1.0, 1.0);<br/>}</pre></div>
<p>And we would reference that varying inside of our fragment shader as follows:</p>
<div><pre>in vec4 vVertexColor;</pre></div>
<p>Keep in the mind that the <em>Storage Qualifier</em>, the declaration of varyings, needs to match between the vertex and fragment shaders.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The in and out variables</h1>
                
            
            
                
<p>These keywords describe the direction of the <em>input</em> and <em>output</em>. As seen with the <em>attribute</em> and <em>varying</em> declarations, when we use <kbd>in</kbd>, that variable is supplied to the shader. When we use <kbd>out</kbd>, the shader exposes that variable. Let's see how these keywords are used in earlier versions of WebGL within the vertex and fragment shader.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Changing attribute to in</h1>
                
            
            
                
<p>In WebGL 1 with <em>ESSL 100</em>, you might have this:</p>
<div><pre>attribute vec4 aVertexPosition;<br/>attribute vec3 aVertexNormal;</pre></div>
<p>In WebGL 2 with <em>ESSL 300</em>, this becomes the following:</p>
<div><pre>in vec4 aVertexPosition;<br/>in vec3 aVertexNormal;</pre></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Changing varying to in / out</h1>
                
            
            
                
<p>WebGL 1 with <em>ESSL 100</em>, you declare a varying in both the vertex and fragment shaders, like so:</p>
<div><pre>varying vec4 vVertexPosition;<br/>varying vec3 vVertexNormal;</pre></div>
<p>In WebGL 2 with <em>ESSL 300</em>, in the vertex shader, the varyings become this:</p>
<div><pre>out vec4 vVertexPosition;<br/>out vec3 vVertexNormal;</pre></div>
<p>And in the fragment shader, they become this:</p>
<div><pre>in vec4 vVertexPosition;<br/>in vec3 vVertexNormal;</pre></div>
<p>Now, let's plug the attributes, uniforms, and varyings into the code and see what the vertex shader and fragment shader look like.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Vertex Shader</h1>
                
            
            
                
<p>Let's cover a sample vertex shader:</p>
<div><pre>#version 300 es<br/><br/>uniform mat4 uModelViewMatrix;<br/>uniform mat4 uProjectionMatrix;<br/>uniform mat4 uNormalMatrix;<br/>uniform vec3 uLightDirection;<br/>uniform vec3 uLightDiffuse;<br/>uniform vec3 uMaterialDiffuse;<br/><br/>in vec3 aVertexPosition;<br/>in vec3 aVertexNormal;<br/><br/>out vec4 vVertexColor;<br/><br/>void main(void) {<br/>  vec3 normal = normalize(vec3(uNormalMatrix * vec4(aVertexNormal, 1.0)));<br/>  <br/>  vec3 lightDirection = normalize(uLightDirection);<br/><br/>  float LambertianTerm = dot(normal, -lightDirection);<br/><br/>  vVertexColor = vec4(uMaterialDiffuse * uLightDiffuse * LambertianTerm, <br/>   1.0);<br/><br/>  gl_Position = uProjectionMatrix * uModelViewMatrix * <br/>   vec4(aVertexPosition, 1.0);<br/>}</pre>
<p>On first inspection, we can identify the attributes, uniforms, and varyings that we will use, along with some matrices that we will discuss later. We can also see that the vertex shader has a <kbd>main</kbd> function that does not accept parameters and instead returns <kbd>void</kbd>. Inside, we can see some ESSL functions, such as <kbd>normalize</kbd> and <kbd>dot</kbd>, along with some arithmetical operators.</p>
</div>
<div><kbd>#version 300 es</kbd><br/>
<br/>
This string must be the very first line of your shader. No comments or blank lines are allowed before it! <kbd>#version 300 es</kbd> tells WebGL that you want to use WebGL 2's shader language (GLSL ES 3.00). If that isn’t written as the first line, the shader language defaults to WebGL 1.0's GLSL ES 1.00, which has fewer features.</div>
<p>There are three uniforms that we haven’t discussed yet:</p>
<div><pre>uniform mat4 uModelViewMatrix;<br/>uniform mat4 uProjectionMatrix;<br/>uniform mat4 uNormalMatrix;</pre></div>
<p>We can see that these three uniforms are <kbd>4x4</kbd> matrices. These matrices are required in the vertex shader to calculate the location for vertices and normals whenever we move the camera. There are a couple of operations here that involve using these matrices:</p>
<div><pre>vec3 normal = normalize(vec3(uNormalMatrix * vec4(aVertexNormal, 1.0)));<br/></pre></div>
<p>The previous line of code calculates the <em>transformed normal</em>:</p>
<div><pre>gl_Position = uProjectionMatrix * uModelViewMatrix * vec4(aVertexPosition, 1.0);</pre></div>
<p>This line calculates the <em>transformed vertex position</em>. <kbd>gl_Position</kbd> is a special output variable that stores the transformed vertex position.</p>
<p>We will come back to these operations in <a href="62d4de32-0b5b-4339-8fcc-80f739e80ec2.xhtml">Chapter 4</a>, <em>Cameras</em>. For now, we should acknowledge that these uniforms and operations deal with camera and world transformations (rotation, scale, and translation).</p>
<p>Returning to the main function’s code, we can clearly see that the Lambertian reflection model is being implemented. The <kbd>dot</kbd> product of the normalized normal and light direction vector is obtained and then multiplied by the light and material diffuse components. Finally, this result is passed into the <kbd>vVertexColor</kbd> varying to be used in the fragment shader, as follows:</p>
<pre>vVertexColor = vec4(uMaterialDiffuse * uLightDiffuse * LambertianTerm, 1.0);</pre>
<p>Also, as we are calculating the color in the vertex shader and then automatically interpolating it for the fragments of every triangle, we are using the Goraud interpolation method.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Fragment Shader</h1>
                
            
            
                
<p>The fragment shader is very simple. The first three lines define the precision of the shader. This is mandatory according to the ESSL specification. Similarly, for the vertex shader, we define our input; in this case, just one varying variable, and then we have the main function:</p>
<div><pre>#version 300 es<br/><br/>// Fragment shaders don't have a default precision so we need<br/>// to pick one. mediump is a good default. It means "medium precision"<br/>precision mediump float;<br/><br/>in vec4 vVertexColor;<br/>// we need to declare an output for the fragment shader<br/>out vec4 fragColor;<br/> <br/>void main() {<br/>  fragColor = vVertexColor;<br/>}</pre></div>
<p>We just need to assign the <kbd>vVertexColor</kbd> varying to the <kbd>fragColor</kbd> output variable.</p>
<p>No More <kbd>gl_FragColor</kbd><br/>
<br/>
In WebGL 1, your fragment shader would set the <kbd>gl_FragColor</kbd> special variable to compute the output of the shader: <kbd>gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0);</kbd>.<br/>
<br/>
In WebGL 2, <em>ESSL 300</em> forces you to declare your own output variable and then set it. You can pick any name you want, but names cannot begin with <kbd>gl_</kbd>.</p>
<div><p>Remember that the value of the <kbd>vVertexColor</kbd> varying will be different from the one calculated in the vertex shader since WebGL will interpolate it by taking the corresponding calculated colors for the vertices surrounding the correspondent fragment (pixel).</p>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Writing ESSL Programs</h1>
                
            
            
                
<p>Now, let's take a moment to step back and look at the big picture. ESSL allows us to implement a lighting strategy, provided that we define a shading method and a light reflection model. In this section, we will take a sphere as the object that we want to illuminate, and we will see how the selection of a lighting strategy changes the scene:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/804a04e3-c218-43ac-84ed-132bf0d0f2d5.png" style="width:41.75em;height:24.08em;"/></p>
<p>We will see two scenarios for Goraud interpolation: one with Lambertian and one with Phong reflections. We will only see one case for Phong interpolation; under Phong shading, the Lambertian reflection model is no different from a Phong reflection model where the ambient and specular components are set to <kbd>0</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Goraud Shading with Lambertian Reflections</h1>
                
            
            
                
<p>The Lambertian reflection model only considers the interaction of diffuse material and diffuse light properties. In short, we assign the final color as follows:</p>
<div><pre>aVertexColor = Id;</pre></div>
<p>The following value is seen:</p>
<div><pre>Id = lightDiffuseProperty * materialDiffuseProperty * lambertCoefficient;</pre></div>
<p>Under Goraud shading, the <strong>Lambert coefficient</strong> is obtained by calculating the dot product of the vertex normal and the inverse of the light-direction vector. Both vectors are normalized before finding the dot product.</p>
<p>Let's take a look at the vertex shader and the fragment shader from the provided example, <kbd>ch03_01_goraud_lambert.html</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d615d810-3bf0-4d37-b061-56a63068af19.png"/></p>
<p>Here's the vertex shader:</p>
<div><pre>#version 300 es<br/>precision mediump float;<br/><br/>uniform mat4 uModelViewMatrix;<br/>uniform mat4 uProjectionMatrix;<br/>uniform mat4 uNormalMatrix;<br/>uniform vec3 uLightDirection;<br/>uniform vec3 uLightDiffuse;<br/>uniform vec3 uMaterialDiffuse;<br/><br/>in vec3 aVertexPosition;<br/>in vec3 aVertexNormal;<br/><br/>out vec4 vVertexColor;<br/><br/>void main(void) {<br/>  // Calculate the normal vector<br/>  vec3 N = normalize(vec3(uNormalMatrix * vec4(aVertexNormal, 1.0)));<br/>  // Normalized light direction<br/>  vec3 L = normalize(uLightDirection);<br/>  // Dot product of the normal product and negative light direction vector<br/>  float lambertTerm = dot(N, -L);<br/>  // Calculating the diffuse color based on the Lambertian reflection model<br/>  vec3 Id = uMaterialDiffuse * uLightDiffuse * lambertTerm;<br/>  vVertexColor = vec4(Id, 1.0);<br/>  // Setting the vertex position<br/>  gl_Position = uProjectionMatrix * uModelViewMatrix * vec4(aVertexPosition, 1.0);<br/>}</pre></div>
<p>Here's the fragment shader:</p>
<div><pre>#version 300 es<br/>precision mediump float;<br/><br/>// Expect the interpolated value fro, the vertex shader<br/>in vec4 vVertexColor;<br/><br/>// Return the final color as fragColor<br/>out vec4 fragColor;<br/><br/>void main(void)  {<br/>  // Simply set the value passed in from the vertex shader<br/>  fragColor = vVertexColor;<br/>}</pre></div>
<p>We can see that the final vertex color that we processed in the vertex shader is carried into a varying variable to the fragment shader. Remember that the value that arrives to the fragment shader is <em>not</em> the original value that we calculated in the vertex shader. The fragment shader interpolates the <kbd>vVertexColor</kbd> variable to generate a final color for the respective fragment. This interpolation takes into account the vertices that enclose the current fragment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Updating Uniforms in Real Time</h1>
                
            
            
                
<p>Let's cover an example of how we'd update shader uniforms interactively:</p>
<ol>
<li>Open the <kbd>ch03_01_goraud_lambert.html</kbd> file in your browser:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/904b2bee-303a-4f77-8954-04c4a28e4002.png"/></p>
<ol start="2">
<li>Notice that in this example, the controls<em> </em>widget is at the top right of the page. If you're curious about how it works, you can check the <kbd>initControls</kbd> function inside of the example code.</li>
</ol>
<p>The Settings Widget<strong><br/>
<br/></strong> The settings widget was created using <strong>DatGui</strong>, an open source library. While we won't cover the intuitive DatGui API, it may be useful to read the documentation and the code in the provided examples to see how it works. For more information, you can check out <a href="https://github.com/dataarts/dat.gui">https://github.com/dataarts/dat.gui.</a></p>
<ol start="3">
<li><strong>Translate X, Y, Z</strong>: These<strong> </strong>control the direction of the light. By changing these sliders, you will modify the <kbd>uLightDirection</kbd> uniform:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/4fb6fe04-13c9-46c9-bc4e-5d9b6f6582b3.png"/></p>
<ol start="4">
<li><strong>Sphere Color</strong>: This changes the <kbd>uMaterialDiffuse</kbd> uniform, which represents the diffuse color of the sphere. Here, you use the color selection widget, which allows you to try different colors. <kbd>onChange</kbd> of <kbd>Sphere Color</kbd> in the <kbd>initControls</kbd> function receives the updates from the widget and updates the <kbd>uMaterialDiffuse</kbd> uniform.</li>
<li><strong>Light Diffuse Color</strong>: This changes the <kbd>uLightDiffuse</kbd> uniform, which represents the diffuse color of the light source. There is no reason why the light color must be white. We achieve this by assigning the slider value to the RGB components of <kbd>uLightDiffuse</kbd> while we keep the alpha channel set to <kbd>1.0</kbd>. We do this inside the <kbd>onChange</kbd> function under the lights settings, which receives the slider updates.</li>
<li>Try different settings for the light source position, the diffuse material, and light properties.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We’ve seen an example of a simple scene illuminated using Goraud interpolation and a Lambertian reflection model. We have also seen the immediate effects of changing uniform values for the Lambertian lighting model.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Have a Go: Moving Light</h1>
                
            
            
                
<p>We mentioned before that we use matrices to move the camera around the scene. We can also use matrices to move lights. To do this, perform the following steps:</p>
<ol>
<li>Open <kbd>ch03_02_moving-light.html</kbd> in your editor. The vertex shader is very similar to the previous diffuse model example. However, there is one extra line:</li>
</ol>
<div><pre style="padding-left: 60px">vec3 light = vec3(uModelViewMatrix * vec4(uLightDirection, 0.0));</pre></div>
<ol start="2">
<li>Here, we are transforming the <kbd>uLightDirection</kbd> vector and assigning it to the <kbd>light</kbd> variable. Notice that the <kbd>uLightDirection</kbd> uniform is a vector with three components (<kbd>vec3</kbd>) and that the <kbd>uModelViewMatrix</kbd> is a 4x4 matrix. In order to complete the multiplication, we need to transform this uniform into a four-component vector (<kbd>vec4</kbd>). We achieve this with the following construct:</li>
</ol>
<div><pre style="padding-left: 60px">vec4(uLightDirection, 0.0);</pre></div>
<ol start="3">
<li>The <kbd>uModelViewMatrix</kbd> matrix contains the <em>Model-View transformation matrix</em>. We will see how all this works in <a href="62d4de32-0b5b-4339-8fcc-80f739e80ec2.xhtml">Chapter 4</a>, <em>Cameras</em>. For now, suffice to say that this matrix allows us to update vertices’ positions, and in this example, the light's position as well.</li>
<li>Take another look at the vertex shader. In this example, we are rotating the sphere and the light. Every time the <kbd>draw</kbd> function is invoked, we rotate the <kbd>modelViewMatrix</kbd> matrix a little bit on the y-axis:</li>
</ol>
<div><pre style="padding-left: 60px">mat4.rotate(modelViewMatrix, modelViewMatrix, angle * Math.PI / 180, [0, 1, 0]);</pre></div>
<ol start="5">
<li>If you examine the code more closely, you will notice that the <kbd>modelViewMatrix</kbd> matrix is mapped to the <kbd>uModelViewMatrix</kbd> uniform:</li>
</ol>
<pre style="padding-left: 60px">gl.uniformMatrix4fv(program.uModelViewMatrix, false, modelViewMatrix);</pre>
<ol start="6">
<li>Run the example in your browser. You will see a sphere and a light source rotating on the y-axis:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/ed0b6249-3e56-494e-9c35-c63ac7a41a89.png" style="width:41.25em;height:9.92em;"/></p>
<ol start="7">
<li>Look for the <kbd>initLights</kbd> function and change the light orientation so that the light is pointing in the negative z-axis direction:</li>
</ol>
<pre style="padding-left: 60px">gl.uniform3f(program.uLightDirection, 0, -1, -1);</pre>
<ol start="8">
<li>Save the file and run it again. What happened? Change the light direction uniform so that it points to <kbd>[-1, 0, 0]</kbd>. Save the file and run it again on your browser. What happened? You should see that changing these values manipulates the light's orientation.</li>
<li>Set the light back to the 45-degree angle<em> </em>by changing the <kbd>uLightDirection</kbd> uniform so that it returns to its initial value:</li>
</ol>
<pre style="padding-left: 60px">gl.uniform3f(program.uLightDirection, 0, -1, -1);</pre>
<ol start="10">
<li>Go to <kbd>draw</kbd> and find the following line:</li>
</ol>
<div><pre style="padding-left: 60px">mat4.rotate(modelViewMatrix, modelViewMatrix, angle * Math.PI / 180, [0, 1, 0]);</pre></div>
<ol start="11">
<li>Change it to this:</li>
</ol>
<pre style="padding-left: 60px">mat4.rotate(modelViewMatrix, modelViewMatrix, angle * Math.PI / 180, [1, 0, 0]);<br/></pre>
<ol start="12">
<li>Save the file and launch it again in your browser. What happens? You should notice that the light moves on a different axis.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>As you can see, the vector that is passed as the third argument to <kbd>mat4.rotate</kbd> determines the axis of the rotation. The first component corresponds to the x-axis, the second to the y-axis, and the third to the z-axis.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Goraud Shading with Phong Reflections</h1>
                
            
            
                
<p>Different from the Lambertian reflection model, the Phong reflection model considers three properties: the ambient, diffuse, and specular, and ultimately yields a more realistic reflection. Following the same analogy that we used in the previous section, consider the following example:</p>
<pre>finalVertexColor = Ia + Id + Is;</pre>
<p>Where:</p>
<div><pre>Ia = lightAmbient * materialAmbient;<br/>Id = lightDiffuse * materialDiffuse * lambertCoefficient;<br/>Is = lightSpecular * materialSpecular * specularCoefficient;</pre>
<p>Notice that:</p>
</div>
<ul>
<li>As we use Goraud interpolation, we still use vertex normals to calculate the diffuse term. This will change when using Phong interpolation, where we will use fragment normals.</li>
<li>Both light and material have three properties: the ambient, diffuse, and specular<em> </em>colors.</li>
<li>On these equations, we can see that <kbd>Ia</kbd>, <kbd>Id</kbd>, and <kbd>Is</kbd> receive contributions from their respective light and material properties.</li>
</ul>
<p>Based on our knowledge of the Phong reflection model, let's see how to calculate the specular coefficient in ESSL:</p>
<div><pre>float specular = pow(max(dot(lightReflection, eyeVector), 0.0), shininess);</pre></div>
<p>Where:</p>
<ul>
<li><kbd>eyeVector</kbd> is the view vector or camera vector</li>
<li><kbd>lightReflection</kbd> is the reflected light vector</li>
<li><kbd>shininess</kbd> is the specular exponential factor or shininess</li>
<li><kbd>lightReflection</kbd> is calculated as <kbd>lightReflection = reflect(lightDirection, normal);</kbd></li>
<li><kbd>normal</kbd> is the vertex normal, and <kbd>lightDirection</kbd> is the light direction that we have been using to calculate the Lambert coefficient</li>
</ul>
<p>Let's take a look at the ESSL implementation for the vertex and fragment shaders. Here's the vertex shader:</p>
<div><pre>#version 300 es<br/>precision mediump float;<br/><br/>uniform mat4 uModelViewMatrix;<br/>uniform mat4 uProjectionMatrix;<br/>uniform mat4 uNormalMatrix;<br/>uniform vec3 uLightDirection;<br/>uniform vec4 uLightAmbient;<br/>uniform vec4 uLightDiffuse;<br/>uniform vec4 uMaterialDiffuse;<br/><br/>in vec3 aVertexPosition;<br/>in vec3 aVertexNormal;<br/><br/>out vec4 vVertexColor;<br/><br/>void main(void) {<br/>  vec3 N = vec3(uNormalMatrix * vec4(aVertexNormal, 1.0));<br/>  vec3 light = vec3(uModelViewMatrix * vec4(uLightDirection, 0.0));<br/>  vec3 L = normalize(light);<br/>  float lambertTerm = dot(N,-L);<br/>  vec4 Ia = uMaterialDiffuse * uLightAmbient;<br/>  vec4 Id =  uMaterialDiffuse * uLightDiffuse * lambertTerm;<br/>  vVertexColor = vec4(vec3(Ia + Id), 1.0);<br/>  gl_Position = uProjectionMatrix * uModelViewMatrix * vec4(aVertexPosition, 1.0);<br/>}</pre></div>
<p>We can obtain negative dot products for the Lambert term when the geometry of our objects is concave or when the object is in the way between the light source and our point of view. In either case, the negative of the light-direction vector and the normals will form an obtuse angle, producing a negative dot product, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/ad5df219-e71e-4fd6-8bfd-afe923d9f2d1.png" style="width:31.75em;height:26.25em;"/></p>
<p>For that reason, we are using the ESSL built-in clamp function to restrict the dot product to the positive range. If we obtain a negative dot product, the clamp function will set the lambert term to zero and the respective diffuse contribution will be discarded, generating the correct result.</p>
<p>Given that we are still using Goraud interpolation, the fragment shader is the same as before:</p>
<div><pre>#version 300 es<br/>precision mediump float;<br/><br/>in vec4 vVertexColor;<br/><br/>out vec4 fragColor;<br/><br/>void main(void)  {<br/>  fragColor = vVertexColor;<br/>}</pre></div>
<p>In the next section, we will explore the scene to see what it looks like when we have negative Lambert coefficients that have been clamped to the <kbd>[0,1]</kbd> range.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Goraud Shading</h1>
                
            
            
                
<p>Let's cover an example where we implement lighting with Goraud shading:</p>
<ol>
<li>Open the <kbd>ch03_03_goraud_phong.html</kbd> file in your browser. You will see something similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/c2a15f52-eefd-4b83-bc06-f9a04d57fd49.png"/></p>
<ol start="2">
<li>The interface looks a little bit more elaborate than the diffuse lighting example. Let's stop here for a moment to explain these widgets:
<ul>
<li>Light Color (light diffuse term): As mentioned at the beginning of this chapter, we can have an example where our light is not white. We have included a color selector widget here for the light color so that you can experiment with different combinations.</li>
<li><strong>Light Ambient Term</strong>: The light ambient property. In this example, this is a gray value: <kbd>r = g = b</kbd>.</li>
<li>Light Specular Term: The light specular property. This is a gray value: <kbd>r = g = b</kbd>.</li>
<li>Translate X,Y,Z: The coordinates that define the light's orientation.</li>
<li>Sphere Color (material diffuse term): The material diffuse property. We have included a color selector so that you can try different combinations for the <kbd>r</kbd>, <kbd>g</kbd>, and <kbd>b</kbd> channels.</li>
<li>Material Ambient Term: The material ambient property. We have included it just for the sake of it. But as you might have noticed in the diffuse example, this vector is not always used.</li>
<li>Material Specular Term: The material specular property. This is a gray value : r = g = b.</li>
<li>Shininess: The specular exponential factor for the Goraud model.</li>
<li>Background Color (<kbd>gl.clearColor</kbd>): This widget simply allows us to change the background color. </li>
</ul>
</li>
<li>The specular reflection in the Phong reflection model depends on the shininess, the specular property of the material, and the specular property of the light. When the specular property of the material is close to <kbd>0</kbd>, the material <em>loses </em>its specular property. Check this behavior with the widget provided:
<ul>
<li>What happens when the specularity of the material is low and the shininess is high?</li>
<li>What happens when the specularity of the material is high and the shininess is low?</li>
<li>Using the widgets, try different combinations for the light and material properties.</li>
</ul>
</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<ul>
<li>We saw how the different parameters of the Phong lighting model interact with each other.</li>
<li>We modified the light orientation, the properties of the light, and the material to observe different behaviors of the Phong lighting model.</li>
<li>Unlike the Lambertian reflection model, the Goraud lighting model has two extra terms: the ambient and specular components. We saw how these parameters affect the scene.</li>
</ul>
<p>Just like the Lambertian reflection model, the Phong reflection model obtains the vertex color in the vertex shader. This color is interpolated in the fragment shader to obtain the final pixel color. This is because, in both cases, we are using Goraud interpolation. Let's now move the heavy processing to the fragment shader and study how we implement the Phong interpolation method.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Phong Shading</h1>
                
            
            
                
<p>Unlike the Goraud interpolation, where we calculated the final color for each vertex, the Phong interpolation calculates the final color for every fragment. This means that the calculation of the ambient, diffuse, and specular terms in the Phong model are performed in the fragment shader instead of the vertex shader. As you can imagine, this is computationally more intensive than performing a simple interpolation like in the two previous scenarios where we were using Goraud interpolation. However, we obtain a scene that seems more realistic.</p>
<p>After this translation, you may be wondering what is left for the vertex shader to do. Well, in this case, we will create varyings that will allow us to do all of the calculations in the fragment shader. For example, the vertex normals are a great fit.</p>
<p>Whereas before we had a normal per vertex, now we need to generate a normal for every pixel so that we can calculate the Lambert coefficient for each fragment. We do so by interpolating the normals that we pass to the fragment shader. Nevertheless, the code is very simple. All we need to know is how to create a varying that stores the normal for the vertex we are processing in the vertex shader and to obtain the interpolated value in the fragment shader (courtesy of ESSL). That's all! Conceptually, this is represented in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/4b90323c-0e88-4abf-b4c1-268eb87b2376.png" style="width:37.67em;height:16.50em;"/></p>
<p>Now, let's take a look at the vertex shader under Phong shading:</p>
<div><pre>#version 300 es<br/>precision mediump float;<br/><br/>uniform mat4 uModelViewMatrix;<br/>uniform mat4 uProjectionMatrix;<br/>uniform mat4 uNormalMatrix;<br/><br/>in vec3 aVertexPosition;<br/>in vec3 aVertexNormal;<br/><br/>out vec3 vNormal;<br/>out vec3 vEyeVector;<br/><br/>void main(void) {<br/>  vec4 vertex = uModelViewMatrix * vec4(aVertexPosition, 1.0);<br/>  vNormal = vec3(uNormalMatrix * vec4(aVertexNormal, 1.0));<br/>  vEyeVector = -vec3(vertex.xyz);<br/>  gl_Position = uProjectionMatrix * uModelViewMatrix * vec4(aVertexPosition, 1.0);<br/>}</pre></div>
<p>Unlike our examples with the Goraud interpolation, the vertex shader looks really simple. There is no final color calculation and we are using two varyings to pass information to the fragment shader. The fragment shader will now look like the following:</p>
<div><pre>#version 300 es<br/>precision mediump float;<br/><br/>uniform float uShininess;<br/>uniform vec3 uLightDirection;<br/>uniform vec4 uLightAmbient;<br/>uniform vec4 uLightDiffuse;<br/>uniform vec4 uLightSpecular;<br/>uniform vec4 uMaterialAmbient;<br/>uniform vec4 uMaterialDiffuse;<br/>uniform vec4 uMaterialSpecular;<br/><br/>in vec3 vNormal;<br/>in vec3 vEyeVector;<br/><br/>out vec4 fragColor;<br/><br/>void main(void) {<br/>  vec3 L = normalize(uLightDirection);<br/>  vec3 N = normalize(vNormal);<br/>  float lambertTerm = dot(N, -L);<br/>  vec4 Ia = uLightAmbient * uMaterialAmbient;<br/>  vec4 Id = vec4(0.0, 0.0, 0.0, 1.0);<br/>  vec4 Is = vec4(0.0, 0.0, 0.0, 1.0);<br/><br/>  if (lambertTerm &gt; 0.0) {<br/>    Id = uLightDiffuse * uMaterialDiffuse * lambertTerm;<br/>    vec3 E = normalize(vEyeVector);<br/>    vec3 R = reflect(L, N);<br/>    float specular = pow( max(dot(R, E), 0.0), uShininess);<br/>    Is = uLightSpecular * uMaterialSpecular * specular;<br/>  }<br/><br/>  fragColor = vec4(vec3(Ia + Id + Is), 1.0);<br/>}</pre></div>
<p>When we pass vectors as varyings, it is possible that they denormalize in the interpolation step. Therefore, you may have noticed that both <kbd>vNormal</kbd> and <kbd>vEyeVector</kbd> are normalized again in the fragment shader.</p>
<p>As we mentioned before, under Phong lighting, the Lambertian reflection model can be seen as a Phong reflection model where the ambient and specular components are set to <kbd>0</kbd>. Therefore, we will only cover the general case in the next section where we will see what the sphere scene looks like when using Phong shading and Phong lighting combined.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Phong Shading with Phong Lighting</h1>
                
            
            
                
<p>Let's cover an example of implementing lighting using Phong shading:</p>
<ol>
<li>Open the <kbd>ch03_04_sphere_Phong.html</kbd> file in your browser. The page will look similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/55730d4f-55ca-408e-9829-021b13c0d1e8.png" style="width:41.50em;height:23.00em;"/></p>
<ol start="2">
<li>The interface is very similar to the Goraud example's interface. As previously described, it is quite evident how the Phong shading combined with Phong lighting delivers a more realistic scene. Experiment with the controls widget to see the immediate result of this new lighting model.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We have seen Phong shading and Phong lighting in action. We explored the source code for the vertex and fragment shaders. We also modified the different parameters of the model and observed the immediate effect of the changes on the scene.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Back to WebGL</h1>
                
            
            
                
<p>It’s time to go back to our JavaScript code, but we now need to consider how to close the gap between our JavaScript code and our ESSL code. First, we need to take a look at how we create a <strong>program</strong> using our WebGL context. Please remember that we refer to both the vertex shader and fragment shader as the program. Second, we need to know how to initialize attributes and uniforms.</p>
<p>Let's take a look at the structure of the web apps we have developed so far:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/6a485ddc-b5d6-43af-a00a-db620262e0a0.png" style="width:36.33em;height:24.08em;"/></p>
<p>Each application has a vertex shader and a fragment shader embedded in the web page. In addition, there is a script section where we write all of our WebGL code. Finally, we have the HTML code that defines the page components, such as titles and the location of the widgets and the <kbd>canvas</kbd>.</p>
<p>In the JavaScript code, we are calling the <kbd>init</kbd> function on the <kbd>onload</kbd> event of the web page. This is the entry point for our application. The first thing that <kbd>init</kbd> does is obtain a WebGL context for the <kbd>canvas</kbd> within <kbd>initProgram</kbd>, and then calls a series of functions that initialize the program, the WebGL buffers, and the lights. Finally, it gets into a render loop where every time that the loop goes off, the <kbd>draw</kbd> function is invoked.</p>
<p class="mce-root">In this section, we will take a closer look at the <kbd>initProgram</kbd> and <kbd>initLights</kbd> functions. <kbd>initProgram</kbd> allows us to create and compile an ESSL program while <kbd>initLights</kbd> allows us to initialize and pass values to the uniforms defined in the programs. It is inside <kbd>initLights</kbd> where we will define the light's position, direction, and color components (ambient, diffuse, and specular) as well as default values for material properties.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a Program</h1>
                
            
            
                
<p>To start, open up <kbd>ch03_05_wall.html</kbd> in an editor. Let's take a step-by-step look at <kbd>initProgram</kbd>:</p>
<div><pre>function initProgram() {<br/>  const canvas = utils.getCanvas('webgl-canvas');<br/>  utils.autoResizeCanvas(canvas);<br/><br/>  gl = utils.getGLContext(canvas);<br/>  gl.clearColor(0.9, 0.9, 0.9, 1);<br/>  gl.clearDepth(100);<br/>  gl.enable(gl.DEPTH_TEST);<br/>  gl.depthFunc(gl.LEQUAL);<br/><br/>  const vertexShader = utils.getShader(gl, 'vertex-shader');<br/>  const fragmentShader = utils.getShader(gl, 'fragment-shader');<br/><br/>  program = gl.createProgram();<br/>  gl.attachShader(program, vertexShader);<br/>  gl.attachShader(program, fragmentShader);<br/>  gl.linkProgram(program);<br/><br/>  if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {<br/>    console.error('Could not initialize shaders');<br/>  }<br/><br/>  gl.useProgram(program);<br/><br/>  program.aVertexPosition = gl.getAttribLocation(program, <br/>   'aVertexPosition');<br/>  program.aVertexNormal = gl.getAttribLocation(program, 'aVertexNormal');<br/><br/>  program.uProjectionMatrix = gl.getUniformLocation(program, <br/>   'uProjectionMatrix');<br/>  program.uModelViewMatrix = gl.getUniformLocation(program, <br/>   'uModelViewMatrix');<br/>  program.uNormalMatrix = gl.getUniformLocation(program, <br/>   'uNormalMatrix');<br/>  program.uLightDirection = gl.getUniformLocation(program, <br/>   'uLightDirection');<br/>  program.uLightAmbient = gl.getUniformLocation(program, 'uLightAmbient');<br/>  program.uLightDiffuse = gl.getUniformLocation(program, 'uLightDiffuse');<br/>  program.uMaterialDiffuse = gl.getUniformLocation(program, <br/>   'uMaterialDiffuse');<br/>}</pre></div>
<p>First, we retrieve a WebGL context, as we've seen in previous chapters. Then, we use the <kbd>utils.getShader</kbd> utility function to retrieve the contents of the vertex shader and the fragment shader:</p>
<div><pre>const canvas = utils.getCanvas('webgl-canvas');<br/>utils.autoResizeCanvas(canvas);<br/><br/>gl = utils.getGLContext(canvas);<br/>gl.clearColor(0.9, 0.9, 0.9, 1);<br/>gl.clearDepth(100);<br/>gl.enable(gl.DEPTH_TEST);<br/>gl.depthFunc(gl.LEQUAL);<br/><br/>const vertexShader = utils.getShader(gl, 'vertex-shader');<br/> const fragmentShader = utils.getShader(gl, 'fragment-shader');</pre></div>
<p>The program's creation occurs in the following lines:</p>
<div><pre>program = gl.createProgram();<br/>gl.attachShader(program, vertexShader);<br/>gl.attachShader(program, fragmentShader);<br/>gl.linkProgram(program);<br/><br/>if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {<br/>  alert('Could not initialize shaders');<br/>}<br/><br/>gl.useProgram(program);</pre></div>
<p>Here, we have used several functions provided by the WebGL context. These include the ones shown in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%;border-color: #000000">
<tbody>
<tr>
<td class="CDPAlignLeft CDPAlign" style="width: 41.6128%"><strong>WebGL Function</strong></td>
<td class="CDPAlignLeft CDPAlign" style="width: 51.3872%"><strong>Description</strong></td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign" style="width: 41.6128%"><kbd>createProgram()</kbd></td>
<td class="CDPAlignLeft CDPAlign" style="width: 51.3872%">Creates a new program (<em>program</em>).</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign" style="width: 41.6128%"><kbd>attachShader(program, shader)</kbd></td>
<td class="CDPAlignLeft CDPAlign" style="width: 51.3872%">Attaches a shader to the current program.</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign" style="width: 41.6128%"><kbd>linkProgram(program)</kbd></td>
<td class="CDPAlignLeft CDPAlign" style="width: 51.3872%">Creates executable versions of the vertex and fragment shaders that are passed to the GPU.</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign" style="width: 41.6128%"><kbd>getProgramParameter(program, parameter)</kbd></td>
<td class="CDPAlignLeft CDPAlign" style="width: 51.3872%">This is part of the WebGL state-machine query mechanism. It allows you to query the program parameters. We use this function to verify whether the program has been successfully linked.</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign" style="width: 41.6128%"><kbd>useProgram(program)</kbd></td>
<td class="CDPAlignLeft CDPAlign" style="width: 51.3872%">It will load the program onto the GPU if the program contains valid code (that is, it has been successfully linked).</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Finally, we create a <strong>mapping</strong> between JavaScript variables and the program attributes and uniforms:</p>
<div><pre>program.aVertexPosition = gl.getAttribLocation(program, 'aVertexPosition');<br/>program.aVertexNormal = gl.getAttribLocation(program, 'aVertexNormal');<br/><br/>program.uProjectionMatrix = gl.getUniformLocation(program, 'uProjectionMatrix');<br/>program.uModelViewMatrix = gl.getUniformLocation(program, 'uModelViewMatrix');<br/>program.uNormalMatrix = gl.getUniformLocation(program, 'uNormalMatrix');<br/>program.uLightDirection = gl.getUniformLocation(program, 'uLightDirection');<br/>program.uLightAmbient = gl.getUniformLocation(program, 'uLightAmbient');<br/>program.uLightDiffuse = gl.getUniformLocation(program, 'uLightDiffuse');<br/>program.uMaterialDiffuse = gl.getUniformLocation(program, 'uMaterialDiffuse');</pre></div>
<p>Instead of creating several JavaScript variables here (one per program <kbd>attribute</kbd> or <kbd>uniform</kbd>), we are attaching properties to the <kbd>program</kbd> object. This does not have anything to do with WebGL. It is just a convenience step to keep all of our JavaScript variables as part of the program object.</p>
<p>WebGL Programs<strong><br/>
<br/></strong> Since we are attaching many of the important variables to our WebGL program, you may be wondering why we don't attach it to our WebGL context rather than the program. In our example, we're using a single <strong>program</strong> because our example is small. As WebGL applications grow, you may find that you have several programs that you switch throughout your application with the <kbd>gl.useProgram</kbd> function.</p>
<p>All of this information pertains to <kbd>initProgram</kbd>. Here, we have used the following WebGL API functions:</p>
<table border="1" style="border-collapse: collapse;width: 100%;border-color: #000000">
<tbody>
<tr>
<td class="CDPAlignLeft CDPAlign"><strong>WebGL Function</strong></td>
<td class="CDPAlignLeft CDPAlign"><strong>Description</strong></td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign"><kbd>getAttribLocation(program, name)</kbd></td>
<td class="CDPAlignLeft CDPAlign">This function receives the current program object and a string that contains the name of the attribute that needs to be retrieved. This function then returns a <strong>reference</strong> to the respective <strong>attribute</strong>.</td>
</tr>
<tr>
<td class="CDPAlignLeft CDPAlign"><kbd>getUniformLocation(program, name)</kbd></td>
<td class="CDPAlignLeft CDPAlign">This function receives the current program object and a string that contains the name of the uniform that needs to be retrieved. This function then returns a <strong>reference</strong> to the respective <strong>uniform</strong>.</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Using this mapping, we can initialize the uniforms and attributes from our JavaScript code, as we will see in the next section.</p>
<p>Another addition to WebGL 2 is an increasingly optimized approach for getting item locations from the vertex shader. In our example, we use <kbd>getAttribLocation</kbd> and <kbd>getUniformLocation</kbd> for getting the locations of these items. If you inspect their return values, you'll see that they return <em>whole numbers</em>.</p>
<p>Whole numbers are simply the numbers 0, 1, 2, 3, 4, 5, ... (and so on).</p>
<p>Convention dictates that with large 3D applications, you can leverage tested design patterns and data structures to organize your code, which may include organizing shader resources in a predetermined or programmatic order.</p>
<p>One example would be to leverage the <strong>layout qualifier </strong>to look up resource locations. Here's a simplified example from <a href="d2019a49-9e84-448c-8799-e296187476d1.xhtml">Chapter 2</a>, <em>Rendering</em>, where we looked up and enabled both <kbd>aVertexPosition</kbd> and <kbd>aVertexColor</kbd> using <kbd>getAttribLocation</kbd>:</p>
<div><pre>const vertexPosition = gl.getAttribLocation(program, 'aVertexPosition');<br/>gl.enableVertexAttribArray(vertexPosition);<br/><br/>const colorLocation = gl.getAttribLocation(program, 'aVertexColor');<br/>gl.enableVertexAttribArray(colorLocation);</pre></div>
<p>And here is the associated vertex shader:</p>
<div><pre>#version 300 es<br/><br/>in vec4 aVertexPosition;<br/>in vec3 aVertexColor;<br/><br/>out vec3 vVertexColor;<br/><br/>void main() {<br/>  vVertexColor = aVertexColor;<br/>  gl_Position = aVertexPosition;<br/>}</pre></div>
<p>These would turn into the following:</p>
<div><pre>const vertexPosition = 0;<br/>gl.enableVertexAttribArray(vertexPosition);<br/><br/>const colorLocation = 1;<br/>gl.enableVertexAttribArray(colorLocation);</pre></div>
<p>And here is the updated vertex shader:</p>
<div><pre>#version 300 es<br/><br/><strong>layout (location=0) in vec4 aVertexPosition;<br/>layout (location=1) in vec3 aVertexColor</strong><strong>;</strong><br/><br/>out vec3 vVertexColor;<br/><br/>void main() {<br/>  vVertexColor = aVertexColor;<br/>  gl_Position = aVertexPosition;<br/>}</pre></div>
<p>As you can see, it's a subtle change where we define the locations using indices within the vertex shader and simply enable the items using those indices.</p>
<p>Performance Hits<strong><br/>
<br/></strong> Every time we need to look up or set shader values from the JavaScript context, it comes at a performance cost. Because of this, we should always be careful of how often we perform such operations.</p>
<p>Although the layout qualifier is optimal, we will continue leveraging the traditional variable and definition lookup throughout this book, given that it’s more readable and requires less overhead.</p>
<p>Layout Qualifiers<strong><br/>
<br/></strong> For more information on layout and other qualifiers, please visit <a href="https://www.khronos.org/opengl/wiki/Layout_Qualifier_(GLSL)">https://www.khronos.org/opengl/wiki/Layout_Qualifier_(GLSL)</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Initializing Attributes and Uniforms</h1>
                
            
            
                
<p>Once we have compiled and installed the program, the next step is to initialize the attributes and variables. We will initialize our uniforms using the <kbd>initLights</kbd> function:</p>
<div><pre>function initLights() {<br/>  gl.uniform3fv(program.uLightDirection, [0, 0, -1]);<br/>  gl.uniform4fv(program.uLightAmbient, [0.01, 0.01, 0.01, 1]);<br/>  gl.uniform4fv(program.uLightDiffuse, [0.5, 0.5, 0.5, 1]);<br/>  gl.uniform4f(program.uMaterialDiffuse, 0.1, 0.5, 0.8, 1);<br/>}</pre></div>
<p>In this example, you can see that we’re using the references obtained with <kbd>getUniformLocation</kbd> (we did this in <kbd>initProgram</kbd>).</p>
<p>These are the functions that the WebGL API provides to set and get uniform values:</p>
<table border="1" style="border-collapse: collapse;width: 100%;border-color: #000000">
<tbody>
<tr>
<td style="width: 21.4908%"><strong>WebGL Function</strong></td>
<td style="width: 77.9284%"><strong>Description</strong></td>
</tr>
<tr>
<td style="width: 21.4908%"><kbd>uniform[1234][fi]</kbd></td>
<td style="width: 77.9284%">Specifies 1-4 <kbd>float</kbd> or <kbd>int</kbd> values of a uniform variable.</td>
</tr>
<tr>
<td style="width: 21.4908%"><kbd>uniform[1234][fi]v</kbd></td>
<td style="width: 77.9284%">Specifies the value of a uniform variable as an array of 1-4 <kbd>float</kbd> or <kbd>int</kbd> values.</td>
</tr>
<tr>
<td style="width: 21.4908%"><kbd>getUniform(program)</kbd></td>
<td style="width: 77.9284%">Retrieves the contents of a uniform variable. The reference parameter has been previously obtained with <kbd>getUniformLocation</kbd>.</td>
</tr>
</tbody>
</table>
<p>In <a href="d2019a49-9e84-448c-8799-e296187476d1.xhtml" target="_blank">Chapter 2</a>, <em>Rendering</em>, we learned that a four-step process is required to initialize and use attributes. Recall that we do the following:</p>
<ol>
<li>Bind a VBO.</li>
<li>Point an attribute to the currently-bound VBO.</li>
<li>Enable the attribute.</li>
<li>Unbind the VBO.</li>
</ol>
<p>The key piece here is step <em>2</em>. We do this with the following instruction:</p>
<div><pre>gl.vertexAttribPointer(index, size, type, normalize, stride, offset);</pre></div>
<p>If you check out the <kbd>ch03_05_wall.html</kbd> example, you will see that we do this inside the <kbd>draw</kbd> function:</p>
<div><pre>gl.bindBuffer(gl.ARRAY_BUFFER, verticesBuffer);<br/>gl.vertexAttribPointer(program.aVertexPosition, 3, gl.FLOAT, false, 0, 0);<br/><br/>gl.bindBuffer(gl.ARRAY_BUFFER, normalsBuffer);<br/>gl.vertexAttribPointer(program.aVertexNormal, 3, gl.FLOAT, false, 0, 0);</pre></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Bridging the Gap Between WebGL and ESSL</h1>
                
            
            
                
<p>It’s now useful to test out how we integrate our ESSL program with our WebGL code by taking the code from <kbd>ch03_05_wall.html</kbd> and making some modifications.</p>
<p>Imagine a wall composed of the sections A, B, and C, and you are facing section B with a flashlight in your hand (frontal view). Intuitively, you know that section A and section C will be darker than section B. This fact can be modeled by starting at the color of the center of section B and darkening the color of the surrounding pixels as we move away from the center:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1e344c4e-1d5b-4f95-8d55-6eb1b1e69d09.png"/></p>
<p>Let's summarize the code we need to cover:</p>
<ul>
<li>The ESSL program containing the vertex and fragment shaders. For the wall, we will select Goraud shading with a Diffuse/Lambertian reflection model.</li>
<li>The <kbd>initProgram</kbd> function. We need to make sure that we map all of the attributes and uniforms that we defined in the ESSL code, including the normals:</li>
</ul>
<div><pre style="padding-left: 60px">program.aVertexNormal= gl.getAttribLocation(program, 'aVertexNormal');</pre></div>
<ul>
<li>The <kbd>initBuffers</kbd> function. Here, we need to create our geometry. We can represent the wall with eight vertices that define six triangles, such as the ones shown in the previous diagram. In <kbd>initBuffers</kbd>, we will apply what we learned in the previous chapters to set up the appropriate VAOs and buffers. This time, we need to set up an additional buffer: the VBO that contains information about normals. The code to set up the normals VBO looks like this:</li>
</ul>
<div><pre style="padding-left: 60px">function initBuffers() {<br/>  const vertices = [<br/>    -20, -8, 20, // 0<br/>    -10, -8, 0,  // 1<br/>    10, -8, 0,   // 2<br/>    20, -8, 20,  // 3<br/>    -20, 8, 20,  // 4<br/>    -10, 8, 0,   // 5<br/>    10, 8, 0,    // 6<br/>    20, 8, 20    // 7<br/>  ];<br/><br/>  indices = [<br/>    0, 5, 4,<br/>    1, 5, 0,<br/>    1, 6, 5,<br/>    2, 6, 1,<br/>    2, 7, 6,<br/>    3, 7, 2<br/>  ];<br/><br/>  <strong>// Create VAO<br/>  vao = gl.createVertexArray();<br/><br/>  // Bind Vao<br/>  gl.bindVertexArray(vao);<br/><br/>  const normals = utils.calculateNormals(vertices, indices);<br/><br/>  const verticesBuffer = gl.createBuffer();<br/>  gl.bindBuffer(gl.ARRAY_BUFFER, verticesBuffer);<br/>  gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), <br/>   gl.STATIC_DRAW);<br/>  // Configure instructions<br/>  gl.enableVertexAttribArray(program.aVertexPosition);<br/>  gl.vertexAttribPointer(program.aVertexPosition, 3, gl.FLOAT, <br/>   false, 0, 0);<br/><br/>  const normalsBuffer = gl.createBuffer();<br/>  gl.bindBuffer(gl.ARRAY_BUFFER, normalsBuffer);<br/>  gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(normals), <br/>   gl.STATIC_DRAW);<br/>  // Configure instructions<br/>  gl.enableVertexAttribArray(program.aVertexNormal);<br/>  gl.vertexAttribPointer(program.aVertexNormal, 3, gl.FLOAT, false, <br/>   0, 0);<br/><br/>  indicesBuffer = gl.createBuffer();<br/>  gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, indicesBuffer);<br/>  gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(indices), <br/>   gl.STATIC_DRAW)</strong><strong>;</strong><br/><br/>  // Clean<br/>  gl.bindVertexArray(null);<br/>  gl.bindBuffer(gl.ARRAY_BUFFER, null);<br/>  gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, null);<br/>}</pre></div>
<ul>
<li>To calculate the normals, we use the <kbd>calculateNormals(vertices, indices)</kbd> helper function. You can find this method in the <kbd>common/js/utils.js</kbd> file.</li>
<li><kbd>initLights</kbd>: We covered this function already and know how to do that.</li>
<li>There’s only one minor but important change to make inside the <kbd>draw</kbd> function. We need to make sure that the VBOs are bound before we use <kbd>drawElements</kbd>. The code to do that looks like this:</li>
</ul>
<div><pre style="padding-left: 60px">function draw() {<br/>  gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);<br/>  gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);<br/><br/>  mat4.perspective(projectionMatrix, 45, gl.canvas.width / <br/>   gl.canvas.height, 0.1, 10000);<br/>  mat4.identity(modelViewMatrix);<br/>  mat4.translate(modelViewMatrix, modelViewMatrix, [0, 0, -40]);<br/><br/>  gl.uniformMatrix4fv(program.uModelViewMatrix, false, <br/>   modelViewMatrix);<br/>  gl.uniformMatrix4fv(program.uProjectionMatrix, false, <br/>   projectionMatrix);<br/><br/>  mat4.copy(normalMatrix, modelViewMatrix);<br/>  mat4.invert(normalMatrix, normalMatrix);<br/>  mat4.transpose(normalMatrix, normalMatrix);<br/><br/>  gl.uniformMatrix4fv(program.uNormalMatrix, false, normalMatrix);<br/><br/>  try {<br/>    <strong>// Bind VAO<br/>    gl.bindVertexArray(vao);<br/><br/>    gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, indicesBuffer);<br/>    gl.drawElements(gl.TRIANGLES, indices.length, <br/>     gl.UNSIGNED_SHORT, 0)</strong><strong>;</strong><br/><br/>    // Clean<br/>    gl.bindVertexArray(null);<br/>    gl.bindBuffer(gl.ARRAY_BUFFER, null);<br/>    gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, null);<br/>  }<br/>  catch (error) {<br/>    console.error(error);<br/>  }<br/>}</pre></div>
<p>In the following section, we will explore the functions that we just described for building and illuminating the wall.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Working on the Wall</h1>
                
            
            
                
<p>Let's cover an example showcasing the preceding concepts in action:</p>
<ol>
<li>Open the <kbd>ch03_05_wall.html</kbd> file in your browser. You will see something similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/74054bf4-6df6-458c-a4a4-159f4d9e1db1.png"/></p>
<ol start="2">
<li>Open the <kbd>ch03_05_wall.html</kbd> file in a code editor.</li>
<li>Go to the vertex shader. Make sure that you identify the attributes, uniforms, and varyings that are declared there.</li>
<li>Go to the fragment shader. Notice that there are no attributes here, because attributes are exclusive to the vertex shader.</li>
</ol>
<p>Vertex and Fragment Shaders<br/>
<br/>
You can find these shaders inside the script tags with the appropriate ID names. For example, the vertex shader can be found inside <kbd>&lt;script id="vertex-shader" type="x-shader/x-vertex"&gt;</kbd>.</p>
<ol start="5">
<li>Go to the <kbd>init</kbd> function. Verify that we are calling <kbd>initProgram</kbd> and <kbd>initLights</kbd> there.</li>
<li>Go to <kbd>initProgram</kbd>. Make sure that you understand how the program is built and how we obtain references to attributes and uniforms.</li>
<li>Go to <kbd>initLights</kbd>. Update the values of the uniforms, as shown here:</li>
</ol>
<div><pre style="padding-left: 60px">function initLights() {<br/>  gl.uniform3fv(program.uLightDirection, [0, 0, -1]);<br/>  gl.uniform4fv(program.uLightAmbient, [0.01, 0.01, 0.01, 1]);<br/>  gl.uniform4fv(program.uLightDiffuse, [0.5, 0.5, 0.5, 1]);<br/>  gl.uniform4f(program.uMaterialDiffuse, 0.1, 0.5, 0.8, 1);<br/>}</pre></div>
<ol start="8">
<li>Notice that one of the updates consists of changing from <kbd>uniform4f</kbd> to <kbd>uniform4fv</kbd> for the <kbd>uMaterialDiffuse</kbd> uniform. </li>
<li>Save the file.</li>
<li>Open it again (or reload it) in your browser. What happened?</li>
<li>Let's do something a bit more interesting. We are going to create a key listener so that every time we hit a key, the light orientation changes.</li>
<li>Right after the <kbd>initLights</kbd> function, write the following code:</li>
</ol>
<div><pre style="padding-left: 60px">function processKey(ev) {<br/>  const lightDirection = gl.getUniform(program, program.uLightDirection);<br/>  const incrementValue = 10;<br/><br/>  switch (ev.keyCode) {<br/>    // left arrow<br/>    case 37: {<br/>      azimuth -= incrementValue;<br/>      break;<br/>    }<br/>    // up arrow<br/>    case 38: {<br/>      elevation += incrementValue;<br/>      break;<br/>    }<br/>    // right arrow<br/>    case 39: {<br/>      azimuth += incrementValue;<br/>      break;<br/>    }<br/>    // down arrow<br/>    case 40: {<br/>      elevation -= incrementValue;<br/>      break;<br/>    }<br/>  }<br/><br/>  azimuth %= 360;<br/>  elevation %= 360;<br/><br/>  const theta = elevation * Math.PI / 180;<br/>  const phi = azimuth * Math.PI / 180;<br/><br/>  // Spherical to cartesian coordinate transformation<br/>  lightDirection[0] = Math.cos(theta) * Math.sin(phi);<br/>  lightDirection[1] = Math.sin(theta);<br/>  lightDirection[2] = Math.cos(theta) * -Math.cos(phi);<br/><br/>  gl.uniform3fv(program.uLightDirection, lightDirection);<br/>}</pre></div>
<ol start="13">
<li>This function processes the arrow keys and changes the light direction accordingly. There’s a bit of trigonometry (<kbd>Math.cos</kbd>, <kbd>Math.sin</kbd>) involved, but we are simply converting the angles (azimuth and elevation) into Cartesian coordinates.</li>
<li>Please note that we get the current light direction by using the following function:</li>
</ol>
<div><pre style="padding-left: 60px">const lightDirection = gl.getUniform(program, program.uLightDirection);</pre></div>
<ol start="15">
<li>After processing the key strokes, we can save the updated light direction with the following code:</li>
</ol>
<div><pre style="padding-left: 60px">gl.uniform3fv(program.uLightDirection, lightDirection);</pre></div>
<ol start="16">
<li>Save the work and reload the web page:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/b95d6df1-1a53-4f15-b512-2f428a4dd01c.png"/></p>
<ol start="17">
<li>Use the arrow keys to change the light direction.</li>
<li>If you have any problems during the development of this exercise or just want to verify the final result, please check the <kbd>ch03_06_wall_final.html</kbd> file, which contains the completed exercise.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>In this exercise, we created a keyboard listener that allows us to update the light's orientation so that we can move it around the wall and see how it reacts to surface normals. We also saw how the vertex shader and fragment shader input variables are declared and used. We learned how to build a program by reviewing the <kbd>initProgram</kbd> function. We also learned about initializing uniforms in the <kbd>initLights</kbd> function. Finally, we studied the <kbd>getUniform</kbd> function to retrieve the current value of a uniform. Although we haven't covered the examples entirely, this exercise was intended to familiarize you with vertex and fragment shaders so that you can implement various light-shading and reflection models.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">More on Lights: Positional Lights</h1>
                
            
            
                
<p>Before finishing this chapter, let's revisit the topic of lights. So far, for the purpose of our examples, we’ve assumed that our light source is infinitely far away from the scene. This assumption allows us to model the light rays as being parallel to each other. An example of this is sunlight. These lights are <em>directional lights</em>. Now, we are going to consider a case where the light source is relatively close to the object it needs to illuminate. Think, for example, of a desk lamp illuminating the document you’re reading. These lights are <em>positional lights</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b25e6328-5f13-4c4f-a540-f094b2197840.png" style="width:42.58em;height:20.58em;"/></p>
<p>As we experienced before, when working with directional lights, only one variable is required. This is the light direction we represented in the <kbd>uLightDirection</kbd> uniform.</p>
<p>In contrast, when working with positional lights, we need to know the location of the light. We can represent it by using a uniform that we will name <kbd>uLightPosition</kbd>. As is the case when using positional lights, the light rays here are not parallel to each other; as a result, we need to calculate each light ray separately. We will do this by using a varying that we will name <kbd>vLightRay</kbd>.</p>
<p>In the next section, we will investigate how a positional light interacts with a scene.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Positional Lights in Action</h1>
                
            
            
                
<p>Let's cover an example of positional lights in action:</p>
<ol>
<li>Open the <kbd>ch03_07_positional_lighting.html</kbd> file in your browser. The page will look similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/c52b4a9d-4cd9-4133-9ff9-2a84444d671f.png"/></p>
<ol start="2">
<li>The interface of this exercise is very simple. You can use the controls widget to interact with the scene. Unlike in previous exercises, the <strong>Translate X</strong>, <strong>Y</strong>, and <strong>Z</strong> sliders do not represent light direction here. Instead, they allow us to set the light source position. Go ahead and play with them.</li>
<li>For clarity, a little sphere representing the position of the light source has been added to the scene to visualize the light source, but this is not generally required.</li>
<li>What happens when the light source is located on the surface of the cone versus on the surface of the sphere?</li>
<li>What happens when the light source is inside the sphere?</li>
</ol>
<ol start="6">
<li>Let's take a look at the way we calculate the light rays by inspecting the vertex shader in the source code. The light ray calculation is performed in the following two lines of code:</li>
</ol>
<div><pre style="padding-left: 60px">vec4 light = uModelViewMatrix * vec4(uLightPosition, 1.0);<br/>vLightRay = vertex.xyz - light.xyz;</pre></div>
<ol start="7">
<li>The first line allows us to obtain a transformed light position by multiplying the Model-View<em> </em>matrix by the <kbd>uLightPosition</kbd> uniform. If you review the code in the vertex shader, you’ll note that we also use this matrix for calculating transformed vertices and normals. We will discuss these matrix operations in <a href="62d4de32-0b5b-4339-8fcc-80f739e80ec2.xhtml">Chapter 4</a>, <em>Cameras</em>. For now, we can just assume that this is necessary to obtain transformed vertices, normals, and light positions whenever we move the camera. To test this, modify this line by removing the matrix from the equation so that the line looks like the following:</li>
</ol>
<div><pre style="padding-left: 60px">vec4 light = vec4(uLightPosition, 1.0);</pre></div>
<ol start="8">
<li>Save the file and launch it in your browser. What is the effect of not transforming the light position? What you can see is that the camera is moving, but the light source position is not being updated!</li>
<li>We can see that the light ray is calculated as the vector that reaches from the transformed light position (light) to the vertex position.</li>
</ol>
<p>Thanks to the interpolation of varyings that is provided by ESSL, we automatically obtain all the light rays per pixel in the fragment shader:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/763b7af0-fe64-4dd0-9675-e29998fac50d.png" style="width:43.92em;height:15.92em;"/></p>
<p><em><strong>What just happened?</strong></em></p>
<p>We studied the difference between directional lights and positional lights. We also investigated the importance of the Model-View matrix for the correct calculation of positional lights when the camera is moving. Finally, we modeled the procedure to obtain per-vertex light rays.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Virtual Showroom Example</h1>
                
            
            
                
<p>In this chapter, we’ve included an example of the Nissan GTR exercise we saw in <a href="d2019a49-9e84-448c-8799-e296187476d1.xhtml">Chapter 2</a>, <em>Rendering</em>. This time, we’ve used a Phong lighting model with a positional light to illuminate the scene. You can find this example in <kbd>ch03_08_showroom.html</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/15af1d35-37ee-430c-b04b-f39dac44efed.png"/></p>
<p>Here, you can experiment with different light positions. Pay special attention to the nice specular reflections you obtain thanks to the specularity property of the car and the shininess of the light.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Architecture Updates</h1>
                
            
            
                
<p>Let's cover some useful functions that we can refactor to use in later chapters:</p>
<ol>
<li>We've seen how to create and compile a program using shaders. We've also covered how to load and reference attributes and uniforms. Let's include a module that abstracts away this low-level functionality with a simpler API:</li>
</ol>
<pre style="padding-left: 60px">&lt;script type="text/javascript" src="img/Program.js"&gt;&lt;/script&gt; </pre>
<ol start="2">
<li>Like we did previously, we will include this script tag in the <kbd>&lt;head&gt;</kbd> of the HTML document. Be sure to include it after the other module scripts, since they may use the libraries and earlier modules we've covered.</li>
<li>Let's update our <kbd>initProgram</kbd> function inside of <kbd>ch03_08_showroom.html</kbd> so that we can use this module:</li>
</ol>
<pre style="padding-left: 60px">function initProgram() {<br/>  const canvas = document.getElementById('webgl-canvas');<br/>  utils.autoResizeCanvas(canvas);<br/><br/>  gl = utils.getGLContext(canvas);<br/>  gl.clearColor(...clearColor, 1);<br/>  gl.enable(gl.DEPTH_TEST);<br/>  gl.depthFunc(gl.LEQUAL);<br/><br/>  program = new Program(gl, 'vertex-shader', 'fragment-shader');<br/><br/>  const attributes = [<br/>    'aVertexPosition',<br/>    'aVertexNormal'<br/>  ];<br/><br/>  const uniforms = [<br/>    'uProjectionMatrix',<br/>    'uModelViewMatrix',<br/>    'uNormalMatrix',<br/>    'uLightAmbient',<br/>    'uLightPosition',<br/>    'uMaterialSpecular',<br/>    'uMaterialDiffuse',<br/>    'uShininess'<br/>  ];<br/><br/>  program.load(attributes, uniforms);<br/>}</pre>
<ol start="4">
<li>All of the heavy lifting of creating a program, compiling shaders, and attaching uniforms and attributes to our program is done for us.</li>
<li>Let's inspect the <kbd>Program</kbd> class source code. Most of the operations should look familiar to you:</li>
</ol>
<pre style="padding-left: 60px">'use strict';<br/><br/>/*<br/>* Program constructor that takes a WebGL context and script tag IDs<br/>* to extract vertex and fragment shader source code from the page<br/>*/<br/>class Program {<br/><br/>  constructor(gl, vertexShaderId, fragmentShaderId) {<br/>    this.gl = gl;<br/>    this.program = gl.createProgram();<br/><br/>    if (!(vertexShaderId &amp;&amp; fragmentShaderId)) {<br/>      return console.error('No shader IDs were provided');<br/>    }<br/><br/>    gl.attachShader(this.program, utils.getShader(gl, <br/>     vertexShaderId));<br/>    gl.attachShader(this.program, utils.getShader(gl, <br/>     fragmentShaderId));<br/>    gl.linkProgram(this.program);<br/><br/>    if (!this.gl.getProgramParameter(this.program, <br/>     this.gl.LINK_STATUS)) {<br/>      return console.error('Could not initialize shaders.');<br/>    }<br/><br/>    this.useProgram();<br/>  }<br/><br/>  // Sets the WebGL context to use current program<br/>  useProgram() {<br/>    this.gl.useProgram(this.program);<br/>  }<br/><br/>  // Load up the given attributes and uniforms from the given values<br/>  load(attributes, uniforms) {<br/>    this.useProgram();<br/>    this.setAttributeLocations(attributes);<br/>    this.setUniformLocations(uniforms);<br/>  }<br/><br/>  // Set references to attributes onto the program instance<br/>  setAttributeLocations(attributes) {<br/>    attributes.forEach(attribute =&gt; {<br/>      this[attribute] = this.gl.getAttribLocation(this.program, <br/>       attribute);<br/>    });<br/>  }<br/><br/>  // Set references to uniforms onto the program instance<br/>  setUniformLocations(uniforms) {<br/>    uniforms.forEach(uniform =&gt; {<br/>      this[uniform] = this.gl.getUniformLocation(this.program, <br/>       uniform);<br/>    });<br/>  }<br/><br/>  // Get the uniform location from the program<br/>  getUniform(uniformLocation) {<br/>    return this.gl.getUniform(this.program, uniformLocation);<br/>  }<br/><br/>}</pre>
<ol start="6">
<li>We initialize <kbd>Program</kbd> by passing in a reference to the <kbd>gl</kbd> context, the vertex, and the fragment shader <kbd>id</kbd>.</li>
<li>We load and reference the <kbd>attributes</kbd> and <kbd>uniforms</kbd> programs by supplying the <kbd>program</kbd> instance with the array of attributes and uniforms.</li>
<li>The other methods are helper functions that we'll use in later chapters.</li>
<li>You can find an example of these changes in <kbd>ch03_09_showroom-final.html</kbd>.</li>
<li>You may have caught two additional utils methods used throughout this chapter: <kbd>normalizeColor</kbd> and <kbd>denormalizeColor</kbd>. These two methods simply normalize colors from range <kbd>[0-255]</kbd> to <kbd>[0-1]</kbd> or denormalize from <kbd>[0-1]</kbd> to <kbd>[0-255]</kbd>:</li>
</ol>
<pre style="padding-left: 60px">const utils = {<br/><br/>  // Normalize colors from 0-255 to 0-1<br/>  normalizeColor(color) {<br/>    return color.map(c =&gt; c / 255);<br/>  },<br/><br/>  // De-normalize colors from 0-1 to 0-255<br/>  denormalizeColor(color) {<br/>    return color.map(c =&gt; c * 255);<br/>  },<br/>  <br/>  // ...  <br/>};</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Let’s summarize what we learned in this chapter:</p>
<ul>
<li>We learned in detail what light sources, materials, and normals are, and how these elements interact to illuminate a WebGL scene.</li>
<li>We covered the differences between a shading method and a lighting model.</li>
<li>We studied the basics of the Goraud and Phong shading methods, along with the Lambertian and Phong lighting models. With the help of several examples, we also covered how to implement these shading and lighting models in code using ESSL, and how to communicate between the WebGL code and the ESSL code through attributes and uniforms.</li>
<li>We can use the vertex shader and the fragment shader to define a lighting model for our 3D scene.</li>
<li>We covered many of these operations through the lens of the latest and greatest techniques provided to us in WebGL 2's updated shading language.</li>
</ul>
<p>In the next chapter, we will expand on using matrices in ESSL so that we can learn how to use them to represent and move our viewpoint in a 3D scene.</p>


            

            
        
    </body></html>