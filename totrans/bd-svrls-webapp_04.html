<html><head></head><body>
        

                            
                    <h1 class="header-title">Hosting the Website</h1>
                
            
            
                
<p>The frontend is the easiest layer to go serverless. You just need a service to host your website's static files, and the user's browser will download the files, render the pages and execute the client-side JavaScript code. On AWS, the service we will use to host the frontend pages is Amazon S3. In this chapter, you'll also learn how to configure and optimize your website by adding a CDN and the supporting HTTPS connections.</p>
<p>In summary, we are going to cover the following topics:</p>
<ul>
<li>Using Amazon S3 to host static files</li>
<li>Configuring Route 53 to associate your domain name with S3</li>
<li>Using CloudFront to serve files through a CDN</li>
<li>Requesting a free SSL/TLS certificate to support HTTPS connections</li>
</ul>
<p>After this chapter, you'll have learned how to host a frontend in a serverless infrastructure.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Serving static files with Amazon S3</h1>
                
            
            
                
<p>Amazon S3 is extremely useful because it is a cheap service that provides high availability and scalability, requiring zero management effort. The infrastructure is fully managed by AWS. In this section, we are going to use S3 to host our website static files such as HTML, CSS, JavaScript, and images. You will see that this is done by uploading the files to a bucket and configuring S3 to enable website hosting.</p>
<p>Besides hosting static websites, you can also host complex applications. You just need to have a clear separation of concerns: the <em>frontend</em> files (HTML/CSS/JavaScript) will be hosted on S3 and will be used by the browser to render the web page and request additional data to the <em>backend</em> code, which will be hosted and executed by Lambda functions. As we are going to discuss in <a href="6cb9ccdc-61f3-437d-81ac-d05ec652f1a4.xhtml">Chapter 5</a>, <em>Building the Frontend</em>, you can build your frontend as an SPA. This requires the browser to render the pages. Alternatively, you can use Lambda to serve server-side rendered pages.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a bucket</h1>
                
            
            
                
<p>In <a href="246b3080-28af-4aa3-805f-5fbf9908caae.xhtml">Chapter 2</a>, <em>Getting Started with AWS</em>, we created a bucket using the CLI. In this section, we are going to use the Management Console to create another bucket so you can see the configuration options that S3 offers.</p>
<p>Let's perform the following steps to create a bucket:</p>
<ol>
<li>The first step is to browse the S3 console at <a href="https://console.aws.amazon.com/s3">https://console.aws.amazon.com/s3</a> and select the Create bucket option:</li>
</ol>
<div><img class="image-border" height="462" src="img/86450c8f-e00a-47ba-aa93-9548792ddbc2.png" width="752"/></div>
<ol start="2">
<li>We are creating a new bucket to host the website. If you don't have a domain name, you can still use S3 to host the website. You will have to access it from a URL in the following format: <kbd>http://my-bucket-name.com.s3-website-us-east-1.amazonaws.com</kbd>. If you have a domain name, such as <kbd>example.com</kbd>, you <em>must</em> set the bucket name with the same name as that of your domain, in this case, <kbd>example.com</kbd>. It is mandatory that you match the domain with the bucket name to allow Route 53 to make the association. As for the region, choose the one that is closer to your target audience:</li>
</ol>
<div><img class="image-border" height="499" src="img/a6548e0d-5eaf-49f7-b6d0-c610f42eb0c1.png" width="486"/></div>
<ol start="3">
<li>The Set properties screen allows you to add Versioning, Logging, and Tags. However, these are not required for our sample and can be skipped:</li>
</ol>
<div><img class="image-border" height="495" src="img/792064e4-3225-4161-8878-5cc8d907efe7.png" width="482"/></div>
<ol start="4">
<li>In the Set permissions screen, under Manage public permissions, select the option Grant public read access to this bucket. Click on Next to continue:</li>
</ol>
<div><img class="image-border" height="495" src="img/b6960cda-2a9c-4eff-9322-b3370fc135a9.png" width="482"/></div>
<ol start="5">
<li>Finish by reviewing the selected options and clicking on Create bucket.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Enabling web hosting</h1>
                
            
            
                
<p>To enable web hosting in this bucket, perform the following steps:</p>
<ol>
<li>Click on your bucket name and select Properties from the second tab:</li>
</ol>
<div><img class="image-border" height="541" src="img/b84cd453-4265-4c91-84d1-c921b7ee3844.png" width="681"/></div>
<ol start="2">
<li>Inside the Static website hosting card, click on the Disabled button to enable the option:</li>
</ol>
<div><img class="image-border" height="482" src="img/016d1be0-ae61-4729-bea8-742939d11130.png" width="680"/></div>
<ol start="3">
<li>Now select the Use this bucket to host a website option and set Index document and Error document with the value <kbd>index.html</kbd>. Also, note that you can see the bucket's Endpoint address in this image. It's <kbd>http://example.com.s3-website-us-east-1.amazonaws.com</kbd> in this example. Write it down because you will need it later during testing and when you configure the CloudFront distribution:</li>
</ol>
<div><img class="image-border" height="420" src="img/79c53ba3-307f-494d-9958-a454ff6b5e0b.png" width="400"/></div>
<ol start="4">
<li>Click on Save to finish this setting.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Handling errors</h1>
                
            
            
                
<p>There are four common ways to handle errors using S3. I will explain the options you have, but before I do that, let me define what I mean by "handling errors". When a user makes an HTTP request, S3 may fail to get the answer and it will return a status code to explain the failure. An HTTP 500 <em>Internal Server Error</em> could be a possible result, but it would be extremely rare and odd. However, S3 returning either a 404 <em>Not Found</em> or 403 <em>Forbidden</em> error is pretty common.</p>
<p>404 <em>Not Found</em> is an error that will be returned when the user browses your website for a page that does not exist. For example, if you have a page named <kbd>company.io/about</kbd> and the user incorrectly browses <kbd>company.io/abot</kbd>, S3 won't be able to find the <kbd>abot.html</kbd> file and will return a 404 error. Another example is when you use a JavaScript framework to create a <strong>Single-Page Application</strong> (<strong>SPA</strong>). Although your frontend code knows how to serve the <kbd>/about</kbd> page, the framework will not create a physical <kbd>about.html</kbd> file to upload to S3, and a 404 error will be returned even when browsing <kbd>company.io/about</kbd>.</p>
<p>We are going to discuss SPA in <a href="6cb9ccdc-61f3-437d-81ac-d05ec652f1a4.xhtml">Chapter 5</a>, <em>Building the Frontend</em>. For the time being, note that an SPA can serve multiple pages, but there is only one complete HTML file, named <kbd>index.html</kbd> file.</p>
<p>403 <em>Forbidden</em> is an error that happens when the bucket has restricted permissions. If the bucket doesn't allow access to <em>everyone</em> or there is a specific file with restricted access, the 403 error will be returned. In my opinion, I prefer to consider <em>all</em> the files within the bucket public. If there is a page that shouldn't be public, showing the HTML file should not be a problem. The objective is to protect the <em>data</em> and not the <em>layout</em>. Setting authorization and data visibility is something that needs to be handled on the server side, not the client side. Also, if there are static files that must remain private, such as photos, you can save them in another bucket instead of reusing the bucket that was created to host the website. These <em>photos</em> can be considered <em>data</em> and, likewise, need to be controlled by the backend with special care.</p>
<p>Considering that all the files of a bucket will remain public, and that we don't need not worry so much about strange S3 errors, the only problem that we need to handle now is 404 errors.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using redirection rules</h1>
                
            
            
                
<p>A very common approach to handle 404 errors for an SPA is to add a redirection rule in the Static website hosting page using the <kbd>ReplaceKeyPrefixWith</kbd> option:</p>
<pre>
    &lt;RoutingRules&gt;<br/>      &lt;RoutingRule&gt;<br/><strong>        &lt;Condition&gt;</strong><br/><strong>          &lt;HttpErrorCodeReturnedEquals&gt;</strong><br/><strong>            404</strong><br/><strong>          &lt;/HttpErrorCodeReturnedEquals&gt;</strong><br/><strong>        &lt;/Condition&gt;</strong><br/>        &lt;Redirect&gt;<br/>          &lt;Hostname&gt;<br/>            example.com<br/>          &lt;/Hostname&gt;<br/><strong>          &lt;ReplaceKeyPrefixWith&gt;</strong><br/><strong>            #!/</strong><br/><strong>          &lt;/ReplaceKeyPrefixWith&gt;</strong><br/>        &lt;/Redirect&gt;<br/>      &lt;/RoutingRule&gt;<br/>    &lt;/RoutingRules&gt;
</pre>
<p>With this solution, when the user browses <kbd>company.io/about</kbd>, the address will be replaced by <kbd>company.io/#!/about</kbd> and the <kbd>index.html</kbd> file will be returned. If you have an SPA, it can identify which page needs to be displayed and render it correctly. If the page doesn't exist, it will be able to render a generic 404 page. Also, you can configure HTML5's pushState to remove the hashbang sign (<kbd>#!</kbd>) after the page loads, but it will make the page to blink.</p>
<p>The drawback of this approach is that you need to choose between a polluted URL with <kbd>#!</kbd> or experience the page blinking when it is loaded.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using an error document</h1>
                
            
            
                
<p>Instead of setting a redirection rule, you can set an error document with <kbd>index.html</kbd>. This is the easiest solution for SPAs. When the user requests an <kbd>/about</kbd> page that doesn't have a physical <kbd>about.html</kbd> file to match, the <kbd>index.html</kbd> file is loaded and the SPA reads the address and understands that it needs to serve the contents of the <kbd>/about</kbd> page.</p>
<p>This configuration is what we have done in the previous image and it works pretty well without polluting the URL address with <kbd>#!</kbd> and without blinking the page when loading. However, search engines may refuse to index your website because, when browsing the <kbd>/about</kbd> page, the result's <kbd>body</kbd> message will be set using the correct page, but the <kbd>status code</kbd> will still be set as 404. If the Google crawler sees a 404 error, it will understand that the page doesn't exist and the page content is probably a generic error page.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Delegating to CloudFront</h1>
                
            
            
                
<p>You can set the error document to <kbd>index.html</kbd>, as in the previous solution, but using CloudFront instead of S3. CloudFront provides a <em>custom error response</em> setting that allows you to change the status code of the <kbd>response</kbd> object. In this case, we can configure CloudFront to act on 404 errors, returning S3's <kbd>index.html</kbd> file and modifying the <kbd>status code</kbd> to <kbd>200 <em>OK</em></kbd> instead of returning 404.</p>
<p>If you opt for this, the problem is that you will return status code 200 even for pages that doesn't exist. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Prerendering pages</h1>
                
            
            
                
<p>Another issue with the other solutions is that <strong>Search Engine Optimization</strong> (<strong>SEO</strong>) is not considered, because they require JavaScript to be enabled in the browser to render the correct page. Since most web crawlers can't execute JavaScript code, they will not be able to index the webpage. This problem is solved with prerendering.</p>
<p>In the next chapter, you are going to learn how you can prerender pages. This technique creates physical pages for SPA routes. For example, in an SPA, the <kbd>/about</kbd> page doesn't have an <kbd>about.html</kbd> file, but with prerendering, you will be able to create it.</p>
<p>When you prerender all the possible pages and upload the files to S3, there will be no need to worry about 404 errors. If the page exist, S3 will find an HTML file for it. We still need to configure the error document to <kbd>index.html</kbd> to handle pages that doesn't exist, but we don't need to configure CloudFront to force the status code 200.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Supporting www anchors</h1>
                
            
            
                
<p>A domain name without the <strong>www</strong> anchor text is usually referred to as a <em>naked</em> domain. For instance, <kbd>www.example.com</kbd> is a domain with the www anchor and <kbd>example.com</kbd> is a naked domain. The <em>canonical</em> URL is the option that you choose as the main address.</p>
<p>There are pros and cons that you'll need to consider when deciding which address should be your website's main one. One issue with using naked domains is that if your site has cookies, then placing static files in a subdomain, such as <kbd>static.example.com</kbd>, will not be optimized because each browser request for static files will automatically send cookies that were created for <kbd>example.com</kbd>. If your login happens at <kbd>www.example.com</kbd>, you can place static content inside <kbd>static.example.com</kbd> without worrying about cookies.</p>
<p>This issue can be mitigated by creating another domain or using a CDN to retrieve the static files. Despite this, the current trend is to drop the www anchor. It looks more modern to brand your tech company as <kbd>company.io</kbd> than it does using the old <kbd>www.company.com</kbd> format.</p>
<p>Choose your main address, but support both formats. Some people are used to adding www to the address and some of them may forget to include it. In the previous example, we created a domain without the www address. Now we are going to create another bucket, in the <kbd>www.example.com</kbd> format, and set the Static website hosting configuration with the Redirect requests option to target the address without www:</p>
<div><img class="image-border" height="290" src="img/56f84f40-75d7-43b2-8448-22d3e5822a26.png" width="324"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Uploading static files</h1>
                
            
            
                
<p>Uploading is pretty straightforward. Let's perform the following steps to upload static files:</p>
<ol>
<li>First, create a very simple <kbd>index.html</kbd> file for testing purposes:</li>
</ol>
<pre>
        &lt;!DOCTYPE html&gt;<br/>        &lt;html&gt;<br/>            &lt;title&gt;My Page&lt;/title&gt;<br/>        &lt;body&gt;<br/>            &lt;h1&gt;Hosting with S3&lt;/h1&gt;<br/>        &lt;/body&gt;<br/>        &lt;/html&gt;
</pre>
<ol start="2">
<li>When you click on the main bucket name, an Upload button will be made available. Click on it:</li>
</ol>
<div><img class="image-border" height="410" src="img/5001da1d-cb8d-4bac-b7a7-9053555a0886.png" width="515"/></div>
<ol start="3">
<li>The next screen is simple. Click on Add files, select the <kbd>index.html</kbd> file, and continue by clicking on Next:</li>
</ol>
<div><img class="image-border" height="481" src="img/c04237b1-2b72-4ca1-abd2-f850d72ae508.png" width="479"/><br/></div>
<ol start="4">
<li>At the Set permissions step, under Manage public permissions, select the option Grant public read access to this object(s):</li>
</ol>
<div><img class="image-border" src="img/288cd06e-1f6c-47ae-bc94-9caafa7e11ca.png" width="479"/><br/></div>
<ol start="5">
<li>At the Set properties step, keep the default option for Storage class as Standard because files will be frequently accessed. Also, set Encryption to None because all the files will be publicly available, and therefore, adding an extra layer of security is not necessary and will only slow down your responses. As for the Metadata fields, you don't need to set them:</li>
</ol>
<div><img class="image-border" height="442" src="img/a8e62c85-9cd0-4528-a022-579309c94e4d.png" width="439"/></div>
<ol start="6">
<li>Finally, review the options and confirm by clicking on Upload.</li>
<li>Now you can check whether it is working by browsing the bucket's endpoint, for example, <kbd>http://example.com.s3-website-us-east-1.amazonaws.com</kbd>.</li>
</ol>
<p>If it doesn't work on the first try, clear your browser cache and try again because when you test the endpoint before uploading a file, the browser may cache the information indicating that the link is invalid. This may avoid further requests for a short time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Automating website publishing</h1>
                
            
            
                
<p>We have gone through the necessary steps to upload our site's frontend using the S3 console. It's easy and fast, but it's also pretty easy to automate this task. If you haven't configured the AWS CLI, as instructed in <a href="246b3080-28af-4aa3-805f-5fbf9908caae.xhtml">Chapter 2</a>, <em>Getting Started with AWS</em>, do so now and see how useful it is to automate the uploading of files. In fact, what we are going to do is synchronize the bucket contents with a local folder. The CLI will upload only the modified files, which will make the upload much faster in future when your site grows.</p>
<p>You can execute the following command to upload your files:</p>
<pre>
<strong>aws s3 sync ./path/to/folder s3://my-bucket-name --acl public-read</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Serving gzip files</h1>
                
            
            
                
<p>The gzip file format is a standard format used on the Web to compress files and improve the download speed by reducing the transferred file sizes. It can lower the bandwidth costs for you and for your users by providing smaller files. This approach has a huge impact on the perceived performance when loading the website.</p>
<p>It is currently supported by every major browser. By default, for every request, the browser will add an <kbd>Accept-Encoding: gzip</kbd> header. If the server supports gzip, the file is sent in compressed form.</p>
<p>The following diagram shows an HTTP request without gzip support:</p>
<div><img class="image-border" height="121" src="img/26db65cb-1b15-4635-a7ec-9b5e434a5ea4.png" width="415"/></div>
<p>The following diagram shows how much bandwidth you can save with gzip. The compressed response is usually 10 times smaller:</p>
<div><img class="image-border" height="127" src="img/f75fb39d-f681-4660-a8e2-be1e28310e6b.png" width="431"/></div>
<p>With this format, the server needs CPU time to compress the file and the user's browser needs CPU cycles to decompress the same file. However, with modern CPUs, the time to compress/decompress is much lower than the extra time taken to send the uncompressed file through the network. Even with low-end mobile devices, the CPU is much faster than the mobile network.</p>
<p>However, there is a problem. Since we don't have server-side processing with Amazon S3, there is no option to natively compress files in response to requests. You need to compress them locally before uploading the files and setting the metadata to identify <kbd>Content-Encoding</kbd> as <kbd>gzip</kbd>. Thankfully, you can skip the trouble of including this step in the deployment workflow if you use CloudFront. As we are going to see later, CloudFront has an option to automatically compress all your files with gzip.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up Route 53</h1>
                
            
            
                
<p>Route 53 is a DNS management service. You don't need to use it if you want to expose subdomains, such as <kbd>www.example.com</kbd>, however, it is indeed obligatory if you want to serve your website data under a naked domain, such as <kbd>example.com</kbd>, hosted on S3 or CloudFront. This is due to the RFC rule: you can't have a CNAME record for your domain root, it must be an A record.</p>
<p>What's the difference? CNAME and A records are both record types that help the DNS system to translate a domain name into an IP address. While CNAME references another domain, an A record references an IP address.</p>
<p>So, if you don't want to use Route 53, you can use your own domain management system, such as GoDaddy, to add a CNAME that will map your <kbd>www.example.com</kbd> domain to an S3 endpoint, for example, <kbd>www.example.com.s3-website-us-east-1.amazonaws.com</kbd>. This configuration works fine, but you can't do the same for <kbd>example.com</kbd> because the IP address of the <kbd>example.com.s3-website-us-east-1.amazonaws.com</kbd> endpoint changes frequently and your third-party domain controller (GoDaddy, in this example) won't follow the changes.</p>
<p>In this case, you need to use Route 53 because it will allow an A record to be created referencing your S3 bucket endpoint, such as <kbd>example.com.s3-website-us-east-1.amazonaws.com</kbd>. You just need to say that this endpoint is an alias and Route 53 will be able to track the correct IP address to answer DNS queries.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a hosted zone</h1>
                
            
            
                
<p>If you register your domain address with Amazon, a hosted zone will be automatically created. If you register your domain with another vendor, you will need to create a new hosted zone.</p>
<p>A hosted zone allows you to configure your domain's DNS settings. You can set where your naked domain and subdomains are hosted and configure other parameters, for example, a <em>mail exchange</em> record set.</p>
<p>To create a hosted zone, perform the following steps:</p>
<ol>
<li>Browse the Route 53 Management Console at <a href="https://console.aws.amazon.com/route53">https://console.aws.amazon.com/route53</a>. You will see a welcome screen, click on Get started now.</li>
<li>In the next screen, click on Hosted zones in the left menu, followed by the Create Hosted Zone button:</li>
</ol>
<div><img class="image-border" height="544" src="img/37c1fb06-d764-4ba5-b6d7-f4bc5f9cf683.png" width="801"/></div>
<ol start="3">
<li>Type the domain name and confirm:</li>
</ol>
<div><img class="image-border" height="249" src="img/255f2be9-668e-4bc4-9501-41b892dbb95f.png" width="372"/></div>
<ol start="4">
<li>A hosted zone will be created with two record types, namely <kbd>NS</kbd> (Name Server) and <kbd>SOA</kbd> (Start Of Authority):</li>
</ol>
<div><img class="image-border" height="114" src="img/4fae923e-91c6-44ce-afdc-66a86fa97e7a.png" width="537"/></div>
<ol start="5">
<li>If you have registered your domain with another vendor, you must transfer the DNS management to Route 53. This is done by changing your domain registrar Name Servers (NS records) to Amazon's Name Servers.</li>
<li>Your registrar may offer a control panel for your domain with an option such as Manage DNS. Find where the Name Servers are located and edit them to use Amazon's servers.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating record sets</h1>
                
            
            
                
<p>Now let's create two record sets such as one for <kbd>example.com</kbd> and another for <kbd>www.example.com</kbd>. Do this by performing the following steps:</p>
<ol>
<li>Click on Create Record Set:</li>
</ol>
<div><img class="image-border" height="149" src="img/9e8c92d3-2af7-4023-b17a-4b2a60484f00.png" width="501"/></div>
<ol start="2">
<li>In the first record, set the following parameters:
<ul>
<li>Name: Leave this field empty</li>
<li>Type: In this field, select A - IPv4 address</li>
<li>Alias: Check this field with Yes</li>
<li>Alias Target: This field is a drop-down list where you can select the S3 endpoint of your bucket:</li>
</ul>
</li>
</ol>
<div><img class="image-border" height="376" src="img/33fcd17e-73ea-48d1-a315-7a06dcfcc885.png" width="361"/></div>
<ol start="3">
<li>Create another record set. This time, use the following parameter values:
<ul>
<li>Name: Set this field as <kbd>www</kbd></li>
<li>Type: In this field select CNAME - Canonical name</li>
<li>Alias: Check this field with No</li>
<li>Value: Fill this field input with the S3 bucket's endpoint:</li>
</ul>
</li>
</ol>
<div><img class="image-border" height="376" src="img/1166f2ae-aba1-4e06-8bc8-ffe94aed0db3.png" width="361"/></div>
<ol start="4">
<li>Now test your domain name by typing it in your browser address. You will see the <kbd>index.html</kbd> file that you have uploaded to the S3 bucket.</li>
</ol>
<div><p>If you have transferred DNS control from another vendor to AWS, then because of DNS caching, you may need to wait for some minutes or even hours before the transfer completes. You will only be able to see the files that were hosted on Amazon S3 after the transfer finishes.</p>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up CloudFront</h1>
                
            
            
                
<p>CloudFront serves static files as a <strong>Content Delivery Network</strong> (<strong>CDN</strong>). Having file copies next to your users reduces latency and improves your perceived website speed. Another feature, which we will discuss later, is support for HTTPS requests.</p>
<p>In the next sections, we will create a CloudFront distribution and adjust the Route 53 settings to use CloudFront instead of the S3 bucket.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a distribution</h1>
                
            
            
                
<p>A CloudFront distribution is what makes it possible to associate a DNS configuration (Route 53) with CloudFront to distribute static content. A distribution needs an origin server to know where the files are stored. In our case, the origin will be the S3 bucket that was previously configured.</p>
<p>Let's perform the following steps to create a CloudFront distribution:</p>
<ol>
<li>Browse the CloudFront Management Console at <a href="https://console.aws.amazon.com/cloudfront">https://console.aws.amazon.com/cloudfront</a> and click on Create Distribution:</li>
</ol>
<div><img class="image-border" height="293" src="img/44d8951c-f0ee-468a-a555-45287b7bdb1b.png" width="587"/></div>
<ol start="2">
<li>The next step is to select the distribution type. For our website, select Get Started under the Web option:</li>
</ol>
<div><img class="image-border" height="419" src="img/497cc3b0-36e5-440c-9181-5dc7903d4844.png" width="511"/></div>
<ol start="3">
<li>The next screen is a big form that we need to fill out. In the first fieldset, Origin Settings, the Origin Domain Name option will provide a drop-down list of S3 endpoints.</li>
</ol>
<p>You should not use the endpoints provided here because some S3 configurations, such as redirects or error messages, won't be available when using this address.</p>
<p style="padding-left: 60px">Instead, use the endpoint that is available in the bucket properties inside the Static Website Hosting settings. The difference between these S3 endpoints is that the suggested endpoint doesn't have the bucket region (for example, <kbd>example.com.s3.amazonaws.com</kbd>) and the endpoint that we will use does have the region (for example, <kbd>example.com.s3-website-us-east-1.amazonaws.com</kbd>). Origin ID will be set automatically after you provide Origin Domain Name. Leave Origin Path and Origin Custom Headers empty:</p>
<div><img class="image-border" height="247" src="img/bb6b5c32-c102-4a52-a760-87c071d2dd65.png" width="461"/></div>
<ol start="4">
<li>In Default Cache Behavior Settings, set the following parameters:
<ul>
<li>Viewer Protocol Policy as HTTP and HTTPS</li>
<li>Allowed HTTP Methods as all the HTTP verb options</li>
<li>Cached HTTP Methods with OPTIONS checked</li>
<li>Object Caching as Use Origin Cache Headers</li>
</ul>
</li>
</ol>
<div><img class="image-border" height="312" src="img/5da573aa-b0a2-416e-92ad-ab699a02b9c2.png" width="506"/></div>
<ol start="5">
<li>Leave the rest of the fields in this section with their default values, except the Compress Objects Automatically option. This feature is used to compress the files on demand using gzip. As already discussed in this chapter, Amazon S3 doesn't offer automatic compression, but CloudFront does. You just need to set the Yes option:</li>
</ol>
<div><img class="image-border" height="389" src="img/833ee269-532e-4313-be9e-b7a8b4d12c72.png" width="535"/></div>
<ol start="6">
<li>In Distribution Settings, set the following parameters:
<ul>
<li>Set Price Class with the option based on your target audience and the costs that you are willing to pay (better performance means higher costs).</li>
<li>Set Alternate Domain Names (CNAMEs) with the naked domain and the www domain, separated by commas.</li>
<li>In SSL Certificate, choose Default CloudFront Certificate (*.cloudfront.net). We will return to this option later once we issue our own certificate.</li>
<li>Leave the rest of the fields with their default values.</li>
</ul>
</li>
</ol>
<div><img class="image-border" height="292" src="img/38ebfe3e-0c14-4eae-b282-5f93e8870e13.png" width="521"/></div>
<ol start="7">
<li>Now click on Create Distribution.</li>
<li>CloudFront needs a few minutes to replicate the distribution configuration between all the edge points, but you can follow the status in the CloudFront console main page. After it finishes, it will display the status as Deployed. In the following screenshot, you can see the CloudFront distribution address. Copy this link to your browser and test to check whether it is working:</li>
</ol>
<div><img class="image-border" height="165" src="img/8dc3ce93-3830-4bed-90f7-a4f3fb61f141.png" width="733"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Handling CloudFront caching</h1>
                
            
            
                
<p>By default, CloudFront will cache all the files for 24 hours. It means that if you modify the files in your S3 bucket, you won't see any changes when browsing through a CloudFront distribution. Forcing a cache reset in your browser will not help because it's server-side caching. So, what's the recommended practice to handle caching? You have the following two options for this:</p>
<ol>
<li><strong>Server side</strong>: Create a cache invalidation request</li>
<li><strong>Client side</strong>: Add suffixes to all the static files when changing their contents</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Invalidating server-side cache</h1>
                
            
            
                
<p>Creating a cache invalidation request to CloudFront takes some minutes to process because CloudFront will need to contact all the edge locations and request them to clear their individual caches.</p>
<p>Since this will be a recurring task, I don't recommend that you use the CloudFront console. It's better to use the CLI. However, CLI support for CloudFront is currently only available at the preview stage. So you may need to enable it by running the following command:</p>
<pre>
<strong>    aws configure set preview.cloudfront true</strong>
</pre>
<p>To create a cache invalidation request for all files (path <kbd>/*</kbd>), execute the following command:</p>
<pre>
<strong>    aws cloudfront \<br/>      create-invalidation --distribution-id=DISTRIBUTION_ID --paths "/*"</strong>
</pre>
<p>You can find <kbd>DISTRIBUTION_ID</kbd> of your CloudFront distribution by looking at the CloudFront console or running the following CLI command:</p>
<pre>
<strong>    aws cloudfront list-distributions</strong>
</pre>
<p>You can add <kbd>--query DistributionList.Items[0].Id</kbd> to the previous command in order to only output the distribution ID of the first distribution.</p>
<p>This solution requires a long time for the invalidation to take effect, and it doesn't solve the problem of client-side caching.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Invalidating client-side cache</h1>
                
            
            
                
<p>When you browse a web page, the browser will cache all the files that were downloaded to display the page (HTML, CSS, JavaScript, images, and so on) and avoid requiring the same files in the near future to improve performance. However, if you modify the contents of one file, how can you tell the browser to dispose of the previous cached content? You can't create an invalidation request because this time the cache is at the client side, not server side, but you can force the browser to make a new request by changing the name of the file that has been modified. For example, you can change the file name from <kbd>script.js</kbd> to <kbd>script.v2.js</kbd> and use its new name inside the HTML page:</p>
<pre>
    &lt;script src="img/strong>"&gt;&lt;/script&gt;
</pre>
<p>Another option is to add a query string to the filename when declaring it inside the HTML page, such as the following:</p>
<pre>
    &lt;script src="img/strong>"&gt;&lt;/script&gt;
</pre>
<p>In this example, the filename was not changed, but the reference was changed, and this is enough for the browser to consider the previous cache invalid and make a new request to get the updated content.</p>
<p>The problem of both strategies is that you can't cache the HTML page. All other data can be cached, except the HTML page. Otherwise, the client will not understand that it should download a newer version of a file dependency.</p>
<p>To upload HTML files in such a way that they will never be cached, you must set the <kbd>Cache-Control</kbd> header to <kbd>no-cache</kbd> when uploading the files. In our website, after syncing a local folder with your bucket, upload the <kbd>index.html</kbd> file again, but this time, use the <kbd>cp</kbd> (copy) command and add the <kbd>Cache-Control</kbd> header:</p>
<pre>
<strong>    aws s3 cp index.html s3://my-bucket-name \<br/>      --cache-control no-cache --acl public-read</strong>
</pre>
<p>This strategy works pretty well, but it requires you to automate your build process to change the filenames or query string parameters for all changed files. In the next chapter, we are going to build a React app using the "Create React App" tool. Thankfully, this tool is already configured to change the filenames of all the deployments. It adds random strings, as in the <kbd>main.12657c03.js</kbd> file.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Updating Route 53 to use CloudFront</h1>
                
            
            
                
<p>Our current record sets use S3 buckets. You just need to go back to Route 53 and replace it with the new CloudFront distribution. For the naked domain, which uses the A record type, you need to select the Alias option as Yes and the CloudFront distribution in the drop-down menu.</p>
<p>For the <kbd>www</kbd> domain, which uses the CNAME record type, select the Alias option as No. In this case, copy the CloudFront distribution address available in the A record and paste it into the box of the CNAME record.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Supporting HTTPS</h1>
                
            
            
                
<p>Unfortunately, Amazon S3 does not support HTTPS connections, it only supports HTTP. We have set the Route 53 record sets to use a CloudFront distribution, but we haven't enabled support to HTTPS in CloudFront yet.</p>
<p>But why should we support HTTPS? There are many reasons nowadays. Let's list some of them:</p>
<ul>
<li>We are building an online store. We need to handle logins and payment transactions. Doing such things without an encrypted connection is not safe. It's too easy to eavesdrop the network and steal sensitive data.</li>
<li>HTTP/2 is the newest protocol and is much faster than the old HTTP/1.1 version. Currently, <em>all</em> major browsers that support HTTP/2 <em>require</em> HTTPS. It is not possible to support HTTP/2 over an unencrypted HTTP connection.</li>
<li>HTTP/2 with encryption is faster than HTTP/1.1 without encryption. Troy Hunt shows an interesting demo at this link: <a href="https://www.troyhunt.com/i-wanna-go-fast-https-massive-speed-advantage">https://www.troyhunt.com/i-wanna-go-fast-https-massive-speed-advantage</a>. In his test, loading a website with hundreds of small files was 80 percent faster with HTTP/2 over TLS due to the multiplexing feature of the newer protocol.</li>
<li>Another good reason is privacy. Using HTTPS everywhere helps to keep your browsing data safe. It's not enough because the domain names of the sites that you visit will continue to be exposed, but it helps a lot. The pages that you visit and the things that you read or write will not be (easily) compromised because data will always be transferred with encryption.</li>
</ul>
<p>If you are convinced and want to support HTTPS, follow these steps:</p>
<ol>
<li>Create a mail exchange record in Route 53.</li>
<li>Request a free SSL/TLS certificate to AWS.</li>
<li>Edit the CloudFront distribution to use this new certificate.</li>
</ol>
<p>The first step, to create a mail account, is necessary because AWS will only issue a free SSL/TLS certificate if you prove that you own the domain, and this verification is done by following a link sent to the <kbd>admin@example.com</kbd> e-mail address.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a mail exchange record</h1>
                
            
            
                
<p>We need a service that will handle e-mail messages before we request our free certificate to AWS. I suggest Zoho Mail as a free option (up to 5 GB space). In this section, we will see how to configure this service by performing the following steps:</p>
<ol>
<li>First, browse <a href="http://www.zoho.com/mail">www.zoho.com/mail</a> and sign up for a free <em>business</em> e-mail account. This account will be associated with your domain. When selecting the administrator account, choose the name <kbd>admin</kbd>. This name is important because AWS will check your domain ownership by sending a confirmation e-mail to <kbd>admin@example.com</kbd>.</li>
<li>After you create your account, you will be requested to confirm the ownership of the associated domain. There are a few options to prove your ownership, and I prefer using the CNAME method. In the Select your domain's DNS Manager (DNS Hosting provider) from the list option, choose Others.. because AWS is not listed. Now, select CNAME Method, and the CNAME and Destination will be presented. You need to configure a new temporary Route 53 record set with this pair and finish clicking on the Proceed to CNAME Verification button:</li>
</ol>
<div><img class="image-border" height="463" src="img/672b939a-0210-47a1-8ff1-2d0ef765a1f7.png" width="700"/></div>
<ol start="3">
<li>After verifying, confirm the creation of the <kbd>admin</kbd> account. You can add in the sequence other users.</li>
</ol>
<ol start="4">
<li>The next step is to configure MX (mail exchange) records in Route 53. Copy the values that are presented by Zoho:</li>
</ol>
<div><img class="image-border" height="338" src="img/0bc0c885-3afb-4f7c-b02b-160293bfe2df.png" width="700"/></div>
<ol start="5">
<li>Go back to Route 53. Delete the CNAME record set that was created to verify your Zoho account because it is no longer necessary. Now you need to create a new record set of the type MX using the values from the preceding screenshot:</li>
</ol>
<div><img class="image-border" height="434" src="img/ca7e6731-96b1-4121-bb0f-661c3fc49d49.png" width="385"/></div>
<ol start="6">
<li>We are done. You can test whether it is working correctly by sending an e-mail to this new address and by checking your Zoho e-mail account for received e-mails.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Requesting free certificates with AWS Certificate Manager</h1>
                
            
            
                
<p>Let's see how to request free certificates with AWS Certificate Manager by performing the following steps:</p>
<ol>
<li>Request a TLS certificate at <a href="https://console.aws.amazon.com/acm/home?region=us-east-1">https://console.aws.amazon.com/acm/home?region=us-east-1</a>.</li>
</ol>
<p>You need to be on us-east-1 because CloudFront only uses certificates from this region.</p>
<ol start="2">
<li>On the welcome screen, click on Get started. In the next screen, type your naked domain name and the www version and click on Review and request:</li>
</ol>
<div><img class="image-border" height="511" src="img/0b0a779c-f324-4ae8-a5c0-82eed3a82a93.png" width="750"/></div>
<ol start="3">
<li>In the following screen, you just need to click on Confirm and request. Amazon will try to prove your domain ownership by sending an e-mail to <kbd>admin@example.com</kbd>. If you have configured your e-mail account correctly in the previous section, you will receive an e-mail in your Zoho inbox.</li>
<li>The e-mail has a confirmation link that you must click on to prove your ownership. After that, Amazon will issue a new TLS certificate that will be available for your account.</li>
</ol>
<p>You don't need to worry about the certificate expiration date. AWS will renew it automatically when necessary.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring CloudFront to support HTTPS connections</h1>
                
            
            
                
<p>The last step to support HTTPS is to edit the CloudFront distribution to use the new certificate. To perform this task, take a look at the following steps:</p>
<ol>
<li>Browse the CloudFront Management Console at <a href="https://console.aws.amazon.com/cloudfront">https://console.aws.amazon.com/cloudfront</a> and open your distribution.</li>
<li>Under the General tab, click on the Edit option.</li>
</ol>
<ol start="3">
<li>Click the Custom SSL Certificate (example.com): option and select your domain certificate using the drop-down button:</li>
</ol>
<div><img class="image-border" height="480" src="img/169b6130-fe64-4b84-86ab-a5e244a20945.png" width="600"/></div>
<ol start="4">
<li>Save to return to the previous page, then click on the third tab, Behaviors, and click on Edit to edit the existing behavior.</li>
</ol>
<p> </p>
<ol start="5">
<li>Now we can change the Viewer Protocol Policy parameter to Redirect HTTP to HTTPS to force the users to always use HTTPS:</li>
</ol>
<div><img class="image-border" height="347" src="img/b3be0f11-99cb-4981-a1b5-e6304abc7ce2.png" width="457"/></div>
<ol start="6">
<li>After changing these settings, CloudFront will automatically deploy the new configuration to all the edge nodes.</li>
<li>After waiting for some minutes, you can browse your domain to confirm HTTPS support.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, you learned how to configure S3, CloudFront, and Route 53 to host a serverless frontend solution. Now you have your site distributed all around the world for reduced latency and increased speed along with HTTPS support to make the Web more safe and private.</p>
<p>In the next chapter, we are going to build the frontend of our serverless store application using React as an SPA.</p>


            

            
        
    </body></html>