- en: Designing for Failure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Employing proper timeouts and retries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing backpressure and rate limiting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling faults
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resubmitting fault events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing idempotence with an inverse OpLock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing idempotence with Event Sourcing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing failure is the cornerstone of cloud-native. We build autonomous services
    that limit the blast radius when they do fail and continue to operate when other
    services fail. We decouple deployment from release, and we control the batch size
    of each deployment so that we can easily identify the problem when a deployment
    does go wrong. We shift testing to the left into the continuous deployment pipeline
    to catch issues before a deployment, as well as all the way to the right into
    production, where we continuously test the system and alert on anomalies to minimize
    the meantime to recovery. The recipes in this chapter demonstrate how to design
    a service to be resilient and forgiving in the face of failure so that transient
    failures are properly handled, their impact is minimized, and the service can
    self-heal.
  prefs: []
  type: TYPE_NORMAL
- en: Employing proper timeouts and retries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reality of computer networks is that they are unreliable. The reality of
    cloud computing is that it relies on computer networks, therefore it is imperative
    that we implement services to properly handle network anomalies. This recipe demonstrates
    how to properly configure functions and SDK calls with the appropriate timeouts
    and retries.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-timeout-retry` directory with `cd cncb-timeout-retry`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with ` npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following command function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Network hiccups** can happen at any time. Where one request may not go through,
    the next request may go through just fine; therefore, we should have short timeouts
    so that we can retry the process as soon as possible, but not so short that a
    good request times out before it has a chance to complete normally. We must also
    ensure that our timeout and retry cycle has enough time to complete before the
    function times out. The `aws-sdk` is configured by default to time out after two
    minutes and performs three retries with an increasing delay time. Of course, two
    minutes is too long. Setting the `timeout` to 1000 (1 second) will typically be
    long enough for a request to complete and allow for three retries to complete
    before a function timeout of 6 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: If requests time out too frequently then this could be an indication that the
    function has been allocated with too few resources. For example, there is a correlation
    between the `memorySize` allocated to a function and the machine instance size
    that is used. Smaller machine instances also have less network I/O capacity, which
    could lead to more frequent network hiccups. Thus, increasing `memorySize` will
    decrease network volatility.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing backpressure and rate limiting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The various services in a cloud-native system must be able to handle the ebb
    and flow of traffic through the system. Upstream services should never overload
    downstream services, and downstream services must be able to handle peak loads
    without falling behind or overloading services further downstream. This recipe
    shows how to leverage the natural backpressure of stream processors and implement
    additional rate limiting to manage throttling.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need an AWS Kinesis Stream, such as the
    one created in the *Creating an event stream* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-backpressure-ratelimit` directory with `cd cncb-backpressure-ratelimit`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the `simulate` function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following `trigger` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Backpressure* is a critical characteristic of a well-implemented stream processor.
    When using the *imperative programming paradigm*, such as looping over the array
    of records in a batch, the downstream target system could easily be overwhelmed
    because the loop will process the records as fast as it can without regard for
    the throughput capacity of the target system. Alternatively, the **Functional
    Reactive Programming** (**FRP**) paradigm, with a library such as Highland.js
    ([https://highlandjs.org](https://highlandjs.org)) or RxJS ([https://github.com/ReactiveX/rxjs](https://github.com/ReactiveX/rxjs)),
    provides a natural backpressure, because data is pulled downstream only as fast
    as the downstream steps are able to complete their tasks. For example, an external
    system that has low throughput will only pull data as quickly as its capacity
    will allow.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, highly-scalable systems, such as DynamoDB or Kinesis, that
    are able to process data with extremely high throughput rely on throttling to
    restrict capacity. In this case, the natural backpressure of FRP is not enough;
    additional use of the `ratelimit` feature is required. As an example, when I was
    writing this recipe I ran a simulation without rate limiting and then went out
    to dinner. When I came back several hours later, the events generated by the simulation
    were still trying to process. This is because DynamoDB was throttling the requests
    and the exponential backoff and retry built into the `aws-sdk` was taking up too
    much time, leading the function to `timeout` and retry the whole batch again.
    This demonstrates that while retries, as discussed in the *Employing proper timeouts
    and retries* recipe, are important for synchronous requests, they cannot be solely
    relied upon for asynchronous stream processing. Instead, we need to proactively
    limit the rate of flow to avoid throttling and exponential backoff to help ensure
    a batch completes within the function timeout.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we use a simple algorithm to calculate the rate of flow—`WRITE_CAPACITY
    / SHARD_COUNT / 10` per every `100` milliseconds. This ensures that DynamoDB will
    not receive more requests per second than have been allocated. I also used a simple
    algorithm to determine the batch size—`batchSize / (timeout / 2) < WRITE_CAPACITY`.
    This ensures that there should be plenty of time to complete the batch under normal
    conditions, but there will be twice the necessary time available in case there
    is throttling. Note that this is just a logical starting point; this is the area
    where performance tuning in cloud-native systems should be focused. The characteristics
    of your data and target systems will dictate the most effective settings. As we
    will see in the *Autoscaling DynamoDB* recipe, autoscaling adds yet another dimension
    to backpressure, rate limiting, and performance tuning. Regardless, you can start
    with these simple and safe algorithms and tune them over time.
  prefs: []
  type: TYPE_NORMAL
- en: Handling faults
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stream processors are naturally resilient and forgiving of transient errors
    because they employ backpressure and automatically retry failing batches. However,
    hard errors, if not handled properly, can cause a traffic jam that results in
    dropped messages. This recipe will show you how to delegate these errors as fault
    events so that good messages can continue to flow.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-handling-faults` directory with `cd cncb-handling-faults`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the file named `serverless.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the `simulate` function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following `trigger` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe implements the *Stream Circuit Breaker* pattern that I discuss in
    depth in my book, *Cloud Native Development Patterns and Best Practices* ([https://www.packtpub.com/application-development/cloud-native-development-patterns-and-best-practices](https://www.packtpub.com/application-development/cloud-native-development-patterns-and-best-practices)).
    Stream processors that experience hard errors will continue to reprocess those
    events until they expire from the stream—unless the stream processor is implemented
    to set these errors aside by delegating them as fault events for processing elsewhere,
    such as described in the *Resubmitting fault events* recipe. This alleviates the
    traffic jam so that other events can continue to flow.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe simulates events that will fail at different stages in the stream
    processor. Some events simulate upstream bugs that will fail the validation logic
    that asserts that events are being created properly upstream. Other events will
    fail when they are inserted into DynamoDB. The logic also randomly fails some
    events to simulate transient errors that do not produce faults and will automatically
    be retried. In the logs, you will see two fault events published. When a random
    error is generated, you will see in the logs that the function retries the batch.
    If the simulation does not raise a random error then you should rerun it until
    it does.
  prefs: []
  type: TYPE_NORMAL
- en: To isolate events in a stream, we need to introduce the concept of a **unit
    of work** (**UOW**) that groups one or more events from the batch into an atomic
    unit that must succeed or fail together. The UOW contains the original Kinesis
    record (`uow.record`), the event parsed from the record (`uow.event`), and any
    intermediate processing results (`uow.params`) that are attached to the UOW as
    it passes through the stream processor. The UOW is also used to identify errors
    as handled or unhandled. When an expected error is identified by the validation
    logic or caught from external calls, the UOW is adorned to the error and the error
    is re-thrown. The adorned unit of work (`error.uow`) acts as the indicator to
    the `errors` handler that the error was handled in the stream processor and that
    it should `publish` the error as a fault event. Unexpected errors, such as randomly-generated
    errors, are not handled by the stream processor logic and thus will not have an
    adorned UOW. The error handler will `push` these errors downstream to cause the
    function to fail and retry. In the *Creating alerts* recipe, we discuss monitoring
    for fault events, as well as iterator age, so that the team can receive timely
    notifications about stream processor errors.
  prefs: []
  type: TYPE_NORMAL
- en: Resubmitting fault events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have designed our stream processors to delegate errors as fault events so
    that valid events can continue to flow. We monitor for and alert on fault events
    so that appropriate action can be taken to address the root cause. Once the problem
    is resolved, it may be necessary to reprocess the failed events. This recipe demonstrates
    how to resubmit fault events back to the stream processor that raised the fault.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the `monitor`, `simulator`, and `cli` projects from the following templates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Review the file named `serverless.yml` in the `cncb-resubmitting-faults-monitor`
    and `cncb-resubmitting-faults-simulator` directories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the `monitor` and `simulator` stacks, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Review the stacks and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the simulator from the `cncb-resubmitting-faults-simulator` directory,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Confirm that a fault file is written to the bucket specified in the monitor
    stack output `cncb-resubmitting-faults-monitor-*-bucket-*` and note the path.
    If no fault was generated, run the simulator again until one is generated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `./cli/lib/resubmit.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Resubmit the fault with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Empty the `cncb-resubmitting-faults-monitor-*` bucket manually and remove the
    `monitor` and `simulator` stacks once you are finished with `npm run rm:lcl --
    -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Handling faults* recipe, we saw how stream processors delegate hard
    errors by publishing fault events with all the data pertaining to the unit of
    work that failed. In this recipe, a fault monitor consumes these fault events
    and stores them in an S3 bucket. This enables the team to review the specific
    fault to help determine the root cause of the problem. The fault contains the
    specific exception that was caught, the event that failed, and all of the contextual
    information that was attached to the unit of work.
  prefs: []
  type: TYPE_NORMAL
- en: Once the root cause has been addressed, the original event can be submitted
    back to the stream processor that published the fault. This is possible because
    the fault event contains the original Kinesis record (`event.uow.record`) and
    the name of the function to invoke (`event.tags.functionName`). The command line
    utility reads all the fault events from the bucket for the specified path and
    invokes the specific functions. From the perspective of the function logic, this
    direct invocation of the function is no different to it being invoked directly
    from the Kinesis stream. However, the stream processor must be designed to be
    idempotent and to be able to handle events out of order, as we will discuss in
    the *Implementing idempotence with an inverse OpLock* and *Implementing idempotence
    with Event Sourcing* recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing idempotence with an inverse OpLock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a business rule standpoint, it is important that events are processed exactly
    once; otherwise, problems may arise, such as double counting or not counting at
    all. However, our cloud-native systems must be resilient to failure and proactively
    retry to ensure no messages are dropped. Unfortunately, this means that messages
    may be delivered multiple times, for example when a producer re-publishes an event
    or a stream processor retries a batch that may have been partially processed.
    The solution to this problem is to implement all actions to be idempotent. This
    recipe implements idempotency with what I refer to as an inverse OpLock.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-idempotence-inverse-oplock` directory with `cd cncb-idempotence-inverse-oplock`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the file named `serverless.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the simulate function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following `listener` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional optimistic locking prevents multiple users from updating the same
    record at the same time. A record is only updated if the `oplock` field has not
    changed since the user retrieved the data. If the data has changed then an exception
    is thrown and the user is forced to retrieve the data again before proceeding
    with the update. This forces the updates to be performed sequentially, and it
    requires human interaction to resolve any potential conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: The **inverse OpLock** is designed to provide idempotency for asynchronous processing.
    Instead of forcing the transaction to retry, we simply do the opposite—we drop
    the older or duplicate event. A traditional OpLock may be used in the upstream
    Backend For Frontend service to sequence user transactions, where, downstream
    services implement an inverse OpLock to ensure that older or duplicate events
    do not overwrite the most recent data. In this recipe, we use the `uow.event.timestamp`
    as the `oplock` value. In some scenarios, it may be preferential to use the sequence
    number if multiple events happen at the exact same millisecond. `ConditionalCheckFailedException`
    is caught and ignored. All other exceptions are re-thrown with the unit of work
    attached to cause a fault event to be published, as discussed in the *Handling
    faults* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The simulation in this recipe publishes a `thing-created` event, publishes it
    again, and then publishes a `thing-updated` event followed by the `thing-created`
    event a third time. The logs show that the `thing-created` event is only processed
    once and the duplicates are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing idempotence with Event Sourcing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a business rule standpoint, it is important that events are processed exactly
    once; otherwise, problems may arise, such as double counting or not counting at
    all. However, our cloud-native systems must be resilient to failure and proactively
    retry to ensure no messages are dropped. Unfortunately, this means that messages
    may be delivered multiple times, such as when a producer re-publishes an event
    or a stream processor retries a batch that may have been partially processed.
    The solution to this problem is to implement all actions to be idempotent. This
    recipe demonstrates how to use Event Sourcing and a micro event store to implement
    idempotence.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-idempotence-es` directory with `cd cncb-idempotence-es`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the file named `serverless.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the simulate function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following `trigger` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Event Sourcing facilitates idempotence because events are immutable. The same
    event with the same unique ID can be published or processed multiple times with
    the same outcome. The micro event store serves as a buffer that weeds out duplicates.
    The service consumes desired events and stores them in a micro event store with
    a **hashkey** that groups related events, such as the `uow.event.thing.id` of
    the domain object, and a range key based on the `uow.event.id`. This primary key
    is also immutable. As a result, the same event can be saved multiple times, but
    only a single event is produced on the database stream. Thus, the business logic,
    which is discussed in the *Creating a micro event store* or *Implementing an analytics
    BFF* recipe, is only triggered once.
  prefs: []
  type: TYPE_NORMAL
- en: The simulation in this recipe publishes a `thing-created` event, publishes it
    again, and then publishes a `thing-updated` event followed by the `thing-created`
    event a third time. The logs show that the three `thing-created` event instances
    only result in a single event on the DynamoDB Stream.
  prefs: []
  type: TYPE_NORMAL
