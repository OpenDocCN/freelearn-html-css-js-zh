- en: Designing for Failure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为失败而设计
- en: 'In this chapter, the following recipes will be covered:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将涵盖以下食谱：
- en: Employing proper timeouts and retries
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适当的超时和重试
- en: Implementing backpressure and rate limiting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现背压和速率限制
- en: Handling faults
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理故障
- en: Resubmitting fault events
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新提交故障事件
- en: Implementing idempotence with an inverse OpLock
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反向 OpLock 实现幂等性
- en: Implementing idempotence with Event Sourcing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用事件溯源实现幂等性
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Managing failure is the cornerstone of cloud-native. We build autonomous services
    that limit the blast radius when they do fail and continue to operate when other
    services fail. We decouple deployment from release, and we control the batch size
    of each deployment so that we can easily identify the problem when a deployment
    does go wrong. We shift testing to the left into the continuous deployment pipeline
    to catch issues before a deployment, as well as all the way to the right into
    production, where we continuously test the system and alert on anomalies to minimize
    the meantime to recovery. The recipes in this chapter demonstrate how to design
    a service to be resilient and forgiving in the face of failure so that transient
    failures are properly handled, their impact is minimized, and the service can
    self-heal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 处理失败是云原生的基础。我们构建自主服务，当它们失败时限制爆炸半径，并在其他服务失败时继续运行。我们将部署与发布解耦，并控制每次部署的批量大小，以便在部署出错时容易识别问题。我们将测试左移到持续部署管道中，以在部署之前捕获问题，以及一直移到生产环境中，在那里我们持续测试系统并在异常情况下发出警报，以最小化恢复时间。本章中的食谱演示了如何设计一个在失败面前具有弹性和宽容性的服务，以便正确处理瞬态故障，最小化其影响，并使服务能够自我修复。
- en: Employing proper timeouts and retries
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用适当的超时和重试
- en: The reality of computer networks is that they are unreliable. The reality of
    cloud computing is that it relies on computer networks, therefore it is imperative
    that we implement services to properly handle network anomalies. This recipe demonstrates
    how to properly configure functions and SDK calls with the appropriate timeouts
    and retries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机网络的现实是它们不可靠。云计算的现实是它依赖于计算机网络，因此我们必须实施服务来正确处理网络异常。这个食谱演示了如何正确配置函数和 SDK 调用，使用适当的超时和重试。
- en: How to do it...
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Create the project from the following template:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Navigate to the `cncb-timeout-retry` directory with `cd cncb-timeout-retry`.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-timeout-retry` 切换到 `cncb-timeout-retry` 目录。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `serverless.yml` 的文件，其内容如下：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `handler.js` 的文件，其内容如下：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Install the dependencies with `npm install`.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖项。
- en: Run the tests with ` npm test`.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看在 `.serverless` 目录中生成的内容。
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm run dp:lcl -- -s $MY_STAGE` 部署堆栈。
- en: Review the stack and resources in the AWS Console.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看AWS控制台中的堆栈和资源。
- en: 'Invoke the function with the following command:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令调用函数：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Take a look at the following command function logs:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下命令函数日志：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用 `npm run rm:lcl -- -s $MY_STAGE` 删除堆栈。
- en: How it works...
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '**Network hiccups** can happen at any time. Where one request may not go through,
    the next request may go through just fine; therefore, we should have short timeouts
    so that we can retry the process as soon as possible, but not so short that a
    good request times out before it has a chance to complete normally. We must also
    ensure that our timeout and retry cycle has enough time to complete before the
    function times out. The `aws-sdk` is configured by default to time out after two
    minutes and performs three retries with an increasing delay time. Of course, two
    minutes is too long. Setting the `timeout` to 1000 (1 second) will typically be
    long enough for a request to complete and allow for three retries to complete
    before a function timeout of 6 seconds.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络中断**可能随时发生。一个请求可能无法通过，而下一个请求可能顺利通过；因此，我们应该设置较短的超时时间，以便我们能够尽快重试过程，但又不至于太短，以至于一个良好的请求在正常完成之前就超时了。我们还必须确保我们的超时和重试周期在函数超时之前有足够的时间完成。默认情况下，`aws-sdk`
    配置为两分钟后超时，并执行三次带有递增延迟时间的重试。当然，两分钟太长了。将 `timeout` 设置为 1000（1 秒）通常足以让请求完成，并允许在 6
    秒的函数超时之前完成三次重试。'
- en: If requests time out too frequently then this could be an indication that the
    function has been allocated with too few resources. For example, there is a correlation
    between the `memorySize` allocated to a function and the machine instance size
    that is used. Smaller machine instances also have less network I/O capacity, which
    could lead to more frequent network hiccups. Thus, increasing `memorySize` will
    decrease network volatility.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果请求频繁超时，这可能表明函数分配的资源过少。例如，函数分配的 `memorySize` 与使用的机器实例大小之间存在关联。较小的机器实例也有更少的网络
    I/O 容量，这可能导致更频繁的网络中断。因此，增加 `memorySize` 将降低网络波动性。
- en: Implementing backpressure and rate limiting
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现背压和速率限制
- en: The various services in a cloud-native system must be able to handle the ebb
    and flow of traffic through the system. Upstream services should never overload
    downstream services, and downstream services must be able to handle peak loads
    without falling behind or overloading services further downstream. This recipe
    shows how to leverage the natural backpressure of stream processors and implement
    additional rate limiting to manage throttling.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生系统中的各种服务必须能够处理系统中的流量起伏。上游服务不应过载下游服务，下游服务必须能够处理峰值负载而不会落后或过载更下游的服务。本食谱展示了如何利用流处理器的自然背压并实现额外的速率限制来管理节流。
- en: Getting ready
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Before starting this recipe, you will need an AWS Kinesis Stream, such as the
    one created in the *Creating an event stream* recipe.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始此食谱之前，您需要一个 AWS Kinesis Stream，例如在 *创建事件流* 食谱中创建的那个。
- en: How to do it...
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Navigate to the `cncb-backpressure-ratelimit` directory with `cd cncb-backpressure-ratelimit`.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令导航到 `cncb-backpressure-ratelimit` 目录：`cd cncb-backpressure-ratelimit`。
- en: 'Review the file named `serverless.yml` with the following content:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `serverless.yml` 的文件，其内容如下：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `handler.js` 的文件，其内容如下：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Install the dependencies with `npm install`.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装依赖项：`npm install`。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行测试：`npm test -- -s $MY_STAGE`。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看在 `.serverless` 目录中生成的内容。
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令部署堆栈：`npm run dp:lcl -- -s $MY_STAGE`。
- en: Review the stack and resources in the AWS Console.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中查看堆栈和资源。
- en: 'Invoke the `simulate` function with the following command:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令调用 `simulate` 函数：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Take a look at the following `trigger` function logs:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下 `trigger` 函数日志：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用以下命令删除堆栈：`npm run rm:lcl -- -s $MY_STAGE`。
- en: How it works...
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '*Backpressure* is a critical characteristic of a well-implemented stream processor.
    When using the *imperative programming paradigm*, such as looping over the array
    of records in a batch, the downstream target system could easily be overwhelmed
    because the loop will process the records as fast as it can without regard for
    the throughput capacity of the target system. Alternatively, the **Functional
    Reactive Programming** (**FRP**) paradigm, with a library such as Highland.js
    ([https://highlandjs.org](https://highlandjs.org)) or RxJS ([https://github.com/ReactiveX/rxjs](https://github.com/ReactiveX/rxjs)),
    provides a natural backpressure, because data is pulled downstream only as fast
    as the downstream steps are able to complete their tasks. For example, an external
    system that has low throughput will only pull data as quickly as its capacity
    will allow.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*背压* 是实现良好的流处理器的关键特性。当使用 *命令式编程范式*，例如在批处理记录数组中循环时，下游目标系统可能会轻易过载，因为循环会尽可能快地处理记录，而不考虑目标系统的吞吐量能力。另一方面，**函数式响应式编程**（**FRP**）范式，例如使用
    Highland.js ([https://highlandjs.org](https://highlandjs.org)) 或 RxJS ([https://github.com/ReactiveX/rxjs](https://github.com/ReactiveX/rxjs))
    库，提供了自然的背压，因为数据只以下游步骤能够完成任务的速度被拉取。例如，吞吐量低的系统只会以其容量允许的速度快速拉取数据。'
- en: On the other hand, highly-scalable systems, such as DynamoDB or Kinesis, that
    are able to process data with extremely high throughput rely on throttling to
    restrict capacity. In this case, the natural backpressure of FRP is not enough;
    additional use of the `ratelimit` feature is required. As an example, when I was
    writing this recipe I ran a simulation without rate limiting and then went out
    to dinner. When I came back several hours later, the events generated by the simulation
    were still trying to process. This is because DynamoDB was throttling the requests
    and the exponential backoff and retry built into the `aws-sdk` was taking up too
    much time, leading the function to `timeout` and retry the whole batch again.
    This demonstrates that while retries, as discussed in the *Employing proper timeouts
    and retries* recipe, are important for synchronous requests, they cannot be solely
    relied upon for asynchronous stream processing. Instead, we need to proactively
    limit the rate of flow to avoid throttling and exponential backoff to help ensure
    a batch completes within the function timeout.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，像 DynamoDB 或 Kinesis 这样高度可扩展的系统，能够以极高的吞吐量处理数据，它们依赖于节流来限制容量。在这种情况下，FRP 的自然背压不足以应对；需要额外使用
    `ratelimit` 功能。例如，当我编写这个菜谱时，我运行了一个没有速率限制的模拟，然后出去吃饭。当我几个小时后回来时，模拟生成的事件仍在尝试处理。这是因为
    DynamoDB 节流了请求，而 `aws-sdk` 内置的指数退避和重试消耗了太多时间，导致函数 `timeout` 并重新尝试整个批量。这表明，虽然重试，如
    *Employing proper timeouts and retries* 菜谱中讨论的那样，对于同步请求很重要，但不能仅依赖于异步流处理。相反，我们需要积极限制流量速率，以避免节流和指数退避，从而确保批量在函数超时内完成。
- en: In this recipe, we use a simple algorithm to calculate the rate of flow—`WRITE_CAPACITY
    / SHARD_COUNT / 10` per every `100` milliseconds. This ensures that DynamoDB will
    not receive more requests per second than have been allocated. I also used a simple
    algorithm to determine the batch size—`batchSize / (timeout / 2) < WRITE_CAPACITY`.
    This ensures that there should be plenty of time to complete the batch under normal
    conditions, but there will be twice the necessary time available in case there
    is throttling. Note that this is just a logical starting point; this is the area
    where performance tuning in cloud-native systems should be focused. The characteristics
    of your data and target systems will dictate the most effective settings. As we
    will see in the *Autoscaling DynamoDB* recipe, autoscaling adds yet another dimension
    to backpressure, rate limiting, and performance tuning. Regardless, you can start
    with these simple and safe algorithms and tune them over time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用一个简单的算法来计算流量速率——每 `100` 毫秒 `WRITE_CAPACITY / SHARD_COUNT / 10`。这确保了
    DynamoDB 每秒接收的请求数量不会超过已分配的数量。我还使用了一个简单的算法来确定批量大小——`batchSize / (timeout / 2) <
    WRITE_CAPACITY`。这确保了在正常情况下应该有足够的时间来完成批量，但在有节流的情况下，将会有两倍于必要的时间。请注意，这只是一个逻辑起点；这是云原生系统中性能调优应该关注的领域。你的数据和目标系统的特性将决定最有效的设置。正如我们将在
    *Autoscaling DynamoDB* 菜谱中看到的那样，自动扩展为背压、速率限制和性能调优增加了另一个维度。无论如何，你可以从这些简单且安全的算法开始，并在一段时间内调整它们。
- en: Handling faults
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理故障
- en: Stream processors are naturally resilient and forgiving of transient errors
    because they employ backpressure and automatically retry failing batches. However,
    hard errors, if not handled properly, can cause a traffic jam that results in
    dropped messages. This recipe will show you how to delegate these errors as fault
    events so that good messages can continue to flow.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理器对瞬时错误具有天然的抗性和宽容性，因为它们使用背压并自动重试失败的批量。然而，如果处理不当，硬错误可能会导致交通堵塞，导致消息丢失。这个菜谱将向你展示如何将这些错误委派为故障事件，以便良好的消息可以继续流动。
- en: How to do it...
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the project from the following template:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Navigate to the `cncb-handling-faults` directory with `cd cncb-handling-faults`.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-handling-faults` 命令进入 `cncb-handling-faults` 目录。
- en: Review the file named `serverless.yml`.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查名为 `serverless.yml` 的文件。
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查包含以下内容的 `handler.js` 文件：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Install the dependencies with `npm install`.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `.serverless` 目录中生成的内容。
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm run dp:lcl -- -s $MY_STAGE` 部署堆栈。
- en: Review the stack and resources in the AWS Console.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中检查堆栈和资源。
- en: 'Invoke the `simulate` function with the following command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令调用`simulate`函数：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Take a look at the following `trigger` function logs:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下`trigger`函数日志：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，删除堆栈：`npm run rm:lcl -- -s $MY_STAGE`。
- en: How it works...
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This recipe implements the *Stream Circuit Breaker* pattern that I discuss in
    depth in my book, *Cloud Native Development Patterns and Best Practices* ([https://www.packtpub.com/application-development/cloud-native-development-patterns-and-best-practices](https://www.packtpub.com/application-development/cloud-native-development-patterns-and-best-practices)).
    Stream processors that experience hard errors will continue to reprocess those
    events until they expire from the stream—unless the stream processor is implemented
    to set these errors aside by delegating them as fault events for processing elsewhere,
    such as described in the *Resubmitting fault events* recipe. This alleviates the
    traffic jam so that other events can continue to flow.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱实现了我在我的书*云原生开发模式和最佳实践*（[https://www.packtpub.com/application-development/cloud-native-development-patterns-and-best-practices](https://www.packtpub.com/application-development/cloud-native-development-patterns-and-best-practices)）中深入讨论的*流断路器*模式。遇到硬错误的流处理器将继续重新处理这些事件，直到它们从流中过期——除非流处理器被实现为将这些错误放在一边，将它们作为故障事件委托给其他地方处理，如*重新提交故障事件*食谱中所述。这缓解了交通堵塞，以便其他事件可以继续流动。
- en: This recipe simulates events that will fail at different stages in the stream
    processor. Some events simulate upstream bugs that will fail the validation logic
    that asserts that events are being created properly upstream. Other events will
    fail when they are inserted into DynamoDB. The logic also randomly fails some
    events to simulate transient errors that do not produce faults and will automatically
    be retried. In the logs, you will see two fault events published. When a random
    error is generated, you will see in the logs that the function retries the batch.
    If the simulation does not raise a random error then you should rerun it until
    it does.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱模拟了在流处理器中不同阶段可能失败的事件。一些事件模拟了上游的bug，这些bug将导致验证逻辑失败，该逻辑断言事件正在上游正确创建。其他事件在它们被插入到DynamoDB时将失败。逻辑还会随机失败一些事件来模拟不会产生故障的短暂错误，这些错误将自动重试。在日志中，您将看到发布了两条故障事件。当生成随机错误时，您将在日志中看到函数正在重试批次。如果模拟没有生成随机错误，那么您应该重新运行它，直到它生成错误。
- en: To isolate events in a stream, we need to introduce the concept of a **unit
    of work** (**UOW**) that groups one or more events from the batch into an atomic
    unit that must succeed or fail together. The UOW contains the original Kinesis
    record (`uow.record`), the event parsed from the record (`uow.event`), and any
    intermediate processing results (`uow.params`) that are attached to the UOW as
    it passes through the stream processor. The UOW is also used to identify errors
    as handled or unhandled. When an expected error is identified by the validation
    logic or caught from external calls, the UOW is adorned to the error and the error
    is re-thrown. The adorned unit of work (`error.uow`) acts as the indicator to
    the `errors` handler that the error was handled in the stream processor and that
    it should `publish` the error as a fault event. Unexpected errors, such as randomly-generated
    errors, are not handled by the stream processor logic and thus will not have an
    adorned UOW. The error handler will `push` these errors downstream to cause the
    function to fail and retry. In the *Creating alerts* recipe, we discuss monitoring
    for fault events, as well as iterator age, so that the team can receive timely
    notifications about stream processor errors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了隔离流中的事件，我们需要引入**工作单元**（**UOW**）的概念，它将一批中的一个或多个事件组合成一个原子单元，这个单元必须一起成功或失败。UOW包含原始的Kinesis记录（`uow.record`）、从记录中解析的事件（`uow.event`）以及任何附加到UOW的中间处理结果（`uow.params`），这些结果在它通过流处理器时被附加。UOW还用于标识错误是否被处理或未处理。当验证逻辑识别出预期的错误或从外部调用中捕获到错误时，UOW将被装饰到错误上，并且错误将被重新抛出。装饰的工作单元（`error.uow`）作为指示器，告知`errors`处理器错误已在流处理器中被处理，并且应该`发布`错误作为故障事件。意外的错误，如随机生成的错误，不会被流处理器逻辑处理，因此不会有装饰的UOW。错误处理器将`推送`这些错误到下游，导致函数失败并重试。在*创建警报*食谱中，我们讨论了对故障事件以及迭代器年龄的监控，以便团队可以及时收到关于流处理器错误的通知。
- en: Resubmitting fault events
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新提交故障事件
- en: We have designed our stream processors to delegate errors as fault events so
    that valid events can continue to flow. We monitor for and alert on fault events
    so that appropriate action can be taken to address the root cause. Once the problem
    is resolved, it may be necessary to reprocess the failed events. This recipe demonstrates
    how to resubmit fault events back to the stream processor that raised the fault.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设计我们的流处理器将错误作为故障事件进行委派，以便有效的事件可以继续流动。我们监控并警告故障事件，以便可以采取适当的行动来解决问题。一旦问题得到解决，可能需要重新处理失败的事件。本食谱演示了如何将故障事件重新提交给引发故障的流处理器。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create the `monitor`, `simulator`, and `cli` projects from the following templates:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建 `monitor`、`simulator` 和 `cli` 项目：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Review the file named `serverless.yml` in the `cncb-resubmitting-faults-monitor`
    and `cncb-resubmitting-faults-simulator` directories.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 `cncb-resubmitting-faults-monitor` 和 `cncb-resubmitting-faults-simulator`
    目录中的 `serverless.yml` 文件。
- en: 'Deploy the `monitor` and `simulator` stacks, as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式部署 `monitor` 和 `simulator` 堆栈：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Review the stacks and resources in the AWS Console.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中检查堆栈和资源。
- en: 'Run the simulator from the `cncb-resubmitting-faults-simulator` directory,
    as follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式从 `cncb-resubmitting-faults-simulator` 目录运行模拟器：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Confirm that a fault file is written to the bucket specified in the monitor
    stack output `cncb-resubmitting-faults-monitor-*-bucket-*` and note the path.
    If no fault was generated, run the simulator again until one is generated.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认故障文件已写入监控堆栈输出中指定的存储桶 `cncb-resubmitting-faults-monitor-*-bucket-*` 并注意路径。如果没有生成故障，请再次运行模拟器，直到生成故障。
- en: 'Review the file named `./cli/lib/resubmit.js` with the following content:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查名为 `./cli/lib/resubmit.js` 的文件，其内容如下：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Resubmit the fault with the following command:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令重新提交故障：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Empty the `cncb-resubmitting-faults-monitor-*` bucket manually and remove the
    `monitor` and `simulator` stacks once you are finished with `npm run rm:lcl --
    -s $MY_STAGE`.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动清空 `cncb-resubmitting-faults-monitor-*` 存储桶，并在完成 `npm run rm:lcl -- -s $MY_STAGE`
    后移除 `monitor` 和 `simulator` 堆栈。
- en: How it works...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In the *Handling faults* recipe, we saw how stream processors delegate hard
    errors by publishing fault events with all the data pertaining to the unit of
    work that failed. In this recipe, a fault monitor consumes these fault events
    and stores them in an S3 bucket. This enables the team to review the specific
    fault to help determine the root cause of the problem. The fault contains the
    specific exception that was caught, the event that failed, and all of the contextual
    information that was attached to the unit of work.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *处理故障* 食谱中，我们看到了流处理器如何通过发布包含失败工作单元所有相关数据的故障事件来委派硬错误。在本食谱中，故障监控器消费这些故障事件并将它们存储在
    S3 存储桶中。这使得团队能够审查特定的故障，以帮助确定问题的根本原因。故障包含捕获的具体异常、失败的事件以及附加到工作单元的所有上下文信息。
- en: Once the root cause has been addressed, the original event can be submitted
    back to the stream processor that published the fault. This is possible because
    the fault event contains the original Kinesis record (`event.uow.record`) and
    the name of the function to invoke (`event.tags.functionName`). The command line
    utility reads all the fault events from the bucket for the specified path and
    invokes the specific functions. From the perspective of the function logic, this
    direct invocation of the function is no different to it being invoked directly
    from the Kinesis stream. However, the stream processor must be designed to be
    idempotent and to be able to handle events out of order, as we will discuss in
    the *Implementing idempotence with an inverse OpLock* and *Implementing idempotence
    with Event Sourcing* recipes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦解决了根本原因，原始事件可以提交回发布故障的流处理器。这是可能的，因为故障事件包含原始 Kinesis 记录 (`event.uow.record`)
    和要调用的函数名称 (`event.tags.functionName`)。命令行实用程序从指定路径的存储桶中读取所有故障事件并调用特定函数。从函数逻辑的角度来看，这种直接调用函数与直接从
    Kinesis 流调用没有区别。然而，流处理器必须设计为幂等的，并且能够处理乱序事件，正如我们将在 *使用逆 OpLock 实现幂等性* 和 *使用事件溯源实现幂等性*
    食谱中讨论的那样。
- en: Implementing idempotence with an inverse OpLock
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逆 OpLock 实现幂等性
- en: From a business rule standpoint, it is important that events are processed exactly
    once; otherwise, problems may arise, such as double counting or not counting at
    all. However, our cloud-native systems must be resilient to failure and proactively
    retry to ensure no messages are dropped. Unfortunately, this means that messages
    may be delivered multiple times, for example when a producer re-publishes an event
    or a stream processor retries a batch that may have been partially processed.
    The solution to this problem is to implement all actions to be idempotent. This
    recipe implements idempotency with what I refer to as an inverse OpLock.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从业务规则的角度来看，事件必须恰好处理一次非常重要；否则，可能会出现问题，例如重复计数或根本不计数。然而，我们的云原生系统必须能够抵御故障并主动重试以确保不丢失任何消息。不幸的是，这意味着消息可能会被多次投递，例如当生产者重新发布事件或流处理器重试可能已部分处理的批次时。解决这个问题的方法是实现所有操作都具有幂等性。这个配方通过我所说的逆OpLock来实现幂等性。
- en: How to do it...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Create the project from the following template:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Navigate to the `cncb-idempotence-inverse-oplock` directory with `cd cncb-idempotence-inverse-oplock`.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cd cncb-idempotence-inverse-oplock`命令导航到`cncb-idempotence-inverse-oplock`目录。
- en: Review the file named `serverless.yml`.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查名为`serverless.yml`的文件。
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查名为`handler.js`的文件，其内容如下：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Install the dependencies with `npm install`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`npm install`命令安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`npm test -- -s $MY_STAGE`命令运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查`.serverless`目录中生成的内容。
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`npm run dp:lcl -- -s $MY_STAGE`命令部署栈。
- en: Review the stack and resources in the AWS Console.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS控制台中检查栈和资源。
- en: 'Invoke the simulate function with the following command:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令调用模拟函数：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Take a look at the following `listener` function logs:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下`listener`函数的日志：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用`npm run rm:lcl -- -s $MY_STAGE`命令删除栈。
- en: How it works...
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Traditional optimistic locking prevents multiple users from updating the same
    record at the same time. A record is only updated if the `oplock` field has not
    changed since the user retrieved the data. If the data has changed then an exception
    is thrown and the user is forced to retrieve the data again before proceeding
    with the update. This forces the updates to be performed sequentially, and it
    requires human interaction to resolve any potential conflicts.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的乐观锁定防止多个用户同时更新相同的记录。只有当用户检索数据后`oplock`字段没有变化时，记录才会被更新。如果数据已更改，则会抛出异常，并强制用户在继续更新之前重新检索数据。这迫使更新按顺序执行，并需要人工交互来解决任何潜在冲突。
- en: The **inverse OpLock** is designed to provide idempotency for asynchronous processing.
    Instead of forcing the transaction to retry, we simply do the opposite—we drop
    the older or duplicate event. A traditional OpLock may be used in the upstream
    Backend For Frontend service to sequence user transactions, where, downstream
    services implement an inverse OpLock to ensure that older or duplicate events
    do not overwrite the most recent data. In this recipe, we use the `uow.event.timestamp`
    as the `oplock` value. In some scenarios, it may be preferential to use the sequence
    number if multiple events happen at the exact same millisecond. `ConditionalCheckFailedException`
    is caught and ignored. All other exceptions are re-thrown with the unit of work
    attached to cause a fault event to be published, as discussed in the *Handling
    faults* recipe.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆OpLock**旨在为异步处理提供幂等性。我们不是强制事务重试，而是简单地做相反的事情——我们丢弃较旧或重复的事件。在上游Backend For
    Frontend服务中，可以使用传统的OpLock来序列化用户事务，其中下游服务实现逆OpLock以确保较旧或重复的事件不会覆盖最新的数据。在这个配方中，我们使用`uow.event.timestamp`作为`oplock`值。在某些场景中，如果多个事件在确切的同一毫秒发生，可能更倾向于使用序列号。`ConditionalCheckFailedException`被捕获并忽略。所有其他异常都会重新抛出，并附加工作单元以发布故障事件，如*处理故障*配方中所述。'
- en: The simulation in this recipe publishes a `thing-created` event, publishes it
    again, and then publishes a `thing-updated` event followed by the `thing-created`
    event a third time. The logs show that the `thing-created` event is only processed
    once and the duplicates are ignored.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方中的模拟会发布一个`thing-created`事件，再次发布它，然后发布一个`thing-updated`事件，最后第三次发布`thing-created`事件。日志显示`thing-created`事件只被处理一次，重复的则被忽略。
- en: Implementing idempotence with Event Sourcing
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用事件溯源实现幂等性
- en: From a business rule standpoint, it is important that events are processed exactly
    once; otherwise, problems may arise, such as double counting or not counting at
    all. However, our cloud-native systems must be resilient to failure and proactively
    retry to ensure no messages are dropped. Unfortunately, this means that messages
    may be delivered multiple times, such as when a producer re-publishes an event
    or a stream processor retries a batch that may have been partially processed.
    The solution to this problem is to implement all actions to be idempotent. This
    recipe demonstrates how to use Event Sourcing and a micro event store to implement
    idempotence.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从业务规则的角度来看，事件必须被精确处理一次非常重要；否则，可能会出现问题，例如重复计数或根本不计数。然而，我们的云原生系统必须能够抵御故障并主动重试以确保不丢失任何消息。不幸的是，这意味着消息可能会被多次投递，例如当生产者重新发布事件或流处理器重试可能已部分处理的事务批次时。解决这个问题的方法是实现所有操作的可幂等性。本菜谱演示了如何使用事件溯源和微事件存储来实现幂等性。
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Create the project from the following template:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下模板创建项目：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Navigate to the `cncb-idempotence-es` directory with `cd cncb-idempotence-es`.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cd cncb-idempotence-es` 命令导航到 `cncb-idempotence-es` 目录。
- en: Review the file named `serverless.yml`.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看名为 `serverless.yml` 的文件。
- en: 'Review the file named `handler.js` with the following content:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下内容的 `handler.js` 文件：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Install the dependencies with `npm install`.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm install` 安装依赖项。
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm test -- -s $MY_STAGE` 运行测试。
- en: Review the contents generated in the `.serverless` directory.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看在 `.serverless` 目录中生成的内容。
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `npm run dp:lcl -- -s $MY_STAGE` 部署堆栈。
- en: Review the stack and resources in the AWS Console.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 控制台中查看堆栈和资源。
- en: 'Invoke the simulate function with the following command:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令调用模拟函数：
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Take a look at the following `trigger` function logs:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看以下 `trigger` 函数日志：
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，使用 `npm run rm:lcl -- -s $MY_STAGE` 删除堆栈。
- en: How it works...
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Event Sourcing facilitates idempotence because events are immutable. The same
    event with the same unique ID can be published or processed multiple times with
    the same outcome. The micro event store serves as a buffer that weeds out duplicates.
    The service consumes desired events and stores them in a micro event store with
    a **hashkey** that groups related events, such as the `uow.event.thing.id` of
    the domain object, and a range key based on the `uow.event.id`. This primary key
    is also immutable. As a result, the same event can be saved multiple times, but
    only a single event is produced on the database stream. Thus, the business logic,
    which is discussed in the *Creating a micro event store* or *Implementing an analytics
    BFF* recipe, is only triggered once.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 事件溯源通过事件不可变来促进幂等性。具有相同唯一 ID 的相同事件可以被发布或处理多次，并产生相同的结果。微事件存储充当缓冲区，以过滤掉重复项。服务消费所需事件，并将它们存储在具有
    **hashkey** 的微事件存储中，该 **hashkey** 用于分组相关事件，例如领域对象的 `uow.event.thing.id`，以及基于 `uow.event.id`
    的范围键。这个主键也是不可变的。因此，相同的事件可以保存多次，但在数据库流上只产生一个事件。因此，在 *创建微事件存储* 或 *实现分析 BFF* 菜谱中讨论的业务逻辑只会触发一次。
- en: The simulation in this recipe publishes a `thing-created` event, publishes it
    again, and then publishes a `thing-updated` event followed by the `thing-created`
    event a third time. The logs show that the three `thing-created` event instances
    only result in a single event on the DynamoDB Stream.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本菜谱中的模拟发布了一个 `thing-created` 事件，再次发布它，然后发布一个 `thing-updated` 事件，最后第三次发布 `thing-created`
    事件。日志显示，三个 `thing-created` 事件实例只在 DynamoDB 流上产生一个事件。
