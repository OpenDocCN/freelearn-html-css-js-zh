<html><head></head><body>
		<div><h1 id="_idParaDest-180" class="chapter-number"><a id="_idTextAnchor231"/>12</h1>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor232"/>Superhero Landing – Setting Up Flexible Navigation Options</h1>
			<p>The city our superhero framework lands in is always changing and they often don’t know what they are up against when on patrol. Our next step is to make the landing page URL more flexible. We will need to be able to switch from testing in the QA environment to staging. At the same time, they should be robust enough to be able to handle small differences. In this chapter, we’ll look at handling elements that exist in one release or environment but not in another. In addition, we will enhance the log wrapper to include colors.</p>
			<p>We’ll cover the following main topics:</p>
			<ul>
				<li>Using system variables</li>
				<li>Adding data configuration files</li>
				<li>Configuration allure reporting</li>
			</ul>
			<p class="callout-heading">Quick tip</p>
			<p class="callout">Avoid testing in the development environment because there will constantly be changes . Stay focused on the QA and staging environments. Promising to keep tests in a running state in dev will generate more maintenance time. More maintenance means less time creating new tests and analyzing existing results, which means more bugs slip into production, increasing the chance we have money leaving the bottom line. If the powers that be insist, make it clear a small subset of 4-10 tests can be provided just to give the developers a “warm fuzzy” about the state of their environment. We do want to shift left, but spreading our team too thin will be counterproductive.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor233"/>Technical requirements</h1>
			<p>All test examples can be found in this GitHub repository: <a href="https://github.com/PacktPublishing/Enhanced-Test-Automation-with-WebdriverIO">https://github.com/PacktPublishing/Enhanced-Test-Automation-with-WebdriverIO</a>.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor234"/>Using system variables</h1>
			<p>When <a id="_idIndexMarker467"/>running our tests from the command line, we can set up user variables easily to indicate which test environment to use or run. This can be done with an environment <code>{</code><code>process.env.ENV}</code> variable:</p>
			<pre class="source-code">
&gt; Env=dev</pre>			<p>This variable can then be read inside our framework and redirect our login method to the proper environment, like this:</p>
			<pre class="source-code">
prod=www.candymapper.com
dev=www.candymapperr2.com</pre>			<p class="callout-heading">Quick tip</p>
			<p class="callout">Be extra vigilant when testing in production. Discuss with leadership the potential impact it can have. Slowing down the production database with inefficient SQL calls that return a million results will overshadow any bugs that are found. Set up your job runs with a marker that indicates your production environment’s safe test cases.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor235"/>Adding data configuration files</h1>
			<p>Legend<a id="_idIndexMarker468"/> has it that data files were created by a brilliant and enigmatic scientist whose name is whispered only in hushed tones by those who know of its existence. They are said to contain ancient knowledge, sacred algorithms, and hidden codes that can unravel the mysteries of the application under test.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor236"/>Where data is stored for test use</h2>
			<p>Adding data files <a id="_idIndexMarker469"/>to your test code with TypeScript is exactly like how you do it for JavaScript, but with TypeScript, you have the added benefit of leveraging TypeScript’s static typing and modules that will help you catch type-related errors early, making your tests more robust and maintainable.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor237"/>Organizing test data</h2>
			<p>First, create a <a id="_idIndexMarker470"/>directory to store your test data files. You can name it something such as <code>test-data</code> or <code>shared-data</code>. Place your data files (e.g., JSON, CSV, etc.) in this directory.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor238"/>Setting up TypeScript configuration</h2>
			<p>Ensure <a id="_idIndexMarker471"/>that your TypeScript configuration (<code>tsconfig.json</code>) includes the appropriate settings for test files and modules. Take the following example:</p>
			<pre class="source-code">
   // Json file
   {
     "compilerOptions": {
       "target": "es6",
       "outDir": "./dist",
       "esModuleInterop": true
     },
     "include": ["src", "shared-data", "tests"]
   }</pre>			<p>Include the <code>test-data</code> directory and the <code>tests</code> directory in the <code>include</code> section of the <code>tsconfig</code> file.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor239"/>Reading data from files</h2>
			<p>Here, we<a id="_idIndexMarker472"/> use the <code>fs</code> module to read data from the files:</p>
			<pre class="source-code">
   import * as fs from 'fs';
   const jsonData: string = fs.readFileSync('./shared-data/data.json', 'utf-8');
   const parsedData: MyDataInterface = JSON.parse(jsonData);</pre>			<p>Once we have a file system object, we can begin to build data-driven tests</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor240"/>Using test data in tests</h2>
			<p>In your<a id="_idIndexMarker473"/> test files, you can import the necessary data and use it in your test cases, like so:</p>
			<pre class="source-code">
   import { expect } from 'expect-webdriverio';
   import { someFunction } from '../src/someModule';
   import testData from '../shared-data/data.json';
   describe('someFunction', () =&gt; {
     it('should return the correct value', () =&gt; {
       const result = someFunction(testData.input);
       expect(result).toEqual(testData.expectedOutput);
     });
   });</pre>			<p>In the preceding example, we are pulling some data from the <code>data.json</code> file in the shared-data directory. The <a id="_idIndexMarker474"/>input data is then compared to the actual result and asserting the values are matching.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor241"/>Beyond masking – making confidential data invisible</h2>
			<p>As <a id="_idIndexMarker475"/>mentioned earlier, superheroes often go to great lengths to protect their identity, such as by wearing masks or donning a pair of glasses. But if they truly want to be stealthy, nothing beats a vault of secrets.</p>
			<p>The use of data files to keep confidential information such as usernames and access keys is very commonplace today. For security reasons, these should <em class="italic">never</em> be uploaded into your code repository. A good DevSecOps team will parse GitHub and GitLab repos for terms such as “password” and flag your team for<a id="_idIndexMarker476"/> being out of compliance with <strong class="bold">System and Organization Controls 2</strong> (<strong class="bold">SOC II</strong>) if they find any matches.</p>
			<p>Create a <code>.env</code> file at the base of your project to store all your confidential data, then add <code>dotenv</code> to your dependencies once done. This will give <code>process.env</code> access to all the data in the <code>.</code><code>env</code> file:</p>
			<pre class="source-code">
// content of .env
# LambdaTest Credentials
 LT_USERNAME=LT_USERNAME
 LT_ACCESS_KEY=LT_ACCESS_KEY
 LT_HOST_URL=LT_HOST_URL</pre>			<p>For this, we need another node package called <code>dotenv</code>. This package allows developers to store configuration data in a plain text file named <code>.env</code>. Each line in the <code>.env</code> file typically represents an environment variable in the form of <code>KEY=VALUE</code>, such as <code>API_KEY=your_api_key_here</code>. Installing it is simple enough:</p>
			<pre class="source-code">
&gt; yarn add dotenv</pre>			<p>Next, we place this at the top of <code>wdio.config</code> file just below the <code>import </code>statements:</p>
			<pre class="source-code">
require('dotenv').config()
// usage in wdio config
module.exports = {
   // ….
   user: process.env.LT_USERNAME,
   key: process.env.LT_ACCESS_KEY,
   // ….
};</pre>			<p>In this case, we <a id="_idIndexMarker477"/>are creating a system variable to hold <code>LT_USERNAME</code> and <code>LT_ACCESS_KEY</code>. This is how we pass sensitive data without storing our credentials in our repo.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor242"/>Spec and Allure – cub reporter versus star journalist</h2>
			<p>In many <a id="_idIndexMarker478"/>comic books, there are several reporters documenting the big events and crimes in the city. The cub reporter gives inside knowledge for our superhero to save the day and the star journalist provides flashy front-page headlines. Spec and Allure reporters<a id="_idIndexMarker479"/> are similar reporting mechanisms in WebdriverIO. They serve different functions and provide different levels of detail. The<a id="_idIndexMarker480"/> Spec reporter is best for SDETs to use to debug failing test runs on the fly. It tells you if the test passed or failed, shows the name of the test, and reports the time it took to run. If a test fails, the Spec reporter provides the error message and stack trace in the console. This provides you with an immediate understanding of what has happened, but it’s up to you to help provide in-depth contextual data about the test run.</p>
			<p>Allure provides flashy historical graphs that are better suited for showing results to project managers and senior executives. It goes beyond the basics to give you a more complete picture. It produces a stylish and informative report with a lot of additional information, such as the following:</p>
			<ul>
				<li>Test and suite descriptions</li>
				<li>Attach screenshots on failure</li>
				<li>Attach text/plain context to the test report</li>
				<li>Mark your tests with BDD labels and severity</li>
				<li>Test case categorization for tests of a common application area</li>
				<li>Trend history and failure analysis</li>
				<li>Environment information</li>
			</ul>
			<p>So, the<a id="_idIndexMarker481"/> Allure reporter provides<a id="_idIndexMarker482"/> a much<a id="_idIndexMarker483"/> richer, more detailed report than the Spec reporter. It allows for a better understanding of what is happening during testing and offers a more holistic view of your test suite. You can think of it as the difference between a simple headline (Spec reporter) and a full news article complete with photos, analysis, and context (Allure reporter).</p>
			<p>The first step is to add Allure to our project:</p>
			<pre class="source-code">
&gt; yarn add @wdio/allure-reporter
&gt; yarn add allure-commandline</pre>			<p>Then, inside the <code>wdio.conf.ts</code> file, we will add the configuration:</p>
			<pre class="source-code">
    reporters: ["spec", ["allure",
    {
       outputDir: "./reports/allure-results",
       disableWebdriverStepsReporting: false,
       disableWebdriverScreenshotsReporting: false,
     }]],</pre>			<p>This section directs where the reporting detail will be stored and includes two options that can be enabled or disabled. Both options are enabled by default (<code>false</code>), allowing Allure to provide detailed step-by-step reporting and include relevant screenshots to enhance the visibility and understandability of your test results. The only reason to disable these options would be to save disk space, which is not recommended. Excluding Webdriver steps reporting and screenshots reporting from the generated report only makes our analysis job harder.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor243"/>Configuring Allure reporting</h1>
			<p>If you <a id="_idIndexMarker484"/>did not set <code>Allure</code> as a reporter previously, it can be done manually. This is a two-step process: <code>Allure</code>, skip to <em class="italic">step 2</em>:</p>
			<ol>
				<li>To install Allure, type the following:<pre class="source-code">
&gt; yarn add @wdio/allure-reporter</pre><p class="list-inset">This will install Allure as a <code>devDependancies</code>. We can verify the package is added to the <code>package.json</code> file.</p></li>			</ol>
			<div><div><img src="img/B19395_Figure_2.17.jpg" alt="Figure 12.1 – Allure reporter dependency is added to package.json"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Allure reporter dependency is added to package.json</p>
			<ol>
				<li value="2">The <a id="_idIndexMarker485"/>Allure package is added to the dev dependencies. Next, the output directories for the HTML report and screen captures must be configured in <code>wdio.config.ts</code>:</li>
			</ol>
			<div><div><img src="img/B19395_12_2.jpg" alt="Figure 12.2 – Adding Allure configuration to the wdio.config.ts file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Adding Allure configuration to the wdio.config.ts file</p>
			<p class="list-inset">In the <code>wdio.config.ts</code> file, <code>outputDir</code> directs where the HTML files and screen captures are to be stored. Let’s use <code>allure-results</code>. Now run the test again:</p>
			<pre class="source-code">
&gt; yarn wdio</pre>			<p class="list-inset">This will launch the <code>example.e2e.ts</code> test. It also generates results in the <code>allure-results</code> folder for Allure to build a dashboard.</p>
			<div><div><img src="img/B19395_12_3.jpg" alt="Figure 12.3 – New support files created by Allure to create the HTML report page"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – New support files created by Allure to create the HTML report page</p>
			<ol>
				<li value="3">To<a id="_idIndexMarker486"/> display the results, type the following:<pre class="source-code">
&gt; allure generate -–clean &amp;&amp; allure open</pre></li>				<li>The Bash terminal can also execute combined statements like this:<pre class="source-code">
&gt; allure generate –clean; allure open</pre></li>			</ol>
			<p>We have installed and configured both WebdriverIO and the Allure dashboard service to display pretty result graphs for our stakeholders. But there is one constant in test automation, and that is change. We need to keep the versions of all our support packages up to date. If there are conflicts, there is bound to be trouble. Fortunately, there is an easy solution for that.</p>
			<p>This information does not need to be stored in the repository, so we will add it to our <code>.</code><code>gitignore</code> file:</p>
			<pre class="source-code">
allure-report
allure-results
Screenshots</pre>			<p>At the<a id="_idIndexMarker487"/> top of each test, we should be consistent with an Allure reporting tag to help organize and categorize our test cases in the report. This includes tags for test owners (authors), features, stories, and descriptions. Advanced reporting can include links back to Jira tickets using TMS links. Let us begin with the <code>Owner</code> tag:</p>
			<pre class="source-code">
AllureReporter.addOwner("Paul Grossman");</pre>			<p>The first question any caped crusader wants answered is, “Who did it?” In earlier chapters, we noted that code can be quickly traced back to its owner in VS Code with GitLens. Since the original author of the test knows the tests they wrote best, your team members should be in the habit of adding their names to each test they write. Next, we need to organize our tests by feature:</p>
			<pre class="source-code">
allureReporter.addFeature("Automation Hello World");</pre>			<div><div><img src="img/B19395_12_4.jpg" alt="Figure 12.4 – Allure report displaying one passing test under the “Automation Hello World“ feature&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Allure report displaying one passing test under the “Automation Hello World“ feature</p>
			<p>A <code>Feature</code> tag <a id="_idIndexMarker488"/>describes what area of the application is being tested by this and other tests. Test cases can be grouped for more efficient execution. This could be a small subset of tests that relate to only one area. This would eliminate separating tests by smoke and regression suites. Tests also need some detail regarding the functionality of the test itself.</p>
			<p>This is the command for adding a descriptive tag name to the test in the report:</p>
			<pre class="source-code">
<code>AllureReporter.addDescription("Verify the user can login");</code></pre>			<p>This description can be seen in the following screenshot, highlighted in blue.</p>
			<div><div><img src="img/B19395_12_5.jpg" alt="Figure 12.5 – Description indicating the test will assert the login functionality&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Description indicating the test will assert the login functionality</p>
			<p>The <code>Description</code> tag is the summary of the validation performed by the test itself. This is generally copied word for word from the title of a ticket in an issue-tracking tool referencing an existing manual test. It could also be the title of a ticket in a separate automation project that links across to the manual test in a separate project. Those ticket numbers should be matched with a <code>Story</code> traceability tag.</p>
			<p>This is how we add a story description to a test report:</p>
			<pre class="source-code">
<code>allureReporter.addStory("TA-001");</code></pre>			<p>This information is then attached as the name of the test.</p>
			<div><div><img src="img/B19395_12_6.jpg" alt="Figure 12.6 – The ”TA-001” story added with a Jira ticker reference&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – The ”TA-001” story added with a Jira ticker reference</p>
			<p>Tests need <a id="_idIndexMarker489"/>traceability to individual story detail information. There is little point in duplicating the text in the actual story ticket, so just providing the ticket number can be sufficient. It can be appended to a saved URL in a browser for a quick lookup.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor244"/>Adding custom comments to the Allure report</h2>
			<p>In <a href="B19395_08.xhtml#_idTextAnchor165"><em class="italic">Chapter 8</em></a>, we <a id="_idIndexMarker490"/>discussed creating a wrapper for <code>Expect</code>. We can add custom reporting with the <code>addattachment()</code> function:</p>
			<pre class="source-code">
allureReporter.addAttachment('Assertion Failure: ', `Invalid Assertion Type = ${assertionType}`, 'text/plain');</pre>			<p>In this example, we intentionally fail with an invalid assertion verb, <code>equa</code>. <code>expectAdv</code> reports a detailed error to the Allure report describing the cause.</p>
			<div><div><img src="img/B19395_12_7.jpg" alt="Figure 12.7 – The problem string “equa” is reported as an error&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – The problem string “equa” is reported as an error</p>
			<p>The best <a id="_idIndexMarker491"/>advice is to be as efficient as possible. Traceability could be combined with a single description:</p>
			<pre class="source-code">
AllureReporter.addDescription("TA-001 : Verify the user can login");</pre>			<p>Within the <code>Log</code> wrapper, we can provide details to both reporters. But not everything. That would cause a high signal-to-noise ratio. So, let us just log errors and warnings:</p>
			<pre class="source-code">
let SEND_TO_ALURE = false</pre>			<p>In addition, the Spec reporter can be made a little more flashy with some color. Let’s say we want any text that indicates a result passed to be displayed in the console in green, while a test that failed is displayed in red. The <code>Log</code> wrapper can be amended to watch for the <code>PASS:</code> and <code>FAIL:</code> text. These strings can be surrounded by lines with ANSI color markers.</p>
			<p>First, let’s add <code>Allure</code> to our project:</p>
			<pre class="source-code">
const { addFeature, addDescription } = require('@wdio/allure-reporter').default;
describe('My feature', () =&gt; {
    it('should do some things', () =&gt; {
        browser.url('https://webdriver.io');
        // Add a step to the report
        addFeature('Navigate to WebdriverIO website');
        browser.url('https://webdriver.io');
        // Add a description to the report
        addDescription('This is a description of what the test should         do');
    });
});</pre>			<p>Next, let’s<a id="_idIndexMarker492"/> identify the basic colors at the top of our helper file starting with green:</p>
			<pre class="source-code">
const ANSI_GREEN = `\x1b[38;2;140;225;50m` // PASS</pre>			<p>The color escape sequence is broken down here:</p>
			<ul>
				<li> <code>\x1b</code> is the escape character, which starts the sequence.</li>
				<li><code>[</code> is the <strong class="bold">Control Sequence Introducer</strong> (<strong class="bold">CSI</strong>), which <a id="_idIndexMarker493"/>tells the terminal to interpret the following characters as a command.</li>
				<li><code>38</code> is the <code>foreground</code> text to a <a id="_idIndexMarker494"/>custom ANSI color. <code>48</code> sets the background color. Use 30–37 to set the color to one of the eight default foreground colors and 40–47 for the eight default background colors.</li>
				<li><code>2</code> specifies that the color will be faint set using RGB values. Other options include <code>3</code> for italics, <code>5</code> and <code>6</code> for blinking text, <code>7</code> for inverse text, and <code>9</code> for crossed-out text.</li>
				<li><code>140;225;50</code> are the red, green, and blue values, respectively, for the color to be set. In this case, they define a shade of green.</li>
				<li><code>m</code> is the final character, which marks the end of the escape sequence.</li>
			</ul>
			<p>As you can see, we can get quite creative with the color and formatting of the text. Next, we add red for failing messages and yellow for warning messages:</p>
			<pre class="source-code">
const ANSI_RED= `\x1b[38;2;145;250;45m`    // FAIL
const ANSI_YELLOW = `\x1b[38;2;145;226;45m`  // WARNING</pre>			<p>When<a id="_idIndexMarker495"/> we output locators, they should have their own color as well:</p>
			<pre class="source-code">
const ANSI_PURPLE= `\x1b[38;2;250;235;80m`  // Locator</pre>			<p>Any text encased in single quotes could be auto-formatted to its own color as well:</p>
			<pre class="source-code">
const ANSI_WHITE= `\x1b[97m`  // TEXT entered into a field</pre>			<p>Finally, we want to reset any color settings back to the default so we can distinguish between messaging from our framework from that of WebdriverIO:</p>
			<pre class="source-code">
const ANSI_RESET= `\x1b[0m` //Reset</pre>			<p>These colors may not be perfect for everyone. You can find a palette of ANSI RGB color combinations to customize to your liking here: <a href="https://github.com/hinell/palette-print.bash">https://github.com/hinell/palette-print.bash</a>.</p>
			<p>Now let’s enhance the log wrapper to get some color:</p>
			<pre class="source-code">
if (message.includes("Warning: ")) {
    message = ANSI_YELLOW + message + ANSI_RESET
    SEND_TO_ALLURE = true
else if (message.includes("Error: ") || message.includes(Promise"){
    message = ANSI_RED + message + ANSI_RESET
    SEND_TO_ALLURE = true
} else {
   message = ANSI_GREEN + message + ANSI_RESET
}</pre>			<p>When we output out strings using accent marks, we can uniquely identify them and colorize them:</p>
			<pre class="source-code">
message  = message .replace(/`([^`]+)`/g, `${ANSI_WHITE}$1${ANSI_RESET}`);</pre>			<p>We <a id="_idIndexMarker496"/>could embed color for our xPath locators from the log method too:</p>
			<pre class="source-code">
message = message.replace(/\/{1,2}[\w\-\.\:]*\[[^\]]*\]/g, `${ANSI_PURPLE}$1${ANSI_RESET}`);</pre>			<p>The same goes for CSS locators:</p>
			<pre class="source-code">
message = message.replace(/[#.|]?[a-zA-Z]+\s?)+[{] /g, `${ANSI_PURPLE}$1${ANSI_RESET}`);</pre>			<p>Now, when passing a result, it could be displayed at runtime in color based on the content:</p>
			<pre class="source-code">
global.log(`FAIL: Invalid Assertion Type = ${assertionType}`);</pre>			<p>But it would be more reliable to do this from the <code>Click</code>, <code>Select</code>, <code>Enter</code>, and <code>Expect</code> method wrappers instead.</p>
			<p>Finally, we can redirect any error logging to an Allure report like this:</p>
			<pre class="source-code">
if (SEND_TO_ALURE){
addStep(str);
}</pre>			<h2 id="_idParaDest-194"><a id="_idTextAnchor245"/>Webhooks and screen captures</h2>
			<p>Our <a id="_idIndexMarker497"/>final step is to add a screen capture at the end of our test cases. It is your decision whether you want to take a screen capture only on failing test cases. However, based on our experience, we think taking a screen capture regardless will give you the opportunity to see what the difference <a id="_idIndexMarker498"/>between a passing versus a failing test is when you’re looking at a historical run saved in Jenkins:</p>
			<pre class="source-code">
    /**
     * Function to be executed after a test (in Mocha/Jasmine only)
     * @param {object}  test             test object
     * @param {object}  context          scope object the test was executed with
     * @param {Error}   result.error     error object in case the test fails, otherwise `undefined`
     * @param {*}       result.result    return object of test function
     * @param {number}  result.duration  duration of test
     * @param {boolean} result.passed    true if test has passed, otherwise false
     * @param {object}  result.retries   information about spec related retries, e.g. `{ attempts: 0, limit: 0 }`
     */
    afterTest: async function (
        test,
        context,
        {error, result, duration, passed, retries}
    ) {
      if (!passed) {
        await browser.takeScreenshot();
      }
    },</pre>			<p>This is accomplished <a id="_idIndexMarker499"/>by adding the preceding lines of code to the <code>afterTest</code> hook of the <code>WDIO.config</code> file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>onPrepare</code>, <code>onWorkerStart</code>, <code>onWorkerEnd</code>, and <code>onComplete</code> hooks are executed in a different process and therefore cannot share any global data with the other hooks that live in the worker process.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor246"/>Summary</h1>
			<p>In this chapter, we embarked on a heroic journey akin to traversing the dynamic realms of a superhero multiverse. We mastered the art of directing our test scenarios to various domains of operation—be it QA, stage, or, when the situation demands it, dev and even production. Alongside this, we infused our console log with a spectrum of hues, akin to a caped crusader’s vibrant costume. Our Allure reports, much like a meticulously organized utility belt, now display information with precision and clarity. We also unlocked the power of data files, safeguarding the keys to our digital city—sensitive credentials—from the prying eyes of nefarious adversaries.</p>
			<p>Navigating through these diverse environments mirrors the complex task of a guardian navigating through parallel universes—each familiar in contour but unique in content. As we prepare to soar into the next chapter, we will fortify our tests with the resilience of a superhero’s shield, ensuring they withstand the trials of missing elements that may have vanished into the ether. Furthermore, we will broaden our horizons into the vast expanse of cross-browser testing, ensuring our digital endeavors are as versatile as a shape-shifting hero’s array of abilities.</p>
		</div>
	</body></html>