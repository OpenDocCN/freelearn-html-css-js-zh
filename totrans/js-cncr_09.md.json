["```js\nco(function* () {\n    // TODO: co-routine amazeballs.\n});\n```", "```js\n// This is the ES7 syntax, where the function is\n// marked as \"async\". Then, the \"await\" calls\n// pause execution till their operands resolve.\n(async function() {\n    var result;\n    result = await Promise.resolve('hello');\n    console.log('async result', `\"${result}\"`);\n    // → async result \"hello\"\n\n    result = await Promise.resolve('world');\n    console.log('async result', `\"${result}\"`);\n    // → async result \"world\"\n}());\n```", "```js\n// We need the \"co()\" function.\nvar co = require('co');\n\n// The differences between the ES7 and \"co()\" are\n// subtle, the overall structure is the same. The\n// function is a generator, and we pause execution\n// by yielding generators.\nco(function*() {\n    var result;\n    result = yield Promise.resolve('hello');\n    console.log('co result', `\"${result}\"`);\n    // → co result \"hello\"\n\n    result = yield Promise.resolve('world');\n    console.log('co result', `\"${result}\"`);\n    // → co result \"world\"\n});\n```", "```js\nvar co = require('co');\n\nco(function* () {\n\n    // The promise that's yielded here isn't resolved\n    // till 1 second later. That's when the yield statement\n    // returns the resolved value.\n    var first = yield new Promise((resolve, reject) => {\n        setTimeout(() => {\n            resolve([ 'First1', 'First2', 'First3' ]);\n        }, 1000);\n    });\n\n    // Same idea here, except we're waiting 2 seconds\n    // before the \"second\" variable gets it's value.\n    var second = yield new Promise((resolve, reject) => {\n        setTimeout(() => {\n            resolve([ 'Second1', 'Second2', 'Second3' ]);\n        }, 2000);\n    });\n\n    // Both \"first\" and \"second\" are resolved at this\n    // point, so we can use both to map a new array.\n    return first.map((v, i) => [ v, second[i] ]);\n\n}).then((value) => {\n    console.log('zipped', value);\n    // → \n    // [ \n    //   [ 'First1', 'Second1' ],\n    //   [ 'First2', 'Second2' ],\n    //   [ 'First3', 'Second3' ] \n    // ]\n});\n```", "```js\nvar co = require('co');\n\n// A simple user collection.\nvar users = [\n    { name: 'User1' },\n    { name: 'User2' },\n    { name: 'User3' },\n    { name: 'User4' }\n];\n\nco(function* () {\n\n    // The \"userID\" value is asynchronous, and execution\n    // pause at this yield statement till the promise\n    // resolves.\n    var userID = yield new Promise((resolve, reject) => {\n        setTimeout(() => {\n            resolve(1);\n        }, 1000);\n    });\n\n    // At this point, we have a \"userID\" value. This\n    // nested co-routine will look up the user based\n    // on this ID. We nest coroutines like this because\n    // \"co()\" returns a promise.\n    var user = yield co(function* (id) {\n        let user = yield new Promise((resolve, reject) => {\n            setTimeout(() => {\n                resolve(users[id]);\n            }, 1000);\n        });\n\n        // The \"co()\" promise is resolved with the\n        // \"user\" value.\n        return user;\n    }, userID);\n\n    console.log(user);\n    // → { name: 'User2' }\n});\n```", "```js\nvar co = require('co');\n\n// A simple user collection.\nvar users = [\n    { name: 'User1' },\n    { name: 'User2' },\n    { name: 'User3' },\n    { name: 'User4' }\n];\n\n// The \"getUser()\" function will create a new\n// co-routine whenever it's called, forwarding\n// any arguments as well.\nvar getUser = co.wrap(function* (id) {\n    let user = yield new Promise((resolve, reject) => {\n        setTimeout(() => {\n            resolve(users[id]);\n        }, 1000);\n    });\n\n    // The \"co()\" promise is resolved with the\n    // \"user\" value.\n    return user;\n});\n\nco(function* () {\n\n    // The \"userID\" value is asynchronous, and execution\n    // pause at this yield statement till the promise\n    // resolves.\n    var userID = yield new Promise((resolve, reject) => {\n        setTimeout(() => {\n            resolve(1);\n        }, 1000);\n    });\n\n    // Instead of a nested co-routine, we have a function\n    // that can now be used elsewhere.\n    var user = yield getUser(userID);\n\n    console.log(user);\n    // → { name: 'User2' }\n});\n```", "```js\n// Eat some CPU cycles...\n// Taken from http://adambom.github.io/parallel.js/\nfunction work(n) {\n    var i = 0;\n    while (++i < n * n) {}\n    return i;\n}\n\n// Adds some functions to the event loop queue.\nprocess.nextTick(() => {\n    var promises = [];\n\n    // Creates 500 promises in the \"promises\"\n    // array. They're each resolved after 1 second.\n    for (let i = 0; i < 500; i++) {\n        promises.push(new Promise((resolve) => {\n            setTimeout(resolve, 1000);\n        }));\n    }\n\n    // When they're all resolved, log that\n    // we're done handling them.\n    Promise.all(promises).then(() => {\n        console.log('handled requests');\n    });\n});\n\n// This takes a lot longer than the 1 second\n// it takes to resolve all the promises that\n// get added to the queue. So this handler blocks\n// 500 user requests till it finishes..\nprocess.nextTick(() => {\n    console.log('hogging the CPU...');\n    work(100000);\n});\n```", "```js\n// We need the \"child_process\" to fork new\n// node processes.\nvar child_process = require('child_process');\n\n// Forks our worker process.\nvar child = child_process.fork(`${__dirname}/child`);\n\n// This event is emitted when the child process\n// responds with data.\nchild.on('message', (message) => {\n\n    // Displays the result, and kills the child\n    // process since we're done with it.\n    console.log('work result', message);\n    child.kill();\n});\n\n// Sends a message to the child process. We're\n// sending a number on this end, and the\n// \"child_process\" ensures that it arrives as a\n// number on the other end.\nchild.send(100000);\nconsole.log('work sent...');\n\n// Since the expensive computation is happening in\n// another process, normal requests flow through\n// the event loop like normal.\nprocess.nextTick(() => {\n    console.log('look ma, no blocking!');\n});\n```", "```js\n// Eat some CPU cycles...\n// Taken from http://adambom.github.io/parallel.js/\nfunction work(n) {\n    var i = 0;\n    while (++i < n * n) {}\n    return i;\n}\n\n// The \"message\" event is emitted when the parent\n// process sends a message. We then respond with\n// the result of performing expensive CPU operations.\nprocess.on('message', (message) => {\n    process.send(work(message));\n});\n```", "```js\n// Our required modules...\nvar child_process = require('child_process');\nvar os = require('os');\n\n// Spawns our child process - the \"ls\" system\n// command. The command line flags are passed\n// as an array.\nvar child = child_process.spawn('ls', [\n    '-lha',\n    __dirname\n]);\n\n// Our output accumulator is an empty string\n// initially.\nvar output = '';\n\n// Adds output as it arrives from process.\nchild.stdout.on('data', (data) => {\n    output += data;\n});\n\n// We're done getting output from the child\n// process - so log the output and kill it.\nchild.stdout.on('end', () => {\n    output = output.split(os.EOL);\n    console.log(output.slice(1, output.length - 2));\n    child.kill();\n});\n```", "```js\nvar child_process = require('child_process');\n\n// Forks our \"worker\" process and creates a \"resolvers\"\n// object to store our promise resolvers.\nvar worker = child_process.fork(`${__dirname}/worker`),\n    resolvers = {};\n\n// When the worker responds with a message, pass\n// the message output to the appropriate resolver.\nworker.on('message', (message) => {\n    resolvers[message.id](message.output);\n    delete resolvers[message.id];  \n});\n\n// IDs are used to map responses from the worker process\n// to the promise resolver functions.\nfunction* genID() {\n    var id = 0;\n\n    while (true) {\n        yield id++;\n    }\n}\n\nvar id = genID();\n\n// This function sends the given \"input\" to the worker,\n// and returns a promise. The promise is resolved with\n// the return value of the worker.\nfunction send(input) {\n    return new Promise((resolve, reject) => {\n        var messageID = id.next().value;\n\n        // Store the resolver function in the \"resolvers\"\n        // map.\n        resolvers[messageID] = resolve;\n\n        // Sends the \"messageID\" and the \"input\" to the\n        // worker.\n        worker.send({\n            id: messageID,\n            input: input\n        });\n    });\n}\n\nvar array;\n\n// Builds an array of numbers to send to the worker\n// individually for processing.\narray = new Array(100)\n    .fill(null)\n    .map((v, i) => (i + 1) * 100);\n\n// Sends each number in \"array\" to the worker process\n// as a message. When each promise is resolved, we can\n// reduce the results.\nvar first = Promise.all(array.map(send)).then((results) => {\n    console.log('first result', \n        results.reduce((r, v) => r + v));\n    // → first result 3383500000\n});\n\n// Creates a smaller array, with smaller numbers - it \n// should take less time to process than the previous \n// array.\narray = new Array(50)\n    .fill(null)\n    .map((v, i) => (i + 1) * 10);\n\n// Process the second array, log the reduced result.\nvar second = Promise.all(array.map(send))\n    .then((results) => {\n        console.log('second result',\n            results.reduce((r, v) => r + v));\n        // → second result 4292500\n});\n\n// When both arrays have finished being processed, we need\n// to kill the worker in order to exit our program.\nPromise.all([ first, second ]).then(() => {\n    worker.kill();\n});\n```", "```js\n// Eat some CPU cycles...\n// Taken from http://adambom.github.io/parallel.js/\nfunction work(n) {\n    var i = 0;\n    while (++i < n * n) {}\n    return i;\n}\n\n// Respond to the main process with the result of\n// calling \"work()\" and the message ID.\nprocess.on('message', (message) => {\n    process.send({\n        id: message.id,\n        output: work(message.input)\n    });\n});\n```", "```js\n// The modules we need...\nvar http = require('http');\nvar cluster = require('cluster');\nvar os = require('os');\n\n// Eat some CPU cycles...\n// Taken from http://adambom.github.io/parallel.js/\nfunction work(n) {\n    var i = 0;\n    while (++i < n * n) {}\n    return i;\n}\n\n// Check which type of process this is. It's either\n// a master or a worker.\nif (cluster.isMaster) {\n\n    // The level of parallelism that goes into\n    // \"workers\".\n    var workers = os.cpus().length;\n\n    // Forks our worker processes.\n    for (let i = 0; i < workers; i++) {\n        cluster.fork();\n    }\n\n    console.log('listening at http://localhost:8081');\n    console.log(`worker processes: ${workers}`);\n\n// If this process isn't the master, then it's\n// a worker. So we create the same HTTP server as\n// every other worker.\n} else {\n    http.createServer((req, res) => {\n        res.setHeader('Content-Type', 'text/plain');\n        res.end(`worker ${cluster.worker.id}: ${work(100000)}`);\n    }).listen(8081);\n}\n```", "```js\n// The modules we need...\nvar http = require('http'),\n    httpProxy = require('http-proxy');\n\n// The \"proxy\" server is how we send\n// requests to other hosts.\nvar proxy = httpProxy.createProxyServer();\n\nhttp.createServer((req, res) => {\n\n    // If the request is for the site root, we\n    // return some HTML with some links'.\n    if (req.url === '/') {\n        res.setHeader('Content-Type', 'text/html');\n        res.end(`\n            <html>\n                <body>\n                    <p><a href=\"hello\">Hello</a></p>\n                    <p><a href=\"world\">World</a></p>\n                </body>\n            </html>\n        `);\n\n    // If the URL is \"hello\" or \"world\", we proxy\n    // the request to the appropriate micro-service\n    // using \"proxy.web()\".\n    } else if (req.url === '/hello') {\n        proxy.web(req, res, {\n            target: 'http://localhost:8082'\n        });\n    } else if (req.url === '/world') {\n        proxy.web(req, res, {\n            target: 'http://localhost:8083'\n        });\n    } else {\n        res.statusCode = 404;\n        res.end();\n    }\n}).listen(8081);\nconsole.log('listening at http://localhost:8081');\n```", "```js\nvar http = require('http');\n\n// Get the port as a command line argument,\n// so we can run multiple instances of the\n// service.\nvar port = process.argv[2];\n\n// Eat some CPU cycles...\n// Taken from http://adambom.github.io/parallel.js/\nfunction work() {\n    var i = 0,\n        min = 10000,\n        max = 100000,\n        n = Math.floor(Math.random() * (max - min)) + min;\n    while (++i < n * n) {}\n    return i;\n}\n\n// Responds with plain text, after blocking\n// the CPU for a random interval.\nhttp.createServer((req, res) => {\n    res.setHeader('Content-Type', 'text/plain');\n    res.end(work().toString());\n}).listen(port);\n\nconsole.log(`listening at http://localhost:${port}`); \n```", "```js\nvar http = require('http'),\n    httpProxy = require('http-proxy');\n\nvar proxy = httpProxy.createProxyServer();\n\n// These are the service targets. They have a \"host\",\n// and a \"busy\" property. Initially they're\n// not busy because we haven't sent any work.\nvar targets = [\n    {\n        host: 'http://localhost:8082',\n        busy: false\n    }\n    {\n        host: 'http://localhost:8083',\n        busy: false\n    }\n];\n\n// Every request gets queued here, in case all\n// our targets are busy.\nvar queue = [];\n\n// Process the request queue, by proxying requests\n// to targets that aren't busy.\nfunction processQueue() {\n\n    // Iterates over the queue of messages.\n    for (let i = 0; i < queue.length; i++) {\n\n        // Iterates over the targets.\n        for (let target of targets) {\n\n            // If the target is busy, skip it.\n            if (target.busy) {\n                continue;\n            }\n\n            // Marks the target as busy - from this\n            // point forward, the target won't accept\n            // any requests untill it's unmarked.\n            target.busy = true;\n\n            // Gets the current item out of the queue.\n            let item = queue.splice(i, 1)[0];\n\n            // Mark the response, so we know which service\n            // worker the request went to when it comes\n            // back.\n            item.res.setHeader('X-Target', i);\n\n            // Sends the proxy request and exits the\n            // loop.\n            proxy.web(item.req, item.res, {\n                target: target.host\n            });\n\n            break;\n        }\n    }\n}\n\n// Emitted by the http-proxy module when a response\n// arrives from a service worker.\nproxy.on('proxyRes', function(proxyRes, req, res) {\n\n    // This is the value we set earlier, the index\n    // of the \"targets\" array.\n    var target = res.getHeader('X-Target');\n\n    // We use this index to unmark it. Now it'll\n    // except new proxy requests.\n    targets[target].busy = false;\n\n    // The client doesn't need this internal\n    // information, so remove it.\n    res.removeHeader('X-Target');\n\n    // Since a service worker just became available,\n    // process the queue again, in case there's pending\n    // requests.\n    processQueue();\n});\n\nhttp.createServer((req, res) => {\n\n    // All incoming requests are pushed onto the queue.\n    queue.push({\n        req: req,\n        res: res\n    });\n\n    // Reprocess the queue, leaving the request there\n    // if all the service workers are busy.\n    processQueue();\n}).listen(8081);\n\nconsole.log('listening at http://localhost:8081');\n```"]