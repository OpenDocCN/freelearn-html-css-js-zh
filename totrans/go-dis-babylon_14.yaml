- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extended Topics, Extended
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a chapter about endings, but it is also a chapter about beginnings.
    Our journey together on this long haul may be approaching its destination, but
    this is just the beginning of your personal journey with Babylon.js. In this chapter,
    we abandon any pretense of linear or sequential progress, and instead, we will
    be bouncing between several disparate topics that will each provide individual
    jumping-off points to help you go the distance with Babylon.js.
  prefs: []
  type: TYPE_NORMAL
- en: When navigating unfamiliar streets, it can be useful to have a guide, someone
    who is knowledgeable about an area. Someone with deep practical experience, who
    knows how to guide visitors and new arrivals to the best places and sights. Our
    Space-Dispatcher has located several talented individuals to show us areas of
    Babylon.js that we didn’t get to see or learn about during our trip.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’re going to visit two active construction sites in the metatropolies
    of BJS. At the first of those sites, we’ll learn about ongoing efforts to bring
    the simple elegance of Babylon.js out of the web and directly onto device hardware
    with Babylon Native. The second of those sites encompasses the exciting world
    (“metaverse”) of **augmented reality** (**AR**) and **virtual reality** (**VR**)
    in the form of **WebXR** – the new standard for web-based AR/VR applications.
  prefs: []
  type: TYPE_NORMAL
- en: After those stops, we’ll meet our first guide, BJS community member and serial
    helper of people on the forum, Andrei Stepanov, who will take us through the loading
    docks and into the Babylon.js Mall. He’ll show us glittering displays of the latest
    gadgets on a tour of how easy it is to use BJS with Content Management Systems
    and e-commerce platforms. Parting ways with Mr. Stepanov, we next visit a shiny
    new transport terminal as we go to meet our last guide, Erich Loftis.
  prefs: []
  type: TYPE_NORMAL
- en: Erich has been ranging out on a journey of his own for some time now, and he’s
    going to entertain and enlighten us with the story of his quest seeking the Holy
    Grail of photorealism in 3D graphics – **Real-Time Ray (Path) Tracing**. That’s
    just a preview of what’s to come because it’s time to take a hard right and put
    on a hard hat as we pull into our first construction site for AR and VR with **WebXR**.
  prefs: []
  type: TYPE_NORMAL
- en: There’s always more to learn in any given technical arena, and that applies
    double or more when the topic is rapidly changing. **WebXR** is the standard for
    developing web-based AR and VR, and it qualifies under the “double-or-more” policy
    with its rapidly evolving mix of standard and support. As we learn about **WebXR**,
    we’re not going to focus on every feature of the standard – that would be like
    trying to ice-skate up a hill during a heatwave. What we’re going to focus on
    are the features and capabilities of Babylon.js that allow you as the developer
    to write applications that make use of **WebXR** while lowering the risks involved
    in those changing standards and APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the topics that we’ll be covering in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: AR and VR with **WebXR**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tour of the **Babylon.js Native project**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating 3D content into a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracing out a path to advanced rendering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AR and VR with WebXR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The inexorable march of Moore’s law has brought increasingly greater computing
    power into increasingly smaller microchips at a steady rate for long enough that
    the casual consumer has a staggering amount of raw computational silicone contained
    in their smartphones and tablets. There’s enough processing throughput in the
    average smartphone now that it’s realistic to entertain scenarios such as AR and
    VR.
  prefs: []
  type: TYPE_NORMAL
- en: AR is a category of applications that encompasses a large variety of different
    use cases and scenarios. The common feature shared by these scenarios is that
    they make use of a device’s camera, location, orientation, and other sensors to
    emplace 3D content into a depiction of the real world. VR is very similar to AR,
    save that instead of the content being immersed in the user’s world (the real
    world), the user is immersed in the content (the virtual world).
  prefs: []
  type: TYPE_NORMAL
- en: Whether considering an AR and VR experience, it is important to keep in mind
    that both are more of a spectrum than a binary quality – there’s no rule that
    says something must use *X* percent of features to be considered an AR or a VR
    app. That would be a silly piece of gatekeeping.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are looking for a great band name, Reality-Virtuality Spectrum/Continuum
    are both cool sounding ones! Read more about the Virtuality Spectrum at [https://creatxr.com/the-virtuality-spectrum-understanding-ar-mr-vr-and-xr/](https://creatxr.com/the-virtuality-spectrum-understanding-ar-mr-vr-and-xr/).
  prefs: []
  type: TYPE_NORMAL
- en: Consider this – an application may only support basic head tracking and stereoscopic
    views, but it is still a VR application. Similarly, a simple application that
    draws a rabbit’s ears over a person’s image in a video feed could technically
    be considered an AR application. Most of the time when discussing AR and VR in
    context of web development, it is assumed that the focus is on the VR side of
    things. Historically, that has been accurate, but it won’t always be the case.
    By examining some historical context, it will be clearer how this might have come
    to pass and when to expect that to change.
  prefs: []
  type: TYPE_NORMAL
- en: An Abridged History of AR/VR on the WWW
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the wide world of web development, there have been numerous attempts to bring
    about a standardized set of APIs for VR content, such as the **VRML** standard.
    The last-but-one effort was called **WebVR**, which was aimed at VR content with
    little to no consideration for AR – not out of neglect, but simply because AR
    didn’t exist in any commercially accessible form until relatively recently (let’s
    call it ca. 2015 or so).
  prefs: []
  type: TYPE_NORMAL
- en: By 2018, it had become clear that to make AR a commercially viable application,
    it needed to be able to run on the web. The problem is deceptively simple but
    deviously hard to solve. Consumers don’t want to have to install five separate
    apps to browse five separate furniture stores just to display selections of furniture
    in the prospective buyer’s living room, but they’re happy to go to a website that
    offers the same! Unfortunately, requirements for even basic AR involve accessing
    device and sensor data that normally isn’t available to the browser JavaScript
    sandbox, where performance can also sometimes be suboptimal.
  prefs: []
  type: TYPE_NORMAL
- en: The **WebXR** standard was introduced in 2018 by an industry-wide consortium
    of hardware and software manufacturers. This standard encapsulates and abstracts
    many areas that were left out of the previous **WebVR** standard, such as object/body
    part tracking, unified controller interfaces that account for the many different
    inputs possible with AR/VR, and in general, everything needed to program a world-class
    experience. All the cool kids (Apple, Google, Meta/Facebook, Samsung, Microsoft,
    et al) are a part of this standards body, which means that developers and consumers
    alike should be able to benefit from an explosion of innovation in the commercial
    AR/VR space. Or at least that should have been the case. Devices dedicated to
    AR, such as Microsoft’s HoloLens, as well as devices dedicated to VR, such as
    Oculus, have started to proliferate the consumer electronics market, but progress
    in general for supporting the **WebXR** standard has been stunted at best by the
    actions – or rather, a lack of action – from one of the most influential members
    of that consortium.
  prefs: []
  type: TYPE_NORMAL
- en: 'While most of the consortium members have been busy working to implement key
    **WebXR** features and standards, one of its members – Apple – has sat mostly
    on the sidelines. They have recently released their new iOS hardware-based application
    SDK known as **ARKit**, which is a potential reason for Apple’s inaction on supporting
    **WebXR**. Allowing the hardware access that **WebXR** requires would effectively
    involve breaking the iron grip that **WebKit** has on web rendering on iOS. That’s
    unfortunate, because in the United States, iOS enjoys roughly 60 percent of the
    market share, meaning that most of the US market is inaccessible to companies,
    individuals, and organizations who want to develop and provide AR experiences
    and products on the web (for contrast, iOS holds less than 30 of the percent market
    share worldwide outside of the US. Android owns the bulk of the overseas market).
    The news doesn’t get too much better on the Apple front: as of summer 2022, it
    does not appear likely that Apple will release support for **WebXR** in its **WebKit**
    rendering engine at any point within the upcoming 6 to 12 months.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Pending anti-trust litigation and legislation debate is ongoing in numerous
    courts and legislatures around the world. It is possible that the outcome of some
    of these matters could result in Apple allowing alternative web engines (such
    as Chromium) to be used in iOS. All bets are off if that happens!
  prefs: []
  type: TYPE_NORMAL
- en: 'With all that depressing talk of **WebXR** not being supported on iOS, constantly
    shifting standards, and frequent breaking changes, what’s the silver lining? How
    is the glass half-full, and why would you want to subject yourself to this type
    of software engineering misery? Let’s all say it together now: “Because Babylon.js’
    Got You” with the **WebXR** Experience Helper – blunting sharp pains into dull
    aches.'
  prefs: []
  type: TYPE_NORMAL
- en: Building Tomorrow, Today with the WebXR Experience Helper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s a founding precept of Babylon.js that backward compatibility is of paramount
    importance. Code written 10 years ago on BJS 1.0 still largely works in BJS 5.0,
    which is quite an achievement when talking about tech and the web! When dealing
    with something like **WebXR**, where features and APIs can come and go quickly
    though, does it even make sense to try and build a production application against
    such a moving target?
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Recalling our previous discussion about rhetorical questions and their answers,
    you should already know the answer to that question to be “YES!”
  prefs: []
  type: TYPE_NORMAL
- en: The BJS `WebXRExperienceHelper` is a component that does exactly what it says
    it does on the box – that is, to help with **WebXR** implementation by setting
    up all of the necessary elements for an immersive session. The **Default Experience**
    provided is set up for a VR session along with basic features such as pointer
    tracking and teleportation while, of course, providing the ability to enable,
    attach, and use other features in collaboration with the **FeatureManager**.
  prefs: []
  type: TYPE_NORMAL
- en: The important concept to understand about how the **FeatureManager** works is
    the process of enabling a given feature – at either a specific version, the “latest,”
    or “stable” version – and making it available to be attached to a Scene. Enabling
    a feature and attaching to the Scene is, along with their associated converse
    operations such as disabling and detaching, a two-step process for the application
    code. Two steps for the application, but hidden under the hood lies a whole host
    of sub-operations. Things such as browser feature detection, device capability
    enumeration, and more all occur during the feature enabling stage. The result
    of the enabling process leaves the **WebXRSession** with a new set of **Observables**
    related to the newly enabled feature(s). These Observables are now available to
    be used to attach those features to a given Scene.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why this is an important concept is because while it isn’t necessary
    to use `WebXRExperienceHelper` or `FeatureManager`, those components provide your
    code with the critical ability to isolate itself from the effects of external
    changes. Production applications can make use of the latest VR/AR functionality
    available on a user’s device with confidence that they won’t suddenly break when
    the standard or a web browser’s support for the standard changes. The abstractions
    provided allow developers to write, extend, and maintain applications that leverage
    cutting-edge browser capabilities while gracefully degrading functionality for
    devices that don’t.
  prefs: []
  type: TYPE_NORMAL
- en: '**WebXR** has some incredibly exciting features and capabilities available
    today in Chrome- and Mozilla-based browsers, though some might require users to
    “unhide” features via flags. The types and features of applications built using
    **WebXR** are just beginning to be explored, and the Babylon.js team intends to
    be there to help developers use them the entire way. Unfortunately, that’s all
    the time we’ve got for this construction site visit – there are other places to
    go and things to see, after all, and we have a schedule to keep!'
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*WebXR Experience Helpers*: [https://doc.babylonjs.com/divingDeeper/webXR/webXRExperienceHelpers](https://doc.babylonjs.com/divingDeeper/webXR/webXRExperienceHelpers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*WebXR Features Manager*: [https://doc.babylonjs.com/divingDeeper/webXR/webXRFeaturesManager](https://doc.babylonjs.com/divingDeeper/webXR/webXRFeaturesManager)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Demos and Playgrounds: [https://doc.babylonjs.com/divingDeeper/webXR/webXRDemos](https://doc.babylonjs.com/divingDeeper/webXR/webXRDemos)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our next visit will be to the grounds of a sprawling new technology campus in
    the Babylon.js “Metatropolis.” This campus is the home of the **Babylon Native**
    project – an impressive, ambitious, and particularly complex undertaking. Among
    other areas of study, Native offers one potential solution to the problems posed
    around iOS support for **WebXR**. Let’s learn more about Native and what that
    solution looks like as part of our campus tour of the Babylon Native ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: A Tour of the Babylon.js Native Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Babylon.js is primarily used as part of a web application, but that’s not the
    only place where it can add value. Sometimes, an application needs to target multiple
    platforms with the same code base. Other times, an existing device application
    wants to be able to easily add 3D rendering activities that are secondary to the
    application’s purpose (for example, in a scientific simulation, the renderer is
    simply drawing the output of the simulation onto the screen). Specific requirements
    might include the need for AR capabilities on platforms that include iOS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each of those scenarios (and more that aren’t listed), there is a place
    for Babylon.js to add value to an application. What’s commonly referred to as
    “Babylon Native” in the singular, proper sense is actually a collection of technologies
    that apply to a specific range of scenarios. Every scenario is different and should
    have a solution tailored to the specific needs of the situation, and the set of
    technologies that comprise Babylon Native allows you as the developer to pick
    and choose where and when to apply them. One way to understand the technologies
    is to show them along a spectrum with a fully native app at one end and a fully
    web-native app at the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Spectrum of application types. Source: https://github.com/BabylonJS/BabylonNative/blob/master/Documentation/WhenToUseBabylonNative.md](img/Figure_14.01_B17266.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1 – Spectrum of application types. Source: [https://github.com/BabylonJS/BabylonNative/blob/master/Documentation/WhenToUseBabylonNative.md](https://github.com/BabylonJS/BabylonNative/blob/master/Documentation/WhenToUseBabylonNative.md)'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram (taken from the BJS Native docs, linked in the caption)
    is one method of depicting the Native Collective that shows the relative scale
    of how close to the native device hardware a particular component or framework
    lies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In his blog post about the technical underpinnings of BJS Native at [https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb](https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb),
    Sergio explains how the Babylon Native parts fit from a different perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – A layered diagram of how Babylon Native works in the absence
    of WebGL. Diagram source: https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb](img/Figure_14.02_B17266.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2 – A layered diagram of how Babylon Native works in the absence
    of WebGL. Diagram source: [https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb](https://babylonjs.medium.com/a-babylon-native-backstage-tour-f9004bebc7fb)'
  prefs: []
  type: TYPE_NORMAL
- en: Whether using **Babylon React Native** or simply **Babylon Native**, the preceding
    diagram shows how the unifying abstraction layer of **Babylon Native** covers
    the ugly and sometimes chaotic mess of talking to various hardware components,
    such as the BGFX cross-platform graphics driver with ARCore and ARKit for other
    device sensor and input API abstractions. Having these concepts in mind, we can
    now consider a few potential usage scenarios where it makes sense to take a good
    look at the options presented by Babylon Native.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing Babylon Native
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decision on whether Babylon Native is a good fit for a given project can
    be complex. The docs for Native have an entire page devoted to a questionnaire
    to help you determine what approaches are worth the most research – and what aren’t
    – and while helpful, they can be better understood via a contrived scenario.
  prefs: []
  type: TYPE_NORMAL
- en: If your application is based on **React Native**, there is a light integration
    option and a full integration option. The light option is to use a **WebView**
    to host the WebGL context and canvas. This has the advantage of being able to
    take advantage of the **Just-In-Time** (**JIT**) compilation of JavaScript, meaning
    JS code will tend to be faster than when not using a WebView for some platforms.
    The full integration option is to use **Babylon React Native**. Here’s what we
    might imagine the app this looks like.
  prefs: []
  type: TYPE_NORMAL
- en: The Evolution of a Babylon Native App
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LARP’in app is an app for **Live Action Role Players** – people who like
    to take the table out of tabletop games and act out the gameplay themselves using
    the app to coordinate events, chat, and so on, with all the different luxuries
    that people have come to expect from a modern Web Application. The “Player App”
    is built using React and has enjoyed a steady run of releases, enhancing and extending
    the site’s functionality. The app’s creators want to allow event schedulers to
    be able to manage events offline (because event spaces sometimes don’t have reception)
    so they’ve added PWA capabilities, making everyone happy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then one day, some LARP’in LARPers were playing *Pokémon Go* when they had
    the realization that while LARPing is cool, what’s even cooler is LARPing… with
    AR! Players would be able to see visualizations of their spells cast, detect traps
    with skill rolls, and walk around exploring a fantasy world brought to life. Their
    existing LARP tools consist of some home-brewed Bluetooth-connected devices embedded
    into items (for example, a sword) that register hits and similar game-management
    tasks by lighting up or beeping, but that’s the extent of it. Many of the members
    have iOS devices, while others are on Android, and there are even a few odd souls
    clinging to heavily tweaked versions of Windows Mobile (bless their souls). In
    2021, the group won first prize at a cosplay competition, which came with enough
    funds to allow the group to purchase a set of **HoloLens** headsets along with
    an **Oculus VR** device for a member whose health problems prevented them from
    attending events in person. The AR-enhanced Player App would need to be able to
    talk to these devices to be useful as well as utilize existing functionality within
    the Player App (for example, displaying the player’s inventory). Finally, the
    group has developed a custom C# desktop application they appropriately call the
    “GM App” to connect to these BT devices and to act as a game’s referee (often
    called a **GM** or **Game Master**). The app’s maintainers have the wonderful
    opportunity here to evolve the app toward their vision in valuable and discrete
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Bring the app over into a **React Native** application that otherwise behaves
    exactly as it currently does.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add basic rendering capabilities with Babylon in a **WebView**. This will allow
    the team to release the same functionality with the same code base as the web
    app.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build local mesh connectivity between BT and WiFi devices that feeds data into
    **React Native** app.
  prefs: []
  type: TYPE_NORMAL
- en: Integrate a **Babylon Native** rendering of a pure 3D scene in the C# application
    to show GMs different views of the action (picture a sword fight where the swords
    have sensors embedded in them, with the scene depicting the state of the swords
    as relayed by sensors).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transition rendering responsibilities from a **WebView** to **Babylon React
    Native**. Use Babylon.js with **WebXR** to leverage device capabilities to render
    scenes onto a live image stream or to a VR set in a remote location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enjoy LARPing!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example isn’t intended to be comprehensive or exhaustive, but it does cover
    a decent range of potential use cases by implication. When embarking on a Native
    project, it is worth considering whether the same goals might be accomplished
    more easily using a different framework such as Unity or Unreal. It is also important
    to keep in mind that the current (summer 2022) state of the project at the time
    of writing is still immature, and thus there are limitations and gaps in supported
    functionality. Check the links in the next section to get the latest information
    on what is supported and what isn’t in Babylon Native.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the project is rapidly evolving, so too is the documentation. Here are some
    places to start reading more about Babylon Native and Babylon React Native at
    the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.babylonjs.com/native/](https://www.babylonjs.com/native/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.babylonjs.com/reactnative/](https://www.babylonjs.com/reactnative/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although it was short, our overview of the Babylon Native campus has covered
    the more important guideposts and signs that mark the various trails throughout
    the area. As a collection of technologies, Babylon Native is all about fitting
    the right set of tools to the right situation. Web apps that already use React
    or apps using React Native are the most stable and advanced implementations currently
    available, but Babylon Native is the path to follow if you’re looking to build
    an AR app that runs on iOS. Each of those approaches has its benefits and drawbacks,
    some potentially quite significant. The good news is that regardless of which
    approach is chosen, the code you write that interacts with Babylon.js doesn’t
    need to change for multiplatform targeting scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we’ve got business to attend to with our first guide, Andrei Stepanov.
    Andrei has been working with Babylon.js and **Content Management Systems** (**CMSs**)
    for a long time now, so he’s the perfect person to give us a quick tour of how
    BJS can be used in e-commerce and CMS business scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating 3D Content into a Website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to understanding how to make Babylon.js work in real-world, customer-centric
    business scenarios, there aren’t many people more knowledgeable about the topic
    than Andrei, who posts to the BJS community forums under the name of “Labris.”
    As a senior 3D developer at MetaDojo ([https://metadojo.io](https://metadojo.io)),
    he satisfies and delights clients with 3D experiences built to spec. Not content
    with just talking about how to build and create with Babylon.js, Andrei is also
    the creator of the **BabylonPress** site ([https://babylonpress.org](https://babylonpress.org)),
    which serves as a showcase of different examples and patterns that use BJS in
    conjunction with the **WordPress** CMS.
  prefs: []
  type: TYPE_NORMAL
- en: Babylon.js and CMS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Babylon.js lets us build very complex JS 3D applications from scratch. At the
    same time, there are a lot of cases when we need to integrate Babylon.js into
    an already existing website with CMS – an application that enables users to create,
    edit, publish and store digital content – or just to some HTML template.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous ways to do this, on different levels. They will depend on
    specific needs, especially on the “3D User Experience,” which you need to provide.
    Since the number and variety of different CMS wouldn’t allow us the luxury of
    describing all possible solutions in this space, I will explain in the next few
    subsections just some of the most common solutions and approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The Babylon Viewer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Babylon.js has an official extension, Babylon Viewer, which may simplify a lot
    of time for integration. It even has its own HTML tags, `<babylon></babylon>`,
    between which you define all needed parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To display a 3D model in a prepared environment – with already tuned lights,
    shadows, reflections, and so on – you just need to add a script reference to the
    viewer like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, add a `<babylon>` tag and set the model attribute to point to a `.gltf`
    or `.glb` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the `.gtlf` and `.glb` formats, `.babylon`, `.obj`, and `.stl` formats.
    Its simplicity allows easy integration of Babylon Viewer into any CMS and makes
    it an ideal choice for cases where you need to display a lot of different 3D models
    (e-commerce, game websites, and 3D artist blogs) in a user-editable CMS. More
    information about different Babylon Viewer configurations is available here: [https://doc.babylonjs.com/extensions/babylonViewer/configuringViewer](https://doc.babylonjs.com/extensions/babylonViewer/configuringViewer).'
  prefs: []
  type: TYPE_NORMAL
- en: Babylon Viewer 3D WordPress Plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Built on the base of **Babylon Viewer**, there also exists a community extension:
    the **Babylon Viewer 3D Wordpress plugin**. This allows you to display 3D models
    and 3D scenes with the help of a **Shortcode**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can use the 3D Viewer in `README` file at its home on GitHub at [https://github.com/eldinor/babylon-wordpress-plugin](https://github.com/eldinor/babylon-wordpress-plugin).
  prefs: []
  type: TYPE_NORMAL
- en: Kiosk Mode and Iframes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With regards to iframe implementations, it is worth mentioning that the **Babylon
    Sandbox** ([https://sandbox.babylonjs.com/](https://sandbox.babylonjs.com/)) has
    a special “kiosk” mode that allows you to use its functionality with any 3D model
    in appropriate format. As an example, have a look at this beautiful example (a
    3D model of an ancient mosquito in amber) of **GLTF** transparency in the **Khronos
    Group** article: [https://www.khronos.org/news/press/new-gltf-extensions-raise-the-bar-on-3d-asset-visual-realism](https://www.khronos.org/news/press/new-gltf-extensions-raise-the-bar-on-3d-asset-visual-realism).'
  prefs: []
  type: TYPE_NORMAL
- en: The different query string elements embedded within the URL allow the content
    creator or manager to define the source 3D file and all other parameters, such
    as camera position, auto-rotation behavior, the skybox, and environment texture.
  prefs: []
  type: TYPE_NORMAL
- en: To use “kiosk mode,” define the URL according to the following table. The first
    parameter starts with `?` after [https://sandbox.babylonjs.com/](https://sandbox.babylonjs.com/);
    all others start with `&` before the parameter. Also note that since Babylon.js
    is an open source project, you can create and host your own version of the Sandbox!
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 14.1 – Table of parameters for iframes for the BJS Sandbox'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_14.01_B17266.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 14.1 – Table of parameters for iframes for the BJS Sandbox
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end, you’ll get something like this – quite a long HTML link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://sandbox.babylonjs.com/?kiosk=true&assetUrl=https://raw.githubusercontent.com/wallabyway/gltf-presskit-transparency/main/docs/MosquitoInAmber_withRefraction.glb&cameraPosition=-0.14,0.005,0.03&autoRotate=true&skybox=true&environment=https://assets.babylonjs.com/environments/studio.env](https://sandbox.babylonjs.com/?kiosk=true&assetUrl=https://raw.githubusercontent.com/wallabyway/gltf-presskit-transparency/main/docs/MosquitoInAmber_withRefraction.glb&cameraPosition=-0.14,0.005,0.03&autoRotate=true&skybox=true&environment=https://assets.babylonjs.com/environments/studio.env)'
  prefs: []
  type: TYPE_NORMAL
- en: The BJS Playground and Iframes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another option that is especially useful for displaying scenes directly from
    Babylon Playground is a special HTML template. Just add `frame.xhtml` before the
    Playground URL and it will show the render area in full screen, but with a bottom
    toolbar showing FPS, reload and edit buttons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example: [https://www.babylonjs-playground.com/frame.xhtml#6F0LKI#2](https://www.babylonjs-playground.com/frame.xhtml%236F0LKI%232).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To show only the render area, use `full.xhtml` as the prefix. More info about
    Playground URL formats is available here: [https://doc.babylonjs.com/toolsAndResources/tools/playground#playground-url-formats](https://doc.babylonjs.com/toolsAndResources/tools/playground#playground-url-formats).
    The result of this option is that you can then use that URL as the source for
    an iframe image element – see [https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe)
    for how to define an iframe element.'
  prefs: []
  type: TYPE_NORMAL
- en: Babylon.js within a CMS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, if you are looking for more close integration between Babylon.js and
    a CMS, you would need to take into consideration these universal steps. Make sure
    that you have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Babylon.js scripts are loaded properly. Depending on the CMS, you can also
    load Babylon.js conditionally if there is 3D content to be displayed on the web
    page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CMS supports the uploading of 3D files (most modern CMSs have a limited
    set of allowed file extensions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A proper canvas element to display. It makes sense to assign a unique ID to
    each Babylon canvas (for example, with the help of a post ID or other CMS variable).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canvas and BJS Engine elements properly hooked up to respond to resizes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the complexity and the scale of applications depends only on your creativity.
    Server-side languages can preprocess any needed data before delivering it to a
    JS client, allowing us to build a truly 3D CMS, where all user experiences and
    interactions happen in 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: Babylon.js is not just another JavaScript framework to use for two-dimensional
    websites; it is one of the key components required to build multi-user 3D worlds
    and metaverses, at least with the current meaning of this term.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a big difference between loading a 3D model onto a single web page and
    bringing potentially lots of 3D models onto lots of different web pages. Managing
    the content and change processes is of utmost importance, but with the Guidance
    of Andrei, you’ll be ready to face those challenges and more. Now, what does 3D
    content in an e-commerce or CMS app have to do with a roughly 50-year-old technique
    for photo-realistic renders? Why, Babylon.js, of course! It’s time to continue
    our tour as we transition from the highly practical to the highly experimental
    side of 3D programming.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing out a Path to Advanced Rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our last stop on our *Extended Topics, Extended* tour is with musician, engineer,
    and graphics wizard Erich Loftis. He’s going to guide us with the story of his
    journey to achieving **Real-Time Path Tracing** (**RTPT**) with Babylon.js. RTPT
    – also referred to as **Ray Tracing** or just **RT** – is a rendering technique
    built on top of Path Tracing that companies such as Nvidia and AMD are only just
    beginning to make available in AAA commercial titles, and only in select ways.
    Through the retelling of Erich’s journey, the reason why the technique has been
    so difficult to accomplish in real-time games and simulations will hopefully become
    abundantly clear.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Tracing and its History by Erich Loftis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RT is a technique for rendering realistic images and effects on a computer.
    It follows the laws of optics and models how physical light rays behave in the
    real world. Therefore, RT can produce truly photo-realistic images. **RT** is
    *the* standard for photo-realistic offline rendering. Because it has found its
    way into real-time applications and games (where Rasterization was the undisputed
    king), it’s important to have at least a basic understanding of how it all works
    under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the awesome **Babylon.js** engine, we can use this understanding
    to make our own **Physically Based Renderer** that runs right inside the browser.
    This is important because it opens the door to experiencing photo-realistic graphics
    on any platform or device, even your cellphone. Of course, the journey to get
    to this point wasn’t exactly the most straightforward or easy, but as you’ll see
    from the demos and examples, the effort is totally worth it!
  prefs: []
  type: TYPE_NORMAL
- en: My Own RT Journey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It was this dream of experiencing **RT** on all devices that led me to create
    a Path Tracing Renderer for **three.js,** starting back in 2015\. For the past
    7+ years, I have been slowly but steadily researching, building, refining, and
    optimizing a browser-based renderer that not only produces high-quality, photo-realistic
    images but also aims to do so at 30–60 frames a second! Please check out my ongoing
    project on **GitHub**, where you can try dozens of clickable demos: [https://github.com/erichlof/THREE.js-PathTracing-Renderer](https://github.com/erichlof/THREE.js-PathTracing-Renderer).'
  prefs: []
  type: TYPE_NORMAL
- en: A while ago, back in 2020, a Babylon.js developer reached out to me and asked
    if I could possibly make a similar renderer for the BJS engine. I must state here
    that I have primarily worked with **three.js** all these years, but I have always
    admired and been impressed by the amazing **Babylon.js** library. When I agreed
    to do the port of my ray/path tracing system from three.js to BJS, I was equally
    impressed with the BJS forum community. They are so friendly and helpful and are
    just awesome folks! I couldn’t have gotten our BJS renderer up and running without
    their help and support. So, before we dive in, a quick shout out to you all –
    thank you, BJS community!
  prefs: []
  type: TYPE_NORMAL
- en: RT or Rasterization?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What does it take to get interactive, real-time RT working inside of BJS and
    a browser? Firstly, let’s take a quick look at the two main techniques for rendering
    3D graphics. Once we see how that works, we’ll also see why we would want to try
    this RT route with BJS.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to displaying 3D graphics on a 2D screen, there are two main
    approaches: **Rasterization** and **RT**. In a nutshell, Rasterization works by
    first taking the **scene geometry**, in the form of 3D vertices, and then projecting
    those to the screen in the form of many flat 2D triangles. Any pixels on the device’s
    display that happen to occupy a screen triangle’s area are sent to the pixel shader
    (also known as a fragment shader). When the fragment shader is run on a pixel,
    its final display color is computed. All of these colored pixels make up the final
    image that we see on our devices.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, RT renders images by addressing each pixel on the display first.
    For every pixel on the screen, a geometric **Ray** is constructed that starts
    at the camera’s position. Pointing from the camera, this **Camera Ray** then shoots
    out toward its pixel wherever it lies on the view plane (usually your screen).
    After piercing through the target pixel, the Camera Ray continues out into the
    3D scene. It will then model how a physical light ray in the real world would
    interact with its environment.
  prefs: []
  type: TYPE_NORMAL
- en: Only now at this point in the pipeline do we consider the scene geometry. Each
    of the camera pixel rays is tested for intersection against every 3D shape in
    the scene. Wherever the ray hits a surface, the color and lighting at that location
    are recorded and a “bounce” ray is then spawned and sent in a new direction. This
    direction is dictated by the material properties of the hit surface location.
    Further, the bounce ray must check the entire scene geometry (every 3D shape or
    triangle) for any intersections just as its parent ray did, thus repeating the
    whole process again and again for however long you are willing to wait for it
    to complete. After a pixel’s camera ray and its spawned bounce rays finish interacting
    with the scene, the ray tracer reports back the final color for that pixel. Just
    like Rasterization, we end up with a screen full of colored pixels but with a
    totally different path taken to arrive at these results!
  prefs: []
  type: TYPE_NORMAL
- en: Both rendering approaches have trade-offs in terms of realism and speed. **Rasterization**
    (comprising 99% of all 3D graphics) has full GPU hardware support, so it is very
    fast and efficient. There’s a drawback, though. As soon as the GPU is done projecting
    and rasterizing the scene’s triangles to the 2D screen, the surrounding 3D scene
    information is lost. To retrieve this lost global scene information, sophisticated
    techniques such as light mapping, shadow mapping, reflection probes, and others
    must be used. In other words, a lot of graphics knowledge and extra effort is
    required to get close to RT-quality visuals.
  prefs: []
  type: TYPE_NORMAL
- en: RT, on the other hand, automatically produces the ultimate in realistic graphics,
    right out of the box! Lighting effects that are difficult if not impossible with
    Rasterization just naturally fall out of the RT algorithm. However, as of 2022,
    RT is not widely supported by most GPU hardware. All CPUs can run RT programs,
    but CPUs aren’t designed to be massively parallel. Therefore, traditional CPU-based
    software RT is very slow in comparison to hardware-accelerated Rasterization on
    the GPU. Even if the RT software is moved inside a shader that runs entirely on
    the GPU (as our project here will do), several RT algorithm optimizations must
    be made in that shader, and/or a decent acceleration structure such as a **Bounding
    Volume Hierarchy** (**BVH**) is required if we can have any hope of RT at interactive
    frame rates.
  prefs: []
  type: TYPE_NORMAL
- en: Taking the RT Route
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, knowing most of these trade-offs in advance (and some not until I was years-deep
    into the project – ha!), I decided to go the RT route. I’ll now fast-forward to
    when I started implementing RT with Babylon.js as the host engine. I’ll give an
    overview of the necessary setup, as well as a few code snippets to show some of
    the implementation details. Let’s jump right in!
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are now following the RT approach, we must find a way to construct
    a viewing ray from the camera through to each and every pixel on the screen. A
    common method for gaining access to the screen pixels is to create a **Full-Screen
    Post-Process Effect**, or just **Post-Process** for short (as you learned in [*Chapter
    10*](B17266_10_Final_AM.xhtml#_idTextAnchor207), *Improving the Environment with
    Lighting and Materials*). Since the post-process is a common operation, BJS has
    a really handy library wrapper that takes care of all the **WebGL** boilerplate
    code and post-process setup for us. In BJS, this helper is called an **EffectWrapper**.
    Here’s an example of a typical post-process creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, here is where the setup gets a little tricky, not because of `pathTracingEffect`),
    we ray trace on all pixels and save their color results by using a **Render Target
    Texture** (**RTT**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This large `screenCopyEffect`) and then fed back through to the first post-process
    (`pathTracingEffect`) on the next animation frame. Now, our GPU ray tracer can
    use its previous result (its own pixel color history) to blend with the fresh
    new pixel color results that it is currently calculating from RT. In other words,
    it keeps blending and mixing with itself again and again. Over a couple of hundred
    frames or so, this ping-pong feedback process will quickly produce very smooth
    anti-aliased results that seem to magically converge right before our eyes! The
    last piece of the rendering setup puzzle is a final monitor output post-process
    (named `screenOutputEffect`). Its job is to perform **noise filtering**, then
    **Tone Mapping** (which you learned about in *Tone Mapping and Basic Post-Processing*
    section of[*Chapter 10*](B17266_10_Final_AM.xhtml#_idTextAnchor207)*, Improving
    the Environment with Lighting and Materials*), and then finally some **gamma correction**
    (also in *Tone Mapping and Basic Post-Processing* section of [*Chapter 10*](B17266_10_Final_AM.xhtml#_idTextAnchor207)*,
    Improving the Environment with Lighting and Materials*) to produce more pleasing
    color output on digital monitors and screens.
  prefs: []
  type: TYPE_NORMAL
- en: 'All in all, we need a total of three post-processing effects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pathTracingEffect`: This performs all of the RT calculations on every single
    pixel. It will take whatever pixel history given to it by the following `screenCopyEffect`
    to use for blending with itself. It outputs to **RenderTargetTexture (RTT)**,
    which is finally fed to the following post-process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`screenCopyEffect`: This takes that supplied RTT output from the preceding
    post-process and copies/saves it to its own RTT. It then sends this saved copy
    back through to the preceding `pathTracingEffect` to use for blending with itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`screenOutputEffect`: This post-process is responsible for the screen’s final
    color output. It takes the preceding `pathTracingEffect` **RTT** (which holds
    all the refined, **ping-pong** blended, ray-traced pixel results so far), applies
    its special filters and pixel color adjustments, and then directly outputs to
    your screen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The first two effects make up the **ping-pong buffers**, or feedback loop.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our custom system set up for progressively refining our ray-traced
    images over time and can correctly display the final pixel color output, we just
    need to do one more thing – the actual RT! Let’s switch gears for a moment and
    briefly discuss the similarities and differences between RT and **Path Tracing**
    (**PT**), and what our ray tracers/path tracers will need in order to do their
    magic in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: The Path to PT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To best understand how RT and PT are related, let’s follow a brief timeline/lineage
    of RT discoveries and techniques in CG history. In 1968, Arthur Appel invented
    Ray Casting, a groundbreaking technique in which mathematical rays are shot out
    from the camera through every pixel. Whatever these camera rays hit first out
    in the 3D scene determines what we see in our image. Then, in 1979, Turner Whitted
    invented RT, which relies on Appel’s earlier 1968 Ray Casting technique but does
    it many times recursively while following the laws of optics, in order to capture
    physically accurate reflections and refractions from specular surfaces (mirrors,
    glass, and so on). Then, in 1986, James Kajiya invented PT, the ultimate evolution
    of RT. Building from all the previous RT techniques, Kajiya added Monte Carlo
    integration (random sampling and averaging) to randomly sample material BRDFs
    (diffuse surfaces in particular), in order to capture physical light effects such
    as caustics and inter-reflected diffuse surface “bounce lighting.” PT gets its
    name from tracing (random sampling) all the possible paths that light rays might
    take as they interact with different types of materials in the scene, and then
    gathering all of these light paths’ contributions to produce a ground-truth, photo-realistic
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this potted RT/PT history, hopefully you can see how PT is related
    to, evolved from, and improves upon RT (and Ray Casting before that). Since I
    wanted the ultimate in realistic graphics, I chose the more sophisticated **Monte
    Carlo PT** method (1986 Kajiya-style), which captures light effects that are impossible
    with Rasterization and even older-style RT. And thanks to our hard work on setting
    up the progressively refining post-process effects system, our randomly sampled
    **Monte Carlo** PT results for all pixels can be correctly averaged and refined
    over time into a **ground-truth image**. This basically means photo-realistic
    rendering in your browser!
  prefs: []
  type: TYPE_NORMAL
- en: PT in the Browser
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let’s discuss scene geometry and what PT requires in terms of how the scene
    is defined. We have two options for telling the PT fragment shader what is in
    the scene. The first and easiest option is to simply write a GLSL function in
    the fragment shader itself that defines the entire scene’s geometry as part of
    the shader. All objects/shapes are hardcoded and listed one after the other. This
    is fine if the number of shapes/objects in your scene does not exceed 20 or so,
    but as soon as you get into the hundreds of objects or, worse yet, use a typical
    model with thousands of triangles (with each triangle being tested by every ray!),
    our path tracer would grind to a halt. To speed things up tremendously and keep
    our PT interactive, we need to use an acceleration structure, such as a **BVH**. A
    **BVH** is basically just a binary tree of bounding boxes that tightly surrounds
    the triangular model(s). When testing for intersection, rays can skip large portions
    of the model if they miss some of the larger bounding boxes. To see how a BVH
    is built, check out my custom BVH builder code at [https://github.com/erichlof/Babylon.js-PathTracing-Renderer/blob/main/js/BVH_Fast_Builder.js](https://github.com/erichlof/Babylon.js-PathTracing-Renderer/blob/main/js/BVH_Fast_Builder.js).
    Recall that the path tracer (inside the fragment shader) must have access to the
    entire scene, and since we can’t fit most large scenes containing thousands of
    triangles into shader **uniforms** (there is a hard limit on most graphics cards),
    we tightly pack the BVH and all its bounding boxes into a data texture. This BVH
    texture will give our GPU path tracer quick and easy access to the entire optimized
    scene geometry (via simple texture lookups).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, all ray tracers and path tracers require a shape intersection library
    to have ray intersection testing with a wide variety of primitive shapes, such
    as spheres, boxes, and triangles. Historically, when RT was just coming into existence,
    computers were only fast enough to intersect rays with simple mathematical shapes.
    Examples of these shapes include spheres, cylinders, cones, and planes, and they
    all belong to a class of shapes known as quadrics. The solution for where a ray
    intersects these quadric shapes is handled by simply solving the quadratic equation
    for that shape. That’s why, when you look at more historical ray-traced images,
    the scenes only contain checkered planes and spheres (or other quadrics) of different
    sizes and materials. In these early years of RT, the math for intersecting rays
    with more complex triangle geometry (like what we use today) was well understood,
    but it would take many years for computers to get fast enough to be able to handle
    testing rays with an entire polygonal 3D model with thousands of triangles. Over
    the last 7 years, I have collected almost every routine I could find for determining
    the intersection of rays against various shapes. Here’s a link to my `PathTracingCommon.js`
    file, which contains all of these intersection routines: [https://github.com/erichlof/Babylon.js-PathTracing-Renderer/blob/main/js/PathTracingCommon.js](https://github.com/erichlof/Babylon.js-PathTracing-Renderer/blob/main/js/PathTracingCommon.js).
    Equally important and also included in this library file are the functions that
    handle **Monte Carlo PT**-style random sampling of different light source types
    (point, spot, directional, area, and HDRI) and material types (BRDFs from the
    *Tone Mapping and Basic Post-Processing of* [*Chapter 10*](B17266_10_Final_AM.xhtml#_idTextAnchor207)*,
    Improving the Environment with Lighting and Materials*) that rays might interact
    with in any given scene.'
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Well, unfortunately, there isn’t enough space in this more general, overview-style
    article to go into detail about my **GLSL** PT shader code (where all the RT/PT
    algorithms happen). However, if you want to see some nice examples of RT/PT in
    GLSL (where I have learned from too), check out a couple of these shaders on **Shadertoy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.shadertoy.com/view/Xtt3DB](https://www.shadertoy.com/view/Xtt3DB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.shadertoy.com/view/XsSSWW](https://www.shadertoy.com/view/XsSSWW)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.shadertoy.com/view/XdcfRr](https://www.shadertoy.com/view/XdcfRr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.shadertoy.com/view/tddSz4](https://www.shadertoy.com/view/tddSz4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And if you would like to go much deeper into the theory and practice of RT
    and PT, I can think of no better resource than **Scratchapixel**. This amazing
    website contains everything you need to know about Rasterization, RT, PT, and
    graphics in general: [https://www.scratchapixel.com/](https://www.scratchapixel.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, to see all of the pieces of this article come together, check out the
    Babylon.js **PathTracing** Renderer: [https://github.com/erichlof/Babylon.js-PathTracing-Renderer](https://github.com/erichlof/Babylon.js-PathTracing-Renderer)'
  prefs: []
  type: TYPE_NORMAL
- en: This is our ongoing project, which has several clickable demos that showcase
    different areas of PT. As with the Space-Truckers OSS project, this BJS **PathTracing**
    Renderer project is open for Pull Requests. If you start getting into this fascinating
    world of RT and PT, we would love to see your contributions! A word of warning
    though – once you start down the road of RT and PT, it can be hard to stop!
  prefs: []
  type: TYPE_NORMAL
- en: Happy rendering!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve seen a lot of new things on our trip through the BJS *Metatropolis*. We’ve
    heard of new wonders under construction but ready for business, such as **VR**
    and **AR** with **WebXR**. To help developers make use of these wonders, we learned
    about how Babylon.js offers the **WebXRExperienceHelper**. Working in conjunction
    with the **FeaturesManager**, it allows developers to code with confidence against
    a rapidly evolving and changing standard.
  prefs: []
  type: TYPE_NORMAL
- en: Babylon.js is a project that places backward compatibility as one of its cornerstone
    principles, and so as hardware improves – or more products open up their hardware
    to **WebXR** APIs – capabilities will “light up” as browser vendors add support.
    While it would be great to include iOS (and **WebKit**) in the supported application
    list today for **WebXR**, and while we can lament for a world that could have
    been, applications using Babylon.js will be ready to best take advantage when
    that day finally does arrive.
  prefs: []
  type: TYPE_NORMAL
- en: Until that happens, developers and designers have several potential approaches
    that will ideally allow the greatest code reuse and lowest friction to implement
    and maintain. The **Babylon.js Native** project is a collection of tools and techniques
    that people working on cross-platform or Native projects can leverage to gain
    maximum productivity and effectiveness. These tools fall into a spectrum going
    from full-on bare-metal BJS Native to the “vanilla” BJS that we’ve come to know
    and love. In between, **Babylon React Native** provides a way for developers already
    using React and React Native to incorporate BJS into their applications, while
    toward the other end of the spectrum, the hosting of a **WebGL** context in a
    **WebView** provides another avenue for potential native device application integration
    in arbitrary software apps.
  prefs: []
  type: TYPE_NORMAL
- en: Babylon.js is more than about making games such as Space-Truckers. As a general
    3D application development platform, BJS gives us access to entire universes of
    possibilities, waiting to be unlocked by curious explorers. Perhaps one of those
    curious explorers will be you! Every coin has a flip side, and the flip side of
    having so many possibilities is that it’s very difficult to give a good account
    of the more interesting ones in the same context as the rest of our journey with
    Space-Truckers. That is where our two guides come into play. As long-time explorers
    into some of these other provinces of BJS, Andrei Stepanov and Erich Loftis have
    much to share with the community.
  prefs: []
  type: TYPE_NORMAL
- en: Through his **Babylon Viewer 3D WordPress Plugin** and his extensive and detailed
    example site, [babylonpress.org](http://babylonpress.org), which shows off the
    viewer, Andrei has opened our eyes to how easy it can be to use **shortcodes**
    to include 3D models as a content editor once the proper script references have
    been injected into the CMS page. By telling us of his journey into PT/RT, Erich
    Loftis has, in turn, opened our eyes to the innovative history of graphics rendering
    technologies and how they’re used in the world of computer graphics.
  prefs: []
  type: TYPE_NORMAL
- en: Each of them has given us their unique insights and approaches to their respective
    topics and helped to guide us to the Terminal Destination for this book. Although
    this is the end of one journey, it is just the beginning of another. Unlike this
    book though, the path for this new journey – your journey – isn’t captured or
    written out anywhere, nor is there any pre-determination on what route that path
    will take. Where this path takes and what it entails is entirely up to you, but
    wherever that destination lies, whether shrouded in mist or lit up with a beacon,
    you’re not alone. The BJS community is there to assist, support, and, of course,
    guide folks. The BJS forums at [https://forum.babylonjs.com](https://forum.babylonjs.com)
    are the best place to go to ask questions, meet folks like Erich and Andrei, and
    learn from other community members.
  prefs: []
  type: TYPE_NORMAL
- en: Good luck on your journey – the world of web-based 3D and the BJS community
    awaits!
  prefs: []
  type: TYPE_NORMAL
