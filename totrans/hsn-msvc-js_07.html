<html><head></head><body>
		<div id="_idContainer079">
			<h1 class="chapter-number" id="_idParaDest-120"><a id="_idTextAnchor121"/>7</h1>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor122"/>Asynchronous Microservices</h1>
			<p>Microservices are designed to be independent and self-contained. Clearly defined communication protocols and APIs ensure these services interact without relying on each other’s internal workings. Defining proper communication between microservices is important for a well-functioning <span class="No-Break">microservices architecture.</span></p>
			<p>In this chapter, we plan to discuss and learn about another important communication mechanism: asynchronous communication <span class="No-Break">between microservices.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding <span class="No-Break">the requirements</span></li>
				<li>Exploring <span class="No-Break">asynchronous communication</span></li>
				<li>Implementing an asynchronous <span class="No-Break">transaction microservice</span></li>
				<li>Adapting an account service to <span class="No-Break">new requirements</span></li>
				<li>Testing our <span class="No-Break">microservices together</span></li>
			</ul>
			<p>Let’s get <span class="No-Break">into it!</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor123"/>Technical requirements</h1>
			<p>To follow along with the chapter, you’ll need an IDE (we prefer Visual Studio Code), Postman, Docker, and a browser of <span class="No-Break">your choice.</span></p>
			<p>It is preferable to download the repository from <a href="https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript">https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript</a> and open the <strong class="source-inline">Ch07</strong> folder to easily follow the <span class="No-Break">code snippets.</span></p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor124"/>Understanding the requirements</h1>
			<p>Up until now, we have developed two<a id="_idIndexMarker515"/> simple microservices and for the current chapter we plan to extend our transaction microservice to meet the <span class="No-Break">following requirements:</span></p>
			<ul>
				<li>Every transaction should support the following statuses: <strong class="source-inline">CREATED</strong>, <strong class="source-inline">FAILED</strong>, <strong class="source-inline">APPROVED</strong>, <strong class="source-inline">DECLINED</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">FRAUD</strong></span><span class="No-Break">.</span></li>
				<li>The transaction service should now have a new method that changes the status of the given transaction to <strong class="source-inline">FRAUD</strong>. It will update the status of the transaction to <strong class="source-inline">FRAUD</strong> and produce a message about <span class="No-Break">the transaction.</span></li>
				<li>The account service will consume this message and after <em class="italic">three</em> fraudulent attempts, the account service should read and suspend/block the <span class="No-Break">given account.</span></li>
			</ul>
			<p>We plan to use asynchronous communication between microservices and any other microservice may use this message for internal purposes. You can check <a href="B09148_02.xhtml#_idTextAnchor027"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> for more information about asynchronous communication <span class="No-Break">between microservices.</span></p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor125"/>Exploring asynchronous communication</h1>
			<p>You can implement asynchronous communication between microservices using various patterns and technologies, each suitable for different use cases and requirements. Here are some of the <span class="No-Break">common ones:</span></p>
			<ul>
				<li><strong class="bold">Message brokers</strong>: Message <a id="_idIndexMarker516"/>brokers facilitate<a id="_idIndexMarker517"/> asynchronous communication by allowing microservices to publish and subscribe to messages. Popular message brokers include <strong class="bold">RabbitMQ</strong>, which<a id="_idIndexMarker518"/> supports multiple messaging protocols and patterns such as pub/sub and routing, and <strong class="bold">Apache Kafka</strong>, designed for high-throughput<a id="_idIndexMarker519"/> and fault-tolerant event streaming – one of the best choices for real-time data processing. An example of a message broker would be a producer service sending a message to a queue or topic and the consumer service subscribing to the queue or topic and <span class="No-Break">processing messages.</span></li>
				<li><strong class="bold">Event streaming platforms</strong>: Event streaming platforms capture and process streams of events. These platforms are <a id="_idIndexMarker520"/>particularly useful for real-time analytics and data pipeline<a id="_idIndexMarker521"/> construction. Popular event streaming platforms include <strong class="bold">Apache Kafka</strong>, which is often used as both a message broker and an event streaming platform, and <strong class="bold">Amazon Kinesis</strong>, a managed service for real-time<a id="_idIndexMarker522"/> data processing at scale. Here is an example: a producer service emits events to a Kafka topic and consumer services consume events from the topic and react <span class="No-Break">to them.</span></li>
				<li><strong class="bold">The Publish-Subscribe pattern</strong>: In the pub/sub pattern, messages are published to a topic and multiple subscribers<a id="_idIndexMarker523"/> can consume these messages asynchronously. Popular services <a id="_idIndexMarker524"/>that use the pub/sub pattern<a id="_idIndexMarker525"/> include <strong class="bold">Google Pub/Sub</strong>, a fully managed real-time messaging service, and <strong class="bold">AWS Simple Notification Service</strong> (<strong class="bold">SNS</strong>), which allows publishing<a id="_idIndexMarker526"/> messages to multiple subscribers. For example, a publisher service publishes an event to a topic and the subscriber services receive notifications and process <span class="No-Break">the event.</span></li>
				<li><strong class="bold">Task queues</strong>: Task queues are used to<a id="_idIndexMarker527"/> distribute tasks to worker services asynchronously. This is<a id="_idIndexMarker528"/> useful for offloading heavy or time-consuming tasks from the main service. Some of the more popular<a id="_idIndexMarker529"/> task queues are <strong class="bold">Celery</strong>, an asynchronous task queue/job queue based on distributed message passing, and <strong class="bold">Amazon Simple Queue Service</strong> (<strong class="bold">SQS</strong>), a fully managed message queue service. Here’s how a task queue works: a<a id="_idIndexMarker530"/> producer service creates a task and places it in the queue and the worker service picks up the task from the queue and <span class="No-Break">processes it.</span></li>
				<li><strong class="bold">Event-driven architecture</strong>: In an <a id="_idIndexMarker531"/>event-driven architecture, services communicate through events. When something notable <a id="_idIndexMarker532"/>happens in one service, it emits an event that other services can listen to and act upon. In event-driven architecture an event source service publishes an event, and the event listener services react to the event and execute <span class="No-Break">their logic.</span></li>
				<li><strong class="bold">WebSockets</strong>: WebSockets allow for full-duplex <a id="_idIndexMarker533"/>communication channels over a single TCP connection, useful for <a id="_idIndexMarker534"/>real-time applications such as chat apps or live updates. Here’s an example: the server pushes updates to clients via WebSockets and clients receive updates in real time and act <span class="No-Break">upon them.</span></li>
				<li><strong class="bold">Server-Sent Events</strong> (<strong class="bold">SSE</strong>): SSE is a server push technology enabling servers to push real-time updates to the client<a id="_idIndexMarker535"/> once an initial client connection is established. Let’s take an <a id="_idIndexMarker536"/>example: the server sends events to clients over an HTTP connection and clients listen to incoming messages and <span class="No-Break">process them.</span></li>
				<li><strong class="bold">gRPC with streaming</strong>: gRPC supports bidirectional streaming, allowing both client and server to send a <a id="_idIndexMarker537"/>sequence of messages using a single connection. gRPC works like this: the client and server can continuously exchange streams of messages as part of a single <span class="No-Break">RPC call.</span></li>
			</ul>
			<p>For this chapter, we will actively use Apache Kafka, an open source, high-performance event streaming platform. It is a popular choice for asynchronous communication between microservices due to its strengths in enabling a robust and scalable event-driven architecture. While we have already talked about how to run services via Docker, this chapter will focus on hosting Apache Kafka <span class="No-Break">on Docker.</span></p>
			<p>Let’s take a quick look at the problems that Apache <span class="No-Break">Kafka solves:</span></p>
			<ul>
				<li><strong class="bold">Communication complexity</strong>: In a microservice environment, you have <em class="italic">multiple sources</em> (every API acts as<a id="_idIndexMarker538"/> a source) and <em class="italic">multiple targets</em> (every API can have multiple sources to write to). The fact that sources and targets are scaled is always accompanied by a communication problem. In this case, the problem is that we should solve the complexities created by the source and target rather than focus on business requirement implementations. Now you have multiple sources and targets, which can create the <span class="No-Break">following issues:</span><ul><li>Every target requires a different protocol <span class="No-Break">to communicate.</span></li><li>Every target has its data format to <span class="No-Break">work with.</span></li><li>Every different target requires maintenance <span class="No-Break">and support.</span></li></ul><p class="list-inset">In simple terms, say you have a microservice application, and every service has its own target. Besides that, every service can have multiple sources, and the services can use common sources. Apache Kafka helps you to avoid complex communication <span class="No-Break">between microservices.</span></p></li>
				<li><strong class="bold">Communication complexity duplication</strong>: Whenever similar systems are developed; we have to rewrite such communication processes again and again. Let’s imagine that we are working on several different projects. Although the domain of these projects is different, and although they solve different problems at an abstract level, the common aspect of these projects is communication complexity. So, it means we’re repeating ourselves and trying to resolve the same<a id="_idIndexMarker539"/> issue <span class="No-Break">every time.</span></li>
				<li><strong class="bold">Fault tolerance</strong>: The system should be able to continue functioning and provide reliable data processing and message delivery even in the presence of various types of failures, such as hardware failures, network issues, or <span class="No-Break">software crashes.</span></li>
				<li><strong class="bold">High performance</strong>: In most cases, such a communication problem (sources - targets) causes the application performance to drop. Regardless of dynamic changes in the number of targets and sources in the application, the program should always support the <span class="No-Break">high-performance attribute.</span></li>
				<li><strong class="bold">Scalability</strong>: The system should be possible to horizontally scale sources and targets. Horizontal scaling, also known as scaling out, is <a id="_idIndexMarker540"/>a technique in software design for increasing the capacity of a system by adding more machines (nodes) to distribute <span class="No-Break">the workload.</span></li>
				<li><strong class="bold">Real-time communication</strong>: One of the possible target and source communication attributes is real-time communication. Depending on the use cases, the system should allow real-time data exchange between the source and <span class="No-Break">the target.</span></li>
				<li><strong class="bold">Log and data aggregation</strong>: This is the ability to combine and process logs and data in certain aggregates. Log and data aggregation play a crucial role in modern software by centralizing and organizing information from various sources, making it easier to analyze, troubleshoot, and <span class="No-Break">optimize applications.</span></li>
				<li><strong class="bold">Data transformation and processing</strong>: The communication between the target and source is not only in the form of data exchange but also information should be based <a id="_idIndexMarker541"/>on the possibility <span class="No-Break">of transformation.</span></li>
			</ul>
			<p>Now let’s talk about the infrastructure we need to use to implement <span class="No-Break">our microservices.</span></p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor126"/>Implementing an asynchronous transaction microservice</h1>
			<p>We will use the same transaction microservice we implemented in <a href="B09148_06.xhtml#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> but with additional changes that will help us <a id="_idIndexMarker542"/>add asynchronous behavior to it. First, we should prepare our infrastructure. Here is what we will have <span class="No-Break">in </span><span class="No-Break">it</span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Apache Kafka</strong>: To create loose <a id="_idIndexMarker543"/>coupling <span class="No-Break">between microservices.</span></li>
				<li><strong class="bold">Kafka UI</strong>: This is a web application <a id="_idIndexMarker544"/>designed for managing Apache Kafka clusters. It <a id="_idIndexMarker545"/>provides a <strong class="bold">graphical user interface</strong> (<strong class="bold">GUI</strong>) instead of the traditional <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) for Kafka, making it<a id="_idIndexMarker546"/> easier to interact with Kafka for <span class="No-Break">many users.</span></li>
				<li><strong class="bold">Zookeeper</strong>: This is open source<a id="_idIndexMarker547"/> software that acts as a central coordinator for large distributed systems. Think of it as a conductor for an orchestra, keeping everything <span class="No-Break">in sync.</span></li>
				<li><strong class="bold">PostgreSQL</strong>: To <span class="No-Break">store</span><span class="No-Break"><a id="_idIndexMarker548"/></span><span class="No-Break"> data.</span></li>
				<li><strong class="bold">PgAdmin</strong>: A graphical tool to<a id="_idIndexMarker549"/> visually see <span class="No-Break">database elements.</span></li>
			</ul>
			<p>We have our <strong class="source-inline">docker-compose.yml</strong> file in our root <span class="No-Break">folder (</span><span class="No-Break"><strong class="source-inline">Ch07/transactionservice</strong></span><span class="No-Break">).</span></p>
			<p>This <strong class="source-inline">docker-compose</strong> file defines a multi-service setup for a PostgreSQL database, a PgAdmin instance for managing the database, and a Kafka messaging system with Zookeeper for coordination. The services are connected through a custom Docker network, <strong class="source-inline">my-app-network</strong>, which enables inter-container communication. For Kafka, ensure the correct network settings are configured to avoid connectivity issues, especially for multi-network setups where <strong class="source-inline">advertised.listeners</strong> may be needed for both internal and external addresses. The PostgreSQL service stores its data in a named volume, <strong class="source-inline">postgres_data</strong>, while <strong class="source-inline">PgAdmin</strong> depends on PostgreSQL to be up and running. The Kafka and Zookeeper services are set up for message brokering, with Kafka UI providing management and monitoring, relying on Zookeeper to maintain a distributed <span class="No-Break">system configuration.</span></p>
			<p>Navigate to the root folder <a id="_idIndexMarker550"/>and run the <strong class="source-inline">docker-compose up -d</strong> command to spin up <span class="No-Break">the infrastructure.</span></p>
			<p>Here is how it should look after a successful run (<span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer074">
					<img alt="Figure 7.1: Docker infrastructure" src="image/B09148_07_001.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1: Docker infrastructure</p>
			<p>After successfully running our docker infrastructure, we are ready to switch to our source code to implement <a id="_idIndexMarker551"/><span class="No-Break">our requirements.</span></p>
			<p>First, we need to update our transaction service to support additional statuses. Open the <strong class="source-inline">schema.prisma</strong> file under the <strong class="source-inline">prisma/migrations</strong> folder and change <strong class="source-inline">enum</strong> to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
enum Status {
  CREATED
  FAILED
  APPROVED
  DECLINED
  FRAUD
}</pre>			<p>As we already know, one of the responsibilities of Prisma is to isolate us from database internals and provide a unique, more understandable language over these internals. That is why we have the <strong class="source-inline">.prisma</strong> extension and to map it to real SQL, we need to run migration. We already know about the migration steps and their impact on your development (check <a href="B09148_06.xhtml#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> for more detailed information), so in this chapter, we just provide the exact command <span class="No-Break">without explanation:</span></p>
			<pre class="console">
npx prisma migrate dev --name transaction-status-updated</pre>			<p>After running the command, you<a id="_idIndexMarker552"/> should end up with an additional folder that contains <strong class="source-inline">migration.sql</strong> and the folder name is a combination of the generation date and the name you provided from the command (<span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer075">
					<img alt="Figure 7.2: Newly generated migration context for statuses" src="image/B09148_07_002.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2: Newly generated migration context for statuses</p>
			<p>The main functionality we plan to add to the transaction service is fraud functionality. This method should change the status of a transaction to <strong class="source-inline">FRAUD</strong> if it is not a failed transaction. After updating the status, it should publish a message to the broker (Apache Kafka in <span class="No-Break">this case).</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor127"/>Getting started with Kafka for NestJS</h2>
			<p>As we learned in <a href="B09148_06.xhtml#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, NestJS has a lot of useful packages to work with different technologies. You don’t need to write any of them to integrate them into your project. This applies to Apache Kafka also. We <a id="_idIndexMarker553"/>don’t need to develop a separate package from scratch for it; just run the following<a id="_idIndexMarker554"/> command to install the <span class="No-Break">required packages:</span></p>
			<pre class="console">
npm install @nestjs/microservices kafkajs</pre>			<p>After successful installation, you will end up with additional changes in your <strong class="source-inline">package.json</strong> file. NestJS has a special pattern combination to configure services. That is why we first need to create our <strong class="source-inline">kafka</strong> module. As we already learned, there is no need to create this file manually. You just need to run the <span class="No-Break">following command:</span></p>
			<pre class="console">
nest generate module kafka</pre>			<p>It should generate a folder called <strong class="source-inline">kafka</strong> that contains the <strong class="source-inline">kafka.module.ts</strong> file. This module should have <strong class="source-inline">KafkaService</strong> as its provider element, but we don’t have a Kafka service. Running the following command will generate <strong class="source-inline">kafka.service.ts</strong> and <span class="No-Break"><strong class="source-inline">kafka.service.spec.ts</strong></span><span class="No-Break"> files:</span></p>
			<pre class="console">
nest generate service kafka</pre>			<p>We don’t need to work on <strong class="source-inline">kafka.service.spec.ts</strong> and it is up to you to remove it. These files are<a id="_idIndexMarker555"/> automatically generated test files, and we won’t run any tests for this chapter. To make things <a id="_idIndexMarker556"/>as simple as possible, we remove it. After running the last command, you should realize that <strong class="source-inline">kafka.module.ts</strong> was also automatically updated. Here is what it <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
import { Module } from '@nestjs/common';
import { KafkaService } from './kafka.service';
import { ConfigModule } from '@nestjs/config';
@Module({
  imports: [ConfigModule],
  providers: [KafkaService],
})
export class KafkaModule {}</pre>			<p>The code in <strong class="source-inline">kafka.module.ts</strong> is straightforward and easy to understand due to its minimal lines.: A bit later we will talk about the <strong class="source-inline">nestjs/config</strong> package also. We will implement the main functionality inside <strong class="source-inline">kafka.service.ts</strong> file. Open your <strong class="source-inline">kafka.service.ts</strong>  file and replace it with the following <span class="No-Break">code lines:</span></p>
			<pre class="source-code">
import { Injectable, OnModuleInit, OnModuleDestroy } from 
  '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { Kafka, Producer } from 'kafkajs';
@Injectable()
export class KafkaService implements OnModuleInit, OnModuleDestroy {
  private readonly producer: Producer;
  private readonly topic: string;
  constructor(private readonly configService: ConfigService) {
    const clientId = this.configService.get&lt;
      string&gt;('KAFKA_CLIENT_ID');
    const brokers = this.configService.get&lt;string&gt;('KAFKA_BROKERS')
      .split(',');
    this.topic = this.configService.get&lt;string&gt;('KAFKA_TOPIC');
    const kafka = new Kafka({ clientId, brokers });
    this.producer = kafka.producer({ retry: { retries: 3 }
      });
  }
  async onModuleInit(): Promise&lt;void&gt; {
    await this.producer.connect();
  }
  async onModuleDestroy(): Promise&lt;void&gt; {
    await this.producer.disconnect();
  }
  async send(value: any, key?: string): Promise&lt;void&gt; {
    const messages = [{ key, value: JSON.stringify(value)
      }];
    await this.producer.send({ topic: this.topic, messages
      });
  }
}</pre>			<p>Now let’s understand <a id="_idIndexMarker557"/>what we <span class="No-Break">just did:</span></p>
			<ul>
				<li><strong class="source-inline">Injectable</strong>: This indicates that the class is injectable into <span class="No-Break">other services.</span></li>
				<li><strong class="source-inline">OnModuleInit</strong> and <strong class="source-inline">OnModuleDestroy</strong>: These are lifecycle hooks for initialization <span class="No-Break">and cleanup.</span></li>
				<li><strong class="source-inline">ConfigService</strong>: This provides <a id="_idIndexMarker558"/>access to environment variables <span class="No-Break">and configuration.</span></li>
				<li><strong class="source-inline">Kafka</strong> and <strong class="source-inline">Producer</strong>: These are classes from the <span class="No-Break"><strong class="source-inline">kafkajs</strong></span><span class="No-Break"> library.</span></li>
				<li><strong class="source-inline">@Injectable()</strong>: This makes the <span class="No-Break"><strong class="source-inline">KafkaService</strong></span><span class="No-Break"> injectable.</span></li>
				<li><strong class="source-inline">implements OnModuleInit</strong> and <strong class="source-inline">OnModuleDestroy</strong>: This implements the <span class="No-Break">lifecycle hooks.</span></li>
				<li><strong class="source-inline">producer</strong>: The Kafka <strong class="source-inline">producer</strong> instance is used for <span class="No-Break">sending messages.</span></li>
				<li><strong class="source-inline">topic</strong>: This is the pre-configured Kafka topic for message delivery (fetched from <span class="No-Break">environment variables).</span></li>
				<li><strong class="source-inline">configService</strong>: This is the injected instance for <span class="No-Break">accessing configuration.</span></li>
				<li>The constructor of the class fetches Kafka configuration values from <span class="No-Break">environment variables:</span><ul><li><strong class="source-inline">KAFKA_CLIENT_ID</strong>: This is the client ID for <span class="No-Break">your application.</span></li><li><strong class="source-inline">KAFKA_BROKERS</strong>: This is a comma-separated list of Kafka <span class="No-Break">broker addresses.</span></li><li><strong class="source-inline">KAFKA_TOPIC</strong>: This is the Kafka topic for <span class="No-Break">sending messages.</span></li><li><strong class="source-inline">const kafka = new Kafka({ clientId, brokers });</strong>: This creates a Kafka client using <span class="No-Break">the configuration.</span></li><li><strong class="source-inline">this.producer = kafka.producer({ retry: { retries: 3 } })</strong>: This<a id="_idIndexMarker559"/> creates a producer instance with a retry configuration for message reliability (set to retry three times <span class="No-Break">by default).</span></li><li><strong class="source-inline">onModuleInit</strong>: This connects the Kafka producer when the NestJS module is initialized, ensuring the producer is ready to <span class="No-Break">send messages.</span></li><li><strong class="source-inline">onModuleDestroy</strong>: This disconnects the Kafka producer when the NestJS module is destroyed, <span class="No-Break">releasing </span><span class="No-Break"><a id="_idIndexMarker560"/></span><span class="No-Break">resources.</span></li><li><strong class="source-inline">send</strong>: This takes a value (any) to be sent and an optional key (string) for message identification. It constructs a message object with a key and value (serialized as JSON) and sends the message to the pre-configured topic using <span class="No-Break">the producer.</span></li></ul></li>
			</ul>
			<p>Sensitive information should not be stored directly in kafka.service.ts. For this chapter, store configuration settings in a <strong class="source-inline">.env</strong> file locally. However, avoid committing this file to version control. For production deployments, consider using a secure vault service, like AWS Secrets Manager or Azure Key Vault, to manage sensitive configurations securely. From the previous code, it is obvious that we store our three main Kafka configurations in a <strong class="source-inline">.env</strong> file. Open your <strong class="source-inline">.env</strong> file and add the following lines to the end of <span class="No-Break">the file:</span></p>
			<pre class="source-code">
#KAFKA Configuration
KAFKA_CLIENT_ID=transaction-service
KAFKA_BROKERS=localhost:29092
KAFKA_TOPIC=transaction-service-topic</pre>			<p>We have already used the <strong class="source-inline">.env</strong> file to configure <strong class="source-inline">postgresql</strong> (<a href="B09148_06.xhtml#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>), but for this chapter, we need to specify a mechanism that can read .<strong class="source-inline">env</strong> files. Another NestJS package, called <strong class="source-inline">config</strong>, will help us to deal with this issue. Let’s install it using the <span class="No-Break">following command:</span></p>
			<pre class="console">
npm install @nestjs/config</pre>			<p>That is all. We have<a id="_idIndexMarker561"/> imported the package to <strong class="source-inline">kafka.service.js</strong> to work with it. Now it is time to talk about Kafka’s essentials. When we produce or consume messages, we need to interact with Apache Kafka, and you <a id="_idIndexMarker562"/>need to understand some basics of Kafka before <span class="No-Break">using it.</span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor128"/>Cluster and brokers in Kafka</h2>
			<p>In production, a Kafka cluster typically consists of multiple brokers, each of which stores and manages partitions for assigned topics. Kafka uses ZooKeeper (or KRaft in newer versions) to coordinate broker metadata and ensure consistent partition distribution across the cluster. A <strong class="bold">broker</strong> is <a id="_idIndexMarker563"/>synonymous with a <strong class="bold">Kafka server</strong>. Each broker is a server. The<a id="_idIndexMarker564"/> purpose of a broker is to <span class="No-Break">serve data.</span></p>
			<p>It doesn’t matter whether it is <a id="_idIndexMarker565"/>physical or not; in the end, a broker should function as a server. While it is technically possible to have<a id="_idIndexMarker566"/> only one broker in a cluster, this is usually done only for <a id="_idIndexMarker567"/>testing or <span class="No-Break">self-learning purposes.</span></p>
			<p>The purposes of a cluster <a id="_idIndexMarker568"/>include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Handling multiple requests in parallel using <span class="No-Break">multiple brokers</span></li>
				<li>Providing <span class="No-Break">high throughput</span></li>
				<li><span class="No-Break">Ensuring scalability</span></li>
			</ul>
			<p>Each broker within a Kafka cluster is also a bootstrap server, containing metadata about all other brokers, topics, and partitions. When consumers join a consumer group, Kafka's group coordinator uses assignment strategies like Range or RoundRobin to assign partitions, ensuring even distribution and balancing the load across consumers. This means that when you connect to one broker, you are automatically connected to the entire cluster. In most cases, a good starting point is to have three brokers. Three brokers in Apache Kafka provide a balance between fault tolerance and efficiency. With three brokers, Kafka can replicate data across multiple nodes, ensuring high availability even if one broker fails. It also enables a replication factor of three, which allows the system to tolerate a broker failure<a id="_idIndexMarker569"/> without losing data, while avoiding the overhead of managing too many brokers. However, in high-load systems, you might end up with hundreds <span class="No-Break">of brokers.</span></p>
			<p>Depending on the topic’s configuration, a broker, as a storage type, consists of multiple <strong class="bold">partitions</strong>. When we create a<a id="_idIndexMarker570"/> topic, we define the number of partitions under that topic. Kafka, as a distributed system, employs the best algorithm to distribute these partitions <span class="No-Break">among brokers.</span></p>
			<p>Let’s consider a Kafka cluster with three brokers. When creating a topic named <strong class="source-inline">tracking_accounts</strong> with three partitions, Kafka will attempt to distribute the partitions among the three brokers. In the best scenario, this will result in one partition per broker. Of course, this<a id="_idIndexMarker571"/> depends on various factors, including load <a id="_idIndexMarker572"/>balancing. You don’t need to intervene; Kafka, as a <strong class="bold">distributed framework</strong>, automatically manages all these <span class="No-Break">internal operations.</span></p>
			<p>If you have three partitions and four brokers, Kafka will attempt to distribute them, assigning one partition to each broker, leaving one broker without a partition. But why create more brokers than the partition count? The value becomes apparent when you encounter issues with a broker going down. As we know, one of the most important attributes of Kafka is its fault tolerance. When a broker fails, Kafka automatically recovers using other brokers. The other important question is how producers and consumers know which broker to communicate with for reading and <span class="No-Break">writing data.</span></p>
			<p>The answer is simple. Since Kafka brokers<a id="_idIndexMarker573"/> also act as <strong class="bold">bootstrap servers</strong>, they possess all the essential information about <span class="No-Break">other servers.</span></p>
			<p>For example, consider any producer. Before producing data, the producer sends a background request to any broker (it doesn’t matter which one – even the nearest broker will do) to retrieve metadata information. This metadata contains all the relevant details about other brokers, their topics, partitions, and leader partitions (which we will cover in <span class="No-Break">future discussions).</span></p>
			<p>Using this metadata, the producer knows<a id="_idIndexMarker574"/> which broker to send data to. We call this process <strong class="bold">Kafka </strong><span class="No-Break"><strong class="bold">broker discovery</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor129"/>Topic and partition concepts in Apache Kafka</h2>
			<p>The responsibility of the Kafka producer is to produce data. On the other hand, the Kafka consumer is a client for your message. The Kafka cluster acts as an isolator and <em class="italic">storage</em> for the producer <a id="_idIndexMarker575"/>and consumer. Before producing data, Kafka brokers need temporary storage to hold the data. These storage boxes are <span class="No-Break">called </span><span class="No-Break"><strong class="bold">topics</strong></span><span class="No-Break">.</span></p>
			<p>A topic is a stream of data <a id="_idIndexMarker576"/>that acts<a id="_idIndexMarker577"/> as a logical isolator <span class="No-Break">over partitions.</span></p>
			<p>The topic is important from the user’s point of view because when reading/writing the data, we’re referring mostly to the topic rather than partitions. (Of course, when defining partitions in the producing/consuming process, it is mandatory to point to the topic name, but in general, it is possible to produce/consume data without directly indicating partitions.) The topic concept helps us mere mortals to interact with Kafka without worrying about the internal storage mechanism. Every topic should have a unique name because the identification process for topics is done through their names. You can create as many topics as you want/your <span class="No-Break">business requires.</span></p>
			<p>Topics are not a thing that can live in one broker in production systems. Instead, using partitions, topics spread out to brokers. This means that, using partitions, a topic lives in multiple brokers. It helps Kafka to make a fault-tolerant, scalable, and <span class="No-Break">distributed system.</span></p>
			<p>Topics are durable, meaning that the data in them is persisted on disk. This makes Kafka a good choice for applications that need to reliably store and process <span class="No-Break">data streams.</span></p>
			<p>But how about partitions? Under the <a id="_idIndexMarker578"/>hood, Kafka uses partitions to store data. Every topic in production consists of multiple partitions. Kafka uses the topic concept for mainly <span class="No-Break">two purposes:</span></p>
			<ul>
				<li>To group partitions under one box for storing “one business <span class="No-Break">point” data</span></li>
				<li>To help users interact with Kafka without worrying about the <span class="No-Break">internal structure</span></li>
			</ul>
			<p>Kafka uses partitions to achieve parallelism and scalability. This means that multiple producers and consumers can work on the same topic at the same time, and the data is evenly distributed across the brokers in <span class="No-Break">the cluster.</span></p>
			<p>So, why do we need the concept of partitions if we have topics? Well, using partitions, Kafka achieves distributive data storage and the <strong class="bold">in-sync replica</strong> (<strong class="bold">ISR</strong>) concept. Partitions help us to distribute topics and achieve <span class="No-Break">fault-tolerant systems.</span></p>
			<p>Every partition is identified by its ID. Every topic can have as many partitions as you want/your business requires. In production, it is very important to define the partition count when creating a topic – otherwise, the system will use the default configuration for the partition count. This means that, without defining the partition count, the system <a id="_idIndexMarker579"/>will automatically create the<a id="_idIndexMarker580"/> number of partitions per topic every time. The partition count should align with business requirements; for example, one topic might need forty partitions, while another may <span class="No-Break">need two-hundred</span><span class="No-Break">.</span></p>
			<p>You can think about partitions <a id="_idIndexMarker581"/>as a collection with a stack algorithm. Every partition is an array, and their indexes are called <strong class="bold">offsets.</strong> A partition<a id="_idIndexMarker582"/> has a dynamic offset count and there is no fixed size for it. Partitions are dynamically extendable, and their sizes can vary within the same topic. Every unit of information in a partition is called a message. Consumers <a id="_idIndexMarker583"/>can read data in a <span class="No-Break">stacked manner.</span></p>
			<p>Kafka partitions are split into Kafka brokers using a round-robin algorithm. This means that each broker in the cluster is assigned an equal number of partitions, as much <span class="No-Break">as possible.</span></p>
			<p>But the process of splitting partitions across Kafka brokers also depends on the <span class="No-Break">following factors:</span></p>
			<ul>
				<li><strong class="bold">Number of partitions</strong>: When you create a Kafka topic, you should specify the number of partitions it can have. This <a id="_idIndexMarker584"/>number determines how many parallel consumers or producers can work with the topic. The number of partitions should be chosen based on the expected workload and the level of <span class="No-Break">parallelism required.</span></li>
				<li><strong class="bold">Broker assignment</strong>: The assignment is typically done in a balanced manner to ensure an even distribution of partitions across brokers, but it can be influenced by partition <span class="No-Break">assignment strategies.</span></li>
				<li><strong class="bold">Partition assignment strategies</strong>: Kafka provides different strategies for partition assignment, mainly controlled by the consumer <span class="No-Break">group coordinator.</span></li>
				<li><strong class="bold">Replication factor</strong>: Kafka ensures fault tolerance through data replication across multiple brokers. Each partition has a specified replication factor, which determines how many copies of the data <span class="No-Break">are</span><span class="No-Break"><a id="_idIndexMarker585"/></span><span class="No-Break"> maintained.</span></li>
			</ul>
			<p>In short, we need partitions in Kafka because they are a core unit of parallelism and distribution and help Kafka to horizontally scale and distribute data. They also enable high throughput and fault tolerance and act as an internal <span class="No-Break">storage mechanism.</span></p>
			<p>It makes sense to note that once a topic is created with a certain number of partitions, it’s not proper to change the number of partitions for that topic. Instead, you need to create a new topic with the required number of partitions and migrate data if needed. Apache Kafka is a huge <a id="_idIndexMarker586"/>concept by its nature and if you want to learn more, you can check my <em class="italic">Apache Kafka for Distributed Systems</em> course on <span class="No-Break">Udemy (</span><a href="https://www.udemy.com/course/apache-kafka-for-distributed-systems/"><span class="No-Break">https://www.udemy.com/course/apache-kafka-for-distributed-systems/</span></a><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor130"/>Configuring Apache Kafka</h2>
			<p>We talked about theoretical <a id="_idIndexMarker587"/>aspects of Apache Kafka and now it is time to implement it in practice. You can use the Kafka CLI to interact <a id="_idIndexMarker588"/>with Kafka, but we have already installed Kafka UI to make our lives easy and not deal with the complexities of <span class="No-Break">command lines.</span></p>
			<p>Our <strong class="source-inline">.env</strong> file defines a topic named <strong class="source-inline">transaction-topic</strong>, and to create it, let’s take the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Open Docker Desktop. Make sure that all the services are running for <span class="No-Break">this chapter.</span></li>
				<li>Open your favorite browser and navigate <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">http://localhost:9100/</strong></span><span class="No-Break">.</span></li>
				<li>From the dashboard on the left, <span class="No-Break">select </span><span class="No-Break"><strong class="bold">Topics</strong></span><span class="No-Break">.</span></li>
				<li>Click the <strong class="bold">Add a Topic</strong> button at the top right and fill in the inputs (<span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer076">
					<img alt="Figure 7.3: Creating a topic for the broker in Apache Kafka" src="image/B09148_07_003.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3: Creating a topic for the broker in Apache Kafka</p>
			<p>After successfully creation, you will see your topic in the <span class="No-Break"><strong class="bold">Topics</strong></span><span class="No-Break"> list.</span></p>
			<p>So far, we configured Apache<a id="_idIndexMarker589"/> Kafka’s topic and created <strong class="source-inline">kafka.service.ts</strong> with <strong class="source-inline">kafka.module.ts</strong>. We plan to have fraud functionality in transactions and that is why we need to change three<a id="_idIndexMarker590"/> more files (<strong class="source-inline">transaction.controller.ts</strong>, <strong class="source-inline">transaction.module.ts</strong>, and <strong class="source-inline">transaction.service.ts</strong>) to integrate our new <span class="No-Break">fraud functionality.</span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>Adding an asynchronous nature to a transaction microservice</h2>
			<p>What we need to do is integrate<a id="_idIndexMarker591"/> configuration reading and Kafka functionalities into the transaction service. The final version of <strong class="source-inline">transaction.module.ts</strong> will look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
import { Module } from '@nestjs/common';
import { TransactionService } from './transaction.service';
import { TransactionController } from './transaction.controller';
import { PrismaModule } from '../prisma/prisma.module';
import { HttpModule } from '@nestjs/axios';
import { KafkaService } from 'src/kafka/kafka.service';
import { ConfigService } from '@nestjs/config';
@Module({
  imports: [PrismaModule,HttpModule],
  controllers: [TransactionController],
  providers:
    [TransactionService,KafkaService,ConfigService],
})
export class TransactionModule {}</pre>			<p>We just added <strong class="source-inline">KafkaService</strong> and <strong class="source-inline">ConfigService</strong>. We plan to inject <strong class="source-inline">KafkaService</strong> into the <strong class="source-inline">transaction.service.ts</strong> file and it has a dependency on <strong class="source-inline">ConfigService</strong>. That is <a id="_idIndexMarker592"/>why we need to add both <strong class="source-inline">KafkaService</strong> and <strong class="source-inline">ConfigService</strong> to the <span class="No-Break"><strong class="source-inline">providers</strong></span><span class="No-Break"> list.</span></p>
			<p>Let’s switch to the <strong class="source-inline">transaction.service.ts</strong> file itself. The modified version of the file is <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
import { Injectable } from "@nestjs/common";
import { CreateTransactionDto } from "./dto/create-transaction.dto";
import { PrismaService } from "src/prisma/prisma.service";
import { HttpService } from "@nestjs/axios";
import { AccountApiResponse } from "./dto/account.dto";
import { KafkaService } from "src/kafka/kafka.service";
@Injectable()
export class TransactionService {
  constructor(
    private readonly prisma: PrismaService,
    private readonly httpService: HttpService,
    private readonly kafkaService: KafkaService
  ) {}
  async create(createTransactionDto: CreateTransactionDto)
  {
    //same as Chapter 6
  }
  findAll() {
    //same as Chapter 6
  }
  findOne(id: number) {
    //same as Chapter 6
  }
 //newly added functionality
    async fraud(id: number) {
    const transaction = await this.findOne(id);
    if (transaction.status !== "FRAUD" &amp;&amp;
      transaction.status !== "FAILED") {
        const newTransaction =
          this.prisma.transaction.update({
            where: { id },
            data: { status: "FRAUD" },
          });
          this.kafkaService.send(transaction, null);
          return newTransaction;
        } else throw new Error("Transaction is not in a valid status");
  }</pre>			<p>As you might have already noticed, we injected <strong class="source-inline">KafkaService</strong> and the transaction has one more function, <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">fraud</strong></span><span class="No-Break">.</span></p>
			<p>This asynchronous function, named <strong class="source-inline">fraud</strong>, is designed to handle marking a transaction as fraudulent. It fetches the transaction details, verifies its current status, updates it to <strong class="source-inline">FRAUD</strong> if valid, potentially sends a notification, and returns the updated transaction object. The function<a id="_idIndexMarker593"/> takes <strong class="source-inline">id: number</strong> as input, representing the unique identifier of the transaction to be flagged as fraudulent. The function begins by using <strong class="source-inline">await</strong> <strong class="source-inline">this.findOne(id)</strong> to retrieve the transaction data asynchronously from a database. It then checks that the transaction’s current status is neither <strong class="source-inline">FRAUD</strong> nor <strong class="source-inline">FAILED</strong> using the strict inequality operator (<strong class="source-inline">!==</strong>). This ensures the function doesn’t attempt to mark an already fraudulent or failed transaction again. If the status doesn’t meet the criteria, an error is thrown with the message <strong class="source-inline">Transaction is not in a valid status</strong> to prevent unexpected behavior. Assuming the status check passes (i.e., the transaction isn’t already fraudulent or failed), the code proceeds to update the transaction data. It utilizes the Prisma library (<strong class="source-inline">this.prisma.transaction.update</strong>) to modify the transaction record. The <strong class="source-inline">where</strong> property specifies that the update should target the specific transaction with the <span class="No-Break">provided ID.</span></p>
			<p>The <strong class="source-inline">data</strong> property defines the changes to be made. In this case, it sets the <strong class="source-inline">status</strong> property of the transaction <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">FRAUD</strong></span><span class="No-Break">.</span></p>
			<p>The function includes the line <strong class="source-inline">this.kafkaService.send(transaction, null)</strong>. This suggests the use of a Kafka message broker to broadcast a notification about the fraudulent transaction. The second argument is a key. A message key is an optional element you can include with a message in Apache Kafka. It plays an important role in how messages are routed and processed within the system. Message keys are primarily used for partitioning messages within a topic. Kafka topics are further divided into partitions, serving as storage units for distributed data. By including a key, you can influence which partition a message gets <span class="No-Break">sent to.</span></p>
			<p>Finally, if the status check is <a id="_idIndexMarker594"/>passed and the update is successful, the function returns the <strong class="source-inline">newTransaction</strong> object. This object contains the updated transaction details, including the newly set <span class="No-Break"><strong class="source-inline">FRAUD</strong></span><span class="No-Break"> status.</span></p>
			<p>In essence, this function provides a mechanism to flag a transaction as fraudulent, considering the current status and sending a notification <span class="No-Break">through Kafka.</span></p>
			<p>The final element is the controller. In the transaction controller, we have a new endpoint with the <span class="No-Break">following behavior:</span></p>
			<pre class="source-code">
 @Post(':id')
  fraud(@Param(‹id›) id: string) {
    return this.transactionService.fraud(+id);
  }</pre>			<p>To test everything together, you should do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Run <strong class="source-inline">npm run start:dev</strong> from the root (the <span class="No-Break"><strong class="source-inline">Ch07</strong></span><span class="No-Break"><strong class="source-inline">/transactionservice</strong></span><span class="No-Break"> folder).</span></li>
				<li>Navigate <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">localhost:3000/api</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>We already have default migrated transactions. You can use their IDs to test our newly created API or you can create a transaction from scratch and test it. Let’s test one of our seed transactions. I’ll use the <strong class="source-inline">id = 1</strong> transaction (<span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer077">
					<img alt="Figure 7.4: Executing the fraud endpoint" src="image/B09148_07_004.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4: Executing the fraud endpoint</p>
			<p>After successfully executing the fraud endpoint, we will end up with the <span class="No-Break">following response:</span></p>
			<pre class="console">
{
  "id": 1,
  "status": "FRAUD",
  "accountId": "662c081370bd2ba6b5f04e94",
  "description": "simple transaction",
  "createdAt": "2024-05-10T08:43:41.389Z",
  "updatedAt": "2024-05-29T17:47:07.233Z"
}</pre>			<p>Now let’s open Apache <a id="_idIndexMarker595"/>Kafka and check our message. Open <strong class="source-inline">localhost:9100</strong> from your favorite browser and go to<strong class="bold"> Topics</strong>. Click on <strong class="source-inline">transaction-service-topic</strong> and select the <strong class="bold">Value</strong> section from the <strong class="bold">Messages</strong> tab (<span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer078">
					<img alt="Figure 7.5: Success message in Apache Kafka" src="image/B09148_07_005.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5: Success message in Apache Kafka</p>
			<p>Great! We’re able to send a message to Apache Kafka and now we need to somehow take this message and <a id="_idIndexMarker596"/>handle it. Reading messages from the source (it is Apache Kafka for us) is called a consuming process and a<a id="_idIndexMarker597"/> reader is <span class="No-Break">a consumer.</span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor132"/>Adapting an account service to new requirements</h1>
			<p>An account service is the main <a id="_idIndexMarker598"/>consumer we need to implement for the given context. First, we need to have the ability to interact with Apache Kafka. Just copy the already implemented account microservice and continue working on that. Navigate to <strong class="source-inline">Ch07/accountService</strong> and run the following command to install the <span class="No-Break"><strong class="source-inline">kafkajs</strong></span><span class="No-Break"> package:</span></p>
			<pre class="console">
npm install kafkajs</pre>			<p>Now we need to develop a separate module to work with Apache Kafka. Apache Kafka has its variables (brokers, topics, etc.) and that is why we use a <strong class="source-inline">.env</strong> file as we did for the transaction service. Under the <strong class="source-inline">accountService</strong> folder, we have <strong class="source-inline">configs/.env</strong>, and we added the following <span class="No-Break">config items:</span></p>
			<pre class="source-code">
#KAFKA Configuration
KAFKA_CLIENT_ID=account-service
KAFKA_BROKERS=localhost:29092
KAFKA_TOPIC=transaction-service-topic
KAFKA_GROUP_ID=account-group</pre>			<p>To read the <strong class="source-inline">.env</strong> file, we use a special configuration that lives under <strong class="source-inline">src/config</strong> and is called <strong class="source-inline">config.js</strong>. But <a id="_idIndexMarker599"/>we need to add the required changes to that file to support new key-value pairs. Here is the final version <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">config.js</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
const dotenv = require('dotenv');
const Joi = require('joi');
const envVarsSchema = Joi.object()
    .keys({
        PORT: Joi.number().default(3000),
        MONGODB_URL:
          Joi.string().required().description('Mongo DB url'),
        KAFKA_CLIENT_ID: Joi.string().required(),
        KAFKA_BROKERS: Joi.string().required(),
        KAFKA_TOPIC: Joi.string().required(),
        KAFKA_GROUP_ID: Joi.string().required()
    })
    .unknown();
function createConfig(configPath) {
    dotenv.config({ path: configPath });
    const { value: envVars, error } = envVarsSchema
        .prefs({ errors: { label: ‹key› } })
        .validate(process.env);
    if (error) {
        throw new Error(`Config validation error:
          ${error.message}`);
    }
    return {
        port: envVars.PORT,
        mongo: {
            url: envVars.MONGODB_URL,
        },
        kafka: {
            clientID: envVars.KAFKA_CLIENT_ID,
            brokers: envVars.KAFKA_BROKERS,
            topic: envVars.KAFKA_TOPIC,
            groupId: envVars.KAFKA_GROUP_ID,
        }
    };
}
module.exports = {
    createConfig,
};</pre>			<p>We just added additional lines to support <strong class="source-inline">config</strong> elements newly added to the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">env </strong></span><span class="No-Break">file.</span></p>
			<p>So far, we added a configuration reading mechanism and a Kafka package. Now it is time to develop a Kafka module to interact with <span class="No-Break">Apache Kafka.</span></p>
			<p>Inside the <strong class="source-inline">src</strong> folder, create a new folder called <strong class="source-inline">modules</strong> and add a new file called <strong class="source-inline">kafkamodule.js</strong>. This<a id="_idIndexMarker600"/>to new requirements”  module should have the <span class="No-Break">following implementation:</span></p>
			<pre class="source-code">
const { Kafka } = require('kafkajs');
const Account = require('../models/account');
const path = require('path');
const { createConfig } = require('../config/config')
const configPath = path.join(__dirname, '../../configs/.env');
const appConfig = createConfig(configPath);
const kafka = new Kafka({
    clientId: appConfig.kafka.clientId,
    brokers: [appConfig.kafka.brokers],
});
const consumer = kafka.consumer({ groupId:
  appConfig.kafka.groupId });
const consumerModule = async () =&gt; {
    await consumer.connect();
    await consumer.subscribe({ topic: appConfig.kafka.topic });
    await consumer.run({
      eachMessage: async ({ topic, partition, message }) =&gt;
      {
        const transaction =
          JSON.parse(message.value.toString());
        const accountId = transaction.accountId;
        try {
              const blockedAccount =
                await Account.findOne({ accountId, status:
                { $ne: 'blocked' } });
              if (!blockedAccount) {
                const updatedAccount =
                  await Account.findOneAndUpdate(
                        { _id: accountId },
                        { $inc: { count: 1 } },
                        { new: true }
                    );
                  if (updatedAccount.count === 3)
                    await Account.findOneAndUpdate(
                            { _id: accountId },
                            { status: 'blocked' },
                            { new: true }
                   );
                }
                else
                    console.log(`not a valid accountId ${accountId}`);
            }
            catch (error) {
                console.log(error);
            }
        },
    });
};
module.exports = consumerModule;</pre>			<p>We need the following <a id="_idIndexMarker601"/>elements to adapt the account microservice to communicate with the <span class="No-Break">transaction service:</span></p>
			<ul>
				<li><strong class="source-inline">Kafka</strong>: To consume the message <span class="No-Break">from Kafka</span></li>
				<li><strong class="source-inline">Account</strong>: To interact with <span class="No-Break">the database</span></li>
				<li><strong class="source-inline">path</strong>: To specify the path to read <span class="No-Break">config files</span></li>
				<li><strong class="source-inline">createConfig</strong>: To retrieve the <span class="No-Break">Kafka configuration</span></li>
			</ul>
			<p>Let’s go through the code step by step to understand what <span class="No-Break">it does:</span></p>
			<ul>
				<li><strong class="source-inline">const configPath = path.join(__dirname, '../../configs/.env');</strong>: This line constructs a path to the <strong class="source-inline">.env</strong> configuration file, which is located two directories up from the current directory, and then in the <span class="No-Break"><strong class="source-inline">configs</strong></span><span class="No-Break"> directory.</span></li>
				<li><strong class="source-inline">const appConfig = createConfig(configPath);</strong>: This line calls the <strong class="source-inline">createConfig</strong> function with <strong class="source-inline">configPath</strong> as an argument. The <strong class="source-inline">createConfig</strong> function reads the configuration file and returns the configuration settings as an <span class="No-Break">object (</span><span class="No-Break"><strong class="source-inline">appConfig</strong></span><span class="No-Break">).</span></li>
				<li>The following lines create an instance of the Kafka client using the <strong class="source-inline">KafkaJS</strong> library. It configures <a id="_idIndexMarker602"/>the client with a <strong class="source-inline">clientId</strong> and a list of brokers, both of which are obtained from the <span class="No-Break"><strong class="source-inline">appConfig</strong></span><span class="No-Break"> object.</span><pre class="source-code">
const kafka = new Kafka({
    clientId: appConfig.kafka.clientId,
    brokers: [appConfig.kafka.brokers],
});</pre></li>				<li><strong class="source-inline">const consumer = kafka.consumer({ groupId: appConfig.kafka.groupId });</strong>: This line creates a new Kafka consumer instance, specifying a <strong class="source-inline">groupId</strong> obtained from the <strong class="source-inline">appConfig</strong> object. The <strong class="source-inline">groupId</strong> is used to manage consumer group coordination <span class="No-Break">in Kafka.</span></li>
				<li>We have implemented the main functionality of our module inside the <strong class="source-inline">consumerModule</strong> function. This function connects to Kafka and subscribers to the <span class="No-Break">given topic.</span></li>
				<li><strong class="source-inline">await consumer.connect();</strong>: This line connects the Kafka consumer to the <span class="No-Break">Kafka broker.</span></li>
				<li><strong class="source-inline">await consumer.subscribe({ topic: appConfig.kafka.topic });</strong>: This line subscribes the consumer to the specified Kafka topic, which is obtained from the <span class="No-Break"><strong class="source-inline">appConfig</strong></span><span class="No-Break"> object.</span></li>
				<li><strong class="source-inline">Consumer.run</strong> starts the consumer to listen for messages from the subscribed Kafka topic. It defines an asynchronous handler function for each message that processes each consumed message. The function takes an object with <strong class="source-inline">topic</strong>, <strong class="source-inline">partition</strong>, and <strong class="source-inline">message</strong> properties. For each message, it parses the retrieved message and extracts the account ID. The real business rules start here. First, we check whether the given account ID exists in our database and that it is not blocked. If the account ID exists, then we should update this account, incrementing its count. If the increment count is <strong class="source-inline">3</strong>, then the status will be updated <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">blocked</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>The rest of the code lines <span class="No-Break">are</span><span class="No-Break"><a id="_idIndexMarker603"/></span><span class="No-Break"> straightforward.</span></p>
			<p>To use the <strong class="source-inline">kafkamodule.js</strong> file capabilities, we need to import it into <strong class="source-inline">app.js</strong> and call it. Here is what <strong class="source-inline">app.js</strong> <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
const express = require('express');
const v1 = require('./routes/v1');
const consumerModule = require('./modules/kafkamodule');
const app = express();
consumerModule();
// service
app.use(express.json());
// V1 API
app.use('/v1', v1);
module.exports = app;</pre>			<p>Well, as you might guess, we missed one important piece of information. Yes – it is a newly created <strong class="source-inline">count</strong>. To track fraud operations, we need to add a new item called <strong class="source-inline">count</strong> to the <em class="italic">account schema</em>. Open <strong class="source-inline">models/account.js</strong> and add the following lines to <span class="No-Break">your schema:</span></p>
			<pre class="source-code">
count: {
            type: Number,
            default: 0, // Optional:(defaults to 0)
        },</pre>			<p>We don’t need to change the account service and account controller to use our new <strong class="source-inline">count :{}</strong>. It is an implementation detail and a user should not be able to interact with this column directly. Everything is ready and we can test <span class="No-Break">our service.</span></p>
			<p>Previously, we ran the<a id="_idIndexMarker604"/> account microservice without Docker Compose, but now we’ve added a <strong class="source-inline">docker-compose.yml</strong> file for it (<strong class="source-inline">Ch07/accountservice/docker-compose.yml</strong>). The transaction service already has its own <strong class="source-inline">docker-compose.yml</strong> file that hosts Kafka. To test both services together, we must run the <strong class="source-inline">docker-compose.yml</strong> files from the <strong class="source-inline">accountservice</strong> and <span class="No-Break"><strong class="source-inline">transactionservice</strong></span><span class="No-Break"> directories.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor133"/>Testing our microservices together</h1>
			<p>We should run transaction and account microservices together to test producing <a id="_idIndexMarker605"/>and consuming processes. First, let’s start with the account microservice. As mentioned before, don’t forget to run the <strong class="source-inline">docker-compose.yml</strong> files for <span class="No-Break">both services.</span></p>
			<p>To test the newly updated account, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Navigate to <strong class="source-inline">Ch07/accountservice/src</strong> from <span class="No-Break">the terminal.</span></li>
				<li>Run the account service from the command line using the <strong class="source-inline">node </strong><span class="No-Break"><strong class="source-inline">index.js </strong></span><span class="No-Break">command.</span></li>
				<li>Open Postman and, from the new tab, paste the service URL (it is <strong class="source-inline">http://localhost:3001/v1/accounts</strong> for us), and for the HTTP method, select <strong class="source-inline">POST</strong>. Select <strong class="bold">Body</strong> | <strong class="bold">raw</strong>, change <strong class="bold">Text</strong> to <strong class="bold">JSON</strong>, and paste <span class="No-Break">the following:</span><pre class="source-code">
{
    "name":"AccName1",
    "number":"Ac12345",
    "type":"root",
    "status":"new"
}</pre><p class="list-inset">You should get the <span class="No-Break">following response:</span></p><pre class="source-code"><strong class="bold">{</strong>
<strong class="bold">    "success": true,</strong>
<strong class="bold">    "Account": {</strong>
<strong class="bold">        "id":{your_account_id}, //for the given request, it is  "6658ae5284432e40604018d5" for us</strong>
<strong class="bold">        "name": "AccName1",</strong>
<strong class="bold">        "number": "Ac12345",</strong>
<strong class="bold">        "type": "root",</strong>
<strong class="bold">        "status": "new"</strong>
<strong class="bold">    }</strong>
<strong class="bold">}</strong></pre></li>			</ol>
			<p>Here is what it looks like in<a id="_idIndexMarker606"/> <span class="No-Break">our database:</span></p>
			<pre class="source-code">
{
  "_id": {
    "$oid": "6658ae5284432e40604018d5"
  },
  "name": "AccName1",
  "number": "Ac12345",
  "type": "root",
  "status": "new",
  …………………
}</pre>			<p>Everything is okay for now based on what we have in the account microservice. Our main trigger, the producer, is the transaction microservice. Let’s produce a message and see whether the account microservice can consume this message or not. We still need the account service to run in <a id="_idIndexMarker607"/>parallel with transaction microservice, and that is why we need to open a new terminal to run the <span class="No-Break">transaction microservice:</span></p>
			<ol>
				<li>Open a new terminal and navigate <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">Ch07/transactionservice</strong></span><span class="No-Break">.</span></li>
				<li>Run <strong class="source-inline">npm run start:dev</strong> to start the <span class="No-Break">transaction microservice.</span></li>
				<li>Navigate to <strong class="source-inline">http://localhost:3000/api/</strong> and select <span class="No-Break"><strong class="source-inline">POST /transaction/</strong></span><span class="No-Break">.</span></li>
				<li>Paste the following JSON to create a <span class="No-Break">new transaction:</span><pre class="source-code">
{
  "accountId": "6658ae5284432e40604018d5",
  "description": "Optional transaction description"
}</pre><p class="list-inset">You should use the account ID created by the account microservice. You will get the <span class="No-Break">following response:</span></p><pre class="source-code"><strong class="bold">{</strong>
<strong class="bold">  "id": {your_id},//it is '37' for us but in your case, it may have a different value</strong>
<strong class="bold">  "status": "CREATED",</strong>
<strong class="bold">  "accountId": "6658ae5284432e40604018d5",</strong>
<strong class="bold">  "description": "Optional transaction description",</strong>
<strong class="bold">………………</strong>
<strong class="bold">}</strong></pre></li>				<li>Now let’s provide this ID (it is <strong class="source-inline">37</strong> for us) to the <strong class="source-inline">POST /transaction/Id </strong>API. It is a fraud endpoint. The response will be like <span class="No-Break">the following:</span><pre class="source-code">
<strong class="bold">{</strong>
<strong class="bold">  "id": 37,</strong>
<strong class="bold">  "status": "FRAUD",</strong>
<strong class="bold">  "accountId": "6658ae5284432e40604018d5",</strong>
<strong class="bold">  "description": "Optional transaction description",</strong>
<strong class="bold">……..</strong>
<strong class="bold">}</strong></pre><p class="list-inset">Before executing the <a id="_idIndexMarker608"/>fraud endpoint, make sure that the Docker infrastructure is running. After running a fraud request, the account microservice should read and update the data. Running the same account ID in a fraud context three times should block the account itself in the <span class="No-Break">account microservice:</span></p><pre class="source-code">{
….
  "type": "root",
  "status": "blocked",
  "count": 3,
………
}</pre></li>			</ol>
			<p>With an understanding of the asynchronous communication technique, you can easily apply it to your projects <span class="No-Break">with confidence.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor134"/>Summary</h1>
			<p>In this chapter, we started our journey by learning the importance of defining proper communication between microservices. We mostly use two main communication forms: async and sync. Choosing one over another is always a context-dependent choice – context is king. Then, we talked about the advantages of asynchronous communication. There are multiple ways of implementing asynchronous communication and we talked about most of the popular choices. Everything has a price and integrated microservices architecture is not an exception. It brings a lot of additional complexity we need to take into account and one of them is <span class="No-Break">asynchronous communication.</span></p>
			<p>We talked about Apache Kafka, which helps us to overcome the problem we have. We learned about essential concepts such as clusters, brokers, topics, messages, and partitions. Our practical examples covered two main microservices. The transaction service was our producer, which produces a message, and the account microservice was a consumer that consumers that message. Of course, there are a lot of subtopics, such as refactoring, exception handling, testing, and deploying, that we haven’t covered yet, and the following chapters will cover these <span class="No-Break">in detail.</span></p>
		</div>
	</body></html>