<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-120"><a id="_idTextAnchor121"/>7</h1>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor122"/>Asynchronous Microservices</h1>
			<p>Microservices are designed to be independent and self-contained. Clearly defined communication protocols and APIs ensure these services interact without relying on each other’s internal workings. Defining proper communication between microservices is important for a well-functioning microservices architecture.</p>
			<p>In this chapter, we plan to discuss and learn about another important communication mechanism: asynchronous communication between microservices.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Understanding the requirements</li>
				<li>Exploring asynchronous communication</li>
				<li>Implementing an asynchronous transaction microservice</li>
				<li>Adapting an account service to new requirements</li>
				<li>Testing our microservices together</li>
			</ul>
			<p>Let’s get into it!</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor123"/>Technical requirements</h1>
			<p>To follow along with the chapter, you’ll need an IDE (we prefer Visual Studio Code), Postman, Docker, and a browser of your choice.</p>
			<p>It is preferable to download the repository from <a href="https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript">https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript</a> and open the <code>Ch07</code> folder to easily follow the code snippets.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor124"/>Understanding the requirements</h1>
			<p>Up until now, we have developed two<a id="_idIndexMarker515"/> simple microservices and for the current chapter we plan to extend our transaction microservice to meet the following requirements:</p>
			<ul>
				<li>Every transaction should support the following statuses: <code>CREATED</code>, <code>FAILED</code>, <code>APPROVED</code>, <code>DECLINED</code>, and <code>FRAUD</code>.</li>
				<li>The transaction service should now have a new method that changes the status of the given transaction to <code>FRAUD</code>. It will update the status of the transaction to <code>FRAUD</code> and produce a message about the transaction.</li>
				<li>The account service will consume this message and after <em class="italic">three</em> fraudulent attempts, the account service should read and suspend/block the given account.</li>
			</ul>
			<p>We plan to use asynchronous communication between microservices and any other microservice may use this message for internal purposes. You can check <a href="B09148_02.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a> for more information about asynchronous communication between microservices.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor125"/>Exploring asynchronous communication</h1>
			<p>You can implement asynchronous communication between microservices using various patterns and technologies, each suitable for different use cases and requirements. Here are some of the common ones:</p>
			<ul>
				<li><strong class="bold">Message brokers</strong>: Message <a id="_idIndexMarker516"/>brokers facilitate<a id="_idIndexMarker517"/> asynchronous communication by allowing microservices to publish and subscribe to messages. Popular message brokers include <strong class="bold">RabbitMQ</strong>, which<a id="_idIndexMarker518"/> supports multiple messaging protocols and patterns such as pub/sub and routing, and <strong class="bold">Apache Kafka</strong>, designed for high-throughput<a id="_idIndexMarker519"/> and fault-tolerant event streaming – one of the best choices for real-time data processing. An example of a message broker would be a producer service sending a message to a queue or topic and the consumer service subscribing to the queue or topic and processing messages.</li>
				<li><strong class="bold">Event streaming platforms</strong>: Event streaming platforms capture and process streams of events. These platforms are <a id="_idIndexMarker520"/>particularly useful for real-time analytics and data pipeline<a id="_idIndexMarker521"/> construction. Popular event streaming platforms include <strong class="bold">Apache Kafka</strong>, which is often used as both a message broker and an event streaming platform, and <strong class="bold">Amazon Kinesis</strong>, a managed service for real-time<a id="_idIndexMarker522"/> data processing at scale. Here is an example: a producer service emits events to a Kafka topic and consumer services consume events from the topic and react to them.</li>
				<li><strong class="bold">The Publish-Subscribe pattern</strong>: In the pub/sub pattern, messages are published to a topic and multiple subscribers<a id="_idIndexMarker523"/> can consume these messages asynchronously. Popular services <a id="_idIndexMarker524"/>that use the pub/sub pattern<a id="_idIndexMarker525"/> include <strong class="bold">Google Pub/Sub</strong>, a fully managed real-time messaging service, and <strong class="bold">AWS Simple Notification Service</strong> (<strong class="bold">SNS</strong>), which allows publishing<a id="_idIndexMarker526"/> messages to multiple subscribers. For example, a publisher service publishes an event to a topic and the subscriber services receive notifications and process the event.</li>
				<li><strong class="bold">Task queues</strong>: Task queues are used to<a id="_idIndexMarker527"/> distribute tasks to worker services asynchronously. This is<a id="_idIndexMarker528"/> useful for offloading heavy or time-consuming tasks from the main service. Some of the more popular<a id="_idIndexMarker529"/> task queues are <strong class="bold">Celery</strong>, an asynchronous task queue/job queue based on distributed message passing, and <strong class="bold">Amazon Simple Queue Service</strong> (<strong class="bold">SQS</strong>), a fully managed message queue service. Here’s how a task queue works: a<a id="_idIndexMarker530"/> producer service creates a task and places it in the queue and the worker service picks up the task from the queue and processes it.</li>
				<li><strong class="bold">Event-driven architecture</strong>: In an <a id="_idIndexMarker531"/>event-driven architecture, services communicate through events. When something notable <a id="_idIndexMarker532"/>happens in one service, it emits an event that other services can listen to and act upon. In event-driven architecture an event source service publishes an event, and the event listener services react to the event and execute their logic.</li>
				<li><strong class="bold">WebSockets</strong>: WebSockets allow for full-duplex <a id="_idIndexMarker533"/>communication channels over a single TCP connection, useful for <a id="_idIndexMarker534"/>real-time applications such as chat apps or live updates. Here’s an example: the server pushes updates to clients via WebSockets and clients receive updates in real time and act upon them.</li>
				<li><strong class="bold">Server-Sent Events</strong> (<strong class="bold">SSE</strong>): SSE is a server push technology enabling servers to push real-time updates to the client<a id="_idIndexMarker535"/> once an initial client connection is established. Let’s take an <a id="_idIndexMarker536"/>example: the server sends events to clients over an HTTP connection and clients listen to incoming messages and process them.</li>
				<li><strong class="bold">gRPC with streaming</strong>: gRPC supports bidirectional streaming, allowing both client and server to send a <a id="_idIndexMarker537"/>sequence of messages using a single connection. gRPC works like this: the client and server can continuously exchange streams of messages as part of a single RPC call.</li>
			</ul>
			<p>For this chapter, we will actively use Apache Kafka, an open source, high-performance event streaming platform. It is a popular choice for asynchronous communication between microservices due to its strengths in enabling a robust and scalable event-driven architecture. While we have already talked about how to run services via Docker, this chapter will focus on hosting Apache Kafka on Docker.</p>
			<p>Let’s take a quick look at the problems that Apache Kafka solves:</p>
			<ul>
				<li><strong class="bold">Communication complexity</strong>: In a microservice environment, you have <em class="italic">multiple sources</em> (every API acts as<a id="_idIndexMarker538"/> a source) and <em class="italic">multiple targets</em> (every API can have multiple sources to write to). The fact that sources and targets are scaled is always accompanied by a communication problem. In this case, the problem is that we should solve the complexities created by the source and target rather than focus on business requirement implementations. Now you have multiple sources and targets, which can create the following issues:<ul><li>Every target requires a different protocol to communicate.</li><li>Every target has its data format to work with.</li><li>Every different target requires maintenance and support.</li></ul><p class="list-inset">In simple terms, say you have a microservice application, and every service has its own target. Besides that, every service can have multiple sources, and the services can use common sources. Apache Kafka helps you to avoid complex communication between microservices.</p></li>
				<li><strong class="bold">Communication complexity duplication</strong>: Whenever similar systems are developed; we have to rewrite such communication processes again and again. Let’s imagine that we are working on several different projects. Although the domain of these projects is different, and although they solve different problems at an abstract level, the common aspect of these projects is communication complexity. So, it means we’re repeating ourselves and trying to resolve the same<a id="_idIndexMarker539"/> issue every time.</li>
				<li><strong class="bold">Fault tolerance</strong>: The system should be able to continue functioning and provide reliable data processing and message delivery even in the presence of various types of failures, such as hardware failures, network issues, or software crashes.</li>
				<li><strong class="bold">High performance</strong>: In most cases, such a communication problem (sources - targets) causes the application performance to drop. Regardless of dynamic changes in the number of targets and sources in the application, the program should always support the high-performance attribute.</li>
				<li><strong class="bold">Scalability</strong>: The system should be possible to horizontally scale sources and targets. Horizontal scaling, also known as scaling out, is <a id="_idIndexMarker540"/>a technique in software design for increasing the capacity of a system by adding more machines (nodes) to distribute the workload.</li>
				<li><strong class="bold">Real-time communication</strong>: One of the possible target and source communication attributes is real-time communication. Depending on the use cases, the system should allow real-time data exchange between the source and the target.</li>
				<li><strong class="bold">Log and data aggregation</strong>: This is the ability to combine and process logs and data in certain aggregates. Log and data aggregation play a crucial role in modern software by centralizing and organizing information from various sources, making it easier to analyze, troubleshoot, and optimize applications.</li>
				<li><strong class="bold">Data transformation and processing</strong>: The communication between the target and source is not only in the form of data exchange but also information should be based <a id="_idIndexMarker541"/>on the possibility of transformation.</li>
			</ul>
			<p>Now let’s talk about the infrastructure we need to use to implement our microservices.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor126"/>Implementing an asynchronous transaction microservice</h1>
			<p>We will use the same transaction microservice we implemented in <a href="B09148_06.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a> but with additional changes that will help us <a id="_idIndexMarker542"/>add asynchronous behavior to it. First, we should prepare our infrastructure. Here is what we will have in it:</p>
			<ul>
				<li><strong class="bold">Apache Kafka</strong>: To create loose <a id="_idIndexMarker543"/>coupling between microservices.</li>
				<li><strong class="bold">Kafka UI</strong>: This is a web application <a id="_idIndexMarker544"/>designed for managing Apache Kafka clusters. It <a id="_idIndexMarker545"/>provides a <strong class="bold">graphical user interface</strong> (<strong class="bold">GUI</strong>) instead of the traditional <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) for Kafka, making it<a id="_idIndexMarker546"/> easier to interact with Kafka for many users.</li>
				<li><strong class="bold">Zookeeper</strong>: This is open source<a id="_idIndexMarker547"/> software that acts as a central coordinator for large distributed systems. Think of it as a conductor for an orchestra, keeping everything in sync.</li>
				<li><strong class="bold">PostgreSQL</strong>: To store<a id="_idIndexMarker548"/> data.</li>
				<li><strong class="bold">PgAdmin</strong>: A graphical tool to<a id="_idIndexMarker549"/> visually see database elements.</li>
			</ul>
			<p>We have our <code>docker-compose.yml</code> file in our root folder (<code>Ch07/transactionservice</code>).</p>
			<p>This <code>docker-compose</code> file defines a multi-service setup for a PostgreSQL database, a PgAdmin instance for managing the database, and a Kafka messaging system with Zookeeper for coordination. The services are connected through a custom Docker network, <code>my-app-network</code>, which enables inter-container communication. For Kafka, ensure the correct network settings are configured to avoid connectivity issues, especially for multi-network setups where <code>advertised.listeners</code> may be needed for both internal and external addresses. The PostgreSQL service stores its data in a named volume, <code>postgres_data</code>, while <code>PgAdmin</code> depends on PostgreSQL to be up and running. The Kafka and Zookeeper services are set up for message brokering, with Kafka UI providing management and monitoring, relying on Zookeeper to maintain a distributed system configuration.</p>
			<p>Navigate to the root folder <a id="_idIndexMarker550"/>and run the <code>docker-compose up -d</code> command to spin up the infrastructure.</p>
			<p>Here is how it should look after a successful run (<em class="italic">Figure 7</em><em class="italic">.1</em>).</p>
			<div><div><img alt="Figure 7.1: Docker infrastructure" src="img/B09148_07_001.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1: Docker infrastructure</p>
			<p>After successfully running our docker infrastructure, we are ready to switch to our source code to implement <a id="_idIndexMarker551"/>our requirements.</p>
			<p>First, we need to update our transaction service to support additional statuses. Open the <code>schema.prisma</code> file under the <code>prisma/migrations</code> folder and change <code>enum</code> to the following:</p>
			<pre class="source-code">
enum Status {
  CREATED
  FAILED
  APPROVED
  DECLINED
  FRAUD
}</pre>			<p>As we already know, one of the responsibilities of Prisma is to isolate us from database internals and provide a unique, more understandable language over these internals. That is why we have the <code>.prisma</code> extension and to map it to real SQL, we need to run migration. We already know about the migration steps and their impact on your development (check <a href="B09148_06.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a> for more detailed information), so in this chapter, we just provide the exact command without explanation:</p>
			<pre class="console">
npx prisma migrate dev --name transaction-status-updated</pre>			<p>After running the command, you<a id="_idIndexMarker552"/> should end up with an additional folder that contains <code>migration.sql</code> and the folder name is a combination of the generation date and the name you provided from the command (<em class="italic">Figure 7</em><em class="italic">.2</em>).</p>
			<div><div><img alt="Figure 7.2: Newly generated migration context for statuses" src="img/B09148_07_002.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2: Newly generated migration context for statuses</p>
			<p>The main functionality we plan to add to the transaction service is fraud functionality. This method should change the status of a transaction to <code>FRAUD</code> if it is not a failed transaction. After updating the status, it should publish a message to the broker (Apache Kafka in this case).</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor127"/>Getting started with Kafka for NestJS</h2>
			<p>As we learned in <a href="B09148_06.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a>, NestJS has a lot of useful packages to work with different technologies. You don’t need to write any of them to integrate them into your project. This applies to Apache Kafka also. We <a id="_idIndexMarker553"/>don’t need to develop a separate package from scratch for it; just run the following<a id="_idIndexMarker554"/> command to install the required packages:</p>
			<pre class="console">
npm install @nestjs/microservices kafkajs</pre>			<p>After successful installation, you will end up with additional changes in your <code>package.json</code> file. NestJS has a special pattern combination to configure services. That is why we first need to create our <code>kafka</code> module. As we already learned, there is no need to create this file manually. You just need to run the following command:</p>
			<pre class="console">
nest generate module kafka</pre>			<p>It should generate a folder called <code>kafka</code> that contains the <code>kafka.module.ts</code> file. This module should have <code>KafkaService</code> as its provider element, but we don’t have a Kafka service. Running the following command will generate <code>kafka.service.ts</code> and <code>kafka.service.spec.ts</code> files:</p>
			<pre class="console">
nest generate service kafka</pre>			<p>We don’t need to work on <code>kafka.service.spec.ts</code> and it is up to you to remove it. These files are<a id="_idIndexMarker555"/> automatically generated test files, and we won’t run any tests for this chapter. To make things <a id="_idIndexMarker556"/>as simple as possible, we remove it. After running the last command, you should realize that <code>kafka.module.ts</code> was also automatically updated. Here is what it looks like:</p>
			<pre class="source-code">
import { Module } from '@nestjs/common';
import { KafkaService } from './kafka.service';
import { ConfigModule } from '@nestjs/config';
@Module({
  imports: [ConfigModule],
  providers: [KafkaService],
})
export class KafkaModule {}</pre>			<p>The code in <code>kafka.module.ts</code> is straightforward and easy to understand due to its minimal lines.: A bit later we will talk about the <code>nestjs/config</code> package also. We will implement the main functionality inside <code>kafka.service.ts</code> file. Open your <code>kafka.service.ts</code>  file and replace it with the following code lines:</p>
			<pre class="source-code">
import { Injectable, OnModuleInit, OnModuleDestroy } from 
  '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { Kafka, Producer } from 'kafkajs';
@Injectable()
export class KafkaService implements OnModuleInit, OnModuleDestroy {
  private readonly producer: Producer;
  private readonly topic: string;
  constructor(private readonly configService: ConfigService) {
    const clientId = this.configService.get&lt;
      string&gt;('KAFKA_CLIENT_ID');
    const brokers = this.configService.get&lt;string&gt;('KAFKA_BROKERS')
      .split(',');
    this.topic = this.configService.get&lt;string&gt;('KAFKA_TOPIC');
    const kafka = new Kafka({ clientId, brokers });
    this.producer = kafka.producer({ retry: { retries: 3 }
      });
  }
  async onModuleInit(): Promise&lt;void&gt; {
    await this.producer.connect();
  }
  async onModuleDestroy(): Promise&lt;void&gt; {
    await this.producer.disconnect();
  }
  async send(value: any, key?: string): Promise&lt;void&gt; {
    const messages = [{ key, value: JSON.stringify(value)
      }];
    await this.producer.send({ topic: this.topic, messages
      });
  }
}</pre>			<p>Now let’s understand <a id="_idIndexMarker557"/>what we just did:</p>
			<ul>
				<li><code>Injectable</code>: This indicates that the class is injectable into other services.</li>
				<li><code>OnModuleInit</code> and <code>OnModuleDestroy</code>: These are lifecycle hooks for initialization and cleanup.</li>
				<li><code>ConfigService</code>: This provides <a id="_idIndexMarker558"/>access to environment variables and configuration.</li>
				<li><code>Kafka</code> and <code>Producer</code>: These are classes from the <code>kafkajs</code> library.</li>
				<li><code>@Injectable()</code>: This makes the <code>KafkaService</code> injectable.</li>
				<li><code>implements OnModuleInit</code> and <code>OnModuleDestroy</code>: This implements the lifecycle hooks.</li>
				<li><code>producer</code>: The Kafka <code>producer</code> instance is used for sending messages.</li>
				<li><code>topic</code>: This is the pre-configured Kafka topic for message delivery (fetched from environment variables).</li>
				<li><code>configService</code>: This is the injected instance for accessing configuration.</li>
				<li>The constructor of the class fetches Kafka configuration values from environment variables:<ul><li><code>KAFKA_CLIENT_ID</code>: This is the client ID for your application.</li><li><code>KAFKA_BROKERS</code>: This is a comma-separated list of Kafka broker addresses.</li><li><code>KAFKA_TOPIC</code>: This is the Kafka topic for sending messages.</li><li><code>const kafka = new Kafka({ clientId, brokers });</code>: This creates a Kafka client using the configuration.</li><li><code>this.producer = kafka.producer({ retry: { retries: 3 } })</code>: This<a id="_idIndexMarker559"/> creates a producer instance with a retry configuration for message reliability (set to retry three times by default).</li><li><code>onModuleInit</code>: This connects the Kafka producer when the NestJS module is initialized, ensuring the producer is ready to send messages.</li><li><code>onModuleDestroy</code>: This disconnects the Kafka producer when the NestJS module is destroyed, releasing <a id="_idIndexMarker560"/>resources.</li><li><code>send</code>: This takes a value (any) to be sent and an optional key (string) for message identification. It constructs a message object with a key and value (serialized as JSON) and sends the message to the pre-configured topic using the producer.</li></ul></li>
			</ul>
			<p>Sensitive information should not be stored directly in kafka.service.ts. For this chapter, store configuration settings in a <code>.env</code> file locally. However, avoid committing this file to version control. For production deployments, consider using a secure vault service, like AWS Secrets Manager or Azure Key Vault, to manage sensitive configurations securely. From the previous code, it is obvious that we store our three main Kafka configurations in a <code>.env</code> file. Open your <code>.env</code> file and add the following lines to the end of the file:</p>
			<pre class="source-code">
#KAFKA Configuration
KAFKA_CLIENT_ID=transaction-service
KAFKA_BROKERS=localhost:29092
KAFKA_TOPIC=transaction-service-topic</pre>			<p>We have already used the <code>.env</code> file to configure <code>postgresql</code> (<a href="B09148_06.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a>), but for this chapter, we need to specify a mechanism that can read .<code>env</code> files. Another NestJS package, called <code>config</code>, will help us to deal with this issue. Let’s install it using the following command:</p>
			<pre class="console">
npm install @nestjs/config</pre>			<p>That is all. We have<a id="_idIndexMarker561"/> imported the package to <code>kafka.service.js</code> to work with it. Now it is time to talk about Kafka’s essentials. When we produce or consume messages, we need to interact with Apache Kafka, and you <a id="_idIndexMarker562"/>need to understand some basics of Kafka before using it.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor128"/>Cluster and brokers in Kafka</h2>
			<p>In production, a Kafka cluster typically consists of multiple brokers, each of which stores and manages partitions for assigned topics. Kafka uses ZooKeeper (or KRaft in newer versions) to coordinate broker metadata and ensure consistent partition distribution across the cluster. A <strong class="bold">broker</strong> is <a id="_idIndexMarker563"/>synonymous with a <strong class="bold">Kafka server</strong>. Each broker is a server. The<a id="_idIndexMarker564"/> purpose of a broker is to serve data.</p>
			<p>It doesn’t matter whether it is <a id="_idIndexMarker565"/>physical or not; in the end, a broker should function as a server. While it is technically possible to have<a id="_idIndexMarker566"/> only one broker in a cluster, this is usually done only for <a id="_idIndexMarker567"/>testing or self-learning purposes.</p>
			<p>The purposes of a cluster <a id="_idIndexMarker568"/>include the following:</p>
			<ul>
				<li>Handling multiple requests in parallel using multiple brokers</li>
				<li>Providing high throughput</li>
				<li>Ensuring scalability</li>
			</ul>
			<p>Each broker within a Kafka cluster is also a bootstrap server, containing metadata about all other brokers, topics, and partitions. When consumers join a consumer group, Kafka's group coordinator uses assignment strategies like Range or RoundRobin to assign partitions, ensuring even distribution and balancing the load across consumers. This means that when you connect to one broker, you are automatically connected to the entire cluster. In most cases, a good starting point is to have three brokers. Three brokers in Apache Kafka provide a balance between fault tolerance and efficiency. With three brokers, Kafka can replicate data across multiple nodes, ensuring high availability even if one broker fails. It also enables a replication factor of three, which allows the system to tolerate a broker failure<a id="_idIndexMarker569"/> without losing data, while avoiding the overhead of managing too many brokers. However, in high-load systems, you might end up with hundreds of brokers.</p>
			<p>Depending on the topic’s configuration, a broker, as a storage type, consists of multiple <strong class="bold">partitions</strong>. When we create a<a id="_idIndexMarker570"/> topic, we define the number of partitions under that topic. Kafka, as a distributed system, employs the best algorithm to distribute these partitions among brokers.</p>
			<p>Let’s consider a Kafka cluster with three brokers. When creating a topic named <code>tracking_accounts</code> with three partitions, Kafka will attempt to distribute the partitions among the three brokers. In the best scenario, this will result in one partition per broker. Of course, this<a id="_idIndexMarker571"/> depends on various factors, including load <a id="_idIndexMarker572"/>balancing. You don’t need to intervene; Kafka, as a <strong class="bold">distributed framework</strong>, automatically manages all these internal operations.</p>
			<p>If you have three partitions and four brokers, Kafka will attempt to distribute them, assigning one partition to each broker, leaving one broker without a partition. But why create more brokers than the partition count? The value becomes apparent when you encounter issues with a broker going down. As we know, one of the most important attributes of Kafka is its fault tolerance. When a broker fails, Kafka automatically recovers using other brokers. The other important question is how producers and consumers know which broker to communicate with for reading and writing data.</p>
			<p>The answer is simple. Since Kafka brokers<a id="_idIndexMarker573"/> also act as <strong class="bold">bootstrap servers</strong>, they possess all the essential information about other servers.</p>
			<p>For example, consider any producer. Before producing data, the producer sends a background request to any broker (it doesn’t matter which one – even the nearest broker will do) to retrieve metadata information. This metadata contains all the relevant details about other brokers, their topics, partitions, and leader partitions (which we will cover in future discussions).</p>
			<p>Using this metadata, the producer knows<a id="_idIndexMarker574"/> which broker to send data to. We call this process <strong class="bold">Kafka </strong><strong class="bold">broker discovery</strong>.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor129"/>Topic and partition concepts in Apache Kafka</h2>
			<p>The responsibility of the Kafka producer is to produce data. On the other hand, the Kafka consumer is a client for your message. The Kafka cluster acts as an isolator and <em class="italic">storage</em> for the producer <a id="_idIndexMarker575"/>and consumer. Before producing data, Kafka brokers need temporary storage to hold the data. These storage boxes are called <strong class="bold">topics</strong>.</p>
			<p>A topic is a stream of data <a id="_idIndexMarker576"/>that acts<a id="_idIndexMarker577"/> as a logical isolator over partitions.</p>
			<p>The topic is important from the user’s point of view because when reading/writing the data, we’re referring mostly to the topic rather than partitions. (Of course, when defining partitions in the producing/consuming process, it is mandatory to point to the topic name, but in general, it is possible to produce/consume data without directly indicating partitions.) The topic concept helps us mere mortals to interact with Kafka without worrying about the internal storage mechanism. Every topic should have a unique name because the identification process for topics is done through their names. You can create as many topics as you want/your business requires.</p>
			<p>Topics are not a thing that can live in one broker in production systems. Instead, using partitions, topics spread out to brokers. This means that, using partitions, a topic lives in multiple brokers. It helps Kafka to make a fault-tolerant, scalable, and distributed system.</p>
			<p>Topics are durable, meaning that the data in them is persisted on disk. This makes Kafka a good choice for applications that need to reliably store and process data streams.</p>
			<p>But how about partitions? Under the <a id="_idIndexMarker578"/>hood, Kafka uses partitions to store data. Every topic in production consists of multiple partitions. Kafka uses the topic concept for mainly two purposes:</p>
			<ul>
				<li>To group partitions under one box for storing “one business point” data</li>
				<li>To help users interact with Kafka without worrying about the internal structure</li>
			</ul>
			<p>Kafka uses partitions to achieve parallelism and scalability. This means that multiple producers and consumers can work on the same topic at the same time, and the data is evenly distributed across the brokers in the cluster.</p>
			<p>So, why do we need the concept of partitions if we have topics? Well, using partitions, Kafka achieves distributive data storage and the <strong class="bold">in-sync replica</strong> (<strong class="bold">ISR</strong>) concept. Partitions help us to distribute topics and achieve fault-tolerant systems.</p>
			<p>Every partition is identified by its ID. Every topic can have as many partitions as you want/your business requires. In production, it is very important to define the partition count when creating a topic – otherwise, the system will use the default configuration for the partition count. This means that, without defining the partition count, the system <a id="_idIndexMarker579"/>will automatically create the<a id="_idIndexMarker580"/> number of partitions per topic every time. The partition count should align with business requirements; for example, one topic might need forty partitions, while another may need two-hundred.</p>
			<p>You can think about partitions <a id="_idIndexMarker581"/>as a collection with a stack algorithm. Every partition is an array, and their indexes are called <strong class="bold">offsets.</strong> A partition<a id="_idIndexMarker582"/> has a dynamic offset count and there is no fixed size for it. Partitions are dynamically extendable, and their sizes can vary within the same topic. Every unit of information in a partition is called a message. Consumers <a id="_idIndexMarker583"/>can read data in a stacked manner.</p>
			<p>Kafka partitions are split into Kafka brokers using a round-robin algorithm. This means that each broker in the cluster is assigned an equal number of partitions, as much as possible.</p>
			<p>But the process of splitting partitions across Kafka brokers also depends on the following factors:</p>
			<ul>
				<li><strong class="bold">Number of partitions</strong>: When you create a Kafka topic, you should specify the number of partitions it can have. This <a id="_idIndexMarker584"/>number determines how many parallel consumers or producers can work with the topic. The number of partitions should be chosen based on the expected workload and the level of parallelism required.</li>
				<li><strong class="bold">Broker assignment</strong>: The assignment is typically done in a balanced manner to ensure an even distribution of partitions across brokers, but it can be influenced by partition assignment strategies.</li>
				<li><strong class="bold">Partition assignment strategies</strong>: Kafka provides different strategies for partition assignment, mainly controlled by the consumer group coordinator.</li>
				<li><strong class="bold">Replication factor</strong>: Kafka ensures fault tolerance through data replication across multiple brokers. Each partition has a specified replication factor, which determines how many copies of the data are<a id="_idIndexMarker585"/> maintained.</li>
			</ul>
			<p>In short, we need partitions in Kafka because they are a core unit of parallelism and distribution and help Kafka to horizontally scale and distribute data. They also enable high throughput and fault tolerance and act as an internal storage mechanism.</p>
			<p>It makes sense to note that once a topic is created with a certain number of partitions, it’s not proper to change the number of partitions for that topic. Instead, you need to create a new topic with the required number of partitions and migrate data if needed. Apache Kafka is a huge <a id="_idIndexMarker586"/>concept by its nature and if you want to learn more, you can check my <em class="italic">Apache Kafka for Distributed Systems</em> course on Udemy (<a href="https://www.udemy.com/course/apache-kafka-for-distributed-systems/">https://www.udemy.com/course/apache-kafka-for-distributed-systems/</a>).</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor130"/>Configuring Apache Kafka</h2>
			<p>We talked about theoretical <a id="_idIndexMarker587"/>aspects of Apache Kafka and now it is time to implement it in practice. You can use the Kafka CLI to interact <a id="_idIndexMarker588"/>with Kafka, but we have already installed Kafka UI to make our lives easy and not deal with the complexities of command lines.</p>
			<p>Our <code>.env</code> file defines a topic named <code>transaction-topic</code>, and to create it, let’s take the following steps:</p>
			<ol>
				<li>Open Docker Desktop. Make sure that all the services are running for this chapter.</li>
				<li>Open your favorite browser and navigate to <code>http://localhost:9100/</code>.</li>
				<li>From the dashboard on the left, select <strong class="bold">Topics</strong>.</li>
				<li>Click the <strong class="bold">Add a Topic</strong> button at the top right and fill in the inputs (<em class="italic">Figure 7</em><em class="italic">.3</em>).</li>
			</ol>
			<div><div><img alt="Figure 7.3: Creating a topic for the broker in Apache Kafka" src="img/B09148_07_003.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3: Creating a topic for the broker in Apache Kafka</p>
			<p>After successfully creation, you will see your topic in the <strong class="bold">Topics</strong> list.</p>
			<p>So far, we configured Apache<a id="_idIndexMarker589"/> Kafka’s topic and created <code>kafka.service.ts</code> with <code>kafka.module.ts</code>. We plan to have fraud functionality in transactions and that is why we need to change three<a id="_idIndexMarker590"/> more files (<code>transaction.controller.ts</code>, <code>transaction.module.ts</code>, and <code>transaction.service.ts</code>) to integrate our new fraud functionality.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>Adding an asynchronous nature to a transaction microservice</h2>
			<p>What we need to do is integrate<a id="_idIndexMarker591"/> configuration reading and Kafka functionalities into the transaction service. The final version of <code>transaction.module.ts</code> will look like this:</p>
			<pre class="source-code">
import { Module } from '@nestjs/common';
import { TransactionService } from './transaction.service';
import { TransactionController } from './transaction.controller';
import { PrismaModule } from '../prisma/prisma.module';
import { HttpModule } from '@nestjs/axios';
import { KafkaService } from 'src/kafka/kafka.service';
import { ConfigService } from '@nestjs/config';
@Module({
  imports: [PrismaModule,HttpModule],
  controllers: [TransactionController],
  providers:
    [TransactionService,KafkaService,ConfigService],
})
export class TransactionModule {}</pre>			<p>We just added <code>KafkaService</code> and <code>ConfigService</code>. We plan to inject <code>KafkaService</code> into the <code>transaction.service.ts</code> file and it has a dependency on <code>ConfigService</code>. That is <a id="_idIndexMarker592"/>why we need to add both <code>KafkaService</code> and <code>ConfigService</code> to the <code>providers</code> list.</p>
			<p>Let’s switch to the <code>transaction.service.ts</code> file itself. The modified version of the file is shown here:</p>
			<pre class="source-code">
import { Injectable } from "@nestjs/common";
import { CreateTransactionDto } from "./dto/create-transaction.dto";
import { PrismaService } from "src/prisma/prisma.service";
import { HttpService } from "@nestjs/axios";
import { AccountApiResponse } from "./dto/account.dto";
import { KafkaService } from "src/kafka/kafka.service";
@Injectable()
export class TransactionService {
  constructor(
    private readonly prisma: PrismaService,
    private readonly httpService: HttpService,
    private readonly kafkaService: KafkaService
  ) {}
  async create(createTransactionDto: CreateTransactionDto)
  {
    //same as Chapter 6
  }
  findAll() {
    //same as Chapter 6
  }
  findOne(id: number) {
    //same as Chapter 6
  }
 //newly added functionality
    async fraud(id: number) {
    const transaction = await this.findOne(id);
    if (transaction.status !== "FRAUD" &amp;&amp;
      transaction.status !== "FAILED") {
        const newTransaction =
          this.prisma.transaction.update({
            where: { id },
            data: { status: "FRAUD" },
          });
          this.kafkaService.send(transaction, null);
          return newTransaction;
        } else throw new Error("Transaction is not in a valid status");
  }</pre>			<p>As you might have already noticed, we injected <code>KafkaService</code> and the transaction has one more function, called <code>fraud</code>.</p>
			<p>This asynchronous function, named <code>fraud</code>, is designed to handle marking a transaction as fraudulent. It fetches the transaction details, verifies its current status, updates it to <code>FRAUD</code> if valid, potentially sends a notification, and returns the updated transaction object. The function<a id="_idIndexMarker593"/> takes <code>id: number</code> as input, representing the unique identifier of the transaction to be flagged as fraudulent. The function begins by using <code>await</code> <code>this.findOne(id)</code> to retrieve the transaction data asynchronously from a database. It then checks that the transaction’s current status is neither <code>FRAUD</code> nor <code>FAILED</code> using the strict inequality operator (<code>!==</code>). This ensures the function doesn’t attempt to mark an already fraudulent or failed transaction again. If the status doesn’t meet the criteria, an error is thrown with the message <code>Transaction is not in a valid status</code> to prevent unexpected behavior. Assuming the status check passes (i.e., the transaction isn’t already fraudulent or failed), the code proceeds to update the transaction data. It utilizes the Prisma library (<code>this.prisma.transaction.update</code>) to modify the transaction record. The <code>where</code> property specifies that the update should target the specific transaction with the provided ID.</p>
			<p>The <code>data</code> property defines the changes to be made. In this case, it sets the <code>status</code> property of the transaction to <code>FRAUD</code>.</p>
			<p>The function includes the line <code>this.kafkaService.send(transaction, null)</code>. This suggests the use of a Kafka message broker to broadcast a notification about the fraudulent transaction. The second argument is a key. A message key is an optional element you can include with a message in Apache Kafka. It plays an important role in how messages are routed and processed within the system. Message keys are primarily used for partitioning messages within a topic. Kafka topics are further divided into partitions, serving as storage units for distributed data. By including a key, you can influence which partition a message gets sent to.</p>
			<p>Finally, if the status check is <a id="_idIndexMarker594"/>passed and the update is successful, the function returns the <code>newTransaction</code> object. This object contains the updated transaction details, including the newly set <code>FRAUD</code> status.</p>
			<p>In essence, this function provides a mechanism to flag a transaction as fraudulent, considering the current status and sending a notification through Kafka.</p>
			<p>The final element is the controller. In the transaction controller, we have a new endpoint with the following behavior:</p>
			<pre class="source-code">
 @Post(':id')
  fraud(@Param(‹id›) id: string) {
    return this.transactionService.fraud(+id);
  }</pre>			<p>To test everything together, you should do the following:</p>
			<ol>
				<li>Run <code>npm run start:dev</code> from the root (the <code>Ch07</code><code>/transactionservice</code> folder).</li>
				<li>Navigate to <code>localhost:3000/api</code>.</li>
			</ol>
			<p>We already have default migrated transactions. You can use their IDs to test our newly created API or you can create a transaction from scratch and test it. Let’s test one of our seed transactions. I’ll use the <code>id = 1</code> transaction (<em class="italic">Figure 7</em><em class="italic">.4</em>).</p>
			<div><div><img alt="Figure 7.4: Executing the fraud endpoint" src="img/B09148_07_004.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4: Executing the fraud endpoint</p>
			<p>After successfully executing the fraud endpoint, we will end up with the following response:</p>
			<pre class="console">
{
  "id": 1,
  "status": "FRAUD",
  "accountId": "662c081370bd2ba6b5f04e94",
  "description": "simple transaction",
  "createdAt": "2024-05-10T08:43:41.389Z",
  "updatedAt": "2024-05-29T17:47:07.233Z"
}</pre>			<p>Now let’s open Apache <a id="_idIndexMarker595"/>Kafka and check our message. Open <code>localhost:9100</code> from your favorite browser and go to<code>transaction-service-topic</code> and select the <strong class="bold">Value</strong> section from the <strong class="bold">Messages</strong> tab (<em class="italic">Figure 7</em><em class="italic">.5</em>):</p>
			<div><div><img alt="Figure 7.5: Success message in Apache Kafka" src="img/B09148_07_005.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5: Success message in Apache Kafka</p>
			<p>Great! We’re able to send a message to Apache Kafka and now we need to somehow take this message and <a id="_idIndexMarker596"/>handle it. Reading messages from the source (it is Apache Kafka for us) is called a consuming process and a<a id="_idIndexMarker597"/> reader is a consumer.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor132"/>Adapting an account service to new requirements</h1>
			<p>An account service is the main <a id="_idIndexMarker598"/>consumer we need to implement for the given context. First, we need to have the ability to interact with Apache Kafka. Just copy the already implemented account microservice and continue working on that. Navigate to <code>Ch07/accountService</code> and run the following command to install the <code>kafkajs</code> package:</p>
			<pre class="console">
npm install kafkajs</pre>			<p>Now we need to develop a separate module to work with Apache Kafka. Apache Kafka has its variables (brokers, topics, etc.) and that is why we use a <code>.env</code> file as we did for the transaction service. Under the <code>accountService</code> folder, we have <code>configs/.env</code>, and we added the following config items:</p>
			<pre class="source-code">
#KAFKA Configuration
KAFKA_CLIENT_ID=account-service
KAFKA_BROKERS=localhost:29092
KAFKA_TOPIC=transaction-service-topic
KAFKA_GROUP_ID=account-group</pre>			<p>To read the <code>.env</code> file, we use a special configuration that lives under <code>src/config</code> and is called <code>config.js</code>. But <a id="_idIndexMarker599"/>we need to add the required changes to that file to support new key-value pairs. Here is the final version of <code>config.js</code>:</p>
			<pre class="source-code">
const dotenv = require('dotenv');
const Joi = require('joi');
const envVarsSchema = Joi.object()
    .keys({
        PORT: Joi.number().default(3000),
        MONGODB_URL:
          Joi.string().required().description('Mongo DB url'),
        KAFKA_CLIENT_ID: Joi.string().required(),
        KAFKA_BROKERS: Joi.string().required(),
        KAFKA_TOPIC: Joi.string().required(),
        KAFKA_GROUP_ID: Joi.string().required()
    })
    .unknown();
function createConfig(configPath) {
    dotenv.config({ path: configPath });
    const { value: envVars, error } = envVarsSchema
        .prefs({ errors: { label: ‹key› } })
        .validate(process.env);
    if (error) {
        throw new Error(`Config validation error:
          ${error.message}`);
    }
    return {
        port: envVars.PORT,
        mongo: {
            url: envVars.MONGODB_URL,
        },
        kafka: {
            clientID: envVars.KAFKA_CLIENT_ID,
            brokers: envVars.KAFKA_BROKERS,
            topic: envVars.KAFKA_TOPIC,
            groupId: envVars.KAFKA_GROUP_ID,
        }
    };
}
module.exports = {
    createConfig,
};</pre>			<p>We just added additional lines to support <code>config</code> elements newly added to the <code>.</code><code>env </code>file.</p>
			<p>So far, we added a configuration reading mechanism and a Kafka package. Now it is time to develop a Kafka module to interact with Apache Kafka.</p>
			<p>Inside the <code>src</code> folder, create a new folder called <code>modules</code> and add a new file called <code>kafkamodule.js</code>. This<a id="_idIndexMarker600"/>to new requirements”  module should have the following implementation:</p>
			<pre class="source-code">
const { Kafka } = require('kafkajs');
const Account = require('../models/account');
const path = require('path');
const { createConfig } = require('../config/config')
const configPath = path.join(__dirname, '../../configs/.env');
const appConfig = createConfig(configPath);
const kafka = new Kafka({
    clientId: appConfig.kafka.clientId,
    brokers: [appConfig.kafka.brokers],
});
const consumer = kafka.consumer({ groupId:
  appConfig.kafka.groupId });
const consumerModule = async () =&gt; {
    await consumer.connect();
    await consumer.subscribe({ topic: appConfig.kafka.topic });
    await consumer.run({
      eachMessage: async ({ topic, partition, message }) =&gt;
      {
        const transaction =
          JSON.parse(message.value.toString());
        const accountId = transaction.accountId;
        try {
              const blockedAccount =
                await Account.findOne({ accountId, status:
                { $ne: 'blocked' } });
              if (!blockedAccount) {
                const updatedAccount =
                  await Account.findOneAndUpdate(
                        { _id: accountId },
                        { $inc: { count: 1 } },
                        { new: true }
                    );
                  if (updatedAccount.count === 3)
                    await Account.findOneAndUpdate(
                            { _id: accountId },
                            { status: 'blocked' },
                            { new: true }
                   );
                }
                else
                    console.log(`not a valid accountId ${accountId}`);
            }
            catch (error) {
                console.log(error);
            }
        },
    });
};
module.exports = consumerModule;</pre>			<p>We need the following <a id="_idIndexMarker601"/>elements to adapt the account microservice to communicate with the transaction service:</p>
			<ul>
				<li><code>Kafka</code>: To consume the message from Kafka</li>
				<li><code>Account</code>: To interact with the database</li>
				<li><code>path</code>: To specify the path to read config files</li>
				<li><code>createConfig</code>: To retrieve the Kafka configuration</li>
			</ul>
			<p>Let’s go through the code step by step to understand what it does:</p>
			<ul>
				<li><code>const configPath = path.join(__dirname, '../../configs/.env');</code>: This line constructs a path to the <code>.env</code> configuration file, which is located two directories up from the current directory, and then in the <code>configs</code> directory.</li>
				<li><code>const appConfig = createConfig(configPath);</code>: This line calls the <code>createConfig</code> function with <code>configPath</code> as an argument. The <code>createConfig</code> function reads the configuration file and returns the configuration settings as an object (<code>appConfig</code>).</li>
				<li>The following lines create an instance of the Kafka client using the <code>KafkaJS</code> library. It configures <a id="_idIndexMarker602"/>the client with a <code>clientId</code> and a list of brokers, both of which are obtained from the <code>appConfig</code> object.<pre class="source-code">
const kafka = new Kafka({
    clientId: appConfig.kafka.clientId,
    brokers: [appConfig.kafka.brokers],
});</pre></li>				<li><code>const consumer = kafka.consumer({ groupId: appConfig.kafka.groupId });</code>: This line creates a new Kafka consumer instance, specifying a <code>groupId</code> obtained from the <code>appConfig</code> object. The <code>groupId</code> is used to manage consumer group coordination in Kafka.</li>
				<li>We have implemented the main functionality of our module inside the <code>consumerModule</code> function. This function connects to Kafka and subscribers to the given topic.</li>
				<li><code>await consumer.connect();</code>: This line connects the Kafka consumer to the Kafka broker.</li>
				<li><code>await consumer.subscribe({ topic: appConfig.kafka.topic });</code>: This line subscribes the consumer to the specified Kafka topic, which is obtained from the <code>appConfig</code> object.</li>
				<li><code>Consumer.run</code> starts the consumer to listen for messages from the subscribed Kafka topic. It defines an asynchronous handler function for each message that processes each consumed message. The function takes an object with <code>topic</code>, <code>partition</code>, and <code>message</code> properties. For each message, it parses the retrieved message and extracts the account ID. The real business rules start here. First, we check whether the given account ID exists in our database and that it is not blocked. If the account ID exists, then we should update this account, incrementing its count. If the increment count is <code>3</code>, then the status will be updated to <code>blocked</code>.</li>
			</ul>
			<p>The rest of the code lines are<a id="_idIndexMarker603"/> straightforward.</p>
			<p>To use the <code>kafkamodule.js</code> file capabilities, we need to import it into <code>app.js</code> and call it. Here is what <code>app.js</code> looks like:</p>
			<pre class="source-code">
const express = require('express');
const v1 = require('./routes/v1');
const consumerModule = require('./modules/kafkamodule');
const app = express();
consumerModule();
// service
app.use(express.json());
// V1 API
app.use('/v1', v1);
module.exports = app;</pre>			<p>Well, as you might guess, we missed one important piece of information. Yes – it is a newly created <code>count</code>. To track fraud operations, we need to add a new item called <code>count</code> to the <em class="italic">account schema</em>. Open <code>models/account.js</code> and add the following lines to your schema:</p>
			<pre class="source-code">
count: {
            type: Number,
            default: 0, // Optional:(defaults to 0)
        },</pre>			<p>We don’t need to change the account service and account controller to use our new <code>count :{}</code>. It is an implementation detail and a user should not be able to interact with this column directly. Everything is ready and we can test our service.</p>
			<p>Previously, we ran the<a id="_idIndexMarker604"/> account microservice without Docker Compose, but now we’ve added a <code>docker-compose.yml</code> file for it (<code>Ch07/accountservice/docker-compose.yml</code>). The transaction service already has its own <code>docker-compose.yml</code> file that hosts Kafka. To test both services together, we must run the <code>docker-compose.yml</code> files from the <code>accountservice</code> and <code>transactionservice</code> directories.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor133"/>Testing our microservices together</h1>
			<p>We should run transaction and account microservices together to test producing <a id="_idIndexMarker605"/>and consuming processes. First, let’s start with the account microservice. As mentioned before, don’t forget to run the <code>docker-compose.yml</code> files for both services.</p>
			<p>To test the newly updated account, follow these steps:</p>
			<ol>
				<li>Navigate to <code>Ch07/accountservice/src</code> from the terminal.</li>
				<li>Run the account service from the command line using the <code>node </code><code>index.js </code>command.</li>
				<li>Open Postman and, from the new tab, paste the service URL (it is <code>http://localhost:3001/v1/accounts</code> for us), and for the HTTP method, select <code>POST</code>. Select <strong class="bold">Body</strong> | <strong class="bold">raw</strong>, change <strong class="bold">Text</strong> to <strong class="bold">JSON</strong>, and paste the following:<pre class="source-code">
{
    "name":"AccName1",
    "number":"Ac12345",
    "type":"root",
    "status":"new"
}</pre><p class="list-inset">You should get the following response:</p><pre class="source-code"><strong class="bold">{</strong>
<strong class="bold">    "success": true,</strong>
<strong class="bold">    "Account": {</strong>
<strong class="bold">        "id":{your_account_id}, //for the given request, it is  "6658ae5284432e40604018d5" for us</strong>
<strong class="bold">        "name": "AccName1",</strong>
<strong class="bold">        "number": "Ac12345",</strong>
<strong class="bold">        "type": "root",</strong>
<strong class="bold">        "status": "new"</strong>
<strong class="bold">    }</strong>
<strong class="bold">}</strong></pre></li>			</ol>
			<p>Here is what it looks like in<a id="_idIndexMarker606"/> our database:</p>
			<pre class="source-code">
{
  "_id": {
    "$oid": "6658ae5284432e40604018d5"
  },
  "name": "AccName1",
  "number": "Ac12345",
  "type": "root",
  "status": "new",
  …………………
}</pre>			<p>Everything is okay for now based on what we have in the account microservice. Our main trigger, the producer, is the transaction microservice. Let’s produce a message and see whether the account microservice can consume this message or not. We still need the account service to run in <a id="_idIndexMarker607"/>parallel with transaction microservice, and that is why we need to open a new terminal to run the transaction microservice:</p>
			<ol>
				<li>Open a new terminal and navigate to <code>Ch07/transactionservice</code>.</li>
				<li>Run <code>npm run start:dev</code> to start the transaction microservice.</li>
				<li>Navigate to <code>http://localhost:3000/api/</code> and select <code>POST /transaction/</code>.</li>
				<li>Paste the following JSON to create a new transaction:<pre class="source-code">
{
  "accountId": "6658ae5284432e40604018d5",
  "description": "Optional transaction description"
}</pre><p class="list-inset">You should use the account ID created by the account microservice. You will get the following response:</p><pre class="source-code"><strong class="bold">{</strong>
<strong class="bold">  "id": {your_id},//it is '37' for us but in your case, it may have a different value</strong>
<strong class="bold">  "status": "CREATED",</strong>
<strong class="bold">  "accountId": "6658ae5284432e40604018d5",</strong>
<strong class="bold">  "description": "Optional transaction description",</strong>
<strong class="bold">………………</strong>
<code>37</code> for us) to the <code>POST /transaction/Id </code>API. It is a fraud endpoint. The response will be like the following:<pre class="source-code">
<strong class="bold">{</strong>
<strong class="bold">  "id": 37,</strong>
<strong class="bold">  "status": "FRAUD",</strong>
<strong class="bold">  "accountId": "6658ae5284432e40604018d5",</strong>
<strong class="bold">  "description": "Optional transaction description",</strong>
<strong class="bold">……..</strong>
<strong class="bold">}</strong></pre><p class="list-inset">Before executing the <a id="_idIndexMarker608"/>fraud endpoint, make sure that the Docker infrastructure is running. After running a fraud request, the account microservice should read and update the data. Running the same account ID in a fraud context three times should block the account itself in the account microservice:</p><pre class="source-code">{
….
  "type": "root",
  "status": "blocked",
  "count": 3,
………
}</pre></li>			</ol>
			<p>With an understanding of the asynchronous communication technique, you can easily apply it to your projects with confidence.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor134"/>Summary</h1>
			<p>In this chapter, we started our journey by learning the importance of defining proper communication between microservices. We mostly use two main communication forms: async and sync. Choosing one over another is always a context-dependent choice – context is king. Then, we talked about the advantages of asynchronous communication. There are multiple ways of implementing asynchronous communication and we talked about most of the popular choices. Everything has a price and integrated microservices architecture is not an exception. It brings a lot of additional complexity we need to take into account and one of them is asynchronous communication.</p>
			<p>We talked about Apache Kafka, which helps us to overcome the problem we have. We learned about essential concepts such as clusters, brokers, topics, messages, and partitions. Our practical examples covered two main microservices. The transaction service was our producer, which produces a message, and the account microservice was a consumer that consumers that message. Of course, there are a lot of subtopics, such as refactoring, exception handling, testing, and deploying, that we haven’t covered yet, and the following chapters will cover these in detail.</p>
		</div>
	</body></html>