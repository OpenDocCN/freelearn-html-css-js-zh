- en: Building a Continuous Deployment Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the CI/CD pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing unit tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing integration tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing contract tests for a synchronous API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing contract tests for an asynchronous API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assembling transitive end-to-end tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging feature flags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the preceding chapters, we have seen how cloud-native is lean and
    autonomous. Leveraging fully-managed cloud services and establishing proper bulkheads
    empowers self-sufficient, full-stack teams to rapidly and continuously deliver
    autonomous services with the confidence that a failure in any one service will
    not cripple the upstream and downstream services that depend on it. This architecture
    is a major advancement because these safeguards protect us from inevitable human
    errors. However, we must still endeavor to minimize human error and increase our
    confidence in our systems.
  prefs: []
  type: TYPE_NORMAL
- en: To minimize and control potential mistakes, we need to minimize and control
    our batch sizes. We accomplish this by following the practice of *decoupling deployment
    from release*. A **deployment** is just the act of deploying a piece of software
    into an environment, whereas a **release** is just the act of making that software
    available to a set of users. Following **lean** methods, we release functionality
    to users in a series of small, focused experiments that determine whether or not
    the solution is on the right track, so that timely course corrections can be made.
    Each experiment consists of a set of stories, and each story consists of a set
    of small focused tasks. These tasks are our unit of deployment. For each story,
    we plan a roadmap that will continuously deploy these tasks in an order that accounts
    for all inter-dependencies, so that there is zero downtime. The practices that
    govern each individual task are collectively referred to as a **task branch workflow**.
    The recipes in this chapter demonstrate the inner-working of a task branch workflow
    and how, ultimately, we enable these features for users with feature flags.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the CI/CD pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Small batch sizes reduce deployment risk because it is much easier to reason
    about their correctness and much easier to correct them when they are in error.
    Task branch workflow is a Git workflow that is focused on extremely short-lived
    branches, in the range of just hours rather than days. It is similar to an *issue
    branch workflow*, in that each task is tracked as an issue in the project management
    tool. The length of an issue is ambiguous, however, because an issue can be used
    to track an entire feature. This recipe demonstrates how issue tracking, Git branches,
    pull requests, testing, code review, and the CI/CD pipeline work together in a
    task branch workflow to govern small focused units of deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need to have an account on GitLab ([https://about.gitlab.com/](https://about.gitlab.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-pipeline` directory, `cd cncb-pipeline`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the Git repository locally and remotely in `gitlab.com`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Confirm that the project was created in your Gitlab account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new issue in the project named `intialize-project`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press the Create merge request button and note the name of the branch, such
    as `1-initialize-project`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check out the branch locally at with `git pull && git checkout 1-initialize-project`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `.gitlab-ci.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `package.json` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Configure the `DEV_AWS_ACCESS_KEY_ID`, `DEV_AWS_SECRET_ACCESS_KEY`, `PROD_AWS_ACCESS_KEY_ID`,
    `PROD_AWS_SECRET_ACCESS_KEY`, and `NPM_TOKEN` environment variables in the GitLab
    project ([https://gitlab.com/](https://gitlab.com/)) under Settings* | *CI/CD
    |Variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Push the project files to the remote repository, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Before pushing your changes, you should always execute `git pull` to keep your
    task branch in sync with the master branch.
  prefs: []
  type: TYPE_NORMAL
- en: Review the code in the merge request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the progress of the branch pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the `cncb-pipeline-stg` stack in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the `WIP:` prefix from the name of the merge request and accept the merge
    request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is best to start a pull request as early as possible to receive feedback
    as early as possible. The `WIP:` prefix indicates to reviewers that work is still
    progressing. The prefix is purely procedural, but GitLab will not allow a merge
    request with a WIP prefix to be accidentally accepted.
  prefs: []
  type: TYPE_NORMAL
- en: Review the progress of the master pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the `cncb-pipeline-prd` stack in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove both stacks once you are finished.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are using `GitLab.com` simply because it is a freely-available and hosted
    toolset that is well-integrated. There are other alternatives, such as Bitbucket
    Pipelines, that require a little more elbow grease to stand up but they still
    offer comparable features. A `bitbucket-pipelines.yml` file is included in the
    recipes for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen throughout this cookbook, our unit of deployment is a stack,
    as defined by a `serverless.yml` file in the root of a project directory. As we
    see in this recipe, each project is managed in its own Git repository and has
    its own CI/CD pipeline. The pipeline is defined by a configuration file that lives
    in the root of the project as well, such as a `.gitlab-ci.yml` or `bitbucket-pipelines.yml`
    file. These pipelines are integrated with the Git branching strategy and are governed
    by pull requests.
  prefs: []
  type: TYPE_NORMAL
- en: Note that GitLab uses the term *merge request*, whereas other tools use the
    term *pull request*. The two terms can be used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: A task branch workflow begins when an issue or task is pulled in from the backlog
    and used to create a branch in the repository. A pull request is created to govern
    the branch. The pipeline executes all tests on the branch and its progress is
    displayed in the pull request. Once the tests are considered successful, the pipeline
    deploys the stack to the staging environment in the development account. A code
    review is performed in the pull request and discussion is recorded with comments.
    Once everything is in order, the pull request can be accepted to merge the changes
    to the master branch and trigger deployment of the stack to the production environment
    in the production account.
  prefs: []
  type: TYPE_NORMAL
- en: The first line of the pipeline definition denotes that the `node:8` Docker image
    will be used to execute the pipeline. The rest of the pipeline definition orchestrates
    the steps we have been executing manually throughout this cookbook. First, `npm
    install` installs all the dependencies as defined in the `package.json` file.
    Then, we execute all the tests on the given branch. Finally, we deploy the stack
    to the specific environment and region with `npm`; in this case, `npm run dp:stg:e`
    or `npm run dp:prd:e`. The details of each step are encapsulated in the `npm`
    scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, throughout this cookbook, we have been using the `npm run dp:lcl`
    script to perform our deployments. These allow each developer to perform development
    and testing in a personal stack (that is, local or `lcl`) to help ensure that
    the staging (`stg`) environment stays stable and therefore the production (`prd`)
    environment as well.
  prefs: []
  type: TYPE_NORMAL
- en: Environment variables, such as `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`,
    are securely stored by the pipeline and never logged. We define a set of variables
    per account, as identified by the `DEV_` and `PROD_` prefix, and then map them
    in the pipeline definition. In the *Securing your cloud account* recipe, we created `CiCdUser`
    to grant permissions to the pipelines. Here, we need to manually create an access
    key for those users and securely store them as pipeline variables. The keys and
    pipeline variables are then periodically rotated and updated.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline deploys the stack to the staging environment in the development
    account for each task branch, and to the production environment in the production
    account for the master branch. The access key determines which account is used
    and the Serverless Framework `-s` option fully qualifies the name of the stack.
    We then add an additional option called `--acct` to allow us to index into account-scoped
    custom variables, such as `${self:custom.accounts.${opt:acct}.accountNumber}`.
    To help avoid confusion between the production stage and the production account,
    we need to use use slightly different abbreviations, such as `prd` and `prod`.
  prefs: []
  type: TYPE_NORMAL
- en: Writing unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unit testing is arguably the most important type of testing and should certainly
    account for the majority of test cases in the test pyramid. Testing should follow
    a scientific method where we hold some variables constant, adjust the input, and
    measure the output. Unit testing accomplishes this by testing individual units
    in isolation. This allows unit tests to focus on functionality and maximize coverage.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-unit-testing` directory with `cd cncb-unit-testing`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `package.json` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the unit tests, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `test/unit/connector/db.test.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `test/unit/get/index.test.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: (Optional) Repeat the steps from the *Creating the CI/CD pipeline* recipe with
    this project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous recipes, we purposefully simplified the examples to a reasonable
    degree to highlight the specific topics. The code was correct but the recipes
    did not have any unit tests, because the topic had not yet been addressed. The
    first thing you are likely to notice in the recipes in this chapter is that we
    are adding additional structure to the code; for example, each function has its
    own directory and files and we have also added some lightweight layering to the
    code. This structure is intended to facilitate the testing process by making it
    easier to isolate the unit that is under test. So, let's now dig deeper into the
    tools and structure that have been added.
  prefs: []
  type: TYPE_NORMAL
- en: The first tool that is called in the `npm test` script is `nyc`, which is the
    command line interface for the `istanbul` code coverage tool. The `.nycrc` file
    configures the code coverage process. Here, we require 100% coverage. This is
    perfectly reasonable for the scope of our bounded, isolated, and autonomous services.
    It is also reasonable because we are writing the unit tests incrementally as we
    also incrementally build the services in a series of task branch workflows. Furthermore,
    without keeping the coverage at 100%, it would be too easy to skip the testing
    until later on in the development process, which is dangerous in a continuous
    deployment pipeline and defeats the purpose. Fortunately, the structure of the
    code makes it much easier to identify which features are lacking tests.
  prefs: []
  type: TYPE_NORMAL
- en: The `npm pretest` script runs the linting process. `eslint` is a very valuable
    tool. It enforces best practices, automatically fixes many violations, and identifies
    common problems. In essence, linting helps to teach developers how to write better
    code. The linting process can be tuned with the `.eslintignore` and `.eslintrc.js`
    files.
  prefs: []
  type: TYPE_NORMAL
- en: Isolating external dependencies is an essential part of unit testing. Our testing
    tools of choice are `mocha`, `chai`, `sinon`, and `aws-sdk-mock`. There are many
    tools available, but this combination is extremely popular. mocha is the overarching
    testing framework; `chai` is the assertion library; `sinon` provides test spies,
    stubs, and mocks; and `aws-sdk-mock` builds on sinon to simplify testing against
    the `aws-sdk`. To further facilitate this process, we isolate our `aws-sdk` calls
    inside Connector classes. This does have the added benefit of reusing code throughout
    the service, but its primary benefit is to simplify testing. We write unit tests
    specifically for the classes that use `aws-sdk-mock`. Throughout the rest of the
    unit test code, we stub out the connector layer, which greatly simplifies the
    setup of each test, because we have isolated the complexities of the `aws-sdk`.
  prefs: []
  type: TYPE_NORMAL
- en: The `Handler` classes account for the bulk of the testing. These classes encapsulate
    and orchestrate the business logic, and therefore will require the most testing
    permutations. To facilitate this effort, we decouple the `Handler` classes from
    the `callback` function. The `handle` method either returns a *promise* or a *stream*
    and the top-level function then adapts these to the callback. This allows tests
    to easily tap into the processing flow to assert the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Our tests are inherently asynchronous; therefore, it is important to guard against
    evergreen tests that do not fail when the code is broken. For handlers that return
    promises, the best approach is to use `async`/`await` to guard against swallowed
    exceptions. For handlers that return a stream, the best approach is to use the
    collect/tap/done pattern to protect against scenarios where the data does not
    flow all the way through the stream.
  prefs: []
  type: TYPE_NORMAL
- en: Writing integration tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integration tests focus on testing the API calls between dependent services.
    In our cloud-native systems, these are concentrated on intra-service interactions
    with fully-managed cloud services. They ensure that the interactions are properly
    coded to send and receive proper payloads. These calls require the network, but
    networks are notoriously unreliable. This is the major cause of flaky tests that
    randomly and haphazardly fail. Flaky tests, in turn, are a major cause of poor
    team morale. This recipe demonstrates how to use a VCR library to create test
    doubles that allow integration testing to be executed in isolation without a dependency
    on the network or the deployment of external services.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need an AWS Kinesis Stream, such as the
    one created in the *Creating an event stream* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-integration-testing` directory with `cd cncb-integration-testing`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `package.json` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Run the unit test with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the integration tests in `replay` mode, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Review the files in the `./fixtures` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the files related to the synchronous integration tests, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the files related to the asynchronous integration tests, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Deploy the stack with `npm run dp:stg:e`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Delete the `./fixtures` directory and run the integration tests again in `record`
    mode, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: (Optional) Repeat the steps from the *Creating the CI/CD pipeline* recipe with
    this project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integration testing uses all of the same tools that are used for unit testing,
    plus some additional tools. This is an advantage in that the learning curve is
    incremental. The first new tool of interest is the `serverless-offline` plugin.
    This plugin reads the `serverless.yml` file and simulates the API Gateway locally
    to facilitate the testing of synchronous APIs. Next, we use `supertest` to make
    HTTP calls to the locally-running service and assert the responses. Inevitably,
    these services make calls to AWS services using the default or specified access
    key. These are the calls that we want to record and playback in the CI/CD pipeline. `baton-vcr-serverless-plugin`
    initializes the `Replay` VCR library in the `serverless-offline` process. By default,
    the VCR runs in `replay` mode and will fail if a recording is not found under
    the fixtures directory. When writing a new test, the developer runs the tests
    in `record` mode by setting the `REPLAY` environment variable, `REPLAY=record
    npm run test:int`. To ensure that a recording is found, we must hold constant
    all dynamically-generated values that are used in the requests; to accomplish
    this, we inject mocks into the webpack configuration. In this example, the `./test/int/mocks.js`
    file uses `sinon` to mock UUID.
  prefs: []
  type: TYPE_NORMAL
- en: Writing integration tests for asynchronous functions, such as `triggers` and
    `listeners`, is mostly similar. First, we need to manually capture the events
    in the log files, such as a DynamoDB Stream event, and include them in test cases.
    Then, we initialize the `baton-vcr-replay-for-aws-sdk` library when the tests
    begin. From here, the process of recording requests is the same. In the case of
    a `trigger` function, it will record a call to publish an event to Kinesis, where
    a `listener` function will typically record calls to the service's tables.
  prefs: []
  type: TYPE_NORMAL
- en: When writing integration tests, it is important to keep the test pyramid in
    mind. Integration tests are focused on interactions within the specific API calls;
    they do not need to cover all of the different functional scenarios, as that is
    the job of unit tests. The integration tests just need to focus on the structure
    of the messages to ensure they are compatible with what is expected by and returned
    from the external service. To support the creation of the recordings for these
    tests, the external system may need to be initialized with a specific dataset.
    This initialization should be coded as part of the tests and recorded as well.
  prefs: []
  type: TYPE_NORMAL
- en: Writing contract tests for a synchronous API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contract testing and integration testing are two sides of the same coin. Integration
    tests ensure that a consumer is calling a provider service correctly, whereas
    contract tests ensure that the provider service continues to meet its obligations
    to its consumers and that any changes are backward-compatible. These tests are
    also consumer driven. This means that the consumer submits a pull request to the
    provider's project to add these additional tests. The provider is not supposed
    to change these tests. If a contract test breaks, it implies that a backwards-incompatible
    change has been made. The provider has to make the change compatible and then
    work with the consumer team to create an upgrade roadmap.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `bff` directory with `cd cncb-contract-testing-sync/bff`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the unit tests with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the integration tests in `replay` mode with `npm run test:int`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the files in the `./fixtures` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `./test/int/frontend/contract.test.js` with the following
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Review the consumer's `test` and `fixture` files under `./frontend`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (Optional) Repeat the steps from the *Creating the CI/CD pipeline* recipe with
    this project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to integration and contract testing, the devil is in the detail.
    Specifically, the detail of the individual fields, their data types, and their
    valid values. It is all too easy for a provider to change a seemingly mundane
    detail that violates a consumer's understanding of the contract. These changes
    then go unnoticed until the worst possible moment. In this regard, handcrafted
    contract tests are unreliable, because the same misunderstanding about the contract
    usually translates into the test as well. Instead, we need to use the same recordings
    on both sides of the interaction.
  prefs: []
  type: TYPE_NORMAL
- en: The consumer creates an integration test in its project that records the interaction
    with the provider's service. These same recordings are then copied to the provider's
    project and used to drive the contract tests. A consumer-specific `fixtures` subdirectory
    is created for the recordings. The contract tests use the `baton-request-relay`
    library to read the recordings so that they can be used to drive `supertest` to
    execute the same requests in the provider's project.
  prefs: []
  type: TYPE_NORMAL
- en: In our cloud-native systems, these synchronous requests are usually between
    a frontend and its **Backend for Frontend** (**BFF**) service, which is owned
    by the same team. The fact that the same team owns the consumer and provider does
    not negate the value of these tests, because even a subtle change, such as changing
    a short integer to a long integer, can have a drastic impact if all the wrong
    assumptions were made on either side.
  prefs: []
  type: TYPE_NORMAL
- en: Writing contract tests for an asynchronous API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of contract testing for asynchronous APIs is to ensure backwards-compatibility
    between the provider and the consumer—just as it is with synchronous APIs. Testing
    asynchronous communication is naturally flaky, as there is the unreliability of
    networks plus the unreliable latency of asynchronous messaging. All too often
    these tests fail because messages are slow to arrive and the tests timeout. To
    solve this problem, we isolate the tests from the messaging system; we record
    the send message request on one end, then relay the message and assert the contract
    on the receiving side.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the upstream and downstream projects from the following templates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `upstream` directory with `cd cncb-contract-testing-async-upstream`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the unit test with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the integration tests in `replay` mode with `npm run test:int`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the files in the `./fixtures/downstream-consumer-x` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `./test/int/downstream-consumer-x/contract.test.js` with
    the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `downstream` directory with `cd ../cncb-contract-testing-async-downstream`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the same steps to run the tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the downstream consumer''s `./test/int/upstream-provider-y` and `./fixture/upstream-provider-y`
    files, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: (Optional) Repeat the steps from the *Creating the CI/CD pipeline* recipe with
    this project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of recording and creating a contract test for an asynchronous API
    is the inverse of a synchronous API. With a synchronous API, the consumer initiates
    the interaction, whereas with an asynchronous API, the provider initiates the
    publishing of the event. However, asynchronous providers are unaware of their
    consumers, so these tests still need to be consumer-driven.
  prefs: []
  type: TYPE_NORMAL
- en: First, the consumer project creates a test in the upstream project and records
    the call to publish an event to the event stream. Then, the consumer relays the
    recording in its own project, using the `baton-event-relay` library, to assert
    that the content of the event is as it expects. Once again, the provider project
    does not own these tests and should not fix the test if it breaks due to a backwards-incompatible
    change.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling transitive end-to-end tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional end-to-end testing is labor-intensive and expensive. As a result,
    traditional batch sizes are large and end-to-end testing is performed infrequently
    or skipped altogether. This is the exact opposite of our objective with continuous
    deployment. We want small batch sizes and we want full testing on every deployment,
    multiple times per day, which is when we typically hear cries of heresy. This
    recipe demonstrates how this can be achieved with the tools and techniques already
    covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the `author-frontend`, `author-bff`, `customer-bff`, and `customer-frontend`
    projects from the following templates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies in each project with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the end-to-end tests in `replay` mode for each project, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Review all of the following fixture files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Review all of the following end-to-end test case files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integration testing and contract testing are two sides of the same coin. We
    have already seen how these tests can be implemented with test doubles that replay
    previously recorded request and response pairs. This allows each service to be
    tested in isolation without the need to deploy any other service. A service provider
    team creates integration tests to ensure their service works as they expect, and
    service consumer teams create contract tests to ensure that the provider's service
    works as they expect. We then build on this so that the test engineers from all
    teams work together to define sufficient end-to-end test scenarios to ensure that
    all of the services are working together. For each scenario, we string together
    a series of integration and contract tests across all projects, where the recordings
    from one project are used to drive the tests in the next project, and so forth.
    Borrowing from the transitive property of equality, if Service *A* produces Payload
    1, which works with Service *B* to produce Payload 2, which works with Service
    *C* to produce the expected Payload 3, then we can assert that when Service *A*
    produces Payload 1 then ultimately Service *C* will produce Payload 3.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we have an authoring application and a customer application.
    Each consists of a frontend project and a BFF project. The `author-frontend` project
    made the `save-thing0` recording. This recording was copied to the `author-bff`
    project and ultimately resulted in the `thing0-created` recording. The `thing0-created`
    recording was then copied to the `customer-bff` project and ultimately resulted
    in the `get-thing0` recording. The `get-thing0` recording was then copied to the
    `customer-frontend` project to support its testing.
  prefs: []
  type: TYPE_NORMAL
- en: The end result is a set of completely autonomous test suites. The test suite
    of a specific service is asserted each time the service is modified, without the
    need to rerun a test suite in each project. Only a backwards-incompatible change
    requires dependent projects to update their test cases and recordings; so, we
    no longer need to maintain an end-to-end testing environment. Database scripts
    are no longer needed to reset databases to a known state, as the data is embodied
    in test cases and recordings. These transitive end-to-end tests form the tip of
    a comprehensive testing pyramid that increases our confidence that we have minimized
    the chance of human error in our continuous deployment pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging feature flags
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The practice of decoupling deployment from release is predicated on the use
    of feature flags. We are continuously deploying small batches of change to mitigate
    the risks of each deployment. These changes are deployed all the way to production,
    so we need a feature flag mechanism to disable these capabilities until we are
    ready to release them and make them generally available. We also need the ability
    to enable these capabilities for a subset of users, such as beta users and internal
    testers. It is also preferable to leverage the natural feature flags of a system,
    such as permissions and preferences, to minimize the technical debt that results
    from littering code with custom feature flags. This recipe will show you how to
    leverage the claims in a JWT token to enable and disable features.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before starting this recipe, you will need an AWS Cognito user pool, such as
    the one created in the *Creating a federated identity pool* recipe. The user pool
    should have the following two groups defined: `Author` and `BetaUser`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-feature-flag` directory with `cd cncb-feature-flag`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `src/Authorize.js`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `src/Home.js`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Review the file named `src/App.js` and update the `clientId` and `domain` fields
    with the values for the user pool stack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the app locally with `npm start`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the `Sign Up` link and follow the instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the `Sign Out` link and then `Sign Up` another user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each user to a different group in the AWS user pool console—one to the
    `Author` group and the other to the `BetaUser` group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Sign In` as each user and notice how the screen renders differently for each
    one.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First and foremost, zero-downtime deployment has to be treated as a first-class
    requirement and must be accounted for in the task roadmap and system design. If
    an enhancement is just adding a new optional field on an existing domain entity
    then a feature flag isn't required at all. If a field is just being removed, the
    order of deployment is important, from most dependent to least dependent. If an
    entirely new feature is being added, then access to the whole feature can be restricted.
    The most interesting scenarios are when a significant change is being made to
    an existing and popular feature. If the change is significant then it may be best
    to support two versions of the feature simultaneously, such as two versions of
    a page—in which case the scenario is essentially the same as the entirely new
    feature scenario. If a new version is not warranted, then care should be taken
    not to tie the code in knots.
  prefs: []
  type: TYPE_NORMAL
- en: In this simplified ReactJS example, the JWT token is accessible after sign-in
    through the `auth` property. Instances of the `HasRole` component are configured
    with  `allowedRoles`. The component checks if the JWT token has a matching group
    in the `cognito:groups` field. If a match is found then the `children` components
    are rendered; otherwise, `null` is returned and nothing is rendered.
  prefs: []
  type: TYPE_NORMAL
