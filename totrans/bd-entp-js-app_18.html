<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Robust Infrastructure with Kubernetes</h1>
                </header>
            
            <article>
                
<p class="code-line">In the previous chapter, we used Docker to pre-build and package different parts of our application, such as Elasticsearch and our API server, into Docker images. These images are portable and can be deployed independently onto any environment. Although this revised approach automated some aspects of our workflow, we are still<span> </span><strong>manually</strong><span> </span>deploying our containers on a<span> </span><strong>single</strong><span> </span>server.</p>
<p class="code-line">This lack of automation presents the risk of human error. Deploying on a single server introduces a <strong>single point of failure</strong> (<strong>SPOF</strong>), which reduces the reliability of our application.</p>
<p class="code-line">Instead, we should provide redundancy by spawning multiple instances of each service, and deploying them across different physical servers and data centers. In other words, we should deploy our application on a cluster.</p>
<p class="code-line">Clusters allow us to have high availability, reliability, and scalability. When an instance of a service becomes unavailable, a failover mechanism can redirect unfulfilled requests to the still-available instances. This ensures that the application, as a whole, remains responsive and functional.</p>
<p class="code-line">However, coordinating and managing this distributed, redundant cluster is non-trivial, and requires many moving parts to work in concert. These include the following:</p>
<ul>
<li class="code-line">Service discovery tools</li>
<li class="code-line">Global configuration store</li>
<li class="code-line">Networking tools</li>
<li class="code-line">Scheduling tools</li>
<li class="code-line">Load balancers</li>
<li class="code-line">...and many more</li>
</ul>
<p class="code-line">Cluster Management Tools is a platform which manages these tools and provides a layer of abstraction for developers to work with. A prime example is<span> </span><em>Kubernetes</em>, which was open-sourced by Google in 2014.</p>
<div class="packt_infobox"><span>Because most Cluster Management Tools also use containers to deploy, they are often also called </span><strong>Container Orchestration</strong> <strong>systems</strong>.</div>
<p class="code-line">In this chapter, we will learn how to:</p>
<ul>
<li class="code-line">Make our application more robust by deploying it with Kubernetes on DigitalOcean</li>
<li class="code-line">Understand the features of a robust system; namely <strong>availability</strong>, <strong>reliability</strong>, <strong>throughput</strong>, and <strong>scalability</strong></li>
<li class="code-line">Examine the types of components a <strong>Cluster Management Tool</strong> would normally manage, how they work together, and how they contribute to making our system more robust</li>
<li class="code-line">Get hands-on and deploy and manage our application as a distributed Kubernetes cluster</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High availability</h1>
                </header>
            
            <article>
                
<p class="code-line">Availability is a measure of the proportion of time that a system is able to fulfill its intended function. For an API, it means the percentage of time that the API can respond correctly to a client's requests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring availability</h1>
                </header>
            
            <article>
                
<p class="code-line">Availability is usually measured as the percentage of time the system is functional (<em>Uptime</em>) over the total elapsed time:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7816e04c-3bb0-484f-8900-0b6d50eff372.png" style="width:20.83em;height:2.75em;"/></p>
<p class="code-line"><span>This is typically represented as "nines". For example, a system with an availability level of "four nines" will have an uptime of 99.99% or higher.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Following the industry standard</h1>
                </header>
            
            <article>
                
<p class="code-line">Generally speaking, the more complex a system, the more things can go wrong; this translates to a lower availability. In other words, it is much easier to have a 100% uptime for a static website than for an API.</p>
<p class="code-line">So, what is the industry standard for availability for common APIs? Most online platforms offer a <strong>service level agreement</strong> (<strong>SLA</strong>) that includes a clause for the minimum availability of the platform. Here are some examples (accurate at the time of writing):</p>
<ul>
<li class="code-line">Google Compute Engine Service Level Agreement (SLA): 99.99%</li>
<li class="code-line">Amazon Compute Service Level Agreement: 99.99%</li>
<li class="code-line">App Engine Service Level Agreement (SLA): 99.95%</li>
<li class="code-line">Google Maps—Service Level Agreement (“Maps API SLA”): 99.9%</li>
<li class="code-line">Amazon S3 Service Level Agreement: 99.9%</li>
</ul>
<p class="code-line">Evidently, these SLAs provide minimum availability guarantees that range from "three nines" (99.9%) to "four nines" (99.99%); this translates to a maximum downtime of between 52.6 minutes and 8.77 hours per year. Therefore, we should also aim to provide a similar level of availability for our API.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Eliminating single points of failure (SPOF)</h1>
                </header>
            
            <article>
                
<p class="code-line">The most fundamental step to ensure high availability is eliminating<span> </span>(SPOF). A SPOF is a component within a system which, if fails, causes the entire system to fail.</p>
<p class="code-line">For example, if we deploy only one instance of our backend API, the single Node process running the instance becomes a SPOF. If that Node process exits for whatever reason, then our whole application goes down.</p>
<p class="code-line">Fortunately, eliminating a SPOF is relatively simple—replication; you simply have to deploy multiple instances of that component. However, that comes with challenges of its own—when a new request is received, which instance should handle it?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Load balancing versus failover</h1>
                </header>
            
            <article>
                
<p class="code-line">Conventionally, there are two methods to route requests to replicated components:</p>
<ul>
<li class="code-line">
<p class="code-line"><strong>Load balancing</strong>: A load balancer sits in-between the client and the server instances, intercepting the requests and distributing them among all instances:</p>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fc306757-1c22-4a71-8d75-6796dfeb55c4.jpg" style="width:35.25em;height:6.50em;"/></p>
<p style="padding-left: 60px"><span>The way requests are distributed depends on the load balancing algorithm used. Apart from "random" selection, the simplest algorithm is the</span><span> </span><em>round-robin</em><span> algorithm. This is where requests are sequentially routed to each instance in order. For example, if there are two backend servers, A and B, the first request will be routed to A, the second to B, the third back to A, the fourth to B, and so on. This results in requests being evenly distributed:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/340684e2-6628-466d-8e53-ef0fde637b6f.jpg" style="width:30.08em;height:17.25em;"/></p>
<p style="padding-left: 60px"><span>While round-robin is the simplest scheme to implement, it assumes that all nodes are equal </span><span>–</span><span> in terms of available resources, current load, and network congestion. This is often not the case. Therefore,</span><span> </span><em>dynamic<span> </span>round-robin</em><span> </span><span>is often used, which will route more traffic to hosts with more available resources and/or lower load.</span></p>
<p class="mce-root"/>
<ul>
<li class="code-line">
<p class="code-line"><strong>Failover</strong>: Requests are routed to a single<span> </span><em>primary</em><span> </span>instance. If and when the primary instance fails, subsequent requests are routed to a different<span> </span><em>secondary</em>, or<span> </span><em>standby</em>, instance:</p>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/30a4e467-8e2b-4831-bc8f-7d39ebe30ea2.png" style="width:32.17em;height:27.83em;"/></p>
<p class="code-line" style="padding-left: 60px">As with all things, there are pros and cons of each method:</p>
<ul>
<li class="code-line"><strong>Resource Utilization</strong>: With the failover approach, only a single instance is running at any one time; this means you'll be paying for server resources that do not contribute to the normal running of your application, nor improve its performance or throughput. On the other hand, the objective of load balancing is to maximize resource usage; providing high availability is simply a useful side effect.</li>
<li class="code-line"><strong>Statefulness</strong>: Sometimes, failover is the only viable method. Many real-world, perhaps legacy, applications are stateful, and the state can become corrupted if multiple instances of the application are running at the same time. Although you can refactor the application to cater for this, it's still a fact that not all applications can be served behind a load balancer.</li>
</ul>
<ul>
<li class="code-line"><strong>Scalability</strong>: With failover, to improve performance and throughput, you must scale the primary node vertically (by increasing its resources). With load balancing, you can scale both vertically and horizontally (by adding more machines).</li>
</ul>
<p class="code-line">Since our application is stateless, using a distributed load balancer makes more sense as it allows us to fully utilize all resources and provide better performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Load balancing</h1>
                </header>
            
            <article>
                
<p class="code-line">Load balancing can be done in multiple ways—using DNS for load distribution, or employing a Layer 4 or Layer 7 load balancer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DNS load balancing</h1>
                </header>
            
            <article>
                
<p class="code-line">A domain can configure its DNS settings so that multiple IP addresses are associated with it. When a client tries to resolve the domain name to an IP address, it returns a list of all IP addresses. Most clients would then send its requests to the first IP address in the list.</p>
<p class="code-line">DNS load balancing is where the DNS changes the order of these addresses each time a new name resolution request is made. Most commonly, this is done in a round-robin manner.</p>
<p class="code-line">Using this method, client requests should be distributed equally among all backend servers. However, load balancing at the DNS level has some major disadvantages:</p>
<ul>
<li class="code-line"><strong>Lack of health-checks</strong>: The DNS does not monitor the health of the servers. Even if one of the servers in the list goes down, it will still return with the same list of IP addresses.</li>
<li class="code-line">Updating and propagating DNS records to all<span> </span><em>root servers</em>, intermediate DNS servers (<em>resolvers</em>), and clients can take anything from minutes to hours. Furthermore, most DNS servers cache their DNS records. This means that requests may still be routed to failed servers long after the DNS records are updated.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Layer 4/7 load balancers</h1>
                </header>
            
            <article>
                
<p class="code-line">Another way to load balance client requests is to use a<span> </span><em>load balancer</em>. Instead of exposing the backend servers on multiple IP addresses and letting the client pick which server to use, we can instead keep our backend servers hidden behind a private local network. When a client wants to reach our application, it would send the request to the load balancer, which will forward the requests to the backends.</p>
<p class="code-line">Generally speaking, there are two types of load balancers—Layer 4 (L4), Layer 7 (L7). Their names relate to the corresponding layer inside the <strong>Open Systems Interconnection</strong> (<strong>OSI</strong>) reference model—a standard conceptual model that partitions a communication system into abstraction layers:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1859 image-border" src="assets/0bd73b37-b690-4015-a261-411fac6ff45a.png" style="width:37.00em;height:28.00em;"/></p>
<p class="code-line">There are numerous standard protocols at each layer that specify how data should be packaged and transported. For example, FTP and MQTT are both application layer protocols. FTP is designed for file transfer, whereas MQTT is designed for publish-subscribe-based messaging.</p>
<p class="code-line">When a load balancer receives a request, it will make a decision as to which backend server to forward a request to. These decisions are made using information embedded in the request. An L4 load balancer would use information from the transport layer, whereas an L7 load balancer can use information from the application layer, including the request body itself.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Layer 4 load balancers</h1>
                </header>
            
            <article>
                
<p class="code-line">Generally speaking, L4 load balancers use information defined at the<span> </span><strong>Transport layer</strong> (layer 4) of the OSI model. In the context of the internet, this means that L4 load balancers should use information from Transmission Control Protocol (TCP) data packets. However, as it turns out, L4 load balancers also use information from Internet Protocol (IP) packets, which is a layer 3 - the<span> </span><em>network</em><span> </span>layer. Therefore, the name "Layer 4" should be considered a misnomer.</p>
<p class="code-line">Specifically, an L4 load balancer routes requests based on the source/destination IP addresses and ports, with zero regards to the contents of the packets.</p>
<p class="code-line">Usually, an L4 load balancer comes in the form of a dedicated hardware device running proprietary chips and/or software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Layer 7 load balancing</h1>
                </header>
            
            <article>
                
<p class="code-line">A Layer 7 (L7) load balancer is similar to an L4 load balancer, but uses information from the highest layer on the OSI model <span>–</span> the<span> </span><em>application</em><span> </span>layer. For web services like our API, the Hypertext Transfer Protocol (HTTP) is used.</p>
<p class="code-line">An L7 load balancer can use information from the URL, <span>HTTP headers (for example, <kbd>Content-Type</kbd>)</span>, cookies, contents of the message body, client's IP address, and other information to route a request.</p>
<p class="code-line">By working on the application layer, an L7 load balancer has several advantages over an L4 load balancer:</p>
<ul>
<li class="code-line"><strong>Smarter</strong>: Because L7 load balancers can base their routing rules on more information, such as the client's geolocation data, they can offer more sophisticated routing rules than L4 load balancers.</li>
<li class="code-line"><strong>More capabilities</strong>: Because L7 load balancers have access to the message content, they are able to alter the message, such as encrypting and/or compressing the body.</li>
</ul>
<ul>
<li class="code-line"><strong>Cloud Load Balancing</strong>: Because L4 load balancers are typically hardware devices, cloud providers usually do not allow you to configure them. In contrast, L7 load balancers are typically software, which can be fully managed by the developer.</li>
<li class="code-line"><strong>Ease of debugging</strong>: They can use cookies to keep the same client hitting the same backend server. This is a must if you implement stateful logic such as "sticky" sessions, but is also otherwise advantageous when debugging—you only have to parse logs from one backend server instead of all of them.</li>
</ul>
<p class="code-line">However, L7 load balancers are not always "better" than their L4 counterparts. L7 load balancers require more system resources and have high latency, because it must take into consideration more parameters. However, this latency is not significant enough for us to worry about.</p>
<p class="code-line">There are currently a few production-ready L7 load balancers on the market—<strong><span>High Availability Proxy</span></strong> (<strong><span>HAProxy</span></strong>), NGINX, and Envoy. We will look into deploying a distributed load balancer in front of our backend servers later in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High reliability</h1>
                </header>
            
            <article>
                
<p class="code-line">Reliability is a measure of the confidence in a system, and is inversely proportional to the probability of failure.</p>
<p class="code-line">Reliability is measured using several metrics:</p>
<ul>
<li class="code-line"><strong>Mean time between failures</strong><span> </span>(<strong>MTBF</strong>): Uptime/number of failures</li>
<li class="code-line"><strong>Mean time to repair </strong>(<strong>MTTR</strong>): The average time it takes the team to fix a failure and return the system online</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing for reliability</h1>
                </header>
            
            <article>
                
<p class="code-line">The easiest way to increase reliability is to increase test coverage of the system. This is, of course, assuming that those tests are meaningful tests.</p>
<p class="code-line">Tests increase reliability by:</p>
<ul>
<li class="code-line">Increasing MTBF: The more thorough your tests, the more likely you'll catch bugs before the system is deployed.</li>
<li class="code-line">Reducing MTTR: This is because historical test results inform you of the last version which passes all tests. If the application is experiencing a high level of failures, then the team can quickly roll back to the last-known-good version.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High throughput</h1>
                </header>
            
            <article>
                
<p class="code-line">Throughput is a measure of the number of requests that can be fulfilled in a given time interval.</p>
<p class="code-line">The throughput of a system depends on several factors:</p>
<ul>
<li class="code-line"><strong>Network Latency</strong>: The amount of time it takes for the message to get from the client to our application, as well as between different components of the application</li>
<li class="code-line"><strong>Performance</strong>: The computation speed of the program itself</li>
<li class="code-line"><strong>Parallelism</strong>: Whether requests can be processed in parallel</li>
</ul>
<p class="code-line">We can increase throughput using the following strategies:</p>
<ul>
<li class="code-line">Deploying our application geographically close to the client: Generally, this reduces the number of hops that a request must make through proxy servers, and thus reduces network latency. We should also deploy components that depend on each other close together, preferably within the same data center. This also reduces network latency.</li>
<li class="code-line">Ensure servers have sufficient resources: This makes sure that the CPU on your servers are sufficiently fast, and that the servers have enough memory to perform their tasks without having to use swap memory.</li>
<li class="code-line">Deploy multiple instances of an application behind a load balancer: This allows multiple requests to the application to be processed at the same time.</li>
<li class="code-line">Ensure your application code is non-blocking: JavaScript is an asynchronous language. If you write synchronous, blocking code, it will prevent other operations from executing while you wait for the synchronous operation to complete.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High scalability</h1>
                </header>
            
            <article>
                
<p class="code-line">Scalability is a measure of how well a system can grow in order to handle higher demands, while still maintaining the same levels of performance.</p>
<p class="code-line">The demand may arise as part of a sustained growth in user uptake, or it may be due to a sudden peak of traffic (for example, a food delivery application is likely to receive more requests during lunch hours).</p>
<p class="code-line">A highly scalable system should constantly monitor its constituent components and identify components which are working above a "safe" resource limit, and scale that component either<span> </span><em>horizontally</em><span> </span>or<span> </span><em>vertically</em>.</p>
<p class="code-line">We can increase scalability in two ways:</p>
<ul>
<li class="code-line">Scale Vertically or<span> </span><em>scaling Up</em>: Increase the amount of resources (for example, CPU, RAM, storage, bandwidth) to the existing servers</li>
<li class="code-line">Scale Horizontally or<span> </span><em>scaling out</em>: Adding servers to the existing cluster</li>
</ul>
<p class="code-line">Scaling vertically is simple, but there'll always be a limit as to how much CPU, RAM, bandwidth, ports, and even processes the machine can handle. For example, many kernels have a limit on the number of processes it can handle:</p>
<pre><strong>$ cat /proc/sys/kernel/pid_max</strong><br/><strong>32768</strong></pre>
<p class="code-line">Scaling horizontally allows you to have higher maximum limits for resources, but comes with challenges of its own. An instance of the service may hold some temporary state that must be synchronized across different instances.</p>
<p class="code-line">However, because our API is "stateless" (in the sense that all states are in our database and not in memory), scaling horizontally poses less of an issue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clusters and microservices</h1>
                </header>
            
            <article>
                
<p class="code-line">In order to make our system be highly available, reliable, scalable, and produce high throughput, we must design a system that is:</p>
<ul>
<li class="code-line">Resilient/Durable: Able to sustain component failures</li>
<li class="code-line">Elastic: Each service and resource can grow and shrink quickly based on demand</li>
</ul>
<p class="code-line">Such systems can be achieved by breaking monolithic applications into many smaller<span> </span><em>stateless</em><span> </span>components (following the microservices architecture) and deploying them in a<span> </span><em>cluster</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Microservices</h1>
                </header>
            
            <article>
                
<p class="code-line">Instead of having a monolithic code base that caters to many concerns, you can instead break the application down into many services which, when working together, make up the whole application. Each service should:</p>
<ul>
<li class="code-line">Have one or very few concerns</li>
<li class="code-line">Be de-coupled from other services</li>
<li class="code-line">Be stateless (if possible)</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/22f95aca-df87-4acc-913b-40657a9c3cab.jpg" style="width:29.58em;height:24.58em;"/></p>
<div class="packt_figref"/>
<div class="packt_figref">With a monolithic application, all the components must be deployed together as a single unit. if you want to scale your application, you must scale by deploying more instances of the monolith. Furthermore, because there're no clear boundaries between different service, you'd often find tightly-coupled code in the code base. <span>On the other hand, a microservices architecture places each service as a separate, standalone entity. You can scale by replicating only the service that is required. Furthermore, you can deploy the services on varied architecture, even using different vendors.</span></div>
<div class="code-line packt_figref"/>
<p class="code-line">A service should expose an API for other services to interact with, but would otherwise be independent of other services. This means services could be independently deployed and managed.</p>
<p class="mce-root"/>
<p class="code-line">Writing an application that allows for a microservice architecture allows us to achieve high scalability—administrators can simply spawn more instances of an in-demand service. Because the services are independent of each other, they can be deployed independently, where the more in-demand services have more instances deployed.</p>
<p class="code-line">We have made our application stateless and containerized, both of which makes implementing a microservices architecture much easier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clusters</h1>
                </header>
            
            <article>
                
<p class="code-line">To implement a reliable and scalable infrastructure, we must provide redundancy. This means redundancy in:</p>
<ul>
<li class="code-line"><strong>Hardware</strong>: We must deploy our application across multiple physical hosts, each (ideally) at different geographical locations. This is so that if one data center is offline or destroyed, services deployed at the other data centers can keep our application running.</li>
<li class="code-line"><strong>Software</strong>: We must also deploy multiple instances of our services; this is so that the load of handling requests can be distributed across them. Consequently, this yields the following benefits:
<ul>
<li class="code-line">We can route users to the server which provides them with the quickest response time (usually the one closest geographically to the user)</li>
<li class="code-line">We can put one service offline, update it, and bring it back online without affecting the uptime of the entire application</li>
</ul>
</li>
</ul>
<p class="code-line">Deploying applications on a cluster allows you to have hardware redundancy, and load balancers provide software redundancy.</p>
<p class="code-line">A cluster consists of a network of hosts/servers (called<span> </span><em>nodes</em>). Once these nodes are provisioned, you can then deploy instances of your services inside them. Next, you'll need to configure a load balancer that sits in front of the services and distribute requests to the node with the most available service.</p>
<p class="code-line">By deploying redundant services on a cluster, it ensures:</p>
<ul>
<li class="code-line"><strong>High Availability</strong>: If a server becomes unavailable, either through failure or planned maintenance, then the load balancer can implement a<span> </span><em>failover</em><span> </span>mechanism and redistribute requests to the healthy instances.</li>
<li class="code-line"><strong>High Reliability</strong>: Redundant instances remove the<span> </span><em>single point of failure</em>. It means our whole system becomes<span> </span><em>fault-tolerant</em>.</li>
</ul>
<ul>
<li class="code-line"><strong>High Throughput</strong>: By having multiple instances of the service across geographical regions, it allows for low latency.</li>
</ul>
<p class="code-line">This may be implemented as a<span> </span><strong>Redundant Array Of Inexpensive Servers</strong><span> </span>(<strong>RAIS</strong>), the server equivalent of RAID, or<span> </span><em>Redundant Arrays Of Inexpensive Disks</em>. Whenever a server fails, the service will still be available by serving them from the healthy servers.</p>
<p class="code-line">However, if you are using a cloud provider like DigitalOcean, they would take care of the hardware redundancy for you. All that's left for us to do is deploy our cluster and configure our load balancer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster management</h1>
                </header>
            
            <article>
                
<p class="code-line">Deploying our application in a microservices manner inside a cluster is simple enough in principle, but actually quite complex to implement.</p>
<p class="code-line">First, you must<span> </span><em>provision</em><span> </span>servers to act as nodes inside your cluster. Then, we'll need to set up a handful of tools that work in concert with each other to manage your cluster. These tools can be categorized into two groups:</p>
<ul>
<li class="code-line"><strong>Cluster-level tools</strong>: Works at the cluster level, and makes global decisions that affect the whole cluster</li>
<li class="code-line"><strong>Node-level tools</strong>: Resides within each node. It takes instructions from, and feedback to, cluster-level tools in order to coordinate the management of services running inside the node.</li>
</ul>
<p class="code-line">For the cluster-level tools, you'll need the following:</p>
<ul>
<li class="code-line"><strong>A scheduler</strong>: This dictates which node a particular service will be deployed on.</li>
<li class="code-line"><strong>A Discovery Service</strong>: This keeps a record of how many instances of each service are deployed, their states (for example, starting, running, terminating and so on.), where they're deployed, and so on. It allows for<span> </span><em>service discovery.</em></li>
<li class="code-line"><strong>A Global Configuration Store</strong>: Stores cluster configurations such as common environment variables.</li>
</ul>
<p class="code-line">On the node-level, you'll need the following tools:</p>
<ul>
<li class="code-line"><strong>Local configuration management tools</strong>: To keep local configuration states and to synchronize with cluster-level configurations. We have our cluster configurations stored inside the Global Configuration Store; however, we also need a way to retrieve those settings into each node. Furthermore, when those configurations are changed, we need a way to fetch the updated configuration and reload the application/services if required.<span> <kbd>confd</kbd></span><span> (<a href="https://github.com/kelseyhightower/confd">https://github.com/kelseyhightower/confd</a>) </span>is the most popular tool.</li>
<li class="code-line"><strong>Container runtime</strong>: Given that a node is assigned to run a service, it must have the necessary programs to do so. Most services deployed on modern microservice infrastructures use containers to encapsulate the service. Therefore, all major cluster management tools will be bundled with some kind of container runtime, such as Docker.</li>
</ul>
<p class="code-line">Now, let's take a look at each cluster-level tool in more detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cluster-level tools</h1>
                </header>
            
            <article>
                
<p class="code-line">As mentioned previously, cluster-level tools work at the cluster level, and make global decisions that affect the whole cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discovery service</h1>
                </header>
            
            <article>
                
<p class="code-line">At the moment, our API container can communicate with our Elasticsearch container because, under the hood, they're connected to the same network, on the same host machine:</p>
<pre><strong>$ docker network ls</strong><br/><strong>NETWORK ID NAME DRIVER SCOPE</strong><br/><strong>d764e66872cf bridge bridge local</strong><br/><strong>61bc0ca692fc host host local</strong><br/><strong>bdbbf199a4fe none null local</strong><br/><br/><strong>$ docker network inspect bridge --format='{{range $index, $container := .Containers}}{{.Name}} {{end}}'</strong><br/><strong>elasticsearch hobnob</strong></pre>
<p class="code-line">However, if these containers are deployed on separate machines, using different networks, how can they communicate with each other?</p>
<p class="code-line">Our API container must obtain network information about the Elasticsearch container so that it can send requests to it. One way to do this is by using a<span> </span><em>service discovery</em><span> </span>tool.</p>
<p class="code-line">With service discovery, whenever a new container (running a service) is initialized, it registers itself with the <strong>Discovery Service</strong>, providing information about itself, which includes its IP address. The <strong>Discovery Service</strong> then stores this information in a simple key-value store.</p>
<p class="code-line">The service should update the <strong>Discovery Service</strong> regularly with its status, so that the Discovery Service always has an up-to-date state of the service at any time.</p>
<p class="code-line">When a new service is initiated, it will query the <strong>Discovery Service</strong> to request information about the services it needs to connect with, such as their IP address. Then, the <strong>Discovery Service</strong> will retrieve this information from its key-value store and return it to the new service:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/343e6df5-2202-41e1-8c4d-1c71079a7520.png" style="width:38.75em;height:27.33em;"/></p>
<p class="code-line">Therefore, when we deploy our application as a cluster, we can employ a service discovery tool to facilitate our API's communication with our Elasticsearch service.</p>
<p class="code-line">Popular service discovery tools include the following:</p>
<ul>
<li class="code-line"><span><kbd>etcd</kbd>, </span>by CoreOS (<a href="https://github.com/coreos/etcd">https://github.com/coreos/etcd</a>)</li>
<li class="code-line">Consul, by HashiCorp (<a href="https://www.consul.io/">https://www.consul.io/</a>)</li>
<li class="code-line">Zookeeper, by Yahoo, now an Apache Software Foundation (<a href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scheduler</h1>
                </header>
            
            <article>
                
<p class="code-line">While the <strong>Discovery Service</strong> holds information about the state and location of each service, it does not make the decision of which host/node the service should be deployed on. This process is known as<span> </span><strong>host selection</strong><span> </span>and is the job of a<span> </span><em>scheduler</em>:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/671cb9c9-069b-42b8-8cd6-75dbcc6c0fcd.png" style="width:41.50em;height:27.75em;"/></p>
<p class="code-line">The scheduler's decision can be based on a set of rules, called<span> </span><strong>policies</strong>, which takes into account the following:</p>
<ul>
<li class="code-line">The nature of the request.</li>
<li class="code-line">Cluster configuration/settings.</li>
</ul>
<ul>
<li class="code-line"><strong>Host density</strong>: An indication of how busy a the host system on the node is. If there are multiple nodes inside the cluster, we should prefer to deploy any new services on a node with the lowest host density. This information can be obtained from the Discovery Service, which holds information about all deployed services.</li>
<li class="code-line"><strong>Service (anti-)affinity</strong>: Whether two services should be deployed together on the same host. This depends on:
<ul>
<li class="code-line"><strong>Redundancy requirements</strong>: The same application should not be deployed on the same node(s) if there are other nodes that are not running the service. For instance, if our API service has already been deployed on two of three hosts, the scheduler may prefer to deploy on the remaining host to ensure maximum redundancy.</li>
<li class="code-line"><strong>Data locality</strong>: The scheduler should try placing computation code next to the data it needs to consume to reduce network latency.</li>
</ul>
</li>
<li class="code-line"><strong>Resource requirements</strong>: Of existing services running on nodes, as well as the service to be deployed</li>
<li class="code-line">Hardware/software constraints</li>
<li class="code-line">Other policies/rules set by the cluster administrator</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Global configuration store</h1>
                </header>
            
            <article>
                
<p class="code-line">Oftentimes, as is the case with our services, environment variables need to be set before the service can run successfully. So far, we've specified the environment variables to use by using the<span> <kbd>docker run</kbd> <kbd>--env-file</kbd> flag:</span></p>
<pre><strong>$ docker run --env-file ./.env --name hobnob -d -p 8080:8080 hobnob:0.1.0</strong></pre>
<p class="code-line">However, when deploying services on a cluster, we no longer run each container manually—we let the scheduler and node-level tools do this for us. Furthermore, we need all our services to share the same environment variables. Therefore, the most obvious solution is to provide a<span> </span><em>Global Configuration Store</em><span> </span>that stores configuration that is to be shared among all of the nodes and services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Provisioning tools</h1>
                </header>
            
            <article>
                
<p class="code-line">Provisioning means starting new hosts (be it physical or virtual) and configuring them in a way that allows them to run Cluster Management Tools. After provisioning, the host is ready to become a node inside the cluster and can receive work.</p>
<p class="code-line">This may involve using Infrastructure Management tools like Terraform to spin up new hosts, and Configuration Management tools like Puppet, Chef, Ansible or Salt, to ensure the configuration set inside each host are consistent with each other.</p>
<p class="code-line">While provisioning can be done before deploying our application, most Cluster Management software has a provisioning component built into it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Picking a cluster management tool</h1>
                </header>
            
            <article>
                
<p class="code-line">Having to manage these different cluster management components individually is tedious and error-prone. Luckily, cluster management tools exist that provides a common API that allows us to configure these tools in a consistent and automated manner. You'd use the Cluster management tool's API instead of manipulating each component individually.</p>
<p class="code-line">Cluster management tools are also known as<span> </span><em>cluster orchestration tools</em><span> </span>or<span> </span><em>container orchestration tools</em>. Although there may be slight nuances between the different terms, we can regard them as the same for the purpose of this chapter.</p>
<p class="code-line">There are a few popular cluster management tools available today:</p>
<ul>
<li class="code-line">Marathon (<a href="https://mesosphere.github.io/marathon/">https://mesosphere.github.io/marathon/</a>): By Mesosphere and runs on Apache Mesos.</li>
<li class="code-line">Swarm (<a href="https://docs.docker.com/engine/swarm/">https://docs.docker.com/engine/swarm/</a>): The Docker engine includes a<span> </span><em>swarm mode</em><span> </span>that manages Docker containers in clusters called<span> </span><em>swarms</em>. You may also group certain containers together using Docker Compose.</li>
<li class="code-line">Kubernetes: The<span> </span><em>de facto</em><span> </span>cluster management tool.</li>
</ul>
<p class="code-line">We will be using Kubernetes because it has the most mature ecosystem, and is the<span> </span><em>de facto</em><span> </span>industry standard.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Control Planes and components</h1>
                </header>
            
            <article>
                
<p class="code-line">The components we described previously—scheduler, Discovery Service, Global Configuration Store, and so on—are common to all Cluster Management Tools that exist today. The difference between them is how they package these components and abstract away the details. In Kubernetes, these components are aptly named Kubernetes<span> </span><em>Components</em>.</p>
<p class="code-line">We will distinguish between generic "components" with Kubernetes Components by using the capital case for the latter.</p>
<p class="code-line">In Kubernetes terminology, a "component" is a process that implements some part of the Kubernetes cluster system; examples include the<span> </span><kbd>kube-apiserver</kbd><span> </span>and<span> </span><kbd>kube-scheduler</kbd>. The sum of all components forms what you think of as the "Kubernetes system", which is formally known as the<span> </span><em>Control Plane</em>.</p>
<p class="code-line">Similar to how we categorized the cluster tools into cluster-level tools and node-level tools, Kubernetes categorizes Kubernetes Components into<span> </span><em>Master Components</em><span> </span>and<span> </span><em>Node Components</em>, respectively. Node Components operates within the node it is running on; Master Components work with multiple nodes or the entire cluster, hold cluster-level settings, configuration, and state, and make cluster-level decisions. Master Components collectively makes up the<span> </span><em>Master Control Plane</em>.</p>
<p class="code-line">Kubernetes also has<span> </span><em>Addons—</em>components which are not strictly required but provide useful features such as Web UI, metrics, and logging.</p>
<p class="code-line">With this terminology in mind, let's compare the generic cluster architecture we've described with Kubernetes'.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Master components</h1>
                </header>
            
            <article>
                
<p class="code-line">The discovery service and global configuration store and scheduler are implemented with the<span> </span><kbd>etcd</kbd><span> </span>and<span> </span><kbd>kube-scheduler</kbd><span> m</span>aster components:</p>
<ul>
<li class="code-line">
<p class="code-line"><span><kbd>etcd</kbd> is</span> a consistent and highly-available <strong>key-value</strong> (<strong>KV</strong>) store used as both the Discovery Service and Global Configuration Store.</p>
<p class="code-line">Because the discovery service and global configuration store both hold information about the services, and each are accessible by all nodes,<span> <kbd>etcd</kbd></span><span> </span>can serve both purposes. Whenever a service registers itself with the discovery service, it will also be returned a set of configuration settings.</p>
</li>
<li class="code-line">
<p class="code-line"><kbd>kube-scheduler</kbd><span> is a scheduler. It</span> keeps <span>track of which applications are unassigned to a node (and thus not running) and makes the decision as to which node to assign it to.</span></p>
</li>
</ul>
<p>In addition to these essential cluster management components, Kubernetes also provides additional Master Components to make working with Kubernetes easier.</p>
<p class="code-line">By default, all Master Components run on a single<span> </span><em>Master Node</em>, which runs only the Master Components and not other containers/services. However, they can be configured to be replicated in order to provide redundancy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">kube-apiserver</h1>
                </header>
            
            <article>
                
<p class="code-line">Kubernetes runs as a daemon that exposes a RESTful Kubernetes API server—<span><kbd>kube-apiserver</kbd>. <kbd>kube-apiserver</kbd></span><span> </span>acts as the interface to the Master Control Plane. Instead of communicating with each Kubernetes Component individually, you'd instead make calls to<span> <kbd>kube-apiserver</kbd>, which</span> will communicate with each component on your behalf:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/70dc31f4-3fce-42e9-9937-f0421f46f4ac.png" style="width:32.33em;height:36.50em;"/></p>
<p class="code-line">There are many benefits to this, including the following:</p>
<ul>
<li class="code-line">You have a central location where all changes pass through. This allows you to record a history of everything that has happened in your cluster.</li>
<li class="code-line">The API provides a uniform syntax.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">kube-control-manager</h1>
                </header>
            
            <article>
                
<p class="code-line">As we will demonstrate later, a central concept of Kubernetes, and the reason why you'd use a Cluster Management Tool in the first place, is that you don't have to<span> </span><em>manually</em><span> </span>manipulate the cluster yourself.</p>
<p class="code-line">Doing so will consist of sending a request to one component, receiving a response, and based on that response, sending another request to another component. This is the<span> </span><em>imperative</em><span> </span>approach, and is time-consuming because it requires you to manually write programs to implement this logic.</p>
<p class="code-line">Instead, Kubernetes allows us to specify the desired state of our cluster using configuration files, and Kubernetes will automatically coordinate the different Kubernetes Components to make it happen. This is the<span> </span><em>declarative </em>approach and is what Kubernetes recommends.</p>
<p class="code-line">Linking this to what we already know, the job of the whole Kubernetes system (the Control Plane) then becomes a system that tries to align the current state of the cluster with the desired state.</p>
<p class="code-line">Kubernetes does this through<span> </span><em>Controllers</em>. Controllers are the processes that actually carry out the actions of keeping the state of the cluster with the desired state.</p>
<p class="code-line">There are many types of controllers; here are two examples:</p>
<ul>
<li class="code-line">Node Controllers, for ensuring that the cluster has the desired number of nodes. For example, when a node fails, the Node Controller is responsible for spawning a new node.</li>
<li class="code-line">Replication Controllers, for ensuring each application has the desired number of replicas.</li>
</ul>
<p class="code-line">The role of controllers<span> for </span><kbd>kube-controller-manager</kbd><span> </span>become clearer once we've explained Kubernetes Objects and deploy our first service on Kubernetes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Node components</h1>
                </header>
            
            <article>
                
<p class="code-line">Node-level tools are implement as Node Components in Kubernetes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Container runtime</h1>
                </header>
            
            <article>
                
<p class="code-line">Kubernetes runs applications and services inside containers, and it expects that each node in the cluster already has the respective container runtime installed; this can be done with a provisioning tool like Terraform.</p>
<p class="code-line">However, it does not dictate any particular container format, as long as it is a format that abides by the <strong>Open Container Initiative</strong> (<strong>OCI</strong>)'s runtime specification (<a href="https://github.com/opencontainers/runtime-spec">https://github.com/opencontainers/runtime-spec</a>). For instance, you can use Docker, rkt (by CoreOS), or runc (by OCI) / CRI-O (by Kubernetes team) as the container format and runtime.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">kubelet</h1>
                </header>
            
            <article>
                
<p class="code-line">In the generic cluster architecture, our cluster needs a local configuration management tool like<span> <kbd>confd</kbd></span><span> </span>to pull updates from the Discovery Service and Global Configuration Stores. This ensures applications running on the node are using the most up-to-date parameters.</p>
<p class="code-line">In Kubernetes, this is the job of<span> <kbd>kubelet</kbd>. However, <kbd>kubelet</kbd></span><span> </span>does a lot more than just updating the local configuration and restarting services. It also monitors each service, make sure they are running<span> </span><em>and</em><span> </span>healthy, and reports their status back to<span> <kbd>etcd</kbd> via <kbd>kube-apiserver</kbd>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">kube-proxy</h1>
                </header>
            
            <article>
                
<p class="code-line">Each application (including replicas) deployed in the cluster is assigned virtual IPs. However, as applications are shut down and re-deployed elsewhere, their virtual IPs can change. We will go into more details later, but Kubernetes provides a<span> </span><em>Services</em><span> </span>Object that provides a static IP address for our end users to call.<span> </span><kbd>kube-proxy</kbd> is <span>a network proxy that runs on each node, and acts as a simple load balancer that forwards (or proxies) requests from the static IP address to the virtual IP address of one of the replicated applications.</span></p>
<p class="code-line">The role of<span> <kbd>kube-proxy</kbd></span><span> </span>will become more apparent when we create services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes objects</h1>
                </header>
            
            <article>
                
<p class="code-line">Now that you understand the different Components that make up the Kubernetes system, let's shift our attention to<span> </span><em>Kubernetes API Objects</em>, or<span> </span><em>Objects</em><span> </span>(with a capital O), for short.</p>
<p class="code-line">As you already know, with Kubernetes, you don't need to interact directly with individual Kubernetes Components; instead, you interact with<span> <kbd>kube-apiserver</kbd> </span>and the API server will coordinate actions on your behalf.</p>
<p class="code-line">The API abstracts away raw processes and entities into abstract concepts called Objects. For instance, instead of asking the API server to "Run these groups of related containers on a node", you'd instead ask "Add this Pod to the cluster". Here, the group of containers is abstracted to a<span> </span><em>Pod</em><span> </span>Object. When we work with Kubernetes, all we're doing is sending requests to the Kubernetes API to manipulate these Objects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The four basic objects</h1>
                </header>
            
            <article>
                
<p class="code-line">There are four basic Kubernetes Objects:</p>
<ul>
<li class="code-line"><strong>Pod</strong>: A group of closely-related containers that should be managed as a single unit</li>
<li class="code-line"><strong>Service</strong>: An abstraction that proxies requests from a static IP to the dynamic, virtual IPs of one of the Pods running the application</li>
<li class="code-line"><strong>Volume</strong>: This provides shared storage for all containers inside the same Pod</li>
<li class="code-line"><strong>Namespace</strong>: This allows you to separate a single physical cluster into multiple virtual clusters</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High-level objects</h1>
                </header>
            
            <article>
                
<p class="code-line">These basic Objects may then be built upon to form higher-level Objects:</p>
<ul>
<li class="code-line"><strong>ReplicaSet</strong>: Manages a set of Pods so that a specified number of replicas are maintained within the cluster.</li>
<li class="code-line"><strong>Deployment:</strong> An even higher-level abstraction than ReplicaSet, a Deployment Object will manage a ReplicaSet to ensure that the right number of replicas are running, but also allows you update your configuration to update/deploy a new ReplicaSet.</li>
</ul>
<ul>
<li class="code-line"><strong>StatefulSet</strong>: Similar to a Deployment, but in a Deployment, when a Pod restarts (for example, due to scheduling), the old Pod is destroyed and a new Pod is created. Although these Pods are created using the same specification, they are different Pods because data from the previous Pod is not persisted. In a StatefulSet, the old Pod can persist its state across restarts.</li>
<li class="code-line"><strong>DaemonSet</strong>: Similar to ReplicaSet, but instead of specifying the number of replicas to run, a DaemonSet is intended to run on every node in the cluster.</li>
<li class="code-line"><strong>Job</strong>: Instead of keeping Pods running indefinitely, a Job object spawns new Pods to carry out tasks with a finite timeline, and ensures that the Pods terminates successfully after the task completes.</li>
</ul>
<p class="code-line">The aforementioned higher-level Objects rely on the four basic Objects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Controllers</h1>
                </header>
            
            <article>
                
<p class="code-line">These higher-level Objects are ran and managed by<span> </span><em>Controllers</em>, which actually perform the actions that manipulate the Objects.</p>
<p class="code-line">For example, when we create a Deployment, a<span> </span><em>Deployment controller</em><span> </span>manages the Pods and ReplicaSet specified from the configuration. It is the controller who is responsible for making changes to get the actual state to the desired state.</p>
<p class="code-line">Most Objects have a corresponding Controller—a ReplicaSet object is managed by a ReplicaSet controller, a DaemonSet is managed by the DaemonSet controller, and so on.</p>
<p class="code-line">Apart from these, there are numerous other Controllers, with the most common ones listed as follows:</p>
<ul>
<li class="code-line"><strong>Node Controller</strong>: Responsible for noticing and responding when nodes go down</li>
<li class="code-line"><strong>Replication Controller</strong>: Responsible for maintaining the correct number of pods for every replication controller object in the system</li>
<li class="code-line"><strong>Route Controller</strong></li>
<li class="code-line"><strong>Volume Controller</strong></li>
<li class="code-line"><strong>Service Controller</strong>: Works on the load balancer and direct requests to the corresponding Pods</li>
</ul>
<ul>
<li class="code-line"><strong>Endpoints Controller</strong>: Populates the Endpoints object that links Service Objects and Pods</li>
<li class="code-line"><strong>Service Account and Token Controllers</strong>: Creates default accounts and API access tokens for new namespaces</li>
</ul>
<p class="code-line">These higher-level objects, and the Controllers that implements them, manage basic Objects on your behalf, providing additional conveniences that you'd come to expect when working with Cluster Management Tools. We will demonstrate the use of these Objects as we migrate our application to run on Kubernetes later in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the local development environment</h1>
                </header>
            
            <article>
                
<p class="code-line">Now that you understand the different Components of Kubernetes and the abstractions (Objects) that the API provides, we are ready to migrate the deployment of our application to using Kubernetes. In this section, we will learn the basics of Kubernetes by running it on our local machine. Later on in this chapter, we will build on what we've learned and deploy our application on multiple VPSs, managed by a cloud provider.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checking hardware requirements</h1>
                </header>
            
            <article>
                
<p class="code-line">To run Kubernetes locally, your machine needs to fulfill the following hardware requirements:</p>
<ul>
<li class="code-line">Have 2 GB or more of available RAM</li>
<li class="code-line">Have two or more CPU cores</li>
<li class="code-line">Swap space is disabled</li>
</ul>
<p class="code-line">Make sure you are using a machine which satisfies those requirements.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleaning our environment</h1>
                </header>
            
            <article>
                
<p class="code-line">Because Kubernetes manages our application containers for us, we no longer need to manage our own Docker containers. Therefore, let's provide a clean working environment by removing any Docker containers and images related to our application. You can do this by running<span> </span><kbd>docker ps -a</kbd><span> </span>and<span> </span><kbd>docker images</kbd><span> </span>to see a list of all containers and images, and then using<span> </span><kbd>docker stop &lt;container&gt;</kbd>,<span> </span><kbd>docker rm &lt;container&gt;</kbd><span>, </span>and<span> </span><kbd>docker rmi &lt;image&gt;</kbd><span> </span>to remove the relevant ones.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Disabling swap memory</h1>
                </header>
            
            <article>
                
<p class="mce-root">Running Kubernetes locally requires you to turn Swap Memory off. You can do so by running<span> </span><kbd>swapoff -a</kbd>:</p>
<pre><strong>$ sudo swapoff -a</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing kubectl</h1>
                </header>
            
            <article>
                
<p>Although we can interact with the Kubernetes API by sending raw HTTP requests using a program like<span> </span><kbd>curl</kbd>, Kubernetes provides a convenient command-line client called<span> </span><kbd>kubectl</kbd>. Let's install it:</p>
<pre><strong>$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.3/bin/linux/amd64/kubectl &amp;&amp; chmod +x ./kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl</strong></pre>
<div class="packt_tip">You can find alternate installation methods at <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubernetes.io/docs/tasks/tools/install-kubectl/</a>.</div>
<p class="mce-root">You can check that the installation was successful by running<span> </span><kbd>kubectl version</kbd>:</p>
<pre class="mce-root"><strong>$ kubectl version</strong><br/><strong>Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.3", GitCommit:"2bba0127d85d5a46ab4b778548be28623b32d0b0", GitTreeState:"clean", BuildDate:"2018-05-21T09:17:39Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}</strong></pre>
<p class="mce-root">Finally, the<span> </span><kbd>kubectl</kbd><span> </span>provides autocompletion; to activate it, simply run the following code:</p>
<pre class="mce-root"><strong>$ echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Minikube</h1>
                </header>
            
            <article>
                
<p class="code-line">Minikube is a free and open source tool by the Kubernetes team that enables you to easily run a single-node Kubernetes cluster locally. Without Minikube, you'd have to install and configure<span> <kbd>kubectl</kbd> and <kbd>kubeadm</kbd> (used for provisioning) yourself.</span></p>
<p class="code-line">So, let's install Minikube by following the instructions found at<span> </span><a href="https://github.com/kubernetes/minikube/releases">https://github.com/kubernetes/minikube/releases</a>. For Ubuntu, we can choose to either run the install script or install the<span> </span><kbd>.deb</kbd><span> </span>package.</p>
<p class="mce-root">At the time of writing this book, the<span> </span><kbd>.deb</kbd><span> </span>package installation is still experimental, so we will opt for the install script instead. For example, to install Minikube v0.27.0, we can run the following:</p>
<pre><strong>$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.27.0/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/</strong></pre>
<div class="packt_tip">You can use the same command to update<span> </span><kbd>minikube</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing a Hypervisor or Docker Machine</h1>
                </header>
            
            <article>
                
<p class="code-line">Normally, Minikube runs a single-node cluster inside a virtual machine (VM), and this requires the installation of a hypervisor like VirtualBox or KVM. This requires a lot of setting up and is not great for performance.</p>
<p class="code-line">Instead, we can instruct Minikube to run Kubernetes components directly on our machine outside of any VMs. This requires the Docker runtime and Docker Machine to be installed on our machine. Docker runtime should already be installed if you followed our previous chapter, so let's install Docker Machine:</p>
<pre><strong>$ base=https://github.com/docker/machine/releases/download/v0.14.0 &amp;&amp;</strong><br/><strong>  curl -L $base/docker-machine-$(uname -s)-$(uname -m) &gt;/tmp/docker-machine &amp;&amp;</strong><br/><strong>  sudo install /tmp/docker-machine /usr/local/bin/docker-machine</strong></pre>
<p class="code-line">After installation, run<span> </span><kbd>docker-machine version</kbd><span> </span>to confirm that the installation was successful:</p>
<pre><strong>$ docker-machine version</strong><br/><strong>docker-machine version 0.14.0, build 89b8332</strong></pre>
<div class="packt_infobox">Running your cluster with Minikube on Docker is only available on Linux machines. If you are not using a Linux machine, go to the Minikube documentation to follow instructions on setting up a VM environment and using a VM driver. The rest of the chapter will still work for you. Just remember to use the correct<span> </span><kbd>--vm-driver</kbd><span> </span>flag when running<span> </span><kbd>minikube start</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating our cluster</h1>
                </header>
            
            <article>
                
<p>With the Kubernetes daemon (installed and ran by<span> </span><kbd>minikube</kbd>) and the Kubernetes client (<kbd>kubectl</kbd>) installed, we can now run<span> </span><kbd>minikube start</kbd><span> </span>to create and start our cluster. We'd need to pass in<span> </span><kbd>--vm-driver=none</kbd><span> </span>as we are not using a VM.</p>
<div class="mce-root packt_infobox">If you are using a VM, remember to use the correct<span> </span><kbd>--vm-driver</kbd><span> </span>flag.</div>
<p class="mce-root">We need to run the<span> </span><kbd>minikube start</kbd><span> </span>command as<span> </span><kbd>root</kbd><span> </span>because the<span> </span><kbd>kubeadm</kbd><span> </span>and<span> </span><kbd>kubelet</kbd><span> </span>binaries need to be downloaded and moved to<span> </span><kbd>/usr/local/bin</kbd>, which requires root privileges.</p>
<p class="mce-root">However, this usually means that all the files created and written during the installation and initiation process will be owned by<span> </span><kbd>root</kbd>. This makes it hard for a normal user to modify configuration files.</p>
<p class="mce-root">Fortunately, Kubernetes provides several environment variables that we can set to change this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting environment variables for the local cluster</h1>
                </header>
            
            <article>
                
<p>Inside<span> </span><kbd>.profile</kbd><span> </span>(or its equivalents, such as<span> </span><kbd>.bash_profile</kbd><span> </span>or<span> </span><kbd>.bashrc</kbd>), add the following lines at the end:</p>
<pre class="mce-root">export MINIKUBE_WANTUPDATENOTIFICATION=false<br/>export MINIKUBE_WANTREPORTERRORPROMPT=false<br/>export MINIKUBE_HOME=$HOME<br/>export CHANGE_MINIKUBE_NONE_USER=true<br/>export KUBECONFIG=$HOME/.kube/config</pre>
<p class="mce-root"><kbd>CHANGE_MINIKUBE_NONE_USER</kbd><span> </span>tells<span> </span><kbd>minikube</kbd><span> </span>to assign the current user as the owner of the configuration files.<span> </span><kbd>MINIKUBE_HOME</kbd><span> </span>tells<span> </span><kbd>minikube</kbd><span> to </span>store the Minikube-specific configuration on<span> </span><kbd>~/.minikube</kbd>, and<span> </span><kbd>KUBECONFIG</kbd><span> </span>tells<span> </span><kbd>minikube</kbd><span> </span>to store the Kubernetes-specific configuration on<span> </span><kbd>~/.kube/config</kbd>.</p>
<p class="mce-root">To apply these environment variables to the current shell, run the following command:</p>
<pre class="mce-root"><strong>$ . .profile</strong></pre>
<p class="mce-root">Lastly, we'll need to actually create a<span> </span><kbd>.kube/config</kbd><span> </span>configuration file to our home directory:</p>
<pre class="mce-root"><strong>$ mkdir -p $HOME/.kube</strong><br/><strong>$ touch $HOME/.kube/config</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running minikube start</h1>
                </header>
            
            <article>
                
<p class="mce-root">With our environment variables set, we're finally ready to run<span> </span><kbd>minikube start</kbd>:</p>
<pre class="mce-root"><strong>$ sudo -E minikube start --vm-driver=none</strong><br/><strong>Starting local Kubernetes v1.10.0 cluster...</strong><br/><strong>Starting VM...</strong><br/><strong>Getting VM IP address...</strong><br/><strong>Moving files into cluster...</strong><br/><strong>Setting up certs...</strong><br/><strong>Connecting to cluster...</strong><br/><strong>Setting up kubeconfig...</strong><br/><strong>Starting cluster components...</strong><br/><strong>Kubectl is now configured to use the cluster.</strong></pre>
<p class="mce-root">This command performs several operations under the hood:</p>
<ul>
<li class="mce-root">Provisions any VMs (if we're using VM). This is done internally by libmachine from Docker Machine.</li>
<li class="mce-root">Sets up configuration files and certificates under<span> </span><kbd>./kube</kbd><span> </span>and<span> </span><kbd>./minikube</kbd>.</li>
<li class="mce-root">Starts up the local Kubernetes cluster using<span> </span><kbd>localkube</kbd>.</li>
<li class="mce-root">Configures<span> </span><kbd>kubectl</kbd><span> </span>to communicate with this cluster.</li>
</ul>
<p class="mce-root">Since we are developing locally using the<span> </span><kbd>--vm-driver=none</kbd><span> </span>flag, our machine becomes the only node within the cluster. You can confirm this by using<span> </span><kbd>kubectl</kbd><span> </span>to see whether the node is registered with the Kubernetes API and<span> </span><kbd>etcd</kbd>:</p>
<pre class="mce-root"><strong>$ kubectl get nodes</strong><br/><strong>NAME STATUS ROLES AGE VERSION</strong><br/><strong>minikube Ready master 15m v1.10.0</strong></pre>
<p class="mce-root">All Master Components, such as the scheduler (<kbd>kube-scheduler</kbd>), as well as Node Components, such as<span> </span><kbd>kubelet</kbd>, are running on the same node, inside Docker containers. You can check them out by running<span> </span><kbd>docker ps</kbd>:</p>
<pre class="mce-root"><strong>$ docker ps -a --format "table {{.ID}}\t{{.Image}}\t{{.Command}}\t{{.Names}}"</strong><br/><strong>CONTAINER ID        IMAGE                        COMMAND                  NAMES</strong><br/><strong>3ff67350410a        4689081edb10                 "/storage-provisioner"   k8s_storage-provisioner_storage-provisioner_kube-system_4d9c2fa3-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>ec2922978b10        e94d2f21bc0c                 "/dashboard --insecu…"   k8s_kubernetes-dashboard_kubernetes-dashboard-5498ccf677-sslhz_kube-system_4d949c82-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>f9f5b8fe1a41        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_storage-provisioner_kube-system_4d9c2fa3-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>f5b013b0278d        6f7f2dc7fab5                 "/sidecar --v=2 --lo…"   k8s_sidecar_kube-dns-86f4d74b45-hs88j_kube-system_4cbede66-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>f2d120dce2ed        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_kubernetes-dashboard-5498ccf677-sslhz_kube-system_4d949c82-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>50ae3b880b4a        c2ce1ffb51ed                 "/dnsmasq-nanny -v=2…"   k8s_dnsmasq_kube-dns-86f4d74b45-hs88j_kube-system_4cbede66-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>a8f677cdc43b        80cc5ea4b547                 "/kube-dns --domain=…"   k8s_kubedns_kube-dns-86f4d74b45-hs88j_kube-system_4cbede66-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>d287909bae1d        bfc21aadc7d3                 "/usr/local/bin/kube…"   k8s_kube-proxy_kube-proxy-m5lrh_kube-system_4cbf007c-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>e14d9c837ae4        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_kube-dns-86f4d74b45-hs88j_kube-system_4cbede66-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>896beface410        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_kube-proxy-m5lrh_kube-system_4cbf007c-627a-11e8-a0e4-54e1ad13e25a_0</strong><br/><strong>9f87d1105edb        52920ad46f5b                 "etcd --listen-clien…"   k8s_etcd_etcd-minikube_kube-system_a2c07ce803646801a9f5a70371449d58_0</strong><br/><strong>570a4e5447f8        af20925d51a3                 "kube-apiserver --ad…"   k8s_kube-apiserver_kube-apiserver-minikube_kube-system_8900f73fb607cc89d618630016758228_0</strong><br/><strong>87931be974c0        9c16409588eb                 "/opt/kube-addons.sh"    k8s_kube-addon-manager_kube-addon-manager-minikube_kube-system_3afaf06535cc3b85be93c31632b765da_0</strong><br/><strong>897928af3c85        704ba848e69a                 "kube-scheduler --ad…"   k8s_kube-scheduler_kube-scheduler-minikube_kube-system_31cf0ccbee286239d451edb6fb511513_0</strong><br/><strong>b3a7fd175e47        ad86dbed1555                 "kube-controller-man…"   k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_c871518ac418f1edf0247e23d5b99a40_0</strong><br/><strong>fd50ec94b68f        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_kube-apiserver-minikube_kube-system_8900f73fb607cc89d618630016758228_0</strong><br/><strong>85a38deae7ad        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_etcd-minikube_kube-system_a2c07ce803646801a9f5a70371449d58_0</strong><br/><strong>326fd83d6630        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_kube-addon-manager-minikube_kube-system_3afaf06535cc3b85be93c31632b765da_0</strong><br/><strong>e3dd5b372dab        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_kube-scheduler-minikube_kube-system_31cf0ccbee286239d451edb6fb511513_0</strong><br/><strong>6c2ac7c363d0        k8s.gcr.io/pause-amd64:3.1   "/pause"                 k8s_POD_kube-controller-manager-minikube_kube-system_c871518ac418f1edf0247e23d5b99a40_0</strong></pre>
<p class="mce-root">As a last check, run<span> </span><kbd>systemctl status kubelet.service</kbd><span> </span>to ensure<span> that </span><kbd>kubelet</kbd><span> </span>is running as a daemon on the node:</p>
<pre class="mce-root"><strong>$ sudo systemctl status kubelet.service</strong><br/><strong>● kubelet.service - kubelet: The Kubernetes Node Agent</strong><br/><strong>   Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enable</strong><br/><strong>  Drop-In: /etc/systemd/system/kubelet.service.d</strong><br/><strong>           └─10-kubeadm.conf</strong><br/><strong>   Active: active (running) since Mon 2018-05-28 14:22:59 BST; 2h 5min ago</strong><br/><strong>     Docs: http://kubernetes.io/docs/</strong><br/><strong> Main PID: 23793 (kubelet)</strong><br/><strong>    Tasks: 18 (limit: 4915)</strong><br/><strong>   Memory: 55.5M</strong><br/><strong>      CPU: 8min 28.571s</strong><br/><strong>   CGroup: /system.slice/kubelet.service</strong><br/><strong>           └─23793 /usr/bin/kubelet --fail-swap-on=false --allow-privileged=true --clu</strong></pre>
<p class="mce-root">Everything is now set up. You can confirm this by running<span> </span><kbd>minikube status</kbd><span> </span>and<span> </span><kbd>kubectl cluster-info</kbd>:</p>
<pre class="mce-root"><strong>$ minikube status</strong><br/><strong>minikube: Running</strong><br/><strong>cluster: Running</strong><br/><strong>kubectl: Correctly Configured: pointing to minikube-vm at 10.122.98.148</strong><br/><br/><strong>$ kubectl cluster-info</strong><br/><strong>Kubernetes master is running at https://10.122.98.148:8443</strong><br/><strong>KubeDNS is running at https://10.122.98.148:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the context</h1>
                </header>
            
            <article>
                
<p>If you change the local network that your computer is connected to, the cluster's IP may change. If you try to use<span> </span><kbd>kubectl</kbd><span> </span>to connect to the cluster after this change, you'll see an error saying that the<span> </span><kbd>network is unreachable</kbd>:</p>
<pre><strong>$ kubectl cluster-info</strong><br/><strong>Kubernetes master is running at https://10.122.35.199:8443</strong><br/><strong>Unable to connect to the server: dial tcp 10.122.35.199:8443: connect: network is unreachable</strong></pre>
<p>Whenever you see an error like this, run<span> </span><kbd>minikube status</kbd><span> </span>to check the state of the cluster:</p>
<pre class="mce-root"><strong>$ minikube status</strong><br/><strong>minikube: Running</strong><br/><strong>cluster: Running</strong><br/><strong>kubectl: Misconfigured: pointing to stale minikube-vm.</strong><br/><strong>To fix the kubectl context, run minikube update-context</strong></pre>
<p class="mce-root">In this case, it informs us that <kbd>kubectl</kbd><span> </span>is "pointing to the stale<span> </span><kbd>minikube-vm</kbd>" and we should run<span> </span><kbd>minikube update-context</kbd><span> </span>to update<span> </span><kbd>kubectl</kbd><span> </span>to point to the new cluster IP:</p>
<pre class="mce-root"><strong>$ minikube update-context</strong><br/><strong>Reconfigured kubeconfig IP, now pointing at 192.168.1.11</strong></pre>
<p class="mce-root">After doing this, check that<span> </span><kbd>kubectl</kbd><span> </span>is able to communicate with the Kubernetes API server:</p>
<pre class="mce-root"><strong>$ kubectl cluster-info</strong><br/><strong>Kubernetes master is running at https://192.168.1.11:8443</strong><br/><strong>KubeDNS is running at https://192.168.1.11:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Resetting the cluster</h1>
                </header>
            
            <article>
                
<p class="code-line">Working with Kubernetes can be tricky, especially at the beginning. If you ever get stuck with a problem and can't resolve it, you can use<span> <kbd>kubeadm reset</kbd></span><span> </span>to reset everything related to our Kubernetes cluster, and start again from scratch:</p>
<pre><strong>$ sudo kubeadm reset</strong><br/><strong>[preflight] Running pre-flight checks.</strong><br/><strong>[reset] Stopping the kubelet service.</strong><br/><strong>[reset] Unmounting mounted directories in "/var/lib/kubelet"</strong><br/><strong>[reset] Removing kubernetes-managed containers.</strong><br/><strong>[reset] No etcd manifest found in "/etc/kubernetes/manifests/etcd.yaml". Assuming external etcd.</strong><br/><strong>[reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]</strong><br/><strong>[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]</strong><br/><strong>[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]</strong></pre>
<p class="mce-root"/>
<p class="code-line">Try it now. Then, run the same<span> </span><kbd>minikube start</kbd><span> </span>command as before to recreate the cluster:</p>
<pre><strong>$ sudo -E minikube start --vm-driver=none</strong><br/><strong>$ minikube status</strong><br/><strong>minikube: Running</strong><br/><strong>cluster: Running</strong><br/><strong>kubectl: Correctly Configured: pointing to minikube-vm at 192.168.1.11</strong><br/><strong>$ kubectl cluster-info</strong><br/><strong>Kubernetes master is running at https://192.168.1.11:8443</strong><br/><strong>KubeDNS is running at https://192.168.1.11:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating our first Pod</h1>
                </header>
            
            <article>
                
<p class="code-line">Now that we have a cluster running locally, let's deploy our Elasticsearch service on it. With Kubernetes, all services run inside containers. Conveniently for us, we are already familiar with Docker, and Kubernetes supports the Docker container format.</p>
<p class="code-line">However, Kubernetes doesn't actually deploy containers individually, but rather, it deploys<span> </span><em>Pods</em>. As already mentioned, Pods are a type of basic Kubernetes Objects—abstractions provided by the Kubernetes API. Specifically, Pods are a logical grouping of containers that should be deployed and managed together. In Kubernetes, Pods are also the lowest-level unit that Kubernetes manages.</p>
<p class="code-line">Containers inside the same Pod share the following:</p>
<ul>
<li class="code-line"><strong>Lifecycle</strong>: All containers inside a Pod are managed as a single unit. When a pod starts, all the containers inside the pod will start (this is known as a<span> </span><strong>shared fate</strong>). When a Pod needs to be relocated to a different node, all containers inside the pod will relocate (also known as<span> </span><strong>co-scheduling</strong>).</li>
<li class="code-line"><strong>Context</strong>: A Pod is isolated from other Pods similar to how one Docker container is isolated from another Docker container. In fact, Kubernetes uses the same mechanism of namespaces and groups to isolate a pod.</li>
<li class="code-line"><strong>Shared network</strong>: All containers within the pod share the same IP address and port space, and can communicate with each other using<span> <kbd>localhost:&lt;port&gt;</kbd>.</span> They can also communicate with each other using inter-process communications (IPC).</li>
</ul>
<ul>
<li class="code-line"><strong>Shared storage</strong>: Containers can access a shared volume that will be persisted outside of the container, and will survive even if the containers restart:</li>
</ul>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/0e055736-d842-4cb4-b4e0-a8c0ef3b38c1.png" style="width:21.83em;height:25.58em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Pods with kubelet</h1>
                </header>
            
            <article>
                
<p class="code-line">Pods are run by the<span> </span><kbd>kubelet</kbd><span> </span>service that runs inside each node. There are three ways to instruct<span> </span><kbd>kubelet</kbd><span> </span>to run a Pod:</p>
<ul>
<li class="code-line">By directly passing it the Pod configuration file (or a directory container configuration files) using<span> <kbd>kubelet --config &lt;path-to-pod-config&gt;</kbd>. <kbd>kubelet</kbd> </span>will poll this directory every 20 seconds for changes, and will start new containers or terminate containers based on any changes to the configuration file(s).</li>
<li class="code-line">By specifying an HTTP endpoint which returns with the Pod configuration files. Like the file option,<span> <kbd>kubelet</kbd> </span>polls the endpoint every 20 seconds.</li>
<li class="code-line">By using the Kubernetes API server to send any new pod manifests to<span> <kbd>kubelet</kbd>.</span></li>
</ul>
<p class="code-line">The first two options are not ideal because:</p>
<ul>
<li class="code-line">It relies on polling, which means that the nodes cannot react quickly to changes</li>
<li class="code-line">The Kubernetes API server is not aware of these pods, and thus cannot manage them</li>
</ul>
<p class="code-line">Instead, we should use<span> <kbd>kubelet</kbd></span><span> </span>to communicate our intentions to the Kubernetes API server, and let it coordinate how to deploy our Pod.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Pods with kubectl run</h1>
                </header>
            
            <article>
                
<p>First, confirm that no Elasticsearch containers are currently running on our machine:</p>
<pre><strong>$ docker ps -a \</strong><br/><strong> --filter "name=elasticsearch" \</strong><br/><strong> --format "table {{.ID}}\t{{.Image}}\t{{.Command}}\t{{.Names}}"</strong><br/><strong>CONTAINER ID IMAGE COMMAND NAMES</strong></pre>
<p>We can now use<span> </span><kbd>kubectl run</kbd><span> </span>to run an image inside a Pod, and deploy it onto our cluster:</p>
<pre><strong>$ kubectl run elasticsearch --image=docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2 --port=9200 --port=9300</strong><br/><strong>deployment.apps "elasticsearch" created</strong></pre>
<p>Now, when we check the Pods that have been deployed onto our cluster, we can see a new<span> </span><kbd>elasticsearch-656d7c98c6-s6v58</kbd><span> </span>Pod:</p>
<pre><strong>$ kubectl get pods</strong><br/><strong>NAME READY STATUS RESTARTS AGE</strong><br/><strong>elasticsearch-656d7c98c6-s6v58 0/1 ContainerCreating 0 9s</strong></pre>
<p>It may take some time for the Pod to initiate, especially if the Docker image is not available locally and needs to be downloaded. Eventually, you should see the<span> </span><kbd>READY</kbd><span> </span>value become<span> </span><kbd>1/1</kbd>:</p>
<pre class="mce-root"><strong>$ kubectl get pods</strong><br/><strong>NAME READY STATUS RESTARTS AGE</strong><br/><strong>elasticsearch-656d7c98c6-s6v58 1/1 Running 0 1m</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding high-level Kubernetes objects</h1>
                </header>
            
            <article>
                
<p class="code-line">The more observant of you might have noticed the following output after you ran <kbd>kubectl</kbd>:</p>
<pre>deployment.apps "elasticsearch" created</pre>
<p class="code-line">When we run<span> </span><kbd>kubectl run</kbd>, Kubernetes does not create a Pod directly; instead, Kubernetes automatically creates a Deployment Object that will manage the Pod for us. Therefore, the following two commands are functionally equivalent:</p>
<pre><strong>$ kubectl run &lt;name&gt; --image=&lt;image&gt;</strong><br/><strong>$ kubectl create deployment &lt;name&gt; --image=&lt;image&gt;</strong></pre>
<p>To demonstrate this, you can see a list of active Deployments using<span> </span><kbd>kubectl get deployments</kbd>:</p>
<pre><strong>$ kubectl get deployments</strong><br/><strong>NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</strong><br/><strong>elasticsearch   1         1         1            1           2s</strong></pre>
<p>The benefit of using a Deployment object is that it will manage the Pods under its control. This means that if the Pod fails, the Deployment will automatically restart the Pod for us.</p>
<p class="code-line">Generally, we should not<span> </span><em>imperatively</em><span> </span>instruct Kubernetes to create a low-level object like Pods, but<span> </span><em>declaratively </em>create a higher-level Kubernetes Object and let Kubernetes manage the low-level Objects for us.</p>
<p class="code-line">This applies to ReplicaSet as well—you shouldn't deploy a ReplicaSet; instead, deploy a Deployment Object that uses ReplicaSet under the hood.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Declarative over imperative</h1>
                </header>
            
            <article>
                
<p class="code-line">Pods, Deployments, and ReplicaSet are examples of Kubernetes Objects. Kubernetes provides you with multiple approaches to run and manage them.</p>
<ul>
<li class="code-line"><kbd>kubectl run</kbd>—imperative: You provide instructions through the command line to the Kubernetes API to carry out</li>
<li class="code-line"><kbd>kubectl create</kbd>—imperative: You provide instructions, in the form of a configuration file, to the Kubernetes API to carry out</li>
</ul>
<ul>
<li class="code-line"><kbd>kubectl apply</kbd>—declarative: You tell the Kubernetes API the desired state of your cluster using configuration file(s), and Kubernetes will figure out the operations required to reach that state</li>
</ul>
<p class="mce-root"><kbd>kubectl create</kbd><span> </span>is a slight improvement to<span> </span><kbd>kubectl run</kbd><span> </span>because the configuration file(s) can now be version controlled; however, it is still not ideal due to its imperative nature.</p>
<p class="code-line">If we use the imperative approach, we'd be manipulating the Kubernetes object(s) directly, and thus be responsible for monitoring all Kubernetes objects. This essentially defeats the point of having a Cluster Management Tool.</p>
<p class="code-line">The preferred pattern is to create Kubernetes Objects in a declarative manner using a version-controlled<span> </span><em>manif</em><em>est </em>file.</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<thead>
<tr>
<th>Management technique</th>
<th>Operates on</th>
<th>Recommended environment</th>
<th>Supported writers</th>
<th>Learning curve</th>
</tr>
</thead>
<tbody>
<tr>
<td>Imperative commands</td>
<td>Live objects</td>
<td>Development projects</td>
<td>1+</td>
<td>Lowest</td>
</tr>
<tr>
<td>Imperative object configuration</td>
<td>Individual files</td>
<td>Production projects</td>
<td>1</td>
<td>Moderate</td>
</tr>
<tr>
<td>Declarative object configuration</td>
<td>Directories of files</td>
<td>Production projects</td>
<td>1+</td>
<td>Highest</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="code-line">You should also note that the imperative and declarative approaches are mutually exclusive—you cannot have Kubernetes manage everything based on your configuration, and also manipulate objects on your own. Doing so will cause Kubernetes to detect the changes you've made as deviations from the desired state, and will work against you and undo your changes. Therefore, we should consistently use the declarative approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting deployment</h1>
                </header>
            
            <article>
                
<p>With this in mind, let's redeploy our Elasticsearch service in a declarative manner, using<span> </span><kbd>kubectl apply</kbd>. But first, we must delete our existing Deployment. We can do that with<span> </span><kbd>kubectl delete</kbd>:</p>
<pre class="mce-root"><strong>$ kubectl delete deployment elasticsearch</strong></pre>
<pre class="mce-root"><strong>$ kubectl get deployments</strong><br/><strong>No resources found.</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a deployment manifest</h1>
                </header>
            
            <article>
                
<p class="code-line">Now, create a new directory structure at<span> </span><kbd>manifests/elasticsearch</kbd>, and in it, create a new file called<span> </span><kbd>deployment.yaml</kbd>. Then, add the following Deployment configuration:</p>
<pre>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: elasticsearch<br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      app: elasticsearch<br/>  template:<br/>    metadata:<br/>      name: elasticsearch<br/>      labels:<br/>        app: elasticsearch<br/>    spec:<br/>      containers:<br/>      - name: elasticsearch<br/>        image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2<br/>        ports:<br/>        - containerPort: 9200<br/>        - containerPort: 9300</pre>
<p class="mce-root">The configuration file consists of several fields (fields marked<span> </span><kbd>*</kbd><span> </span>are required):</p>
<ul>
<li class="mce-root"><kbd>apiVersion*</kbd>: The version of the API. This affects the scheme expected for the configuration file. The API is broken into modular API Groups. This allows Kubernetes to develop newer features independently. It also provides Kubernetes cluster administrators more fine-grained control over which API features they want to be enabled.<br/>
The core Kubernetes objects are available in the<span> </span><em>core</em><span> </span>group (the<span> </span><em>legacy</em><span> </span>group), and you can specify this by using<span> </span><kbd>v1</kbd><span> </span>as the<span> </span><kbd>apiVersion</kbd><span> </span>property value. Deployments are available under the<span> </span><kbd>apps</kbd><span> </span>group, and we can enable this by using<span> </span><kbd>apps/v1</kbd><span> </span>as the<span> </span><kbd>apiVersion</kbd><span> </span>property value. Other groups include<span> </span><kbd>batch</kbd><span> </span>(provides the<span> </span><kbd>CronJob</kbd><span> </span>object),<span> </span><kbd>extensions</kbd>,<span> </span><kbd>scheduling.k8s.io</kbd>,<span> </span><kbd>settings.k8s.io</kbd><span>, </span>and many more.</li>
<li class="mce-root"><kbd>kind*</kbd>: The type of resource this manifest is specifying. In our case, we want to create a Deployment, so we should specify<span> </span><kbd>Deployment</kbd><span> </span>as the value. Other valid values for<span> </span><kbd>kind</kbd><span> </span>include<span> </span><kbd>Pod</kbd><span> </span>and<span> </span><kbd>ReplicaSet</kbd>, but for reasons mentioned previously, you wouldn't normally use them.</li>
<li class="mce-root"><kbd>metadata</kbd>: Metadata about the Deployment, such as:
<ul>
<li class="mce-root"><kbd>namespace</kbd>: With Kubernetes, you can split a single physical cluster into multiple<span> </span><em>virtual clusters</em>. The default namespace is<span> </span><kbd>default</kbd>, which is sufficient for our use case.</li>
<li class="mce-root"><kbd>name</kbd>: A name to identify the Deployment within the cluster.</li>
</ul>
</li>
<li class="mce-root"><kbd>spec</kbd>: Details the behavior of the Deployment, such as:
<ul>
<li class="mce-root"><kbd>replicas</kbd>: The number of replica Pods, specified in the<span> </span><kbd>spec.template</kbd>, to deploy</li>
<li class="mce-root"><kbd>template</kbd>: The specification for each Pod in the ReplicaSet
<ul>
<li class="mce-root"><kbd>metadata</kbd>: The metadata about the Pod, including a<span> </span><kbd>label</kbd><span> </span>property</li>
<li class="mce-root"><kbd>spec</kbd>: The specification for each individual Pod:
<ul>
<li class="mce-root"><kbd>containers</kbd>: A list of containers that belong in the same Pod and should be managed together.</li>
<li class="mce-root"><kbd>selector</kbd>: The method by which the Deployment controller knows which Pods it should manage. We use the<span> </span><kbd>matchLabels</kbd><span> </span>criteria to match all Pods with the label<span> </span><kbd>app: elasticsearch</kbd>. We then set the label at<span> </span><kbd>spec.template.metadata.labels</kbd>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A note on labels</h1>
                </header>
            
            <article>
                
<p class="code-line">In our manifest file, under<span> </span><kbd>spec.template.metadata.labels</kbd>, we've specified that our Elasticsearch Pods should carry the label<span> </span><kbd>app: elasticsearch</kbd>.</p>
<p class="code-line">Label is one of two methods to attach arbitrary metadata to Kubernetes Objects, with the other being<span> </span><em>annotations</em>.</p>
<p class="code-line">Both labels and annotations are implemented as key-value stores, but they serve different purposes:</p>
<ul>
<li class="code-line">Labels: Used to identify an Object as belonging to a certain group of similar Objects. In other words, it can be used to select a subset of all Objects of the same type. This can be used to apply Kubernetes commands to only a subset of all Kubernetes Objects.</li>
<li class="code-line">Annotations: Any other arbitrary metadata not used to identify the Object.</li>
</ul>
<p class="code-line">A label key consists of two components—an optional prefix, and a name—separated by a forward slash (<kbd>/</kbd>).</p>
<p class="code-line">The prefix exists as a sort of namespace, and allows third-party tools to select only the Objects that it is managing. For instance, the core Kubernetes components have a label with a prefix of<span> </span><kbd>kubernetes.io/</kbd>.</p>
<p class="code-line">Labeled Objects can then be selected using<span> </span><em>label selectors</em>, such as the one specified in our Deployment manifest:</p>
<pre>selector:<br/>  matchLabels:<br/>    app: elasticsearch</pre>
<p class="code-line">This selector instructs the Deployment Controller to manage only these Pods and not others.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running pods declaratively with kubectl apply</h1>
                </header>
            
            <article>
                
<p class="code-line">With the Deployment manifest ready, we can run<span> </span><kbd>kubectl apply</kbd><span> </span>to update the desired state of our cluster:</p>
<pre><strong>$ kubectl apply -f manifests/elasticsearch/deployment.yaml</strong><br/><strong>deployment.apps "elasticsearch" created</strong></pre>
<p class="code-line">This will trigger a set of events:</p>
<ol>
<li class="code-line"><kbd>kubectl</kbd><span> </span>sends the Deployment manifest to the Kubernetes API server (<kbd>kube-apiserver</kbd>).<span> </span><kbd>kube-apiserver</kbd><span> </span>will assign it a unique ID, and adds it on to<span> </span><kbd>etcd</kbd>.</li>
<li class="code-line">The API server will also create the corresponding<span> </span>ReplicaSet<span> </span>and<span> </span>Pod<span> </span>Objects and add it to<span> </span><kbd>etcd</kbd>.</li>
<li class="code-line">The scheduler watches<span> </span><kbd>etcd</kbd><span> </span>and notices that there are Pods that have not been assigned to a node. Then, the scheduler will make a decision about where to deploy the Pods specified by the Deployment.</li>
<li class="code-line">Once a decision is made, it will inform<span> </span><kbd>etcd</kbd><span> </span>of its decision;<span> </span><kbd>etcd</kbd><span> </span>records the decision.</li>
<li class="code-line">The<span> </span><kbd>kubelet</kbd><span> </span>service running on each node will notice this change on<span> </span><kbd>etcd</kbd>, and pull down a PodSpec <span>– </span>the Pod's manifest file. It will then run and manage a new Pod according to the PodSpec.</li>
</ol>
<p class="code-line">During the entire process, the scheduler and<span> </span>kubelets keep<span> </span><kbd>etcd</kbd><span> </span>up to date<span> </span><em>at all times</em><span> </span>via the Kubernetes API.</p>
<p class="code-line">If we query for the state of the Deployment in the first few seconds after we run<span> </span><kbd>kubectl apply</kbd>, we will see that<span> </span><kbd>etcd</kbd><span> </span>has updated its records with our desired state, but the Pods and containers will not be available yet:</p>
<pre><strong>$ kubectl get deployments</strong><br/><strong>NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</strong><br/><strong>elasticsearch   3         3         3            0           2s</strong></pre>
<div class="code-line packt_infobox"><strong>What do the numbers m</strong><strong>ean?</strong><span> </span><kbd>DESIRED</kbd>—the desired number of replicas;<span> </span><kbd>CURRENT</kbd>—the current number of replicas;<span> </span><kbd>UP-TO-<span>–</span></kbd> the current number of replicas that has the most up-to-date configuration (has the copy of the latest Pod template/manifest);<span> </span><kbd>AVAILABLE</kbd>—the number of replicas available to users</div>
<p class="code-line">We can then run<span> </span><kbd>kubectl rollout status</kbd><span> </span>to be notified, in real-time, when each Pod is ready:</p>
<pre><strong>$ kubectl rollout status deployment/elasticsearch</strong><br/><strong>Waiting for rollout to finish: 0 of 3 updated replicas are available...</strong><br/><strong>Waiting for rollout to finish: 1 of 3 updated replicas are available...</strong><br/><strong>Waiting for rollout to finish: 2 of 3 updated replicas are available...</strong><br/><strong>deployment "elasticsearch" successfully rolled out</strong></pre>
<p class="code-line">Then, we can check the deployment again, and we can see that all three replica Pods are available:</p>
<pre><strong>$ kubectl get deployments</strong><br/><strong>NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</strong><br/><strong>elasticsearch   3         3         3            3           2m</strong></pre>
<p class="code-line">We have now successfully switched our approach from an imperative one (using<span> </span><kbd>kubectl run</kbd>), to a declarative one (using manifest files and<span> </span><kbd>kubectl apply</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes Object management hierarchy</h1>
                </header>
            
            <article>
                
<p class="code-line">To solidify your understanding that our Deployment object is managing a ReplicaSet object, you can run<span> </span><kbd>kubectl get rs</kbd><span> </span>to get a list of ReplicaSet in the cluster:</p>
<pre><strong>$ kubectl get rs</strong><br/><strong>NAME                       DESIRED   CURRENT   READY     AGE</strong><br/><strong>elasticsearch-699c7dd54f   3         3         3         3m</strong></pre>
<p class="code-line">The name of a ReplicaSet is automatically generated from the name of the Deployment object that manages it, and a hash value derived from the Pod template:</p>
<pre>&lt;deployment-name&gt;-&lt;pod-template-hash&gt;</pre>
<p class="code-line">Therefore, we know that the<span> </span><kbd>elasticsearch-699c7dd54f</kbd><span> </span>ReplicaSet is managed by the<span> </span><kbd>elasticsearch</kbd><span> </span>Deployment.</p>
<p class="code-line">Using the same logic, you can run<span> </span><kbd>kubectl get pods</kbd><span> </span>to see a list of Pods:</p>
<pre><strong>$ kubectl get pods --show-labels</strong><br/><strong>NAME                             READY  STATUS    LABELS</strong><br/><strong>elasticsearch-699c7dd54f-n5tmq   1/1    Running   app=elasticsearch,pod-template-hash=2557388109</strong><br/><strong>elasticsearch-699c7dd54f-pft9k   1/1    Running   app=elasticsearch,pod-template-hash=2557388109</strong><br/><strong>elasticsearch-699c7dd54f-pm2wz   1/1    Running   app=elasticsearch,pod-template-hash=2557388109</strong></pre>
<p class="code-line">Again, the name of the Pod is the name of its controlling ReplicaSet and a unique hash.</p>
<p class="code-line">You can also see that the Pods have a<span> </span><kbd>pod-template-hash=2557388109</kbd><span> </span>label applied to them. The Deployment and ReplicaSet use this label to identify which Pods it should be managing.</p>
<p class="code-line">To find out more information about an individual Pod, you can run<span> </span><kbd>kubectl describe pods &lt;pod-name&gt;</kbd>, which will produce a human-friendly output:</p>
<pre><strong>$ kubectl describe pods elasticsearch-699c7dd54f-n5tmq</strong><br/><strong>Name: elasticsearch-699c7dd54f-n5tmq</strong><br/><strong>Namespace: default</strong><br/><strong>Node: minikube/10.122.98.143</strong><br/><strong>Labels: app=elasticsearch</strong><br/><strong>                pod-template-hash=2557388109</strong><br/><strong>Annotations: &lt;none&gt;</strong><br/><strong>Status: Running</strong><br/><strong>IP: 172.17.0.5</strong><br/><strong>Controlled By: ReplicaSet/elasticsearch-699c7dd54f</strong><br/><strong>Containers:</strong><br/><strong>  elasticsearch:</strong><br/><strong>    Container ID: docker://ee5a3000a020c91a04fa02ec50b86012f2c27376b773bbf7be4c9ebce9c2551f</strong><br/><strong>    Image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.4</strong><br/><strong>    Image ID: docker-pullable://docker.elastic.co/elasticsearch/elasticsearch-oss@sha256:2d9c774c536bd1f64abc4993ebc96a2344404d780cbeb81a8b3b4c3807550e57</strong><br/><strong>    Ports: 9200/TCP, 9300/TCP</strong><br/><strong>    Host Ports: 0/TCP, 0/TCP</strong><br/><strong>    State: Running</strong><br/><strong>    Ready: True</strong><br/><strong>    Restart Count: 0</strong><br/><strong>    Environment: &lt;none&gt;</strong><br/><strong>    Mounts:</strong><br/><strong>      /var/run/secrets/kubernetes.io/serviceaccount from default-token-26tl8 (ro)</strong><br/><strong>Conditions:</strong><br/><strong>  Type Status</strong><br/><strong>  Initialized True</strong><br/><strong>  Ready True</strong><br/><strong>  PodScheduled True</strong><br/><strong>Volumes:</strong><br/><strong>  default-token-26tl8:</strong><br/><strong>    Type: Secret (a volume populated by a Secret)</strong><br/><strong>    SecretName: default-token-26tl8</strong><br/><strong>    Optional: false</strong><br/><strong>QoS Class: BestEffort</strong><br/><strong>Events:</strong><br/><strong>  Type Reason Age From Message</strong><br/><strong>  ---- ------ ---- ---- -------</strong><br/><strong>  Normal Scheduled 1m default-scheduler Successfully assigned elasticsearch-699c7dd54f-n5tmq to minikube</strong><br/><strong>  Normal SuccessfulMountVolume 1m kubelet, minikube MountVolume.SetUp succeeded for volume "default-token-26tl8"</strong><br/><strong>  Normal Pulled 1m kubelet, minikube Container image "docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.4" already present on machine</strong><br/><strong>  Normal Created 1m kubelet, minikube Created container</strong><br/><strong>  Normal Started 1m kubelet, minikube Started container</strong></pre>
<p class="code-line">Alternatively, you can get information about a Pod in a more structured JSON format by running<span> </span><kbd>kubectl get pod &lt;pod-name&gt;</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Elasticsearch cluster</h1>
                </header>
            
            <article>
                
<p class="code-line">From the output of<span> </span><kbd>kubectl describe pods</kbd><span> </span>(or<span> </span><kbd>kubectl get pod</kbd>), we can see that the IP address of the Pod named<span> </span><kbd>elasticsearch-699c7dd54f-n5tmq</kbd><span> </span>is listed as<span> </span><kbd>172.17.0.5</kbd>. Since our machine<span> </span>is<span> </span>the node that this Pod runs on, we can access the Pod using this private IP address.</p>
<p class="mce-root"/>
<p class="code-line">The Elasticsearch API should be listening to port<span> </span><kbd>9200</kbd>. Therefore, if we make a<span> </span><kbd>GET</kbd><span> </span>request to<span> </span><kbd>http://172.17.0.5:9200/</kbd>, we should expect Elasticsearch to reply with a JSON object:</p>
<pre><strong>$ curl http://172.17.0.5:9200/</strong><br/><strong>{</strong><br/><strong>  "name" : "CKaMZGV",</strong><br/><strong>  "cluster_name" : "docker-cluster",</strong><br/><strong>  "cluster_uuid" : "dCAcFnvOQFuU8pTgw4utwQ",</strong><br/><strong>  "version" : {</strong><br/><strong>    "number" : "6.3.2",</strong><br/><strong>    "lucene_version" : "7.3.1"</strong><br/><strong>    ...</strong><br/><strong>  },</strong><br/><strong>  "tagline" : "You Know, for Search"</strong><br/><strong>}</strong></pre>
<p class="code-line">We can do the same for Pods<span> </span><kbd>elasticsearch-699c7dd54f-pft9k</kbd><span> </span>and<span> </span><kbd>elasticsearch-699c7dd54f-pm2wz</kbd>, which have the IPs<span> </span><kbd>172.17.0.4</kbd><span> </span>and<span> </span><kbd>172.17.0.6</kbd>, respectively:</p>
<pre><strong>$ kubectl get pods -l app=elasticsearch -o=custom-columns=NAME:.metadata.name,IP:.status.podIP</strong><br/><strong>NAME IP</strong><br/><strong>elasticsearch-699c7dd54f-pft9k 172.17.0.4</strong><br/><strong>elasticsearch-699c7dd54f-n5tmq 172.17.0.5</strong><br/><strong>elasticsearch-699c7dd54f-pm2wz 172.17.0.6</strong><br/><br/><strong>$ curl http://172.17.0.4:9200/</strong><br/><strong>{</strong><br/><strong>  "name" : "TscXyKK",</strong><br/><strong>  "cluster_name" : "docker-cluster",</strong><br/><strong>  "cluster_uuid" : "zhz6Ok_aQiKfqYpzsgp7lQ",</strong><br/><strong>  ...</strong><br/><strong>}</strong><br/><strong>$ curl http://172.17.0.6:9200/</strong><br/><strong>{</strong><br/><strong>  "name" : "_nH26kt",</strong><br/><strong>  "cluster_name" : "docker-cluster",</strong><br/><strong>  "cluster_uuid" : "TioZ4wz4TeGyflOyu1Xa-A",</strong><br/><strong>  ...</strong><br/><strong>}</strong></pre>
<p class="code-line">Although these Elasticsearch instances are deployed inside the same Kubernetes cluster, they are each inside their own Elasticsearch cluster (there are currently three Elasticsearch clusters, running independently from each other). We know this because the value of<span> </span><kbd>cluster_uuid</kbd><span> </span>for the different Elasticsearch instances are all different.</p>
<p class="code-line">However, we want our Elasticsearch nodes to be able to communicate with each other, so that data written to one instance will be propagated to, and accessible from, other instances.</p>
<p class="code-line">Let's confirm that this is not the case with our current setup. First, we will index a simple document:</p>
<pre><strong>$ curl -X PUT "172.17.0.6:9200/test/doc/1" -H 'Content-Type: application/json' -d '{"foo":"bar"}'</strong><br/><strong>{"_index":"test","_type":"doc","_id":"1","_version":1,"result":"created","_shards":{"total":2,"successful":1,"failed":0},"_seq_no":0,"_primary_term":1}</strong></pre>
<p class="code-line">Already, we can see that the desired total number of shards is<span> </span><kbd>2</kbd>, but we only have one shard.</p>
<p class="code-line">We can confirm that the document is now indexed and accessible from the same Elasticsearch instance (running at<span> </span><kbd>172.17.0.6:9200</kbd>), but not from any other Elasticsearch instances on our Kubernetes cluster:</p>
<pre><strong>$ curl "172.17.0.6:9200/test/doc/1"</strong><br/><strong>{"_index":"test","_type":"doc","_id":"1","_version":1,"found":true,"_source":{"foo":"bar"}}</strong><br/><br/><strong>$ curl "172.17.0.5:9200/test/doc/1"</strong><br/><strong>{"error":{"type":"index_not_found_exception","reason":"no such index","index":"test"},"status":404}</strong><br/><br/><strong>$ curl "172.17.0.4:9200/test/doc/1"</strong><br/><strong>{"error":{"type":"index_not_found_exception","reason":"no such index","index":"test"},"status":404}</strong></pre>
<p>Before we continue, it's important to make the distinction between an Elasticsearch cluster and a Kubernetes cluster. Elasticsearch is a distributed data storage solution, where all data is distributed among one or more shards, deployed among one or more nodes. An Elasticsearch cluster can be deployed on any machines, and is completely unrelated to a Kubernetes cluster. However, because we are deploying a distributed Elasticsearch services on Kubernetes, the Elasticsearch cluster now resides within the Kubernetes cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Networking for distributed databases</h1>
                </header>
            
            <article>
                
<p class="code-line">Due to the ephemeral nature of Pods, the IP addresses for Pods running a particular service (such as Elasticsearch) may change. For instance, the scheduler may kill Pods running on a busy node, and redeploy it on a more available node.</p>
<p class="code-line">This poses a problem for our Elasticsearch deployment because:</p>
<ul>
<li class="code-line">An Elasticsearch instance running on one Pod would not know the IP addresses of other instances running on other Pods</li>
<li class="code-line">Even if an instance obtains a list of IP addresses of other instances, this list will quickly become obsolete</li>
</ul>
<p class="code-line">This means that Elasticsearch nodes cannot discover each other (this process is called<span> </span><strong>Node Discovery</strong>), and is the reason why changes applied to one Elasticsearch node is not propagated to the others.</p>
<p class="code-line">To resolve this issue, we must understand how Node Discovery works in Elasticsearch, and then figure out how we can configure Kubernetes to enable discovery for Elasticsearch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring Elasticsearch's Zen discovery</h1>
                </header>
            
            <article>
                
<p class="code-line">Elasticsearch provides a discovery module, called<span> </span><strong>Zen Discovery</strong>, that allows different Elasticsearch nodes to find each other.</p>
<p class="code-line">By default, Zen Discovery achieves this by pinging ports<span> </span><kbd>9300</kbd><span> </span>to<span> </span><kbd>9305</kbd><span> </span>on each loopback address (<kbd>127.0.0.0/16</kbd>), and tries to find Elasticsearch instances that respond to the ping. This default behavior provides auto-discovery for all Elasticsearch nodes running on the same machine.</p>
<p class="code-line">However, if the nodes reside on different machines, they won't be available on the loopback addresses. Instead, they will have IP addresses that are private to their network. For Zen Discovery to work here, we must provide a<span> </span><em>seed list</em><span> </span>of hostnames and/or IP addresses that other Elasticsearch nodes are running on.</p>
<p class="code-line">This list can be specified under the<span> </span><kbd>discovery.zen.ping.unicast.hosts</kbd><span> </span>property inside Elasticsearch's configuration file<span> </span><kbd>elasticsearch.yaml</kbd>. But this is difficult because:</p>
<ul>
<li class="code-line">The Pod IP address that these Elasticsearch nodes will be running on is very likely to change</li>
<li class="code-line">Every time the IP changes, we'd have to go inside each container and update<span> </span><kbd>elasticsearch.yaml</kbd></li>
</ul>
<p class="code-line">Fortunately, Elasticsearch allows us to specify this setting as an environment variable. Therefore, we can modify our<span> </span><kbd>deployment.yaml</kbd><span> </span>and add an<span> </span><kbd>env</kbd><span> </span>property under<span> </span><kbd>spec.template.spec.containers</kbd>:</p>
<pre>containers:<br/>- name: elasticsearch<br/>  image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2<br/>  ports:<br/>  - containerPort: 9200<br/>  - containerPort: 9300<br/>  env:<br/>  - name: discovery.zen.ping.unicast.hosts<br/>    value: ""</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Attaching hostnames to Pods</h1>
                </header>
            
            <article>
                
<p class="code-line">But what should the value of this environment variable be? Currently, the IP addresses of the Elasticsearch Pods is random (within a large range) and may change at any time.</p>
<p class="code-line">To resolve this issue, we need to give each Pod a unique hostname that sticks to the Pod, even if it gets rescheduled.</p>
<div class="code-line packt_infobox">When you visit a website, you usually won't type the site's IP address directly onto the browser; instead, you'd use the website's domain name. Even if the host of the website changes to a different IP address, the website will still be reachable on the same domain name. This is similar to what happens when we attach a hostname to a Pod.</div>
<p class="code-line">To achieve this, we need to do two things:</p>
<ol>
<li class="code-line">Provide each Pod with an identity using another Kubernetes Object called<span> </span><em>StatefulSet.</em></li>
<li class="code-line">Attach a DNS subdomain to each Pod using a<span> </span><em>Headless Service</em>, where the value of the subdomain is based on the Pod's identity.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with StatefulSets</h1>
                </header>
            
            <article>
                
<p class="code-line">So far, we've been using the Deployment object to deploy our Elasticsearch service. The Deployment Controller will manage the ReplicaSets and Pods under its control and ensure that the correct numbers are running and healthy.</p>
<p class="code-line">However, a Deployment assumes that each instance is stateless and works independently from each other. More importantly, it assumes that instances are fungible—that one instance is interchangeable with any other.<span> </span><strong>Pods managed by a Deployment have identical identities.</strong></p>
<p class="code-line">This is not the case for Elasticsearch, or other distributed databases, which must hold stateful information that distinguishes one Elasticsearch node from another. These Elasticsearch nodes need individual identities so that they can communicate with each other to ensure data is consistent across the cluster.</p>
<p class="code-line">Kubernetes provides another API Object called<span> </span><strong>StatefulSet</strong>. Like the Deployment object, StatefulSet manages the running and scaling of Pods, but it also guarantees the ordering and uniqueness of each Pod.<span> </span><strong>Pods managed by a StatefulSet have individual identities.</strong></p>
<p class="code-line">StatefulSets are similar to Deployments in terms of definition, so we only need to make minimal changes to our<span> </span><kbd>manifests/elasticsearch/deployment.yaml</kbd>. First, change the filename to<span> </span><kbd>stateful-set.yaml</kbd>, and then change the<span> </span><kbd>kind</kbd><span> </span>property to<span> </span>StatefulSet:</p>
<pre>kind: StatefulSet</pre>
<p class="code-line">Now, all the Pods within the StatefulSet can be identified with a name. The name is composed of the name of the StatefulSet, as well as the<span> </span><em>ordinal index</em><span> </span>of the Pod:</p>
<pre>&lt;statefulset-name&gt;-&lt;ordinal&gt;</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ordinal index</h1>
                </header>
            
            <article>
                
<p class="code-line">The ordinal index, also known as<span> </span><strong>ordinal number</strong><span> </span>in set theory, is simply a set of numbers that are used to order a collection of objects, one after the other. Here, Kubernetes is using them to order, as well as identify each Pod. You can think of it akin to an auto-incrementing index in a SQL column.</p>
<p class="code-line">The "first" Pod in the StatefulSet has an ordinal number of<span> </span><kbd>0</kbd>, the "second" Pod has the ordinal number of<span> </span><kbd>1</kbd>, and so on.</p>
<p class="code-line">Our StatefulSet is named<span> </span><kbd>elasticsearch</kbd><span> </span>and we indicated<span> </span><kbd>3</kbd><span> </span>replicas, so our Pods will now be named<span> </span><kbd>elasticsearch-0</kbd>,<span> </span><kbd>elasticsearch-1</kbd><span>, </span>and<span> </span><kbd>elasticsearch-2</kbd>.</p>
<p class="code-line">Most importantly, a Pod's cardinal index, and thus its identity, is<span> </span><em>sticky—</em>if the Pod gets rescheduled onto another Node, it will keep this same ordinal and identity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with services</h1>
                </header>
            
            <article>
                
<p class="code-line">By using a StatefulSet, each Pod can now be uniquely identified. However, the IP of each Pod is still randomly assigned; we want our Pods to be accessible from a stable IP address. Kubernetes provides the<span> </span><em>Service</em><span> </span>Object to achieve this.</p>
<p class="code-line">The Service Object is very versatile, in that it can be used in many ways. Generally, it is used to provide an IP address to Kuberentes Objects like Pods.</p>
<p class="code-line">The most common use case for a Service Object is to provide a single, stable, externally-accessible<span> </span><em>Cluster IP</em><span> </span>(also known as the<span> </span><em>Service IP</em>) for a distributed service. When a request is made to this Cluster IP, the request will be proxied to one of the Pods running the service. In this use case, the Service Object is acting as a load balancer.</p>
<p class="code-line">However, that's not what we need for our Elasticsearch service. Instead of having a single cluster IP for the entire service, we want each Pod to have its own stable subdomain so that each Elasticsearch node can perform Node Discovery.</p>
<p class="code-line">For this use case, we want to use a special type of Service Object called<strong> Headless Service</strong>. As with other Kubernetes Objects, we can define a Headless Service using a manifest file. Create a new file at<span> </span><kbd>manifests/elasticsearch/service.yaml</kbd><span> </span>with the following content:</p>
<pre>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: elasticsearch<br/>spec:<br/>  selector:<br/>    app: elasticsearch<br/>  clusterIP: None<br/>  ports:<br/>  - port: 9200<br/>    name: rest<br/>  - port: 9300<br/>    name: transport</pre>
<p class="code-line">Let's go through what some of the fields mean:</p>
<ul>
<li class="code-line"><kbd>metadata.name</kbd>: Like other Kuberentes Objects, having a name allows us to identify the Service by name and not ID.</li>
<li class="code-line"><kbd>spec.selector</kbd>: This specifies the Pods that should be managed by the Service Controller. Specifically for Services, this defines the selector to select all the Pods that constitute a service.</li>
<li class="code-line"><kbd>spec.clusterIP</kbd>: This specifies the Cluster IP for the Service. Here, we set it to<span> </span><kbd>None</kbd><span> </span>to indicate that we want a Headless Service.</li>
<li class="code-line"><kbd>spec.ports</kbd>: A mapping of how requests are mapped from a port to the container's port.</li>
</ul>
<p class="code-line">Let's deploy this Service into our Kubernetes cluster:</p>
<div class="code-line packt_infobox">We don't need to actually run the Pods before we define a Service. A Service will frequently evaluate its selector to find new Pods that satisfy the selector.</div>
<pre><strong>$ kubectl apply -f manifests/elasticsearch/service.yaml</strong><br/><strong>service "elasticsearch" created</strong></pre>
<p class="code-line">We can run<span> </span><kbd>kubectl get service</kbd><span> </span>to see a list of running services:</p>
<pre><strong>$ kubectl get services</strong><br/><strong>NAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE</strong><br/><strong>elasticsearch   ClusterIP   None         &lt;none&gt;        9200/TCP,9300/TCP   46s</strong><br/><strong>kubernetes      ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP             4h</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linking StatefulSet to a service</h1>
                </header>
            
            <article>
                
<p class="code-line">First, let's remove our existing<span> </span><kbd>elasticsearch</kbd><span> </span>Deployment Object:</p>
<pre><strong>$ kubectl delete deployment elasticsearch</strong></pre>
<p class="code-line">Now, the final step is to create our StatefulSet, which provides each Pod with a unique identity, and link it to the Service, which gives each Pod a subdomain. We do this by specifying the name of the Service as the<span> </span><kbd>spec.serviceName</kbd><span> </span>property in our StatefulSet manifest file:</p>
<pre>...<br/>spec:<br/>  replicas: 3<br/>  serviceName: elasticsearch<br/>  ...</pre>
<p class="code-line">Now, the Service linked to the StatefulSet will get a domain with the following structure:</p>
<pre>&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;</pre>
<p class="code-line">Our Service's name is<span> </span><kbd>elasticsearch</kbd>. By default, Kubernetes will use the<span> </span><kbd>default</kbd><span> </span>namespace, and<span> </span><kbd>cluster.local</kbd><span> </span>as the Cluster Domain. Therefore, the Service Domain for our Headless Service is<span> </span><kbd>elasticsearch.default.svc.cluster.local</kbd>.</p>
<p class="code-line">Each Pod within the Headless Service will have its own subdomain, which has the following structure:</p>
<pre>&lt;pod-name&gt;.&lt;service-domain&gt;</pre>
<p class="code-line">Or if we expand this out:</p>
<pre>&lt;statefulset-name&gt;-&lt;ordinal&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.&lt;cluster-domain&gt;</pre>
<p class="code-line">Therefore, our three replicas would have the subdomains:</p>
<pre>elasticsearch-0.elasticsearch.default.svc.cluster.local<br/>elasticsearch-1.elasticsearch.default.svc.cluster.local<br/>elasticsearch-2.elasticsearch.default.svc.cluster.local</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating Zen Discovery configuration</h1>
                </header>
            
            <article>
                
<p class="code-line">We can now combine these subdomains into a comma-separated list, and use it as the value for the<span> </span><kbd>discovery.zen.ping.unicast.hosts</kbd><span> </span>environment variable we are passing into the Elasticsearch containers. Update the<span> </span><kbd>manifests/elasticsearch/stateful-set.yaml</kbd><span> </span>file to read the following:</p>
<pre class="mce-root">env:<br/>  - name: discovery.zen.ping.unicast.hosts<br/>    value: "elasticsearch-0.elasticsearch.default.svc.cluster.local,elasticsearch-1.elasticsearch.default.svc.cluster.local,elasticsearch-2.elasticsearch.default.svc.cluster.local"</pre>
<p class="code-line">The final<span> </span><kbd>stateful-set.yaml</kbd><span> </span>should read as follows:</p>
<pre>apiVersion: apps/v1<br/>kind: StatefulSet<br/>metadata:<br/>  name: elasticsearch<br/>spec:<br/>  replicas: 3<br/>  serviceName: elasticsearch<br/>  selector:<br/>    matchLabels:<br/>      app: elasticsearch<br/>  template:<br/>    metadata:<br/>      name: elasticsearch<br/>      labels:<br/>        app: elasticsearch<br/>    spec:<br/>      containers:<br/>        - name: elasticsearch<br/>          image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2<br/>          ports:<br/>            - containerPort: 9200<br/>            - containerPort: 9300<br/>          env:<br/>            - name: discovery.zen.ping.unicast.hosts<br/>              value: "elasticsearch-0.elasticsearch.default.svc.cluster.local,elasticsearch-1.elasticsearch.default.svc.cluster.local,elasticsearch-2.elasticsearch.default.svc.cluster.local"</pre>
<p class="code-line">Now, we can add this StatefulSet to our cluster by running<span> </span><kbd>kubectl apply</kbd>:</p>
<pre><strong>$ kubectl apply -f manifests/elasticsearch/stateful-set.yaml</strong><br/><strong>statefulset.apps "elasticsearch" created</strong></pre>
<p class="code-line">We can check that the StatefulSet is deployed by running<span> </span><kbd>kubectl get statefulset</kbd>:</p>
<pre><strong>$ kubectl get statefulsets</strong><br/><strong>NAME            DESIRED   CURRENT   AGE</strong><br/><strong>elasticsearch   3         3         42s</strong></pre>
<p class="code-line">We should also check that the Pods are deployed and running:</p>
<pre><strong>$ kubectl get pods</strong><br/><strong>NAME              READY     STATUS    RESTARTS   AGE</strong><br/><strong>elasticsearch-0   1/1       Running   0          1m</strong><br/><strong>elasticsearch-1   1/1       Running   0          1m</strong><br/><strong>elasticsearch-2   1/1       Running   0          1m</strong></pre>
<p class="code-line">Note how each Pod now has a name with the structure<span> </span><kbd>&lt;statefulset-name&gt;-&lt;ordinal&gt;</kbd>.</p>
<p class="code-line">Now, let's<span> </span><kbd>curl</kbd><span> </span>port<span> </span><kbd>9200</kbd><span> </span>of each Pod and see if the Elasticsearch Nodes have discovered each other and have collectively formed a single cluster. We will be using the<span> </span><kbd>-o</kbd><span> </span>flag of<span> </span><kbd>kubectl get pods</kbd><span> </span>to extract the IP address of each Pod. The<span> </span><kbd>-o</kbd><span> </span>flag allows you to specify custom formats for your output. For example, you can get a table of Pod names and IPs:</p>
<pre><strong>$ kubectl get pods -l app=elasticsearch -o=custom-columns=NAME:.metadata.name,IP:.status.podIP</strong><br/><strong>NAME IP</strong><br/><strong>elasticsearch-0 172.17.0.4</strong><br/><strong>elasticsearch-1 172.17.0.5</strong><br/><strong>elasticsearch-2 172.17.0.6</strong></pre>
<p class="code-line">We will run the following command to get the Cluster ID of the Elasticsearch node running on Pod<span> </span><kbd>elasticsearch-0</kbd>:</p>
<pre><strong>$ curl -s $(kubectl get pod elasticsearch-0 -o=jsonpath='{.status.podIP}'):9200 | jq -r '.cluster_uuid'</strong><br/><strong>eeDC2IJeRN6TOBr227CStA</strong></pre>
<p class="code-line"><kbd>kubectl get pod elasticsearch-0 -o=jsonpath='{.status.podIP}'</kbd><span> </span>returns the IP address of the Pod. This is then used to<span> </span><kbd>curl</kbd><span> </span>the port<span> </span><kbd>9200</kbd><span> </span>of this IP; the<span> </span><kbd>-s</kbd><span> </span>flag silences the progress information that cURL normally prints to<span> </span><kbd>stdout</kbd>. Lastly, the JSON returned from Elasticsearch is parsed by the<span> </span><kbd>jq</kbd><span> </span>tool which extracts the<span> </span><kbd>cluster_uuid</kbd><span> </span>field from the JSON object.</p>
<p class="code-line">The end result gives a Elasticsearch Cluster ID of<span> </span><kbd>eeDC2IJeRN6TOBr227CStA</kbd>. Repeat the same step for the other Pods to confirm that they've successfully performed Node Discovery and are part of the same Elasticsearch Cluster:</p>
<pre><strong>$ curl -s $(kubectl get pod elasticsearch-1 -o=jsonpath='{.status.podIP}'):9200 | jq -r '.cluster_uuid'</strong><br/><strong>eeDC2IJeRN6TOBr227CStA</strong><br/><br/><strong>$ curl -s $(kubectl get pod elasticsearch-2 -o=jsonpath='{.status.podIP}'):9200 | jq -r '.cluster_uuid'</strong><br/><strong>eeDC2IJeRN6TOBr227CStA</strong></pre>
<p class="code-line">Perfect! Another way to confirm this is to send a<span> </span><kbd>GET /cluster/state</kbd><span> </span>request to any one of the Elasticsearch nodes:</p>
<pre><strong>$ curl "$(kubectl get pod elasticsearch-2 -o=jsonpath='{.status.podIP}'):9200/_cluster/state/master_node,nodes/?pretty"</strong><br/><strong>{</strong><br/><strong>  "cluster_name" : "docker-cluster",</strong><br/><strong>  "compressed_size_in_bytes" : 874,</strong><br/><strong>  "master_node" : "eq9YcUzVQaiswrPbwO7oFg",</strong><br/><strong>  "nodes" : {</strong><br/><strong>    "lp4lOSK9QzC3q-YEsqwRyQ" : {</strong><br/><strong>      "name" : "lp4lOSK",</strong><br/><strong>      "ephemeral_id" : "e58QpjvBR7iS15FhzN0zow",</strong><br/><strong>      "transport_address" : "172.17.0.5:9300",</strong><br/><strong>      "attributes" : { }</strong><br/><strong>    },</strong><br/><strong>    "eq9YcUzVQaiswrPbwO7oFg" : {</strong><br/><strong>      "name" : "eq9YcUz",</strong><br/><strong>      "ephemeral_id" : "q7zlTKCqSo2qskkY8oSStw",</strong><br/><strong>      "transport_address" : "172.17.0.4:9300",</strong><br/><strong>      "attributes" : { }</strong><br/><strong>    },</strong><br/><strong>    "77CpcuDDSom7hTpWz8hBLQ" : {</strong><br/><strong>      "name" : "77CpcuD",</strong><br/><strong>      "ephemeral_id" : "-yq7bhphQ5mF5JX4qqXHoQ",</strong><br/><strong>      "transport_address" : "172.17.0.6:9300",</strong><br/><strong>      "attributes" : { }</strong><br/><strong>    }</strong><br/><strong>  }</strong><br/><strong>}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Validating Zen Discovery</h1>
                </header>
            
            <article>
                
<p class="code-line">Once all ES nodes have been discovered, most API operations are propagated from one ES node to another in a peer-to-peer manner. To test this, let's repeat what we did previously and add a document to one Elasticsearch node and test whether you can access this newly indexed document from a different Elasticsearch node.</p>
<p class="code-line">First, let's index a new document on the Elasticsearch node running inside the<span> </span><kbd>elasticsearch-0</kbd><span> </span>Pod:</p>
<pre><strong>$ curl -X PUT "$(kubectl get pod elasticsearch-0 -o=jsonpath='{.status.podIP}'):9200/test/doc/1" -H 'Content-Type: application/json' -d '{"foo":"bar"}'</strong><br/><strong>{"_index":"test","_type":"doc","_id":"1","_version":1,"result":"created","_shards":{"total":2,"successful":2,"failed":0},"_seq_no":0,"_primary_term":1}</strong></pre>
<p class="code-line">Now, let's try to retrieve this document from another Elasticsearch node (for example, the one running inside Pod<span> </span><kbd>elasticsearch-1</kbd>):</p>
<pre><strong>$ curl "$(kubectl get pod elasticsearch-1 -o=jsonpath='{.status.podIP}'):9200/test/doc/1"</strong><br/><strong>{"_index":"test","_type":"doc","_id":"1","_version":1,"found":true,"_source":{"foo":"bar"}}</strong></pre>
<p class="code-line">Try repeating the same command for<span> </span><kbd>elasticsearch-0</kbd><span> </span>and<span> </span><kbd>elasticsearch-2</kbd><span> </span>and confirm that you get the same result.</p>
<p class="code-line">Amazing! We've now successfully deployed our Elasticsearch service in a distributed manner inside our Kubernetes cluster!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying on cloud provider</h1>
                </header>
            
            <article>
                
<p class="code-line">So far, we've deployed everything locally so that you can experiment freely without costs. But for us to make our service available to the wider internet, we need to deploy our cluster remotely, with a cloud provider.</p>
<p class="code-line">DigitalOcean supports running Kubernetes clusters, and so we will sign in to our DigitalOcean dashboard and create a new cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new remote cluster</h1>
                </header>
            
            <article>
                
<p class="code-line">After signing into your DigitalOcean account, click on the <span class="packt_screen">Kubernetes</span> tab on your dashboard. You should be greeted with the message <span class="packt_screen">Get started with Kubernetes on DigitalOcean.</span> Click on the <span class="packt_screen">Create a Cluster</span> button and you will be shown a screen similar to how you configured your droplet:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/c6b273ac-87cf-4bc0-9e9a-33a9dd3b42dc.png"/></p>
<p class="code-line">Make sure you select at least three Nodes, where each node has at least 4 GB of RAM. Then, click <span class="packt_screen">Create Cluster</span>. You'll be brought back to the main Kubernetes tab, where you can see that the cluster is being provisioned:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/0dc405da-9af2-4c4a-bb4e-cd9bed20c444.png"/></p>
<p class="code-line">Click on the cluster and you'll be brought to the <span class="packt_screen">Overview</span> section for the cluster:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/b6c41811-0166-4695-b74e-3b9fd128978c.png"/></p>
<p class="code-line">Click on the <span class="packt_screen">Download Config</span> button to download the configuration required to connect with our newly-created cluster on DigitalOcean. When you open it up, you should see something similar to this:</p>
<pre>apiVersion: v1<br/>clusters:<br/>- cluster:<br/>    certificate-authority-data: S0tL...FFDENFRJQV0<br/>    server: https://8b8a5720059.k8s.ondigitalocean.com<br/>  name: do-nyc1-hobnob<br/>contexts:<br/>- context:<br/>    cluster: do-nyc1-hobnob<br/>    user: do-nyc1-hobnob-admin<br/>  name: do-nyc1-hobnob<br/>current-context: do-nyc1-hobnob<br/>kind: Config<br/>preferences: {}<br/>users:<br/>- name: do-nyc1-hobnob-admin<br/>  user:<br/>    client-certificate-data: LUMMmxjaJ...VElGVEo<br/>    client-key-data: TFyMrS2I...mhoTmV2LS05kRF</pre>
<p class="code-line">Let's examine the fields to understand why they're there:</p>
<ul>
<li class="code-line"><kbd>apiVersion</kbd>,<span> </span><kbd>kind</kbd>: These fields have the same meaning as before</li>
<li class="code-line"><kbd>clusters</kbd>: Define different clusters to be managed by<span> </span><kbd>kubectl</kbd>, including the cluster's server's hostname, and certificates required to verify the identity of the server</li>
<li class="code-line"><kbd>users</kbd>: Defines user credentials that are used to connect to a cluster; this may be certificates and keys, or simple usernames and passwords. You can use the same user to connect to multiple clusters, although normally you'd create a separate user for each cluster.</li>
<li class="code-line"><kbd>context</kbd>: A grouping of clusters, users, and namespaces.</li>
</ul>
<p class="code-line">It will take a few minutes for the nodes to initialize; in the meantime, let's see how we can configure<span> </span><kbd>kubectl</kbd><span> </span>to interact with our new remote cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Switching contexts</h1>
                </header>
            
            <article>
                
<p class="code-line">When using<span> </span><kbd>kubectl</kbd>, a context is a grouping of clusters, user credentials, and namespaces.<span> </span><kbd>kubectl</kbd><span> </span>uses information stored in these contexts to communicate with any cluster.</p>
<p class="code-line">When we set up our local cluster using Minikube, it creates a default<span> </span><kbd>minikube</kbd><span> </span>context for us. We can confirm this by running<span> </span><kbd>kubectl config current-context</kbd>:</p>
<pre><strong>$ kubectl config current-context</strong><br/><strong>minikube</strong></pre>
<p class="code-line"><kbd>kubectl</kbd><span> </span>gets its configuration from the file specified by the<span> </span><kbd>KUBECONFIG</kbd><span> </span>environment variable. This was set in our<span> </span><kbd>.profile</kbd><span> </span>file to<span> </span><kbd>$HOME/.kube/config</kbd>. If we look inside it, we will see that it is very similar to the config we downloaded from DigitalOcean:</p>
<pre>apiVersion: v1<br/>clusters:<br/>- cluster:<br/>    certificate-authority: ~/.minikube/ca.crt<br/>    server: https://10.122.98.148:8443<br/>  name: minikube<br/>contexts:<br/>- context:<br/>    cluster: minikube<br/>    user: minikube<br/>  name: minikube<br/>current-context: minikube<br/>kind: Config<br/>preferences: {}<br/>users:<br/>- name: minikube<br/>  user:<br/>    client-certificate: ~/.minikube/client.crt<br/>    client-key: ~/.minikube/client.key</pre>
<p class="code-line">The<span> </span><kbd>~/.kube/config</kbd><span> </span>file records the IP address of the cluster's master API server, the credentials for our client to interact with it, and grouped the cluster information and user credentials together in the context object.</p>
<p class="code-line">For<span> </span><kbd>kubectl</kbd><span> </span>to interact with our new DigitalOcean Hobnob cluster, we must update the<span> </span><kbd>KUBECONFIG</kbd><span> </span>environment variable to include our new configuration file.</p>
<p class="code-line">First, copy the configuration file from DigitalOcean to a new file:</p>
<pre><strong>$ cp downloads/hobnob-kubeconfig.yaml ~/.kube/</strong></pre>
<p class="code-line">Now, edit your<span> </span><kbd>~/.profile</kbd><span> </span>file and update the<span> </span><kbd>KUBECONFIG</kbd><span> </span>environment variable to include the new configuration file:</p>
<pre>export KUBECONFIG=$HOME/.kube/config:$HOME/.kube/hobnob-kubeconfig.yaml</pre>
<p class="code-line">Save and source the file to make it apply to the current shell:</p>
<pre><strong>$ . ~/.profile</strong></pre>
<p class="code-line">Now, when we run<span> </span><kbd>kubectl config view</kbd>, we will see that configuration from both of our files has merged together:</p>
<pre><strong>$ kubectl config view</strong><br/><strong>apiVersion: v1</strong><br/><strong>clusters:</strong><br/><strong>- cluster:</strong><br/><strong>    certificate-authority-data: REDACTED</strong><br/><strong>    server: https://8b8a5720059.k8s.ondigitalocean.com</strong><br/><strong>  name: do-nyc1-hobnob</strong><br/><strong>- cluster:</strong><br/><strong>    certificate-authority: ~/.minikube/ca.crt</strong><br/><strong>    server: https://10.122.98.148:8443</strong><br/><strong>  name: minikube</strong><br/><strong>contexts:</strong><br/><strong>- context:</strong><br/><strong>    cluster: do-nyc1-hobnob</strong><br/><strong>    user: do-nyc1-hobnob-admin</strong><br/><strong>  name: do-nyc1-hobnob</strong><br/><strong>- context:</strong><br/><strong>    cluster: minikube</strong><br/><strong>    user: minikube</strong><br/><strong>  name: minikube</strong><br/><strong>current-context: minikube</strong><br/><strong>kind: Config</strong><br/><strong>preferences: {}</strong><br/><strong>users:</strong><br/><strong>- name: do-nyc1-hobnob-admin</strong><br/><strong>  user:</strong><br/><strong>    client-certificate-data: REDACTED</strong><br/><strong>    client-key-data: REDACTED</strong><br/><strong>- name: minikube</strong><br/><strong>  user:</strong><br/><strong>    client-certificate: ~/.minikube/client.crt</strong><br/><strong>    client-key: ~/.minikube/client.key</strong></pre>
<p class="code-line">Now, to make<span> </span><kbd>kubectl</kbd><span> </span>interact with our DigitalOcean cluster instead of our local cluster, all we have to do is change the context:</p>
<pre><strong>$ kubectl config use-context do-nyc1-hobnob</strong><br/><strong>Switched to context "do-nyc1-hobnob".</strong></pre>
<p class="code-line">Now, when we run<span> </span><kbd>kubectl cluster-info</kbd>, we get information about the remote cluster instead of the local one:</p>
<pre><strong>$ kubectl cluster-info</strong><br/><strong>Kubernetes master is running at https://8b8a5720059.k8s.ondigitalocean.com</strong><br/><strong>KubeDNS is running at https://8b8a5720059.k8s.ondigitalocean.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring nodes for Elasticsearch</h1>
                </header>
            
            <article>
                
<p class="code-line">As mentioned in the official Elasticsearch Guide (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults">https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults</a>), we must configure the node running Elasticsearch in a certain way when deploying on production. For instance:</p>
<ul>
<li class="code-line">
<p class="code-line">By default, Elasticsearch uses a<span> </span><kbd>mmapfs</kbd><span> </span>directory to store its indices. However, most systems set a limit of<span> </span><kbd>65530</kbd><span> </span>on mmap counts, which means Elasticsearch may run out of memory for its indices. If we do not change this setting, you'll encounter the following error when trying to run Elasticsearch:</p>
<pre>[INFO ][o.e.b.BootstrapChecks ] [6tcspAO] bound or publishing to a non-loopback address, enforcing bootstrap checks<br/>ERROR: [1] bootstrap checks failed<br/>[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</pre>
<p class="code-line">Therefore, we should change the<span> </span><kbd>vm.max_map_count</kbd><span> </span>kernel setting to at least<span> </span><kbd>262144</kbd>. This can be done temporarily by running<span> </span><kbd>sysctl -w vm.max_map_count=262144</kbd>, or permanently by adding it to a new file at<span> </span><kbd>/etc/sysctl.d/elasticsearch.conf</kbd>.</p>
</li>
<li class="code-line">
<p class="code-line">UNIX systems impose an upper limit on the number of open files, or more specifically, the number of file descriptors. If you go over that limit, the process which is trying to open a new file will encounter the error<span> </span><kbd>Too many open files</kbd>.</p>
<p class="code-line">There's a global limit for the kernel, which is stored at<span> </span><kbd>/proc/sys/fs/file-max</kbd>; on most systems, this is a large number like<span> </span><kbd>2424348</kbd>. There's also a hard and soft limit<span> </span>per user; hard limits can only be raised by<span> the </span>root, while soft limits can be changed by the user, but never go above the hard limit. You can check the soft limit on file descriptors by running<span> </span><kbd>ulimit -Sn</kbd>; on most systems, this defaults to<span> </span><kbd>1024</kbd>. You can check the hard limit by running<span> </span><kbd>ulimit -Hn</kbd>; the hard limit on my machine is<span> </span><kbd>1048576</kbd>, for example.</p>
<p class="code-line">Elasticsearch recommends that we change the soft and hard limit to at least<span> </span><kbd>65536</kbd>. This can be done by running<span> </span><kbd>ulimit -n 65536</kbd><span> </span>as<span> </span><kbd>root</kbd>.</p>
</li>
</ul>
<p class="mce-root"/>
<p class="code-line">We need to make these changes for every node in our cluster. But first, let's return to our DigitalOcean dashboard to see if our nodes have been created successfully.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running commands on multiple servers</h1>
                </header>
            
            <article>
                
<p class="code-line">When on your DigitalOcean dashboard, click into your cluster and go to the <span class="packt_screen">Nodes</span> tab. Here, you should see that the nodes in your cluster have successfully been provisioned:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/30628f97-6df1-4e10-ac6d-7b0f2377a2f2.png"/></p>
<p class="code-line">We can confirm this from the command line by running<span> </span><kbd>kubectl get nodes</kbd>:</p>
<pre><strong>$ kubectl get nodes</strong><br/><strong>NAME STATUS ROLES AGE VERSION</strong><br/><strong>worker-6000 Ready &lt;none&gt; 17h v1.10.1</strong><br/><strong>worker-6001 Ready &lt;none&gt; 17h v1.10.1</strong><br/><strong>....</strong></pre>
<div class="code-line packt_infobox">Because our current context is set to<span> </span><kbd>do-nyc1-hobnob</kbd>, it will get the nodes on our remote cluster, and not the local cluster.</div>
<p class="code-line">Now that the nodes are ready, how do we go about updating the Elasticsearch-specific settings mentioned previously? The simplest way is to SSH into each server and run the following three sets of commands:</p>
<pre><strong># sysctl -w vm.max_map_count=262144</strong><br/><strong># ulimit -n 65536</strong></pre>
<p class="code-line">However, this becomes unmanageable once we have a large number of servers. Instead, we can use a tool called<span> </span><kbd>pssh</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using pssh</h1>
                </header>
            
            <article>
                
<p class="code-line">Tools such as<span> </span><kbd>pssh</kbd><span> </span>(parallel<span> </span><strong>ssh</strong>,<span> </span><a href="https://github.com/robinbowes/pssh">https://github.com/robinbowes/pssh</a>),<span> </span><kbd>pdsh</kbd><span> </span>(<a href="https://github.com/chaos/pdsh">https://github.com/chaos/pdsh</a>), or<span> </span><kbd>clusterssh</kbd><span> </span>(<a href="https://github.com/duncs/clusterssh">https://github.com/duncs/clusterssh</a>) allow you to issue commands simultaneously to multiple servers at once. Out of all of them,<span> </span><kbd>pssh</kbd><span> </span>is the easiest to install.</p>
<p class="code-line"><kbd>pssh</kbd><span> </span>is listed in the APT registry, so we can simply update the registry cache and install it:</p>
<pre><strong>$ sudo apt update</strong><br/><strong>$ sudo apt install pssh</strong></pre>
<p class="code-line">This will actually install<span> </span><kbd>pssh</kbd><span> </span>under the name<span> </span><kbd>parallel-ssh</kbd>; this was done to avoid conflict with the<span> </span><kbd>putty</kbd><span> </span>package.</p>
<p class="code-line">We can now use<span> </span><kbd>kubectl get nodes</kbd><span> </span>to programmatically get the IPs of all nodes in the cluster, and pass it to<span> </span><kbd>parallel-ssh</kbd>:</p>
<pre><strong>$ parallel-ssh --inline-stdout --user root --host "$(kubectl get nodes -o=jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}')" -x "-o StrictHostKeyChecking=no" "sysctl -w vm.max_map_count=262144 &amp;&amp; ulimit -n 65536"</strong><br/><strong>[1] 23:27:51 [SUCCESS] 142.93.126.236</strong><br/><strong>vm.max_map_count = 262144</strong><br/><strong>[2] 23:27:51 [SUCCESS] 142.93.113.224</strong><br/><strong>vm.max_map_count = 262144</strong><br/><strong>...</strong></pre>
<div class="code-line packt_infobox">We are setting the<span> </span><kbd>ssh</kbd><span> </span>parameter<span> </span><kbd>StrictHostKeyChecking</kbd><span> </span>to<span> </span><kbd>no</kbd><span> </span>to temporarily disable<span> </span><kbd>ssh</kbd><span> </span>checking the authenticity of the nodes. This is insecure but offers convenience; otherwise, you'll have to add each node's key to the<span> </span><kbd>~/.ssh/known_hosts</kbd><span> </span>file.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using init containers</h1>
                </header>
            
            <article>
                
<p class="code-line">Using<span> </span><kbd>pssh</kbd><span> </span>is acceptable, but it's an extra command we need to remember. Ideally, this configuration should be recorded inside<span> </span><kbd>stateful-set.yaml</kbd>, so that the commands only run on nodes that have our Elasticsearch StatefulSet deployed there. Kuberentes provides a special type of Container called Init Containers<span>, </span>which allows us to do just that.</p>
<p class="code-line">Init Containers are special Containers that run and exit before your "normal"<span> </span><em>app Containers</em><span> </span>are initiated. When multiple Init Containers are specified, they run in a sequential order. Also, if the previous Init Container exits with a non-zero exit status, then the next Init Container is not ran and the whole Pod fails.</p>
<p class="code-line">This allows you to use Init Containers to:</p>
<ul>
<li class="code-line">Poll for the readiness of other services. For instance, if your service X depends on another service Y, you can use an Init Container to poll service Y, and this exits only when service Y responds correctly. After the Init Container exits, the app container can begin its initialization steps.</li>
<li class="code-line">Update configurations on the node running the Pod.</li>
</ul>
<p class="code-line">Therefore, we can define Init Containers inside<span> </span><kbd>stateful-set.yaml</kbd>, which will update the configurations on nodes running our Elasticsearch StatefulSet.</p>
<p class="code-line">Inside<span> </span><kbd>stateful-set.yaml</kbd>, under<span> </span><kbd>spec.template.spec</kbd>, add a new field called<span> </span><kbd>initContainers</kbd><span> </span>with the following settings:</p>
<pre>initContainers:<br/>  - name: increase-max-map-count<br/>    image: busybox<br/>    command:<br/>    - sysctl<br/>    - -w<br/>    - vm.max_map_count=262144<br/>    securityContext:<br/>      privileged: true<br/>  - name: increase-file-descriptor-limit<br/>    image: busybox<br/>    command:<br/>    - sh<br/>    - -c<br/>    - ulimit -n 65536<br/>    securityContext:<br/>      privileged: true</pre>
<div class="code-line packt_infobox">We are using the<span> </span><kbd>busybox</kbd><span> </span>Docker image.<span> </span><kbd>busybox</kbd><span> </span>is an image that "combines tiny versions of many common UNIX utilities into a single small executable". Essentially, it is an extremely lightweight (&lt;5 MB) image that allows you to run many of the utility commands you'd expect from the GNU operating system.</div>
<p class="code-line">The final<span> </span><kbd>stateful-set.yaml</kbd><span> file </span>should look like this:</p>
<pre>apiVersion: apps/v1<br/>kind: StatefulSet<br/>metadata:<br/>  name: elasticsearch<br/>  labels:<br/>    app: elasticsearch<br/>spec:<br/>  replicas: 3<br/>  serviceName: elasticsearch<br/>  selector:<br/>    matchLabels:<br/>      app: elasticsearch<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: elasticsearch<br/>    spec:<br/>      initContainers:<br/>      - name: increase-max-map-count<br/>        image: busybox<br/>        command:<br/>        - sysctl<br/>        - -w<br/>        - vm.max_map_count=262144<br/>        securityContext:<br/>          privileged: true<br/>      - name: increase-file-descriptor-limit<br/>        image: busybox<br/>        command:<br/>        - sh<br/>        - -c<br/>        - ulimit -n 65536<br/>        securityContext:<br/>          privileged: true<br/>      containers:<br/>        - name: elasticsearch<br/>          image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2<br/>          ports:<br/>          - containerPort: 9200<br/>            name: http<br/>          - containerPort: 9300<br/>            name: tcp<br/>          env:<br/>            - name: discovery.zen.ping.unicast.hosts<br/>              value: "elasticsearch-0.elasticsearch.default.svc.cluster.local,elasticsearch-1.elasticsearch.default.svc.cluster.local,elasticsearch-2.elasticsearch.default.svc.cluster.local"</pre>
<p class="code-line">This configures our nodes in the same way as<span> </span><kbd>pssh</kbd>, but with the added benefit of configuration-as-code, since it's now part of our<span> </span><kbd>stateful-set.yaml</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the Elasticsearch service</h1>
                </header>
            
            <article>
                
<p class="code-line">With our<span> </span><kbd>stateful-set.yaml</kbd><span> </span>ready, it's time to deploy our Service and StatefulSet onto our remote cloud cluster.</p>
<p class="code-line">At the moment, our remote cluster is not running anything apart from the Kubernetes Master Components:</p>
<pre><strong>$ kubectl get all</strong><br/><strong>NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</strong><br/><strong>service/kubernetes   ClusterIP   10.32.0.1    &lt;none&gt;        443/TCP   17h</strong></pre>
<div class="code-line packt_infobox"><br/>
The Kubernetes Master Components are automatically deployed when we create a new cluster using DigitalOcean.</div>
<p class="code-line">To deploy our Service and StatefulSet, we will use<span> </span><kbd>kubectl apply</kbd>:</p>
<pre><strong>$ kubectl apply -f manifests/elasticsearch/service.yaml</strong><br/><strong>service "elasticsearch" created</strong><br/><strong>$ kubectl apply -f manifests/elasticsearch/stateful-set.yaml</strong><br/><strong>statefulset.apps "elasticsearch" created</strong></pre>
<p class="code-line">Give it a minute or so, and run<span> </span><kbd>kubectl get all</kbd><span> </span>again. You should see that the Pods, StatefulSet, and our headless Service are running successfully!</p>
<pre><strong>$ kubectl get all</strong><br/><strong>NAME                  READY     STATUS    RESTARTS   AGE</strong><br/><strong>pod/elasticsearch-0   1/1       Running   0          1m</strong><br/><strong>pod/elasticsearch-1   1/1       Running   0          1m</strong><br/><strong>pod/elasticsearch-2   1/1       Running   0          10s</strong><br/><br/><strong>NAME                    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE</strong><br/><strong>service/elasticsearch   ClusterIP   None         &lt;none&gt;        9200/TCP,9300/TCP   1m</strong><br/><strong>service/kubernetes      ClusterIP   10.32.0.1    &lt;none&gt;        443/TCP             18h</strong><br/><br/><strong>NAME                             DESIRED   CURRENT   AGE</strong><br/><strong>statefulset.apps/elasticsearch   3         3         1m</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Validating Zen Discovery on the remote cluster</h1>
                </header>
            
            <article>
                
<p class="code-line">Let's validate that all three Elasticsearch nodes has been successfully added to the Elasticsearch cluster once more. We can do this by sending a<span> </span><kbd>GET</kbd><span> </span>request to<span> </span><kbd>/_cluster/state?pretty</kbd><span> </span>and checking the output.</p>
<p class="code-line">But since we want to keep the database service internal, we haven't exposed it to an external-reachable URL, so the only way to validate this is to SSH into one of the VPS and query Elasticsearch using its private IP.</p>
<p class="code-line">However,<span> </span><kbd>kubectl</kbd><span> </span>provides a more convenient alternative.<span> </span><kbd>kubectl</kbd><span> </span>has a<span> </span><kbd>port-forward</kbd><span> </span>command, which forwards requests going into a port on<span> </span><kbd>localhost</kbd><span> </span>to a port on one of the Pods. We can use this feature to send requests from our local machine to each Elasticsearch instance.</p>
<p class="code-line">Let's suppose that we have three Pods running Elasticsearch:</p>
<pre><strong>$ kubectl get pods</strong><br/><strong>NAME              READY   STATUS    RESTARTS   AGE</strong><br/><strong>elasticsearch-0   1/1     Running   0          34m</strong><br/><strong>elasticsearch-1   1/1     Running   0          34m</strong><br/><strong>elasticsearch-2   1/1     Running   0          34m</strong></pre>
<p class="code-line">We can set up port forward on<span> </span><kbd>elasticsearch-0</kbd><span> </span>by running the following:</p>
<pre><strong>$ kubectl port-forward elasticsearch-0 9200:9200</strong><br/><strong>Forwarding from 127.0.0.1:9200 -&gt; 9200</strong><br/><strong>Forwarding from [::1]:9200 -&gt; 9200</strong></pre>
<p class="code-line">Now, on a separate terminal, send a<span> </span><kbd>GET</kbd><span> </span>request to<span> </span><kbd>http://localhost:9200/_cluster/state?pretty</kbd>:</p>
<pre><strong>$ curl http://localhost:9200/_cluster/state?pretty</strong><br/><strong>{</strong><br/><strong>  "cluster_name" : "docker-cluster",</strong><br/><strong>  "state_uuid" : "rTHLkSYrQIu5E6rcGJZpCA",</strong><br/><strong>  "master_node" : "TcYdL65VSb-W1ZzXPfB8aA",</strong><br/><strong>  "nodes" : {</strong><br/><strong>    "ns1ZaCTCS9ywDSntHz94vg" : {</strong><br/><strong>      "name" : "ns1ZaCT",</strong><br/><strong>      "ephemeral_id" : "PqwcVrldTOyKSfQ-ZfhoUQ",</strong><br/><strong>      "transport_address" : "10.244.24.2:9300",</strong><br/><strong>      "attributes" : { }</strong><br/><strong>    },</strong><br/><strong>    "94Q-t8Y8SJiXnwVzsGcdyA" : {</strong><br/><strong>      "name" : "94Q-t8Y",</strong><br/><strong>      "ephemeral_id" : "n-7ew1dKSL2LLKzA-chhUA",</strong><br/><strong>      "transport_address" : "10.244.18.3:9300",</strong><br/><strong>      "attributes" : { }</strong><br/><strong>    },</strong><br/><strong>    "TcYdL65VSb-W1ZzXPfB8aA" : {</strong><br/><strong>      "name" : "TcYdL65",</strong><br/><strong>      "ephemeral_id" : "pcghJOnTSgmB8xMh4DKSHA",</strong><br/><strong>      "transport_address" : "10.244.75.3:9300",</strong><br/><strong>      "attributes" : { }</strong><br/><strong>    }</strong><br/><strong>  },</strong><br/><strong>  "metadata" : {</strong><br/><strong>    "cluster_uuid" : "ZF1t_X_XT0q5SPANvzE4Nw",</strong><br/><strong>    ...</strong><br/><strong>  },</strong><br/><strong>  ...</strong><br/><strong>}</strong></pre>
<p class="code-line">As you can see, the<span> </span><kbd>node</kbd><span> </span>field contains three objects, representing each of our Elasticsearch instances. They are all part of the cluster, with a<span> </span><kbd>cluster_uuid</kbd><span> value of </span><kbd>ZF1t_X_XT0q5SPANvzE4Nw</kbd>. Try port forwarding to the other Pods, and confirm that the<span> </span><kbd>cluster_uuid</kbd><span> </span>for those nodes are the same.</p>
<p class="code-line">If everything worked, we have now successfully deployed the same Elasticsearch service on DigitalOcean!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Persisting data</h1>
                </header>
            
            <article>
                
<p class="code-line">However, we're not finished yet! Right now, if all of our Elasticsearch containers fail, the data stored inside them would be lost.</p>
<p class="code-line">This is because containers are<span> </span><em>ephemeral</em>, meaning that any file changes inside the container, be it addition or deletion, only persist for as long as the container persists; once the container is gone, the changes are gone.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="code-line">This is fine for stateless applications, but our Elasticsearch service's primary purpose is to hold state. Therefore, similar to how we persist data using Volumes in Docker, we need to do the same with Kubernetes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Kubernetes Volumes</h1>
                </header>
            
            <article>
                
<p class="code-line">Like Docker, Kubernetes has an API Object that's also called Volume, but there are several differences between the two.</p>
<p class="code-line">With both Docker and Kubernetes, the storage solution that backs a Volume can be a directory on the host machine, or it can be a part of a cloud solution like AWS.</p>
<p class="code-line">And for both Docker and Kubernetes, a Volume is an abstraction for a piece of storage that can be attached or mounted. The difference is which resource it is mounted to.</p>
<p class="code-line">With Docker Volumes, the storage is mounted on to a directory inside the container. Any changes made to the contents of this directory would be accessible by both the host machine and the container.</p>
<p class="code-line">With Kubernetes Volumes, the storage is mapped to a directory inside a Pod. Containers within the same Pod has access to the Pod's Volume. This allows containers inside the same Pod to share information easily.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining Volumes</h1>
                </header>
            
            <article>
                
<p class="code-line">Volumes are created by specifying information about the Volume in the<span> </span><kbd>.spec.volumes</kbd><span> </span>field inside a Pod manifest file. The following manifest snippet will create a Volume of type<span> </span><kbd>hostPath</kbd>, using the parameters defined in the<span> </span><kbd>path</kbd><span> </span>and<span> </span><kbd>type</kbd><span> </span>properties.</p>
<p class="code-line"><kbd>hostPath</kbd><span> </span>is the Volume type most similar to a Docker Volume, where the Volume exists as a directory from the host node's filesystem:</p>
<pre>apiVersion: v1<br/>kind: Pod<br/>spec:<br/>  ...<br/>  volumes:<br/>  - name: host-volume<br/>    hostPath:<br/>      path: /data<br/>      type: Directory</pre>
<p class="code-line">This Volume will now be available to all containers within the Pod. However, the Volume is not automatically mounted onto each container. This is done by design because not all containers may need to use the Volume; it allows the configuration to be explicit rather than implicit.</p>
<p class="code-line">To mount the Volume to a container, specify the<span> </span><kbd>volumeMounts</kbd><span> </span>option in the container's specification:</p>
<pre>apiVersion: v1<br/>kind: Pod<br/>spec:<br/>  containers:<br/>    - name: elasticsearch<br/>      image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.4<br/>      ports:<br/>        - containerPort: 9200<br/>        - containerPort: 9300<br/>      env:<br/>        - name: discovery.type<br/>          value: single-node<br/>      volumeMounts:<br/>        - mountPath: /usr/share/elasticsearch/data<br/>          name: host-volume<br/>  ...</pre>
<p class="code-line">The<span> </span><kbd>mountPath</kbd><span> </span>specifies the directory inside the container where the Volume should be mounted at.</p>
<p class="code-line">To run this Pod, you first need to create a<span> </span><kbd>/data</kbd><span> </span>directory on your host machine and change its ownership to having a<span> </span><kbd>UID</kbd><span> </span>and<span> </span><kbd>GID</kbd><span> </span>of<span> </span><kbd>1000</kbd>:</p>
<pre><strong>$ sudo mkdir data</strong><br/><strong>$ sudo chown 1000:1000 /data</strong></pre>
<p class="mce-root"/>
<p class="code-line">Now, when we run this Pod, you should be able to query it on<span> </span><kbd>&lt;pod-ip&gt;:9200</kbd><span> </span>and see the content written to the<span> </span><kbd>/data</kbd><span> </span>directory:</p>
<pre><strong>$ tree /data</strong><br/><strong>data/</strong><br/><strong>└── nodes</strong><br/><strong>    └── 0</strong><br/><strong>        ├── node.lock</strong><br/><strong>        └── _state</strong><br/><strong>            ├── global-0.st</strong><br/><strong>            └── node-0.st</strong><br/><br/><strong>3 directories, 3 files</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problems with manually-managed Volumes</h1>
                </header>
            
            <article>
                
<p class="code-line">While you can use Volumes to persists data for individual Pods, this won't work for our StatefulSet. This is because each of the replica Elasticsearch nodes will try to write to the same files at the same time; only one will succeed, the others will fail. If you tried, the following hanged state is what you'll encounter:</p>
<pre><strong>$ kubectl get pods</strong><br/><strong>NAME              READY   STATUS             RESTARTS</strong><br/><strong>elasticsearch-0   1/1     Running            0       </strong><br/><strong>elasticsearch-1   0/1     CrashLoopBackOff   7       </strong><br/><strong>elasticsearch-2   0/1     CrashLoopBackOff   7 </strong>      </pre>
<p class="code-line">If we use<span> </span><kbd>kubectl logs</kbd><span> </span>to inspect one of the failing Pods, you'll see the following error message:</p>
<pre><strong>$ kubectl logs elasticsearch-1</strong><br/><strong>[WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]</strong><br/><strong>org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/data/docker-cluster]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?</strong></pre>
<p class="code-line">Basically, before an Elasticsearch instance is writing to the database files, it creates a<span> </span><kbd>node.lock</kbd><span> </span>file. Before other instances try to write to the same files, it will detect this<span> </span><kbd>node.lock</kbd><span> </span>file and abort.</p>
<p class="code-line">Apart from this issue, attaching Volumes directly to Pods is not good for another reason—Volumes persist data at the Pod-level, but Pods can get rescheduled to other Nodes. When this happens, the "old" Pod is destroyed, along with its associated Volume, and a new Pod is deployed on a different Node with a blank Volume.</p>
<p class="code-line">Finally, scaling storage this way is also difficult—if the Pod requires more storage, you'll have to destroy the Pod (so it doesn't write anything to the Volume, create a new Volume, copy contents from the old Volume to the new, and then restart the Pod).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing PersistentVolume (PV)</h1>
                </header>
            
            <article>
                
<p class="code-line">To tackle these issues, Kubernetes provides the<span> </span>PersistentVolume<span> </span>(PV) object. PersistentVolume is a variation of the Volume Object, but the storage capability is associated with the entire cluster, and not with any particular Pod.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consuming PVs with PersistentVolumeClaim (PVC)</h1>
                </header>
            
            <article>
                
<p class="code-line">When an administrator wants a Pod to use storage provided by a PV, the administrator would create a new<span> </span><strong>PersistentVolumeClaim</strong><span> </span>(<strong>PVC</strong>) object and assign that PVC Object to the Pod. A PVC object is simply a request for a suitable PV to be bound to the PVC (and thus the Pod).</p>
<p class="code-line">After the PVC has been registered with the Master Control Plane, the Master Control Plane would search for a PV that satisfies the criteria laid out in the PVC, and bind the two together. For instance, if the PVC requests a PV with at least 5 GB of storage space, the Master Control Plane will only bind that PVC with PVs which have at least 5 GB of space.</p>
<p class="code-line">After the PVC has been bound to the PV, the Pod would be able to read and write to the storage media backing the PV.</p>
<p class="code-line">A PVC-to-PV binding is a one-to-one mapping; this means when a Pod is rescheduled, the same PV would be associated with the Pod.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting a PersistentVolumeClaim</h1>
                </header>
            
            <article>
                
<p class="code-line">When a Pod no longer needs to use the PersistentVolume, the PVC can simply be deleted. When this happens, what happens to the data stored inside the storage media depends on the<span> </span>PersistentVolume's Reclaim Poli<em>cy</em>.</p>
<p class="code-line">If the Reclaim Policy is set to:</p>
<ul>
<li class="code-line">Retain, the PV is retained—the PVC is simply released/unbounded from the PV. The data in the storage media is retained.</li>
<li class="code-line">Delete, it deletes both the PV and the data in the storage media.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deleting a PersistentVolume</h1>
                </header>
            
            <article>
                
<p class="code-line">When you no longer need a PV, you can delete it. But because the actually data is stored externally, the data will remain in the storage media.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problems with manually provisioning PersistentVolume</h1>
                </header>
            
            <article>
                
<p class="code-line">Whil a<span> </span>PersistentVolume<span> </span>decouples storage from individual Pods, it still lacks the automation that we've come to expect from Kubernetes, because the cluster administrator (you) must manually interact with their cloud provider to provision new storage spaces, and then create<span> a </span>PersistentVolume<span> </span>to represent them in Kubernetes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8b4d4ff5-c69d-437c-9c3f-f996836024c1.png" style="width:31.25em;height:18.42em;"/></p>
<p class="code-line">Furthermore, a PVC to PV binding is a one-to-one mapping; this means we must take care when creating our PVs. For instance, let's suppose we have 2 PVCs—one requesting 10 GB and the other 40 GB. If we register two PVs, each of size 25GB, then only the 10 GB PVC would succeed, even though there is enough storage space for both PVCs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic volume provisioning with StorageClass</h1>
                </header>
            
            <article>
                
<p class="code-line">To resolve these issues, Kubernetes provides another API Object called<span> </span><kbd>StorageClass</kbd>. With<span> </span><kbd>StorageClass</kbd>, Kubernetes is able to interact with the cloud provider directly. This allows Kubernetes to provision new storage volumes, and create<span> </span><kbd>PersistentVolumes</kbd> automatically.</p>
<p class="code-line">Basically, a <kbd>PersistentVolume</kbd> is a representation of a piece of storage, whereas <kbd>StorageClass</kbd> is a specification of<span> </span><em>how</em><span> </span>to create<span> </span><kbd>PersistentVolumes</kbd><span> </span><em>dynamically</em>. <kbd>StorageClass</kbd> abstracts the manual processes into a set of fields you can specify inside a manifest file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining a StorageClass</h1>
                </header>
            
            <article>
                
<p class="code-line">For example, if you want to create a<span> </span><kbd>StorageClass</kbd><span> </span>that will create Amazon EBS Volume of type General Purpose SSD (<kbd>gp2</kbd>), you'd define a<span> </span>StorageClass<span> </span>manifest like so:</p>
<pre>kind: StorageClass<br/>apiVersion: storage.k8s.io/v1<br/>metadata:<br/>  name: standard<br/>provisioner: kubernetes.io/aws-ebs<br/>parameters:<br/>  type: gp2<br/>reclaimPolicy: Retain</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="code-line">Here's what each field means (required fields are marked with an asterik (<kbd>*</kbd>):</p>
<ul>
<li class="code-line"><kbd>apiVersion</kbd>: The<span> </span><kbd>StorageClass</kbd><span> </span>object is provided in the<span> </span><kbd>storage.k8s.io</kbd><span> </span>API group.</li>
<li class="code-line"><kbd>*provisioner</kbd>: The name of a<span> </span><em>provisioner</em><span> </span>that would prepare new storage spaces on-demand. For instance, if a Pod requests 10 GB of block storage from the<span> </span><kbd>standard</kbd><span> </span>StorageClass, then the<span> </span><kbd>kubernetes.io/aws-ebs</kbd><span> </span>provisioner will interact directly with AWS to create a new storage volume of at least 10 GB in size.</li>
<li class="code-line"><kbd>*parameters</kbd>: The parameters that are passed to the provisioner so it knows how to provision the storage. Valid parameters depends on the provisioner. For example, both <kbd>kubernetes.io/aws-ebs</kbd><span> </span>and<span> </span><kbd>kubernetes.io/gce-pd</kbd><span> </span>support the<span> </span><kbd>type</kbd><span> </span>parameter.</li>
</ul>
<ul>
<li class="code-line"><kbd>*reclaimPolicy</kbd>: As with <kbd>PersistentVolumes,</kbd> the Reclaim Policy determines whether the data written to the storage media is retained or deleted. This can be either<span> </span><kbd>Delete</kbd><span> </span>or<span> </span><kbd>Retain</kbd>, but it defaults to<span> </span><kbd>Delete</kbd>.</li>
</ul>
<div class="code-line packt_infobox">There are many types of provisioners available. Amazon EBS provisions<span> </span><em>Block storage</em><span> </span>on AWS, but there are other types of storage, namely file and object storage. We will be using block storage here because it provides the lowest latency, and is suitable for use with our Elasticsearch database.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the csi-digitalocean provisioner</h1>
                </header>
            
            <article>
                
<p class="code-line">DigitalOcean provides its own provisioner called CSI-DigitalOcean (<a href="https://github.com/digitalocean/csi-digitalocean">https://github.com/digitalocean/csi-digitalocean</a>). To use it, simply follow the instructions in the<span> </span><kbd>README.md</kbd><span> </span>file. Essentially, you have go to the DigitalOcean dashboard, generate a token, use that to generate a Secret Kubernetes Object, and then deploy the StorageClass manifest file found at<span> </span><a href="https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-latest-stable.yaml">https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-latest-stable.yaml</a>.</p>
<p class="code-line">However, because we are using the DigitalOcean Kubernetes platform, our Secret and the<span> </span><kbd>csi-digitaloceanstorage</kbd><span> </span>class is already configured for us, so we don't actually need to do anything! You can check both the Secret and StorageClass using<span> </span><kbd>kubectl get</kbd>:</p>
<pre><strong>$ kubectl get secret</strong><br/><strong>NAME TYPE DATA AGE</strong><br/><strong>default-token-2r8zr kubernetes.io/service-account-token 3 2h</strong><br/><br/><strong>$ kubectl get storageclass</strong><br/><strong>NAME PROVISIONER AGE</strong><br/><strong>do-block-storage (default) com.digitalocean.csi.dobs 2h</strong></pre>
<p class="code-line">Note down the name of the StorageClass (<kbd>do-block-storage</kbd><span> </span>here).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Provisioning PersistentVolume to StatefulSet</h1>
                </header>
            
            <article>
                
<p class="code-line">We now need to update our<span> </span><kbd>stateful-set.yaml</kbd><span> file </span>to use the<span> </span><kbd>do-block-storage</kbd><span> </span>StorageClass. Under the StatefulSet spec (<kbd>.spec</kbd>), add a new field called<span> </span><kbd>volumeClaimTemplates</kbd><span> </span>with the following value:</p>
<pre>apiVersion: apps/v1<br/>kind: StatefulSet<br/>metadata: ...<br/>spec:<br/>  volumeClaimTemplates:<br/>  - metadata:<br/>      name: data<br/>    spec:<br/>      accessModes:<br/>      - ReadWriteOnce<br/>      resources:<br/>        requests:<br/>          storage: 2Gi<br/>      storageClassName: do-block-storage</pre>
<p class="code-line">This will use the<span> </span><kbd>do-block-storage</kbd><span> </span>class to dynamically provision 2 GB <kbd>PersistentVolumeClaim</kbd> Objects for any containers which mount it. The PVC is given the name<span> </span><kbd>data</kbd><span> </span>as a reference.</p>
<p class="code-line">To mount it to a container, add a<span> </span><kbd>volumeMounts</kbd><span> </span>property under the<span> </span><kbd>spec</kbd><span> </span>property of the container spec:</p>
<pre>apiVersion: apps/v1<br/>kind: StatefulSet<br/>metadata: ...<br/>spec:<br/>  ...<br/>  template:<br/>    ...<br/>    spec:<br/>      initContainers: ...<br/>      containers: ...<br/>      volumeMounts:<br/>        - name: data<br/>          mountPath: /usr/share/elasticsearch/data<br/>  volumeClaimTemplates: ...</pre>
<p class="code-line">Elasticsearch writes its data to<span> </span><kbd>/usr/share/elasticsearch/data</kbd><span>, </span>so that's the data we want to persist.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring permissions on a bind-mounted directory</h1>
                </header>
            
            <article>
                
<p class="code-line">By default, Elasticsearch runs inside the Docker container as the user<span> </span><kbd>elasticsearch</kbd>, with both<span> a </span><kbd>UID</kbd><span> </span>and<span> </span><kbd>GID</kbd><span> </span>of<span> </span><kbd>1000</kbd>. Therefore, we must ensure that the data directory (<kbd>/usr/share/elasticsearch/data</kbd>) and all its content is going to be owned by this the<span> </span><kbd>elasticsearch</kbd><span> </span>user so that Elasticsearch can write to them.</p>
<p class="code-line">When Kubernetes bind-mounted the <kbd>PersistentVolume</kbd> to our<span> </span><kbd>/usr/share/elasticsearch/data</kbd>, it was done using the<span> </span><kbd>root</kbd><span> </span>user. This means that the<span> </span><kbd>/usr/share/elasticsearch/data</kbd><span> </span>directory is no longer owned by the<span> </span><kbd>elasticsearch</kbd><span> </span>user.</p>
<p class="code-line">Therefore, to complete our deployment of Elasticsearch, we need to use an Init Container to fix our permissions. This can be done by running<span> </span><kbd>chown -R 1000:1000 /usr/share/elasticsearch/data</kbd><span> </span>on the node as<span> </span><kbd>root</kbd>.</p>
<p class="code-line">Add the following entry to the<span> </span><kbd>initContainers</kbd><span> </span>array inside<span> </span><kbd>stateful-set.yaml</kbd>:</p>
<pre>- name: fix-volume-permission<br/>  image: busybox<br/>  command:<br/>  - sh<br/>  - -c<br/>  - chown -R 1000:1000 /usr/share/elasticsearch/data<br/>  securityContext:<br/>    privileged: true<br/>  volumeMounts:<br/>  - name: data<br/>    mountPath: /usr/share/elasticsearch/data</pre>
<p class="code-line">This basically mounts the <kbd>PersistentVolume</kbd> and updates its owner before the app Container starts initializing, so that the correct permissions would already be set when the app container executes. To summarize, your final<span> </span><kbd>elasticsearch/service.yaml</kbd><span> </span>should look like this:</p>
<pre>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: elasticsearch<br/>  labels:<br/>    app: elasticsearch<br/>spec:<br/>  selector:<br/>    app: elasticsearch<br/>  clusterIP: None<br/>  ports:<br/>  - port: 9200<br/>    name: rest<br/>  - port: 9300<br/>    name: transport</pre>
<p class="code-line">And your final<span> </span><kbd>elasticsearch/stateful-set.yaml</kbd><span> </span>should look like this:</p>
<pre>apiVersion: apps/v1<br/>kind: StatefulSet<br/>metadata:<br/>  name: elasticsearch<br/>  labels:<br/>    app: elasticsearch<br/>spec:<br/>  replicas: 3<br/>  serviceName: elasticsearch<br/>  selector:<br/>    matchLabels:<br/>      app: elasticsearch<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: elasticsearch<br/>    spec:<br/>      initContainers:<br/>      - name: increase-max-map-count<br/>        image: busybox<br/>        command:<br/>        - sysctl<br/>        - -w<br/>        - vm.max_map_count=262144<br/>        securityContext:<br/>          privileged: true<br/>      - name: increase-file-descriptor-limit<br/>        image: busybox<br/>        command:<br/>        - sh<br/>        - -c<br/>        - ulimit -n 65536<br/>        securityContext:<br/>          privileged: true<br/>      containers:<br/>        - name: elasticsearch<br/>          image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2<br/>          ports:<br/>          - containerPort: 9200<br/>            name: http<br/>          - containerPort: 9300<br/>            name: tcp<br/>          env:<br/>            - name: discovery.zen.ping.unicast.hosts<br/>              value: "elasticsearch-0.elasticsearch.default.svc.cluster.local,elasticsearch-1.elasticsearch.default.svc.cluster.local,elasticsearch-2.elasticsearch.default.svc.cluster.local"<br/>      volumeMounts:<br/>        - name: data<br/>          mountPath: /usr/share/elasticsearch/data<br/>  volumeClaimTemplates:<br/>  - metadata:<br/>      name: data<br/>    spec:<br/>      accessModes:<br/>      - ReadWriteOnce<br/>      resources:<br/>        requests:<br/>          storage: 2Gi<br/>      storageClassName: do-block-storage</pre>
<p class="code-line">Delete your existing Services, StatefulSets, and Pods and try deploying them from scratch:</p>
<pre><strong>$ kubectl apply -f ./manifests/elasticsearch/service.yaml</strong><br/><strong>service "elasticsearch" created</strong><br/><strong>$ kubectl apply -f ./manifests/elasticsearch/stateful-set.yaml</strong><br/><strong>statefulset.apps "elasticsearch" created</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing Kubernetes Objects using the Web UI Dashboard</h1>
                </header>
            
            <article>
                
<p class="code-line">You've been introduced to<span> </span><em>a lot</em><span> </span>of Kubernetes in this chapter—Namespaces, Nodes, Pods, Deployments, ReplicaSet, StatefulSet, DaemonSet, Services, Volumes, PersistentVolumes, and StorageClasses. So, let's take a mini-breather before we continue.</p>
<p class="code-line">So far, we've been using<span> </span><kbd>kubectl</kbd><span> </span>for everything. While<span> </span><kbd>kubectl</kbd><span> </span>is great, sometimes, visual tools can help. The Kubernetes project provides a convenient Web UI Dashboard that allows you to visualize all Kubernetes Objects easily.</p>
<div class="code-line packt_infobox"><br/>
The Kubernetes Web UI Dashboard is different from the DigitalOcean Dashboard.</div>
<p class="code-line">Both<span> </span><kbd>kubectl</kbd><span> </span>and the Web UI Dashboard make calls to the<span> </span><kbd>kube-apiserver</kbd>, but the former is a command-line tool, whereas the latter provides a web interface.</p>
<p class="code-line">By default, the Web UI Dashboard is not deployed automatically. We'd normally need to run the following to get an instance of the Dashboard running on our cluster:</p>
<pre><strong>$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</strong></pre>
<p class="code-line">However, both DigitalOcean and Minikube deploy this Dashboard feature by default, so we don't need to deploy anything.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching the Web UI Dashboard locally</h1>
                </header>
            
            <article>
                
<p class="code-line">To launch the Web UI Dashboard for your local cluster, run<span> </span><kbd>minikube dashboard</kbd>. This will open a new tab on your web browser with an Overview screen like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/77b2e134-9ad8-4da4-b156-31fc7c23c35a.png"/></p>
<p class="code-line">You can use the menu on the left to navigate and view other Kubernetes Objects currently running in our cluster:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/9d1dcb8b-1096-4beb-8124-f0feb93737f3.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching the Web UI Dashboard on a remote cluster</h1>
                </header>
            
            <article>
                
<p class="code-line">To access the Web UI Dashboard deployed on the remote cluster, the easier method is to use<span> </span>kubectl proxy<span> </span>to access the remote cluster's Kubernetes API. Simply run<span> </span><kbd>kubectl proxy</kbd><span>, </span>and the Web UI Dashboard should be available at<span> </span><a href="http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a>.</p>
<p class="code-line">We will continue using<span> </span><kbd>kubectl</kbd><span> </span>for the rest of this chapter, but feel free to switch to the Web UI Dashboard to get a more intuitive view of the cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the backend API</h1>
                </header>
            
            <article>
                
<p class="code-line">We've deployed Elasticsearch, so let's carry on with the rest of the deployment—of our backend API and our frontend application.</p>
<p class="code-line">The<span> </span><kbd>elasticsearch</kbd><span> </span>Docker image used in the deployment was available publicly. However, our backend API Docker image is not available anywhere, and thus our remote Kubernetes cluster won't be able to pull and deploy it.</p>
<p class="code-line">Therefore, we need to build our Docker images and make it available on a Docker registry. If we don't mind our image being downloaded by others, we can publish it on a public registry like Docker Hub. If we want to control access to our image, we need to deploy it on a private registry.</p>
<p class="code-line">For simplicity's sake, we will simply publish our images publicly on Docker Hub.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Publishing our image to Docker Hub</h1>
                </header>
            
            <article>
                
<p class="code-line">First, go to<span> </span><a href="https://hub.docker.com/">https://hub.docker.com/</a><span> </span>and create an account with Docker Hub. Make sure to verify your email.</p>
<p class="code-line">Then, click on <span class="packt_screen">Create</span> | <span class="packt_screen">c</span><span class="packt_screen">reate Repository</span> at the top navigation. Give the repository a unique name and press <span class="packt_screen">Create</span>. You can set the repository to <span class="packt_screen">Public</span> or <span class="packt_screen">Private</span> as per your own preferences (at the time of writing this book, Docker Hub provides one free private repository):</p>
<p class="mce-root"/>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/6031fa93-6807-4158-8e27-de32a7e36ce0.png"/></p>
<p class="code-line">The repository can be identified using<span> </span><kbd>&lt;namespace&gt;/&lt;repository-name&gt;</kbd>, where the namespace is simply your Docker Hub username. You can find it on Docker Hub via the URL<span> </span><kbd>hub.docker.com/r/&lt;namespace&gt;/&lt;repository-name&gt;/</kbd>.</p>
<div class="code-line packt_infobox"><br/>
If you have an organization, the namespace may be the name of the organization.</div>
<p class="code-line">Next, return to your terminal and login using your Docker Hub credentials. For example, my Docker Hub username is<span> </span><kbd>d4nyll</kbd>, so I would run the following:</p>
<pre><strong>$ docker login --username d4nyll</strong></pre>
<p class="code-line">Enter your password when prompted, and you should see a message informing you of your<span> </span><kbd>Login Succeeded</kbd>. Next, build the image (if you haven't already):</p>
<pre><strong>$ docker build -t hobnob:0.1.0 . --no-cache</strong><br/><strong>Sending build context to Docker daemon 359.4kB</strong><br/><strong>Step 1/13 : FROM node:8-alpine as builder</strong><br/><strong>...</strong><br/><strong>Successfully built 3f2d6a073e1a</strong></pre>
<p class="code-line">Then, tag the local image with the full repository name on Docker Hub, as well as a tag that'll appear on Docker Hub to distinguish between different versions of your image. The<span> </span><kbd>docker tag</kbd><span> </span>command you should run will have the following structure:</p>
<pre><strong>$ docker tag &lt;local-image-name&gt;:&lt;local-image-tag&gt; &lt;hub-namespace&gt;/&lt;hub-repository-name&gt;:&lt;hub-tag&gt;</strong></pre>
<p class="code-line">In my example, I would run the following:</p>
<pre><strong>$ docker tag hobnob:0.1.0 d4nyll/hobnob:0.1.0</strong></pre>
<p class="code-line">Lastly, push the image onto Docker Hub:</p>
<pre><strong>$ docker push d4nyll/hobnob</strong><br/><strong>The push refers to repository [docker.io/d4nyll/hobnob]</strong><br/><strong>90e19b6c8d6d: Pushed</strong><br/><strong>49fb9451c65f: Mounted from library/node</strong><br/><strong>7d863d91deaa: Mounted from library/node</strong><br/><strong>8dfad2055603: Mounted from library/node</strong><br/><strong>0.1.0: digest: sha256:21610fecafb5fd8d84a0844feff4fdca5458a1852650dda6e13465adf7ee0608 size: 1163</strong></pre>
<p class="code-line">Confirm it has been successfully pushed by going to<span> </span><kbd>https://hub.docker.com/r/&lt;namespace&gt;/&lt;repository-name&gt;/tags/</kbd>. You should see the tagged image appear there.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Deployment</h1>
                </header>
            
            <article>
                
<p class="code-line">Since our backend API is a stateless application, we don't need to deploy a StatefulSet like we did with Elasticsearch. We can simply use a simpler Kubernetes Object that we've encountered already—Deployment.</p>
<p class="code-line">Create a new manifest at<span> </span><kbd>manifests/backend/deployment.yaml</kbd><span> </span>with the following content:</p>
<pre>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: backend<br/>  labels:<br/>    app: backend<br/>spec:<br/>  selector:<br/>    matchLabels:<br/>      app: backend<br/>  replicas: 3<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: backend<br/>    spec:<br/>      containers:<br/>      - name: backend<br/>        image: d4nyll/hobnob:0.1.0<br/>        ports:<br/>        - containerPort: 8080<br/>          name: api<br/>        - containerPort: 8100<br/>          name: docs<br/>        env:<br/>        - name: ELASTICSEARCH_HOSTNAME<br/>          value: "http://elasticsearch"<br/>        - name: ELASTICSEARCH_PORT<br/>          value: "9200"<br/>        ...</pre>
<p class="code-line">For the<span> </span><kbd>.spec.template.spec.containers[].env</kbd><span> </span>field, add in the same environment variables that we passed in to our Docker image from the previous chapter (the ones we stored inside our<span> </span><kbd>.env</kbd><span> </span>file). However, for the<span> </span><kbd>ELASTICSEARCH_PORT</kbd><span> </span>variable, hard-code it to<span> </span><kbd>"9200"</kbd>, and for<span> </span><kbd>ELASTICSEARCH_HOSTNAME</kbd>, use the value<span> </span><kbd>"http://elasticsearch"</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discovering Services using kube-dns/CoreDNS</h1>
                </header>
            
            <article>
                
<p class="code-line">While Kubernetes Components constitutes the essential parts of the Kubernetes platform, there are also<span> </span><em>add-ons</em>, which extend the core functionalities. They are optional, but some are highly recommended and are often included by default. In fact, the Web UI Dashboard is an example of an add-on.</p>
<p class="code-line">Another such add-on is<span> </span><kbd>kube-dns</kbd>, a DNS server which is used by Pods to resolve hostnames.</p>
<div class="code-line packt_infobox"><em>CoreDNS</em><span> </span>is an alternative DNS server which reached <strong>General Availability</strong> (<strong>GA</strong>) status in Kubernetes 1.11, replacing the existing<span> </span><kbd>kube-dns</kbd><span> </span>addon as the default. For our purposes, they achieve the same results.</div>
<p class="code-line">This DNS server watches the Kubernetes API for new Services. When a new Service is created, a DNS record is created that would route the name<span> </span><kbd>&lt;service-name&gt;.&lt;service-namespace&gt;</kbd><span> </span>to the Service's Cluster IP. Or, in the case of a Headless Service (without a cluster IP), a list of IPs of the Pods that constitutes the Headless Service.</p>
<p class="code-line">This is why we can use<span> </span><kbd>"http://elasticsearch"</kbd><span> </span>as the value of the<span> </span><kbd>ELASTICSEARCH_HOSTNAME</kbd><span> </span>environment variable, because the DNS server will resolve it, even if the Service changes its IP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running Our backend Deployment</h1>
                </header>
            
            <article>
                
<p class="code-line">With our Deployment manifest ready, let's deploy it onto our remote cluster. You should be familiar with the drill by now—simply run<span> </span><kbd>kubectl apply</kbd>:</p>
<pre><strong>$ kubectl apply -f ./manifests/backend/deployment.yaml</strong><br/><strong>deployment.apps "backend" created</strong></pre>
<p class="code-line">Check the status of the Deployment using<span> </span><kbd>kubectl get all</kbd>:</p>
<pre><strong>$ kubectl get all</strong><br/><strong>NAME                           READY     STATUS    RESTARTS   AGE</strong><br/><strong>pod/backend-6d58f66658-6wx4f   1/1       Running   0          21s</strong><br/><strong>pod/backend-6d58f66658-rzwnl   1/1       Running   0          21s</strong><br/><strong>pod/backend-6d58f66658-wlsdz   1/1       Running   0          21s</strong><br/><strong>pod/elasticsearch-0            1/1       Running   0          18h</strong><br/><strong>pod/elasticsearch-1            1/1       Running   0          20h</strong><br/><strong>pod/elasticsearch-2            1/1       Running   0          20h</strong><br/><br/><strong>NAME                    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)          </strong><br/><strong>service/elasticsearch   ClusterIP   None         &lt;none&gt;        9200/TCP,9300/TCP</strong><br/><strong>service/kubernetes      ClusterIP   10.32.0.1    &lt;none&gt;        443/TCP          </strong><br/><br/><strong>NAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</strong><br/><strong>deployment.apps/backend   3         3         3            3           21s</strong><br/><br/><strong>NAME                                 DESIRED   CURRENT   READY     AGE</strong><br/><strong>replicaset.apps/backend-6d58f66658   3         3         3         21s</strong><br/><br/><strong>NAME                             DESIRED   CURRENT   AGE</strong><br/><strong>statefulset.apps/elasticsearch   3         3         20h</strong></pre>
<p class="code-line">You can also check the logs for the backend Pods. If you get back a message saying the server is listening on port<span> </span><kbd>8080</kbd>, the deployment was successful:</p>
<pre><strong>$ kubectl logs pod/backend-6d58f66658-6wx4f</strong><br/><strong>Hobnob API server listening on port 8080!</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a backend Service</h1>
                </header>
            
            <article>
                
<p class="code-line">Next, we should deploy a Service that sits in front of the backend Pods. As a recap, every<span> </span><kbd>backend</kbd><span> </span>Pod inside the<span> </span><kbd>backend</kbd><span> </span>Deployment will have its own IP address, but these addresses can change as Pods are destroyed and created. Having a Service that sits in front of these Pods allow other parts of the application to access these backend Pods in a consistent manner.</p>
<p class="code-line">Create a new manifest file at<span> </span><kbd>./manifests/backend/service.yaml</kbd><span> </span>with the following content:</p>
<pre>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: backend<br/>  labels:<br/>    app: backend<br/>spec:<br/>  selector:<br/>    app: backend<br/>  ports:</pre>
<pre>  - port: 8080<br/>    name: api<br/>  - port: 8100<br/>    name: docs</pre>
<p class="code-line">And deploy it using<span> </span><kbd>kubectl apply</kbd>:</p>
<pre><strong>$ kubectl apply -f ./manifests/backend/service.yaml</strong><br/><strong>service "backend" created</strong><br/><br/><strong>$ kubectl get services</strong><br/><strong>NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE</strong><br/><strong>backend         ClusterIP   10.32.187.38   &lt;none&gt;        8080/TCP,8100/TCP   4s</strong><br/><strong>elasticsearch   ClusterIP   None           &lt;none&gt;        9200/TCP,9300/TCP   1d</strong><br/><strong>kubernetes      ClusterIP   10.32.0.1      &lt;none&gt;        443/TCP             1d</strong></pre>
<p class="code-line">Our<span> </span><kbd>backend</kbd><span> </span>Service is now reachable through its Cluster IP (<kbd>10.32.187.38</kbd><span>, </span>in our example). However, that is a private IP address, accessible only within the cluster. We want our API to be available externally <span>–</span> to the wider internet. To do this, we need to look at one final Kubernetes Object—Ingress.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exposing services through Ingress</h1>
                </header>
            
            <article>
                
<p class="code-line">An Ingress is a Kubernetes Object that sits at the edge of the cluster and manages external access to Services inside the cluster.</p>
<p class="code-line">The Ingress holds a set of rules that takes inbound requests as parameters and routes them to the relevant Service. It can be used for routing, load balancing, terminate SSL, and more.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the NGINX Ingress Controller</h1>
                </header>
            
            <article>
                
<p class="code-line">An Ingress Object requires a Controller to enact it. Unlike other Kubernetes controllers, which are part of the<span> </span><kbd>kube-controller-manager</kbd><span> </span>binary, the Ingress controller is not. Apart from the GCE/Google Kubernetes Engine, the Ingress controller needs to be deployed separately as a Pod.</p>
<p class="code-line">The most popular Ingress controller is the NGINX controller (<a href="https://github.com/kubernetes/ingress-nginx">https://github.com/kubernetes/ingress-nginx</a>), which is officially supported by Kubernetes and NGINX. Deploy it by running<span> </span><kbd>kubectl apply</kbd>:</p>
<pre><strong>$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml</strong><br/><strong>$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml</strong></pre>
<p class="code-line">The<span> </span><kbd>mandatory.yaml</kbd><span> </span>file contains a Deployment manifest that deploys the NGINX Ingress controller as a Pod with the label<span> </span><kbd>app: ingress-nginx</kbd>.</p>
<p class="code-line">The<span> </span><kbd>cloud-generic.yaml</kbd><span> </span>file contains a Service manifest of type<span> </span><kbd>LoadBalancer</kbd>, with a label selector for the label<span> </span><kbd>app: ingress-nginx</kbd>. When deployed, this will interact with the DigitalOcean API to spin up an L4 network load balancer (note that this load balaner is<span> </span><em>outside</em><span> </span>our Kubernetes cluster):</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/dfe922fc-ec13-44db-b9f3-5357ae2ddaf0.png"/></p>
<p class="code-line">The L4 load balancer will provide an external IP address for our end users to hit. The Kubernetes service controller will automatically populate the L4 load balancer with entries for our Pods, and set up health checks and firewalls. The end result is that any requests that hits the L4 load balancer will be forwarded to Pods that matches the Service's selector, which, in our case, is the Ingress controller Pod:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/6c12f1ca-0d08-448d-92b3-6b1fb5f087a3.png" style="width:18.00em;height:20.58em;"/></p>
<p class="code-line">When the request reaches the Ingress controller Pod, it can then examine the host and path of the request, and proxy the request to the relevant Service:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/9a1f616c-c407-4cd1-8c34-1e9d6673ea2b.png" style="width:38.00em;height:20.50em;"/></p>
<p class="code-line">Give it a minute or two, and then check that the controller is created successfully by running<span> </span><kbd>kubectl get pods</kbd>, specifying<span> </span><kbd>ingress-nginx</kbd><span> </span>as the namespace:</p>
<pre><strong>$ kubectl get pods --namespace=ingress-nginx</strong><br/><strong>NAME READY STATUS RESTARTS AGE</strong><br/><strong>default-http-backend-5c6d95c48-8tjc5 1/1 Running 0 1m</strong><br/><strong>nginx-ingress-controller-6b9b6f7957-7tvp7 1/1 Running 0 1m</strong></pre>
<p class="code-line">If you see a Pod named<span> </span><kbd>nginx-ingress-controller-XXX</kbd><span> </span>with the status<span> </span><kbd>Running</kbd>, you're ready to go!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the Ingress resource</h1>
                </header>
            
            <article>
                
<p class="code-line">Now that our Ingress controller is running, we are ready to deploy our Ingress resource. Create a new manifest file at<span> </span><kbd>./manifests/backend/ingress.yaml</kbd><span> </span>with the following content:</p>
<pre>apiVersion: extensions/v1beta1<br/>kind: Ingress<br/>metadata:<br/>  name: test-ingress<br/>  annotations:<br/>    nginx.ingress.kubernetes.io/rewrite-target: /<br/>spec:<br/>  rules:<br/>  - host: api.hobnob.social<br/>    http:<br/>      paths:<br/>      - backend:<br/>          serviceName: backend<br/>          servicePort: 8080<br/>  - host: docs.hobnob.social<br/>    http:<br/>      paths:<br/>      - backend:<br/>          serviceName: backend<br/>          servicePort: 8100</pre>
<p class="code-line">The important parts lies at<span> </span><kbd>.spec.rules</kbd>. This is a list of rules that checks the request's host and path, and if it matches, proxies the request to a specified Service.</p>
<p class="code-line">In our example, we are matching any requests for the domain<span> </span><kbd>api.hobnob.social</kbd><span> </span>to our<span> </span><kbd>backend</kbd><span> </span>service, on port<span> </span><kbd>8080</kbd>; likewise, we'll also forward requests for the host<span> </span><kbd>docs.hobnob.social</kbd><span> </span>to our<span> </span><kbd>backend</kbd><span> </span>Service, but on the<span> </span><kbd>8100</kbd><span> </span>port instead.</p>
<p class="code-line">Now, deploy it with<span> </span><kbd>kubectl apply</kbd>, and then wait for the address of the L4 load balancer to appear in the<span> </span><kbd>kubectl describe<span> </span>output</kbd>:</p>
<pre><strong>$ kubectl apply -f ./manifest/backend/ingress.yaml</strong><br/><strong>ingress.extensions "backend-ingress" created</strong><br/><strong>$ kubectl describe ingress backend-ingress</strong><br/><strong>Name: backend-ingress</strong><br/><strong>Namespace: default</strong><br/><strong>Address: 174.138.126.169</strong><br/><strong>Default backend: default-http-backend:80 (&lt;none&gt;)</strong><br/><strong>Rules:</strong><br/><strong>  Host Path Backends</strong><br/><strong>  ---- ---- --------</strong><br/><strong>  api.hobnob.social</strong><br/><strong>                         backend:8080 (&lt;none&gt;)</strong><br/><strong>  docs.hobnob.social</strong><br/><strong>                         backend:8100 (&lt;none&gt;)</strong><br/><strong>Annotations:</strong><br/><strong>  kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{"nginx.ingress.kubernetes.io/rewrite-target":"/"},"name":"backend-ingress","namespace":"default"},"spec":{"rules":[{"host":"api.hobnob.social","http":{"paths":[{"backend":{"serviceName":"backend","servicePort":8080}}]}},{"host":"docs.hobnob.social","http":{"paths":[{"backend":{"serviceName":"backend","servicePort":8100}}]}}]}}</strong><br/><br/><strong>  nginx.ingress.kubernetes.io/rewrite-target: /</strong><br/><strong>Events:</strong><br/><strong>  Type Reason Age From Message</strong><br/><strong>  ---- ------ ---- ---- -------</strong><br/><strong>  Normal UPDATE 2s nginx-ingress-controller Ingress default/backend-ingress</strong></pre>
<p class="code-line">This means any requests with the hosts<span> </span><kbd>api.hobnob.social</kbd><span> </span>and<span> </span><kbd>docs.hobnob.social</kbd><span> </span>can now reach our distributed service!</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating DNS records</h1>
                </header>
            
            <article>
                
<p class="code-line">Now that the<span> </span><kbd>api.hobnob.social</kbd><span> </span>and<span> </span><kbd>docs.hobnob.social</kbd><span> </span>domains can both be accessed through the load balancer, it's time to update our DNS records to point those subdomains to the load balancer's external IP address:</p>
<p class="code-line CDPAlignCenter CDPAlign"><img src="assets/4a8987e9-3be9-4c40-901d-0a611c729ccb.png"/></p>
<p class="code-line">After the DNS records have been propagated, go to a browser and try<span> </span><kbd>docs.hobnob.social</kbd>. You should be able to see the Swagger UI documentation!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="code-line">In this chapter, we have successfully deployed our Elasticsearch instance and backend API on Kubernetes. We have learned the roles of each Component and the types of Objects each manages.</p>
<p>You've come a long way since we started! To finish it off, let's see if you can use what you've learned to deploy the frontend application on Kubernetes on your own.</p>


            </article>

            
        </section>
    </body></html>