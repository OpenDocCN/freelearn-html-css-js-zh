- en: Advanced Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters of this book, we covered many foundational computer
    graphics concepts that, ultimately, gave us the knowledge and skills to build
    a 3D virtual car showroom. This means that at this point, you have all of the
    information you need to create rich 3D applications with WebGL. However, we've
    only just scratched the surface of what's possible with WebGL! Creative use of
    shaders, textures, and vertex attributes can yield fantastic results. In these
    final chapters, we'll cover a few advanced WebGL concepts that should leave you
    eager to explore more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn various post-processing effects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a particle system using point sprites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how to use normal mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement how to use ray tracing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Post-processing** is the process of adding effects by re-rendering the image
    of the scene with a shader that alters the final image. You can think of this
    as the process of taking a screenshot of your scene (ideally at `60+` frames per
    second), opening it up in your favorite image editor, and applying various filters.
    The difference is, of course, that we can do so in real time!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of simple post-processing effects include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Grayscale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepia tone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inverted colors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Film grain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blur
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wavy/dizzy effect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic technique for creating these effects is relatively simple: create
    a framebuffer with the same dimensions as the `canvas` and have the entire scene
    rendered to it at the beginning of the `draw` cycle. Then, a quad is rendered
    to the default framebuffer using the texture that makes up the framebuffer''s
    color attachment. The shader used during the rendering of the quad is what contains
    the post-process effect. That shader can transform the color values of the rendered
    scene as they get written to the quad to produce the desired visuals.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's investigate the individual steps of this process more closely.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Framebuffer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code we will use to create the framebuffer is nearly the same as what we
    created earlier in [Chapter 8](b06d92d3-3687-476f-a181-e7dd3aac1b8f.xhtml), *Picking*. There
    are, however, a few key differences worth noting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We use the `width` and `height` of the `canvas` to determine our buffer size,
    instead of using the arbitrary values that were used for the picker. Because the
    content of the picker buffer is not for rendering to the screen, we don't need
    to worry about resolution as much. For the post-process buffer, however, we'll
    get the best results if the output matches the dimensions of the `canvas`.
  prefs: []
  type: TYPE_NORMAL
- en: Since the texture will be exactly the same size as the `canvas`, and since we're
    rendering it as a full-screen quad, we've created a situation where the texture
    will be displayed at exactly a `1:1` ratio on the screen. This means that no filters
    need to be applied and that we can use `NEAREST` filtering with no visual artifacts.
    Also, in post-processing cases where we want to warp the texture coordinates (such
    as the wavy effect), we would benefit from using `LINEAR` filtering. We also need
    to use a wrap mode of `CLAMP_TO_EDGE`. That being said, the code is nearly identical
    to the `Picker` we used for framebuffer creation.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Geometry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we could load the quad from a file, the geometry is simple enough
    that we can include it directly in the code. All that''s needed are the vertex
    positions and texture coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Setting up the Shader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The vertex shader for the post-process draw is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that unlike the other vertex shaders we've worked with so far, this one
    doesn't use any matrices. That's because the vertices we declared in the previous
    step are **pre-transformed**.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from [Chapter 4](62d4de32-0b5b-4339-8fcc-80f739e80ec2.xhtml), *Cameras, *that
    we retrieved normalized device coordinates by multiplying the vertex position
    by the Projection matrix. Here, the coordinates mapped all positions to a `[-1,
    1]` range on each axis, which represents the full viewport. In this case, however,
    our vertex positions are already mapped to a `[-1, 1]` range; therefore, no transformation
    is needed because they will map perfectly to the viewport bounds when we render.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fragment shader is where most of the interesting operations happen. The
    fragment shader will be different for every post-process effect. Let''s look at
    a simple **grayscale effect** as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we sample the original color rendered by the scene (available
    through `uSampler`) and output a color that is a weighted average of the red,
    green, and blue channels. The result is a simple grayscale version of the original
    scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0e14317-62cd-42b0-9c74-edfff270a994.png)'
  prefs: []
  type: TYPE_IMG
- en: Architectural Updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've added a new class, `PostProcess`, to assist with the post-processing effects.
    This code can be found in the `common/js/PostProcess.js` file. This class will
    create the appropriate framebuffer and quad geometry, compile the post-process
    shader, and set up the render needed to draw the scene out to the quad.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how this component works with an example!
  prefs: []
  type: TYPE_NORMAL
- en: Time for Action: Post-Process Effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see a few post-processing effects in action:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `ch10_01_post-process.html` file in your browser, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2f595736-ad10-4585-b60e-27a2d09232b3.png)'
  prefs: []
  type: TYPE_IMG
- en: The controls dropdown allows you to switch between different sample effects.
    Try them out to get a feel for the effects they have on the scene. We've already
    looked at grayscale, so let's examine the rest of the filters individually.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **invert effect**, similar to grayscale in that it only modifies the color
    output, inverts each color channel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b6c2a988-ed35-4c64-9093-11daeb61d673.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **wavy effect **manipulates the texture coordinates to make the scene swirl
    and sway. In this effect, we also provide the current time to allow the distortion
    to change as time progresses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/249f5635-ab2e-4923-94c9-8ba0f2c4167d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **blur effect **samples several pixels around the current pixel and uses
    a weighted blend to produce a fragment output that is the average of its neighbors.
    This gives a blurry feel to the scene. A new uniform, `uInverseTextureSize`, provides
    values that are `1` over the `width` and `height` of the viewport. We use these
    values to accurately target individual pixels within the texture. For example, `vTextureCoords.x
    + 2 * uInverseTextureSize.x` will be exactly 2 pixels to the left of the original
    texture coordinate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/938d5656-3092-425d-9bbb-024feb693459.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our final example is a **film grain **effect. This uses a noisy texture to
    create a grainy scene, which simulates the use of an old camera. This example
    is significant because it demonstrates the use of a second texture besides the
    framebuffer when rendering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/164be51d-f5d2-44f8-9653-7d214157d51d.png)'
  prefs: []
  type: TYPE_IMG
- en: '***What just happened?***'
  prefs: []
  type: TYPE_NORMAL
- en: All of these effects are achieved by manipulating the rendered image before
    it is outputted to the screen. Since the amount of geometry processed for these
    effects is small, they are efficient, regardless of the scene's complexity. That
    being said, performance may be affected as the size of the `canvas` or the complexity
    of the post-process shader increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a Go: Funhouse Mirror Effect'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What would it take to create a post-process effect that stretches the image
    near the center of the viewport and squashes it toward the edges?
  prefs: []
  type: TYPE_NORMAL
- en: Point Sprites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Particle effects **is a common technique used in many 3D applications and
    games. A particle effect is a generic term for any special effect created by rendering
    groups of **particles** (displayed as points, textured quads, or repeated geometry),
    typically with some simple physics simulation acting on the individual particles.
    They can be used for simulating smoke, fire, bullets, explosions, water, sparks,
    and many other effects that are difficult to represent by a single geometric model.'
  prefs: []
  type: TYPE_NORMAL
- en: One very efficient way of rendering particles is to use **point sprites**. Throughout
    this book, we've been rendering triangle primitives, but if you render vertices
    with the `POINTS` primitive type, then each vertex will be rendered as a single
    pixel on the screen. A point sprite is an extension of the `POINTS` primitive
    rendering, where each point is provided a size and is textured in the shader.
  prefs: []
  type: TYPE_NORMAL
- en: 'A point sprite is created by setting the `gl_PointSize` value in the vertex
    shader. It can be set to either a constant value or a value calculated from shader
    inputs. If it''s set to a number greater than one, the point is rendered as a
    quad that always faces the screen (also known as a **billboard**). The quad is
    centered on the original point and has a width and height equal to the `gl_PointSize` in
    pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/938e6674-d890-4607-9d57-6a9fb0876d0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the point sprite is rendered, it also generates texture coordinates for
    the quad, covering a simple `0-1` range from the upper left to the lower right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f53f4e7-6be9-4a49-b2ad-a0739bb6197b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The texture coordinates are accessible in the fragment shader by the built-in `vec2
    gl_PointCoord`. Combining these properties gives us a simple point sprite vertex
    shader that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding fragment shader looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of the appropriate draw command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This renders each point in the vertex buffer as a `16x16` texture.
  prefs: []
  type: TYPE_NORMAL
- en: Time for Action: Fountain of Sparks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can use point sprites to create a fountain of sparks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `ch10_02_point-sprites.html` file in your browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/382fc860-9d2a-42be-a19c-2f298a87d2b9.png)'
  prefs: []
  type: TYPE_IMG
- en: This example showcases a simple *fountain of sparks *effect with point sprites.
    You can adjust the size and lifetime of the particles by using the sliders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The particle simulation is performed by maintaining a list of particles that
    are comprised of position, velocity, and lifespan. In every frame, we iterate
    through the list and move the particles according to the velocity; we also apply
    gravity and reduce the remaining lifespan. Once a particle's lifespan has reached
    `0`, it's reset to the origin with a random velocity and updated lifespan.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With every iteration of the particle's simulation, the particle positions and
    lifespans are copied to an array that is then used to update a vertex buffer.
    This vertex buffer is what is rendered to produce the onscreen sprites.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's experiment with some of the other values that control the simulation and
    see how they affect the scene. Open up `ch10_02_point-sprites.html` in your editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, locate the call to `configureParticles` at the bottom of the `configure` function.
    The number passed as an argument, initially set to `1024`, determines how many
    particles are created. Try changing it to a lower or higher value to see the effect
    it has on the particle system. Be careful, though, since extremely high values
    (for example, in the millions) may cause performance issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, find the `resetParticle` function. This function is called any time a
    particle is created or reset. There are several values here that can have a significant
    effect on how the scene renders:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `particle.position` is the `x`, `y`, `z` starting coordinates for the particle.
    Initially, all points start at the world origin `(0, 0, 0)`, but this could be
    set to anything. It's often desirable to have the particles originate from the
    location of another object so as to give the impression that the object is producing
    the particles. You can also randomize the position to make the particles appear
    within a given area.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`particle.velocity` is the initial velocity of the particle. Here, you can
    see that it has been randomized so that particles spread out as they move away
    from the origin. Particles that move in random directions tend to look more like
    explosions or sprays, while those that move in the same direction give the appearance
    of a steady stream. In this case, the `y` value is designed to always be positive,
    while the `x` and `z` values may either be positive or negative. Experiment with
    what happens when you increase or decrease these velocity values or remove the
    random element from one of the components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, `particle.lifespan` determines how long a particle is displayed before
    being reset. This uses the value from the controls while being randomized to provide
    visual variety. If you remove the random element from the particle lifespan, all
    of the particles will expire and reset at the same time, resulting in fireworks-like *bursts *of
    particles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, find the `updateParticles` function. This function is called once per
    frame to update the position and velocity of all particles before pushing the
    new values to the vertex buffer. It''s interesting to note that in terms of manipulating
    the simulation behavior, gravity is applied mid-way through the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `9.8` here is the acceleration applied to the `y` component over time. In
    other words, this is the gravity. We can remove this calculation entirely to create
    an environment where the particles float indefinitely along their original trajectories.
    We can increase the value to make the particles fall very quickly (giving them
    a *heavy* appearance), or we can change the component that the deceleration is
    applied to so that we can change the direction of gravity. For example, subtracting
    from `velocity[0]` makes the particles *fall* sideways.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is also where we apply a simple collision response with the *floor*. Any
    particles with a `y`position less than `0` (below the floor) have their velocities
    reversed and reduced. This gives us a realistic bouncing motion. We can make the
    particles less bouncy by reducing the multiplier (that is, `0.25` instead of `0.75`)
    or even eliminate bouncing altogether by simply setting the `y` velocity to `0`.
    Additionally, we can remove the floor by taking away the check for `y < 0`, which
    will allow the particles to fall indefinitely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's also worth seeing the different effects we can achieve with different textures.
    Try changing the path for the `spriteTexture` in the `configure` function to see
    what it looks like when you use different images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***What just happened?***'
  prefs: []
  type: TYPE_NORMAL
- en: We've seen how point sprites can be used to efficiently render particle effects.
    We've also seen the different ways that we can manipulate a particle simulation
    to achieve various effects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a Go: Bubbles!'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The particle system in place here could be used to simulate bubbles or smoke
    floating upward just as easily as bouncing sparks. How would you change the simulation
    to make the particles float rather than fall?
  prefs: []
  type: TYPE_NORMAL
- en: Normal Mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very powerful and popular technique among real-time 3D applications is **normal
    mapping**. Normal mapping creates the illusion of highly detailed geometry on
    a low-poly model by storing surface normals in a texture map that can then be
    used to calculate the lighting of the objects. This method is especially popular
    in modern games, since this allows developers to strike a balance between high
    performance and scene detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, lighting is calculated by using the surface normals of the triangles
    being rendered, meaning that the entire polygon will be lit as a continuous, smooth
    surface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee00b4a8-4719-401e-a59e-bd24ef025568.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With normal mapping, the surface normals are replaced by normals that are encoded
    in a texture that give the appearance of a rough or bumpy surface. Note that the
    actual geometry is not changed when using a normal map – only how it''s lit changes.
    If you look at a normal mapped polygon from the side, it will still appear to
    be perfectly flat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60c3eee4-74e2-4112-a026-ef7161857799.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The texture used to store the normals is called a **normal map**, and it''s
    typically paired with a specific diffuse texture that complements the surface
    that the normal map is trying to simulate. For example, here is a diffuse texture
    of some flagstones and the corresponding normal map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b264e92-611f-4032-bc33-bcef53831320.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the normal map contains a similar pattern to the diffuse texture.
    Together, the two textures give the appearance that the stones are raised with
    a rough finish, while the grout is sunken in.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapping Techniques** Although normal mapping is a powerful technique for
    efficiently adding more detail to assets, there are many other mapping techniques
    that follow the same line of reasoning. You can read about some of the other techniques
    that are available for use here: [https://en.wikipedia.org/wiki/Category:Texture_mapping](https://en.wikipedia.org/wiki/Category:Texture_mapping).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The normal map contains custom-formatted color information that can be interpreted
    by the shader at runtime as a fragment normal. A fragment normal is essentially
    the same as a vertex normal: it is a three-component vector that points away from
    the surface. The normal texture encodes the three components of the normal vector
    into the three channels of the texture''s texel color. Red represents the `x-axis`,
    green represents the `y-axis`, and blue represents the `z-axis`.'
  prefs: []
  type: TYPE_NORMAL
- en: The normals that have been encoded are typically stored in **tangent space**, as
    opposed to world or object space. Tangent space is the coordinate system for the
    texture coordinates of a face. Normal maps are commonly blue, since the normals
    they represent generally point away from the surface and thus have larger `z`
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time for Action: Normal Mapping in Action'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s cover an example showcasing normal mapping in action:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `ch10_03_normal-map.html` file in a browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/be34f773-74b6-4a56-b78a-89b58c7f91bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Rotate the cube to see the effect that the normal map has on the lit cube. Keep
    in mind that the profile of the cube has not changed. Let's examine how this effect
    is achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we need to add a new attribute to our vertex buffers. There are three
    vectors needed to calculate the tangent space coordinates for lighting: the **normal**,
    the **tangent**,and the **bitangent**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aa0741c1-0221-41ff-ae40-fd2904f6bc31.png)'
  prefs: []
  type: TYPE_IMG
- en: We have already covered normals, so let's investigate the other two vectors.
    The tangent represents the *up* (positive `y`) vector for the texture relative
    to the polygon surface. The bitangent represents the *left* (positive `x`) vector
    for the texture relative to the polygon surface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We only need to provide two of the three vectors as vertex attributes. Traditionally,
    the normal and tangent suffice, as the third vector is calculated as the cross-product
    of the other two in the vertex shader.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is common for 3D modeling packages to generate tangents for you. However,
    if they aren''t provided, they can be calculated from the vertex positions and
    texture coordinates, similar to calculating vertex normals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tangent Generation Algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: We won't cover this algorithm here, but for reference, it has been implemented
    in `common/js/utils.js` as `calculateTangents` and used in `scene.add`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the vertex shader, at the top of `ch10_03_normal-map.html`, the tangent
    needs to be transformed by the Normal matrix. The two transformed vectors can
    be used to calculate the third:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The three vectors can then be used to create a matrix that transforms vectors
    into tangent space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike before, where we applied lighting in the vertex shader, the bulk of
    the lighting calculations needs to happen in the fragment shader so that we can
    incorporate normals from the texture. That being said, we do transform the light
    direction into tangent space in the vertex shader before passing it to the fragment
    shader as a varying:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the fragment shader, we start by extracting the tangent space normal from
    the normal map texture. Since texture texels don''t store negative values, the
    normal components must be encoded to map from a `[-1, 1]` to a `[0, 1]` range.
    Therefore, they must be *unpacked* into the correct range before being used in
    the shader. The algorithm to perform this operation can be easily expressed in
    ESSL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Lighting is calculated nearly the same as the vertex-lit model, which is done
    by using the texture normal and tangent space light direction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To help accentuate the normal mapping effect, the code sample also includes
    the calculation of a specular term.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***What just happened?***'
  prefs: []
  type: TYPE_NORMAL
- en: We've seen how we can use normal information that's been encoded into a texture
    to add a new level of complexity to our lit models without additional geometry.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Tracing in Fragment Shaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common (if somewhat impractical) technique used to demonstrate how powerful
    shaders can be is to use them to **ray trace** a scene. Thus far, all of our rendering
    has been done with **polygon rasterization**, which is the technical term for
    the triangle-based rendering that WebGL incorporates. Ray tracing is an alternate
    rendering technique that traces the path of light through a scene as it interacts
    with mathematically defined geometry.
  prefs: []
  type: TYPE_NORMAL
- en: Ray tracing has several advantages compared to traditional polygonal rendering.
    Primarily, this includes creating more realistic scenes due to a more accurate
    lighting model that can easily account for things like reflection and reflected
    lighting. That said, ray tracing tends to be considerably slower than polygonal
    rendering, which is the reason it's not often used for real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ray tracing a scene is achieved by creating a series of rays (represented by
    an origin and direction) that start at the camera''s location and pass through
    each pixel in the viewport. These rays are then tested against every object in
    the scene to determine whether there are any intersections. If an intersection
    occurs, the closest intersection to the ray origin is returned, determining the
    color of the rendered pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3aa28732-30bc-4a8a-8592-54763c70778e.png)'
  prefs: []
  type: TYPE_IMG
- en: Although there are many algorithms that can be used to determine the color of
    the intersection point – ranging from simple diffuse lighting to multiple bounces
    of rays coming off other objects to simulate reflection – we'll keep our example
    simple. It's important to note that the rendered scene will entirely be the product
    of the shader code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time for Action: Examining the Ray Traced Scene'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s cover an example showcasing the power of ray tracing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `ch10_04_ray-tracing.html` file in your browser. You should see a
    scene with a simple lit, bobbing sphere like the one shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a6829bb4-69c0-4d16-b69b-0e9a56925b16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to trigger the shader, we need a way to draw a full-screen quad. Fortunately,
    we have a class from our post-processing examples earlier in this chapter to help
    us do just that. Since we don''t have a scene to process, we can omit a large
    part of the rendering code and simplify JavaScript''s `draw` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: That's it. The remainder of our scene will be built in to the fragment shader.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are two functions at the core of our shader: one that determines if a
    ray is intersecting a sphere and one that determines the normal of a point on
    the sphere. We''re using spheres because they''re typically the easiest type of
    geometry to raycast, and they also happen to be a type of geometry that is difficult
    to represent accurately with polygons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use these two functions to determine where the ray is intersecting
    with a sphere (if at all), along with what the normal and color of the sphere
    are at that point. To keep things simple, the sphere information is hardcoded
    as global variables, but they could just as easily be provided as uniforms from
    JavaScript:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we can determine the normal and color of a point with a ray, we need
    to generate the rays for casting. We can do this by determining the pixel that
    the current fragment represents and then creating a ray that points from the camera
    position through that pixel. To do so, we will utilize the `uInverseTextureSize`
    uniform that the `PostProcess` class provides to the shader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the ray we just generated, we call the `intersect` function to get the
    information about the sphere''s intersection. Then, we apply the same diffuse
    lighting calculations we''ve been using all along! To keep things simple, we''re
    using directional lighting here, but it would be easy enough to update the lighting
    model to point or spot lights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus far, our example is a static lit sphere. How do we add a bit of motion
    to the scene to give us a better sense of how fast the scene renders and how the
    lighting interacts with the sphere? We do so by adding a simple looping circular
    motion to the sphere by using the `uTime` uniform to modify the `x` and `z` coordinates
    at the beginning of the shader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '***What just happened?***'
  prefs: []
  type: TYPE_NORMAL
- en: We covered how we can construct a 3D scene, lighting and all, entirely in a
    fragment shader. It's a simple scene, of course, but also one that would be nearly
    impossible to render using polygon-based rendering. That's because perfect spheres
    can only be approximated with triangles.
  prefs: []
  type: TYPE_NORMAL
- en: '**Shader Toy** Now that you''ve seen how to construct 3D scenes entirely in
    fragment shaders, you will find the demos on [ShaderToy.com](https://www.shadertoy.com/) both
    beautiful and inspiring.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a Go: Multiple Spheres'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our example, we've kept things simple by rendering only one single sphere.
    That being said, all of the pieces needed to render several spheres are in place!
    How would you render a scene of multiple spheres with different colors and motion?
  prefs: []
  type: TYPE_NORMAL
- en: '**Hint** The main shader function that needs editing is `intersect`.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s summarize what we''ve learned in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: We covered a variety of advanced techniques to create more visually complex
    and compelling scenes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learned how to apply post-processing effects by leveraging a framebuffer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We rendered particle effects using point sprites.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We created the illusion of complex geometry by using normal maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we rendered a scene entirely in a fragment shader using ray casting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These advanced effects are only a glimpse into the vast landscape of effects
    possible with WebGL. Given the power and flexibility of shaders, the possibilities
    are endless!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the major differences between WebGL 1 (OpenGL
    ES 2.0) and WebGL 2 (OpenGL ES 3.0), along with a migration plan to WebGL 2.
  prefs: []
  type: TYPE_NORMAL
