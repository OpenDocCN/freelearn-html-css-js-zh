- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Coding with Streams
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用流进行编码
- en: Streams are one of the most important components and patterns of Node.js. There
    is a motto in the community that goes, "stream all the things!", and this alone
    should be enough to describe the role of streams in Node.js. Dominic Tarr, a top
    contributor to the Node.js community, defines streams as "Node's best and most
    misunderstood idea." There are different reasons that make Node.js streams so
    attractive; again, it's not just related to technical properties, such as performance
    or efficiency, but it's more about their elegance and the way they fit perfectly
    into the Node.js philosophy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 流是Node.js中最重要组件和模式之一。社区中有一个口号：“流一切东西！”，这本身就足以描述流在Node.js中的作用。Node.js社区的主要贡献者Dominic
    Tarr将流定义为“Node最好的、最被误解的想法。”有多个原因使得Node.js流如此吸引人；再次强调，这不仅仅与技术属性相关，如性能或效率，更多的是关于它们的优雅性和它们完美融入Node.js哲学的方式。
- en: This chapter aims to provide a complete understanding of Node.js streams. The
    first half of this chapter serves as an introduction to the main ideas, the terminology,
    and the libraries behind Node.js streams. In the second half, we will cover more
    advanced topics and, most importantly, we will explore useful streaming patterns
    that can make your code more elegant and effective in many circumstances.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在提供对Node.js流的全面理解。本章的前半部分作为对主要思想、术语和Node.js流背后的库的介绍。在后半部分，我们将涵盖更高级的主题，最重要的是，我们将探索有用的流模式，这些模式可以使你的代码在许多情况下更加优雅和有效。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下主题：
- en: Why streams are so important in Node.js
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么流在Node.js中如此重要
- en: Understanding, using, and creating streams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解、使用和创建流
- en: 'Streams as a programming paradigm: leveraging their power in many different
    contexts and not just for I/O'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流作为编程范式：在许多不同场景中利用其力量，而不仅仅是用于I/O
- en: Streaming patterns and connecting streams together in different configurations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流的模式和在不同配置中连接流
- en: Without further ado, let's discover together why streams are one of the cornerstones
    of Node.js.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，让我们一起来发现为什么流是Node.js的基石之一。
- en: Discovering the importance of streams
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现流的重要性
- en: In an event-based platform such as Node.js, the most efficient way to handle
    I/O is in real time, consuming the input as soon as it is available and sending
    the output as soon as the application produces it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于事件的平台如Node.js中，处理I/O最有效的方式是实时，一旦输入可用就消费它，一旦应用程序产生输出就发送它。
- en: In this section, we will give you an initial introduction to Node.js streams
    and their strengths. Please bear in mind that this is only an overview, as a more
    detailed analysis on how to use and compose streams will follow later in this
    chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为你提供一个对Node.js流及其优势的初步介绍。请记住，这只是一个概述，因为更详细的分析如何使用和组合流将在本章的后面部分进行。
- en: Buffering versus streaming
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓冲与流
- en: 'Almost all the asynchronous APIs that we''ve seen so far in this book work
    using *buffer mode*. For an input operation, buffer mode causes all the data coming
    from a resource to be collected into a buffer until the operation is completed;
    it is then passed back to the caller as one single blob of data. The following
    diagram shows a visual example of this paradigm:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎本书中到目前为止我们所见到的所有异步API都是使用*缓冲模式*工作的。对于输入操作，缓冲模式会将来自资源的所有数据收集到一个缓冲区中，直到操作完成；然后它将作为一个单一的数据块传递回调用者。下面的图展示了这一范式的视觉示例：
- en: '![](img/B15729_06_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1](img/B15729_06_01.png)'
- en: 'Figure 6.1: Buffering'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：缓冲
- en: In *Figure 6.1*, we can see that, at time *t1*, some data is received from the
    resource and saved into the buffer. At time *t2*, another data chunk is received—the
    final one—which completes the read operation, so that, at *t3*, the entire buffer
    is sent to the consumer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.1*中，我们可以看到，在时间*t1*，从资源接收了一些数据并保存到缓冲区中。在时间*t2*，接收到了另一个数据块——最后一个数据块，这完成了读取操作，因此，在*t3*，整个缓冲区被发送到消费者。
- en: 'On the other side, streams allow us to process the data as soon as it arrives
    from the resource. This is shown in the following diagram:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，流允许我们在数据从资源到达时立即处理这些数据。这在上面的图中有所展示：
- en: '![](img/B15729_06_02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2](img/B15729_06_02.png)'
- en: 'Figure 6.2: Streaming'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：流
- en: This time, *Figure 6.2* shows you that as soon as each new chunk of data is
    received from the resource, it is immediately passed to the consumer, who now
    has the chance to process it straight away, without waiting for all the data to
    be collected in the buffer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，*图6.2* 显示，一旦从资源接收到每个新的数据块，它就会立即传递给消费者，现在消费者有机会立即处理它，而无需等待所有数据都收集到缓冲区中。
- en: 'But what are the differences between these two approaches? Purely from an efficiency
    perspective, streams can be more efficient in terms of both space (memory usage)
    and time (computation clock time). However, Node.js streams have another important
    advantage: **composability**. Let''s now see what impact these properties have
    on the way we design and write our applications.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但这两种方法之间有什么区别呢？纯粹从效率的角度来看，流在空间（内存使用）和时间（计算时钟时间）方面都可以更高效。然而，Node.js流还有一个重要的优点：**可组合性**。现在让我们看看这些属性对我们设计和编写应用程序的方式有什么影响。
- en: Spatial efficiency
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 空间效率
- en: First of all, streams allow us to do things that would not be possible by buffering
    data and processing it all at once. For example, consider the case in which we
    have to read a very big file, let's say, in the order of hundreds of megabytes
    or even gigabytes. Clearly, using an API that returns a big buffer when the file
    is completely read is not a good idea. Imagine reading a few of these big files
    concurrently; our application would easily run out of memory. Besides that, buffers
    in V8 are limited in size. You cannot allocate more than a few gigabytes of data,
    so we may hit a wall way before running out of physical memory.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，流允许我们做一些通过缓冲数据一次性处理不可能完成的事情。例如，考虑我们必须读取一个非常大的文件的情况，比如说，几百兆字节甚至几吉字节。显然，当文件完全读取时，使用返回大缓冲区的API不是一个好主意。想象一下同时读取这些大文件；我们的应用程序很容易耗尽内存。此外，V8中的缓冲区大小有限。你不能分配超过几个吉字节的数据，所以我们可能会在耗尽物理内存之前就遇到障碍。
- en: 'The actual maximum size of a buffer changes across platforms and versions of
    Node.js. If you are curious to find out what''s the limit in bytes in a given
    platform, you can run this code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 缓冲区的实际最大大小会随着平台和Node.js的版本而变化。如果你想知道在特定平台上字节的限制是多少，你可以运行以下代码：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Gzipping using a buffered API
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用缓冲API进行Gzipping
- en: 'To make a concrete example, let''s consider a simple command-line application
    that compresses a file using the GZIP format. Using a buffered API, such an application
    will look like the following in Node.js (error handling is omitted for brevity):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做一个具体的例子，让我们考虑一个简单的命令行应用程序，该程序使用GZIP格式压缩文件。使用缓冲API，这样的应用程序在Node.js中的样子如下（为了简洁，省略了错误处理）：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can try to put the preceding code in a file named `gzip-buffer.js`
    and then run it with the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以尝试将前面的代码放入一个名为`gzip-buffer.js`的文件中，然后使用以下命令运行它：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we choose a file that is big enough (for instance, about 8 GB), we will
    most likely receive an error message saying that the file that we are trying to
    read is bigger than the maximum allowed buffer size:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择一个足够大的文件（例如，大约8 GB），我们很可能会收到一个错误消息，说我们正在尝试读取的文件大小超过了最大允许的缓冲区大小：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That's exactly what we expected, and it's a symptom of the fact that we are
    using the wrong approach.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们预期的，也是我们使用错误方法的一个迹象。
- en: Gzipping using streams
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用流进行Gzipping
- en: 'The simplest way we have to fix our Gzip application and make it work with
    big files is to use a streaming API. Let''s see how this can be achieved. Let''s
    write a new module with the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修复Gzip应用程序并使其能够处理大文件的最简单方法就是使用流API。让我们看看这是如何实现的。让我们编写一个新的模块，代码如下：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '"Is that it?" you may ask. Yes! As we said, streams are amazing because of
    their interface and composability, thus allowing clean, elegant, and concise code.
    We will see this in a while in more detail, but for now, the important thing to
    realize is that the program will run smoothly against files of any size and with
    constant memory utilization. Try it yourself (but consider that compressing a
    big file may take a while).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: “这就完了？”你可能问。是的！正如我们所说的，流之所以神奇，是因为它们的接口和可组合性，这使得代码干净、优雅和简洁。我们将在稍后更详细地看到这一点，但现在，重要的是要认识到程序将平稳地运行在任何大小的文件上，并且具有恒定的内存利用率。自己试试（但请考虑压缩大文件可能需要一段时间）。
- en: Note that, in the previous example, we omitted error handling for brevity. We
    will discuss the nuances of proper error handling with streams later in this chapter.
    Until then, be aware that most examples will be lacking proper error handling.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了简洁，在先前的例子中我们省略了错误处理。在本章的后面部分，我们将讨论使用流的正确错误处理的细微差别。在此之前，请注意，大多数示例都将缺少适当的错误处理。
- en: Time efficiency
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间效率
- en: Let's now consider the case of an application that compresses a file and uploads
    it to a remote HTTP server, which, in turn, decompresses it and saves it on the
    filesystem. If the client component of our application was implemented using a
    buffered API, the upload would start only when the entire file had been read and
    compressed. On the other hand, the decompression would start on the server only
    when all the data had been received. A better solution to achieve the same result
    involves the use of streams. On the client machine, streams allow us to compress
    and send the data chunks as soon as they are read from the filesystem, whereas
    on the server, they allow us to decompress every chunk as soon as it is received
    from the remote peer. Let's demonstrate this by building the application that
    we mentioned earlier, starting from the server side.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一个应用程序的案例，该应用程序压缩一个文件并将其上传到远程HTTP服务器，然后服务器将其解压缩并保存到文件系统中。如果我们的应用程序的客户端组件是使用缓冲API实现的，上传将仅在读取并压缩整个文件后开始。另一方面，解压缩将在服务器上仅在接收到所有数据后开始。为了达到相同的结果，一个更好的解决方案是使用流。在客户端机器上，流允许我们在从文件系统读取数据块后立即压缩并发送数据，而在服务器上，它们允许我们在从远程对等方接收到每个数据块后立即解压缩。让我们通过构建我们之前提到的应用程序来演示这一点，从服务器端开始。
- en: 'Let''s create a module named `gzip-receive.js` containing the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 `gzip-receive.js` 的模块，包含以下代码：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding example, `req` is a stream object that is used by the server
    to receive the request data in chunks from the network. Thanks to Node.js streams,
    every chunk of data is decompressed and saved to disk as soon as it is received.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，`req` 是一个流对象，服务器使用它从网络中以块的形式接收请求数据。多亏了 Node.js 流，每个数据块在接收后立即解压缩并保存到磁盘上。
- en: You might have noticed that, in our server application, we are using `basename()`
    to remove any possible path from the name of the received file. This is a security
    best practice as we want to make sure that the received file is saved exactly
    within our `received_files` folder. Without `basename()`, a malicious user could
    craft a request that could effectively override system files and inject malicious
    code into the server machine. Imagine, for instance, what happens if `filename`
    is set to `/usr/bin/node`? In such a case, the attacker could effectively replace
    our Node.js interpreter with any arbitrary file.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在我们的服务器应用程序中，我们使用 `basename()` 来从接收到的文件名中移除任何可能的路径。这是一个安全最佳实践，因为我们想确保接收到的文件正好保存到我们的
    `received_files` 文件夹中。如果没有 `basename()`，恶意用户可能会构建一个请求，有效地覆盖系统文件并将恶意代码注入到服务器机器中。例如，想象一下如果
    `filename` 被设置为 `/usr/bin/node` 会发生什么？在这种情况下，攻击者可以有效地用任何任意文件替换我们的 Node.js 解释器。
- en: 'The client side of our application will go into a module named `gzip-send.js`,
    and it looks as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用程序的客户端将进入一个名为 `gzip-send.js` 的模块，其代码如下：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, we are again using streams to read the data from the
    file, and then compressing and sending each chunk as soon as it is read from the
    filesystem.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们再次使用流从文件中读取数据，然后从文件系统读取后立即压缩并发送每个数据块。
- en: 'Now, to try out the application, let''s first start the server using the following
    command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了尝试这个应用程序，我们首先使用以下命令启动服务器：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we can launch the client by specifying the file to send and the address
    of the server (for example, `localhost`):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过指定要发送的文件和服务器地址（例如，`localhost`）来启动客户端：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we choose a file big enough, we can appreciate how the data flows from the
    client to the server. But why exactly is this paradigm—where we have flowing data—more
    efficient compared to using a buffered API? *Figure 6.3* should make this concept
    easier to grasp:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择一个足够大的文件，我们可以欣赏数据从客户端流向服务器的过程。但为什么这种数据流动的范式——与使用缓冲API相比——更有效率？*图6.3* 应该会使这个概念更容易理解：
- en: '![](img/B15729_06_03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15729_06_03.png)'
- en: 'Figure 6.3: Buffering and streaming compared'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：缓冲和流比较
- en: 'When a file is processed, it goes through a number of sequential stages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个文件被处理时，它会经过一系列的连续阶段：
- en: '[Client] Read from the filesystem'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[客户端] 从文件系统读取'
- en: '[Client] Compress the data'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[客户端] 压缩数据'
- en: '[Client] Send it to the server'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[客户端] 发送到服务器'
- en: '[Server] Receive from the client'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[服务器] 从客户端接收'
- en: '[Server] Decompress the data'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[服务器] 解压缩数据'
- en: '[Server] Write the data to disk'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[服务器] 将数据写入磁盘'
- en: To complete the processing, we have to go through each stage like in an assembly
    line, in sequence, until the end. In *Figure 6.3*, we can see that, using a buffered
    API, the process is entirely sequential. To compress the data, we first have to
    wait for the entire file to be read, then, to send the data, we have to wait for
    the entire file to be both read and compressed, and so on.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成处理，我们必须像装配线一样按顺序经过每个阶段，直到结束。在*图6.3*中，我们可以看到，使用缓冲API，整个过程完全是顺序的。为了压缩数据，我们首先必须等待整个文件被读取，然后，为了发送数据，我们必须等待整个文件被读取和压缩，依此类推。
- en: Using streams, the assembly line is kicked off as soon as we receive the first
    chunk of data, without waiting for the entire file to be read. But more amazingly,
    when the next chunk of data is available, there is no need to wait for the previous
    set of tasks to be completed; instead, another assembly line is launched in parallel.
    This works perfectly because each task that we execute is asynchronous, so it
    can be parallelized by Node.js. The only constraint is that the order in which
    the chunks arrive in each stage must be preserved. The internal implementation
    of Node.js streams takes care of maintaining the order for us.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流，一旦我们收到第一块数据，装配线就会启动，而不需要等待整个文件被读取。但更令人惊讶的是，当下一块数据可用时，不需要等待之前的一组任务完成；相反，另一个装配线会并行启动。这之所以能完美工作，是因为我们执行的每个任务都是异步的，因此可以被Node.js并行化。唯一的约束是每个阶段中块到达的顺序必须保持不变。Node.js流内部实现会为我们维护这个顺序。
- en: As we can see from *Figure 6.3*, the result of using streams is that the entire
    process takes less time, because we waste no time waiting for all the data to
    be read and processed all at once.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图6.3*所示，使用流的结果是整个过程所需时间更少，因为我们没有浪费时间等待所有数据一次性被读取和处理。
- en: Composability
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可组合性
- en: The code we have seen so far has already given us an overview of how streams
    can be composed thanks to the `pipe()` method, which allows us to connect the
    different processing units, each being responsible for one single functionality,
    in perfect Node.js style. This is possible because streams have a uniform interface,
    and they can understand each other in terms of API. The only prerequisite is that
    the next stream in the pipeline has to support the data type produced by the previous
    stream, which can be either binary, text, or even objects, as we will see later
    in this chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的代码已经展示了如何通过`pipe()`方法组合流，这使得我们可以连接不同的处理单元，每个单元负责单一的功能，完全符合Node.js的风格。这是可能的，因为流有一个统一的接口，并且它们可以通过API相互理解。唯一的前提是管道中的下一个流必须支持由前一个流产生的数据类型，这可以是二进制、文本，甚至是我们将在本章后面看到的对象。
- en: To take a look at another demonstration of the power of this property, we can
    try to add an encryption layer to the `gzip-send`/`gzip-receive` application that
    we built previously.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看这个属性的强大之处，我们可以尝试向之前构建的`gzip-send`/`gzip-receive`应用程序添加一个加密层。
- en: In order to do this, we will need to apply some small changes to both our client
    and server.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要对我们的客户端和服务器进行一些小的修改。
- en: Adding client-side encryption
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加客户端加密
- en: 'Let''s start with the client:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从客户端开始：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s review what we changed here:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们在这里做了哪些改变：
- en: First of all, we import the `createCipheriv()` `Transform` stream and the `randomBytes()`
    function from the `crypto` module.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入`createCipheriv()` `Transform`流和`randomBytes()`函数，这两个都来自`crypto`模块。
- en: We get the server's encryption secret from the command line. We expect the string
    to be passed as a hexadecimal string, so we read this value and load it in memory
    using a buffer set to `hex` mode.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从命令行获取服务器的加密密钥。我们期望字符串以十六进制字符串的形式传递，因此我们读取这个值，并使用设置为`hex`模式的缓冲区将其加载到内存中。
- en: Finally, we generate a random sequence of bytes that we will be using as an
    initialization vector for the file encryption.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们生成一个随机字节序列，我们将使用它作为文件加密的初始化向量。
- en: 'Now, we can update the piece of code responsible for creating the HTTP request:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以更新负责创建HTTP请求的代码片段：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The main changes here are:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里主要的改变是：
- en: We pass the initialization vector to the server as an HTTP header.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将初始化向量作为HTTP头传递给服务器。
- en: We encrypt the data, just after the Gzip phase.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在Gzip阶段之后加密数据。
- en: That's all for the client side.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端部分到此结束。
- en: Adding server-side decryption
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加服务器端解密
- en: 'Let''s now refactor the server. The first thing that we need to do is import
    some utility functions from the core `crypto` module, which we can use to generate
    a random encryption key (the secret):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重构服务器。我们首先需要做的是从核心 `crypto` 模块导入一些实用函数，我们可以使用这些函数来生成一个随机加密密钥（密钥）：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The generated secret is printed to the console as a hex string so that we can
    share that with our clients.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的密钥以十六进制字符串的形式打印到控制台，以便我们可以与我们的客户共享。
- en: 'Now, we need to update the file reception logic:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要更新文件接收逻辑：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, we are applying two changes:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在应用两个更改：
- en: We have to read the encryption **initialization vector** ([nodejsdp.link/iv](http://nodejsdp.link/iv))
    sent by the client.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须读取客户端发送的加密**初始化向量**([nodejsdp.link/iv](http://nodejsdp.link/iv))。
- en: The first step of our streaming pipeline is now responsible for decrypting the incoming
    data using the `createDecipheriv` `Transform` stream from the `crypto` module.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们流式传输管道的第一步现在负责使用 `crypto` 模块的 `createDecipheriv` `Transform` 流解密传入的数据。
- en: With very little effort (just a few lines of code), we added an encryption layer
    to our application; we simply had to use some already available `Transform` streams
    (`createCipheriv` and `createDecipheriv`) and included them in the stream processing
    pipelines for the client and the server. In a similar way, we can add and combine
    other streams, as if we were playing with Lego bricks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 只需付出很少的努力（几行代码），我们就为我们的应用程序添加了一个加密层；我们只需使用一些已经可用的 `Transform` 流（`createCipheriv`
    和 `createDecipheriv`）并将它们包含在客户端和服务器流处理管道中。以类似的方式，我们可以添加和组合其他流，就像我们在玩乐高积木一样。
- en: The main advantage of this approach is reusability, but as we can see from the
    code so far, streams also enable cleaner and more modular code. For these reasons,
    streams are often used not just to deal with pure I/O, but also as a means to
    simplify and modularize the code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要优势是可重用性，但正如我们从代码中看到的那样，流还使代码更干净、更模块化。出于这些原因，流通常不仅用于处理纯 I/O，而且还作为简化代码和模块化代码的手段。
- en: Now that we have introduced streams, we are ready to explore, in a more structured
    way, the different types of streams available in Node.js.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经引入了流，我们准备以更结构化的方式探索 Node.js 中可用的不同类型的流。
- en: Getting started with streams
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流入门
- en: In the previous section, we learned why streams are so powerful, but also that
    they are everywhere in Node.js, starting from its core modules. For example, we
    have seen that the `fs` module has `createReadStream()` for reading from a file
    and `createWriteStream()` for writing to a file, the HTTP `request` and `response`
    objects are essentially streams, the `zlib` module allows us to compress and decompress
    data using a streaming interface and, finally, even the `crypto` module exposes
    some useful streaming primitives like `createCipheriv` and `createDecipheriv`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了为什么流如此强大，但也了解到它们在 Node.js 中无处不在，从其核心模块开始。例如，我们已经看到 `fs` 模块有 `createReadStream()`
    用于从文件读取和 `createWriteStream()` 用于向文件写入，HTTP 的 `request` 和 `response` 对象本质上也是流，`zlib`
    模块允许我们使用流式接口压缩和解压缩数据，最后，`crypto` 模块甚至公开了一些有用的流式原语，如 `createCipheriv` 和 `createDecipheriv`。
- en: Now that we know why streams are so important, let's take a step back and start
    to explore them in more detail.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了流为什么如此重要，让我们退一步，更详细地探索它们。
- en: Anatomy of streams
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流的解剖结构
- en: 'Every stream in Node.js is an implementation of one of the four base abstract
    classes available in the `stream` core module:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Node.js 中，每个流都是 `stream` 核心模块中可用的四个基本抽象类之一的一个实现：
- en: '`Readable`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Readable`'
- en: '`Writable`'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Writable`'
- en: '`Duplex`'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Duplex`'
- en: '`Transform`'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Transform`'
- en: Each `stream` class is also an instance of `EventEmitter`. Streams, in fact,
    can produce several types of event, such as `end` when a `Readable` stream has
    finished reading, `finish` when a `Writable` stream has completed writing, or
    `error` when something goes wrong.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `stream` 类也是 `EventEmitter` 的一个实例。实际上，流可以产生几种类型的事件，例如当 `Readable` 流完成读取时产生
    `end` 事件，当 `Writable` 流完成写入时产生 `finish` 事件，或者在出现错误时产生 `error` 事件。
- en: 'One reason why streams are so flexible is the fact that they can handle not
    just binary data, but almost any JavaScript value. In fact, they support two operating
    modes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 流之所以如此灵活的一个原因是它们不仅可以处理二进制数据，而且几乎可以处理任何 JavaScript 值。实际上，它们支持两种操作模式：
- en: '**Binary mode**: To stream data in the form of chunks, such as buffers or strings'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二进制模式**：以块的形式流式传输数据，例如缓冲区或字符串'
- en: '**Object mode**: To stream data as a sequence of discrete objects (allowing
    us to use almost any JavaScript value)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象模式**：将数据作为一系列离散对象流式传输（允许我们使用几乎任何JavaScript值）'
- en: These two operating modes allow us to use streams not just for I/O, but also
    as a tool to elegantly compose processing units in a functional fashion, as we
    will see later in this chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种操作模式使我们能够不仅使用流进行I/O，而且还可以将其用作工具，以优雅的方式在函数式风格中组合处理单元，正如我们将在本章后面看到的那样。
- en: Let's start our deep dive of Node.js streams by introducing the class of `Readable`
    streams.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从介绍`Readable`流类开始，深入探讨Node.js流。
- en: Readable streams
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可读流
- en: A `Readable` stream represents a source of data. In Node.js, it's implemented
    using the `Readable` abstract class, which is available in the `stream` module.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`Readable`流表示数据源。在Node.js中，它使用`Readable`抽象类实现，该类在`stream`模块中可用。'
- en: Reading from a stream
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从流中读取
- en: 'There are two approaches to receive the data from a `Readable` stream: **non-flowing**
    (or **paused**)and **flowing**. Let''s analyze these modes in more detail.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Readable`流接收数据有两种方法：**非流动**（或**暂停**）和**流动**。让我们更详细地分析这些模式。
- en: The non-flowing mode
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非流动模式
- en: 'The non-flowing or paused mode is the default pattern for reading from a `Readable`
    stream. It involves attaching a listener to the stream for the `readable` event,
    which signals the availability of new data to read. Then, in a loop, we read the
    data continuously until the internal buffer is emptied. This can be done using
    the `read()` method, which synchronously reads from the internal buffer and returns
    a `Buffer` object representing the chunk of data. The `read()` method has the
    following signature:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 非流动或暂停模式是读取`Readable`流的默认模式。这涉及到将监听器附加到流上，监听`readable`事件，该事件表示有新数据可供读取。然后，在一个循环中，我们持续读取数据，直到内部缓冲区清空。这可以通过使用`read()`方法来完成，该方法从内部缓冲区同步读取并返回一个表示数据块内容的`Buffer`对象。`read()`方法具有以下签名：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Using this approach, the data is imperatively pulled from the stream on demand.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，数据在需要时被强制从流中拉取。
- en: 'To show how this works, let''s create a new module named `read-stdin.js`, which
    implements a simple program that reads from the standard input (which is also
    a `Readable` stream) and echoes everything back to the standard output:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这是如何工作的，让我们创建一个名为`read-stdin.js`的新模块，该模块实现了一个简单的程序，它从标准输入（也是一个`Readable`流）读取，并将所有内容回显到标准输出：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `read()` method is a synchronous operation that pulls a data chunk from
    the internal buffers of the `Readable` stream. The returned chunk is, by default,
    a `Buffer` object if the stream is working in binary mode.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`read()`方法是一个同步操作，它从`Readable`流的内部缓冲区中拉取数据块。默认情况下，如果流以二进制模式工作，返回的块是一个`Buffer`对象。'
- en: In a `Readable` stream working in binary mode, we can read strings instead of
    buffers by calling `setEncoding(encoding)` on the stream, and providing a valid
    encoding format (for example, `utf8`). This approach is recommended when streaming
    UTF-8 text data as the stream will properly handle multibyte characters, doing
    the necessary buffering to make sure that no character ends up being split into
    separate chunks. In other words, every chunk produced by the stream will be a
    valid UTF-8 sequence of bytes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在以二进制模式工作的`Readable`流中，我们可以通过在流上调用`setEncoding(encoding)`并提供一个有效的编码格式（例如，`utf8`）来读取字符串而不是缓冲区。当流传输UTF-8文本数据时，这种方法是推荐的，因为流将正确处理多字节字符，进行必要的缓冲以确保没有字符被分割成单独的数据块。换句话说，流产生的每个数据块都将是一个有效的UTF-8字节序列。
- en: Note that you can call `setEncoding()` as many times as you want on a `Readable`
    stream, even after you've started consuming the data from the stream. The encoding
    will be switched dynamically on the next available chunk. Streams are inherently
    binary; encoding is just a view over the binary data that is emitted by the stream.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可以在`Readable`流上多次调用`setEncoding()`，即使在你已经开始从流中读取数据之后。编码将在下一个可用的数据块上动态切换。流本质上是二进制的；编码只是对流发出的二进制数据的视图。
- en: The data is read solely from within the `Readable` listener, which is invoked
    as soon as new data is available. The `read()` method returns `null` when there
    is no more data available in the internal buffers; in such a case, we have to
    wait for another `readable` event to be fired, telling us that we can read again,
    or wait for the `end` event that signals the end of the stream. When a stream
    is working in binary mode, we can also specify that we are interested in reading
    a specific amount of data by passing a `size` value to the `read()` method. This
    is particularly useful when implementing network protocols or when parsing specific
    data formats.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仅从 `Readable` 监听器中读取，该监听器在新的数据可用时立即被调用。当内部缓冲区中没有更多数据可用时，`read()` 方法返回 `null`；在这种情况下，我们必须等待另一个
    `readable` 事件被触发，告诉我们可以再次读取，或者等待表示流结束的 `end` 事件。当流以二进制模式工作时，我们还可以通过将 `size` 值传递给
    `read()` 方法来指定我们感兴趣读取的数据量。这在实现网络协议或解析特定数据格式时特别有用。
- en: Now, we are ready to run the `read-stdin.js` module and experiment with it.
    Let's type some characters into the console and then press Enter to see the data
    echoed back into the standard output. To terminate the stream and hence generate
    a graceful `end` event, we need to insert an `EOF` (end-of-file) character (using
    Ctrl + Z on Windows or Ctrl + D on Linux and macOS).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行 `read-stdin.js` 模块并对其进行实验。让我们在控制台中输入一些字符，然后按 Enter 键以查看数据被回显到标准输出。为了终止流并生成一个优雅的
    `end` 事件，我们需要插入一个 `EOF`（文件结束）字符（在 Windows 上使用 Ctrl + Z，在 Linux 和 macOS 上使用 Ctrl
    + D）。
- en: 'We can also try to connect our program with other processes. This is possible
    using the pipe operator (`|`), which redirects the standard output of a program
    to the standard input of another. For example, we can run a command such as the
    following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试将我们的程序与其他进程连接起来。这可以通过管道操作符（`|`）实现，它将一个程序的标准输出重定向到另一个程序的标准输入。例如，我们可以运行以下命令：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is an amazing demonstration of how the streaming paradigm is a universal
    interface that enables our programs to communicate, regardless of the language
    they are written in.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人惊叹的演示，说明了流式处理范式是一个通用的接口，它使我们的程序能够通信，无论它们是用哪种语言编写的。
- en: Flowing mode
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 流动模式
- en: 'Another way to read from a stream is by attaching a listener to the `data`
    event. This will switch the stream into using **flowing mode**, where the data
    is not pulled using `read()`, but instead is pushed to the `data` listener as
    soon as it arrives. For example, the `read-stdin.js` application that we created
    earlier will look like this using flowing mode:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从流中读取的另一种方式是通过将监听器附加到 `data` 事件。这将使流切换到使用 **流动模式**，其中数据不是通过 `read()` 拉取，而是数据一到就推送到
    `data` 监听器。例如，我们之前创建的 `read-stdin.js` 应用程序在流动模式下将看起来像这样：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Flowing mode offers less flexibility to control the flow of data compared to
    non-flowing mode. The default operating mode for streams is non-flowing, so to
    enable flowing mode, it's necessary to attach a listener to the `data` event or
    explicitly invoke the `resume()` method. To temporarily stop the stream from emitting
    `data` events, we can invoke the `pause()` method, causing any incoming data to
    be cached in the internal buffer. Calling `pause()` will switch the stream back
    to non-flowing mode.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与非流动模式相比，流动模式在控制数据流方面提供了更少的灵活性。流的默认操作模式是非流动模式，因此要启用流动模式，需要将监听器附加到 `data` 事件或显式调用
    `resume()` 方法。为了暂时停止流从发出 `data` 事件，我们可以调用 `pause()` 方法，这将导致任何传入的数据被缓存在内部缓冲区中。调用
    `pause()` 将将流切换回非流动模式。
- en: Async iterators
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异步迭代器
- en: '`Readable` streams are also async iterators; therefore, we could rewrite our
    `read-stdin.js` example as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`可读`流也是异步迭代器；因此，我们可以将我们的 `read-stdin.js` 示例重写如下：'
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We will discuss async iterators in greater detail in *Chapter 9*, *Behavioral
    Design Patterns*, so don't worry too much about the syntax in the preceding example
    for now. What's important to know is that if you need to write a function that
    consumes an entire `Readable` stream and returns a `Promise`, this syntax could
    come in very handy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第 9 章 *行为设计模式* 中更详细地讨论异步迭代器，所以现在不必太担心前面示例中的语法。重要的是要知道，如果您需要编写一个消耗整个 `Readable`
    流并返回 `Promise` 的函数，这种语法可能会非常有用。
- en: Implementing Readable streams
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现可读流
- en: 'Now that we know how to read from a stream, the next step is to learn how to
    implement a new custom `Readable` stream. To do this, it''s necessary to create
    a new class by inheriting the prototype `Readable` from the `stream` module. The
    concrete stream must provide an implementation of the `_read()` method, which
    has the following signature:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何从流中读取，下一步是学习如何实现一个新的自定义`Readable`流。要做到这一点，需要通过从`stream`模块继承原型`Readable`来创建一个新的类。具体的流必须提供一个`_read()`方法的实现，该方法的签名如下：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The internals of the `Readable` class will call the `_read()` method, which,
    in turn, will start to fill the internal buffer using `push()`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`Readable`类的内部将调用`_read()`方法，然后它将开始使用`push()`填充内部缓冲区：'
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Please note that `read()` is a method called by the stream consumers, while
    `_read()` is a method to be implemented by a stream subclass and should never
    be called directly. The underscore usually indicates that the method is not public
    and should not be called directly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`read()`是由流消费者调用的方法，而`_read()`是由流子类实现的方法，不应直接调用。下划线通常表示该方法不是公开的，不应直接调用。
- en: 'To demonstrate how to implement new `Readable` streams, we can try to implement
    a stream that generates random strings. Let''s create a new module called `random-stream.js`
    that contains the code of our random string generator:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何实现新的`Readable`流，我们可以尝试实现一个生成随机字符串的流。让我们创建一个新的模块`random-stream.js`，其中包含我们的随机字符串生成器的代码：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: At the top of the file, we load our dependencies. There is nothing special there,
    except that we are loading an npm module called `chance` ([nodejsdp.link/chance](http://nodejsdp.link/chance)),
    which is a library for generating all sorts of random values, from numbers to
    strings to entire sentences.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件顶部，我们加载我们的依赖项。那里没有什么特别的地方，除了我们正在加载一个名为`chance`的npm模块（[nodejsdp.link/chance](http://nodejsdp.link/chance)），这是一个用于生成各种随机值的库，从数字到字符串到整个句子。
- en: The next step is to create a new class called `RandomStream`, which specifies
    `Readable` as its parent. In the preceding code, invoking `super(options)` in
    the `RandomStream` constructor will call the constructor of the parent class,
    allowing us to initialize the stream's internal state.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个新的类`RandomStream`，指定`Readable`为其父类。在前面的代码中，在`RandomStream`构造函数中调用`super(options)`将调用父类的构造函数，允许我们初始化流的内部状态。
- en: If you have a constructor that only invokes `super(options)`, you can remove
    it as you will inherit the parent constructor. Just be careful to remember to
    call `super(options)` every time you need to write a custom constructor.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个只调用`super(options)`的构造函数，你可以将其删除，因为你将继承父构造函数。但请务必记住，每次你需要编写自定义构造函数时都要调用`super(options)`。
- en: 'The possible parameters that can be passed through the `options` object include
    the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`options`对象传递的可能的参数包括以下内容：
- en: The `encoding` argument, which is used to convert buffers into strings (defaults
    to `null`)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding`参数，用于将缓冲区转换为字符串（默认为`null`）'
- en: A flag to enable object mode (`objectMode`, defaults to `false`)
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个标志来启用对象模式（`objectMode`，默认为`false`）
- en: The upper limit of the data stored in the internal buffer, after which no more
    reading from the source should be done (`highWaterMark`, defaults to `16KB`)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部缓冲区中存储的数据的上限，在此之后不应再从源读取更多数据（`highWaterMark`，默认为`16KB`）
- en: 'Okay, now let''s explain the `_read()` method:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们解释一下`_read()`方法：
- en: The method generates a random string of length equal to `size` using `chance`.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该方法使用`chance`生成一个长度等于`size`的随机字符串。
- en: It pushes the string into the internal buffer. Note that since we are pushing
    strings, we also need to specify the encoding, `utf8` (this is not necessary if
    the chunk is simply a binary `Buffer`).
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将字符串推入内部缓冲区。请注意，由于我们正在推送字符串，我们还需要指定编码，`utf8`（如果块只是一个二进制`Buffer`，则此操作不是必需的）。
- en: It terminates the stream randomly, with a likelihood of 5 percent, by pushing
    `null` into the internal buffer to indicate an `EOF` situation or, in other words,
    the end of the stream.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它通过将`null`推入内部缓冲区来随机终止流，概率为5%，以指示`EOF`情况，换句话说，流的结束。
- en: Note that the `size` argument in the `_read()` function is an advisory parameter.
    It's good to honor it and push only the amount of data that was requested by the
    caller, even though it is not mandatory to do so.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`_read()`函数中的`size`参数是一个建议参数。尊重它是好的，只推送调用者请求的数据量，尽管这样做不是强制性的。
- en: When we invoke `push()`, we should check whether it returns `false`. When that
    happens, it means that the internal buffer of the receiving stream has reached
    the `highWaterMark` limit and we should stop adding more data to it. This is called
    **backpressure**, and we will be discussing it in more detail in the next section
    of this chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用`push()`时，我们应该检查它是否返回`false`。当这种情况发生时，这意味着接收流的内部缓冲区已达到`highWaterMark`限制，我们应该停止向其中添加更多数据。这被称为**背压**，我们将在本章下一节中更详细地讨论它。
- en: 'That''s it for `RandomStream`, we are now ready to use it. Let''s see how to
    instantiate a `RandomStream` object and pull some data from it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomStream`就到这里，我们现在可以使用它了。让我们看看如何实例化一个`RandomStream`对象并从中拉取一些数据：'
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, everything is ready for us to try our new custom stream. Simply execute
    the `index.js` module as usual and watch a random set of strings flowing on the
    screen.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一切准备就绪，我们可以尝试我们的新自定义流。像往常一样执行`index.js`模块，并观察屏幕上流动的随机字符串集。
- en: Simplified construction
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简化构造
- en: 'For simple custom streams, we can avoid creating a custom class by using the
    `Readable` stream''s *simplified construction* approach. With this approach, we
    only need to invoke `new Readable(options)` and pass a method named `read()` in
    the set of options. The `read()` method here has exactly the same semantic as
    the `_read()` method that we saw in the class extension approach. Let''s rewrite
    our `RandomStream` using the simplified constructor approach:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的自定义流，我们可以通过使用`Readable`流的**简化构造**方法来避免创建一个自定义类。使用这种方法，我们只需要调用`new Readable(options)`并在选项集中传递一个名为`read()`的方法。这里的`read()`方法与我们在类扩展方法中看到的`_read()`方法具有完全相同的语义。让我们用简化构造方法重写我们的`RandomStream`：
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This approach can be particularly useful when you don't need to manage a complicated
    state and allows you to take advantage of a more succinct syntax. In the previous
    example, we created a single instance of our custom stream. If we want to adopt
    the simplified constructor approach but we need to create multiple instances of
    the custom stream, we can wrap the initialization logic in a factory function
    that we can invoke multiple times to create those instances.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不需要管理复杂的状态时，这种方法尤其有用，它允许你利用更简洁的语法。在前面的例子中，我们创建了一个自定义流的单个实例。如果我们想采用简化构造方法但需要创建多个自定义流的实例，我们可以将初始化逻辑包装在一个工厂函数中，我们可以多次调用该函数来创建这些实例。
- en: Readable streams from iterables
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可读流来自可迭代对象
- en: You can easily create `Readable` stream instances from arrays or other **iterable**
    objects (that is, **generators**, **iterators**, and **async iterators**) using
    the `Readable.from()` helper.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以轻松地使用`Readable.from()`辅助工具从数组或其他**可迭代**对象（即**生成器**、**迭代器**和**异步迭代器**）创建`Readable`流实例。
- en: 'In order to get accustomed with this helper, let''s look at a simple example
    where we convert data from an array into a `Readable` stream:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉这个辅助工具，让我们看看一个简单的例子，我们将数据从数组转换为`Readable`流：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As we can see from this code, the `Readable.from()` method is quite simple
    to use: the first argument is an iterable instance (in our case, the `mountains`
    array). `Readable.from()` accepts an additional optional argument that can be
    used to specify stream options like `objectMode`.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从这段代码中我们可以看出，`Readable.from()`方法非常简单易用：第一个参数是一个可迭代实例（在我们的例子中是`mountains`数组）。`Readable.from()`接受一个额外的可选参数，可以用来指定流选项，如`objectMode`。
- en: Note that we didn't have to explicitly set `objectMode` to `true`. By default,
    `Readable.from()` will set `objectMode` to `true`, unless this is explicitly opted
    out by setting it to `false`. Stream options can be passed as a second argument
    to the function.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不必显式地将`objectMode`设置为`true`。默认情况下，`Readable.from()`会将`objectMode`设置为`true`，除非明确选择将其设置为`false`。流选项可以作为函数的第二个参数传递。
- en: 'Running the previous code will produce the following output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码将产生以下输出：
- en: '[PRE24]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Try not to instantiate large arrays in memory. Imagine if, in the previous example,
    we wanted to list all the mountains in the world. There are about 1 million mountains,
    so if we were to load all of them in an array upfront, we would allocate a quite
    significant amount of memory. Even if we then consume the data in the array through
    a `Readable` stream, all the data has already been preloaded, so we are effectively
    voiding the memory efficiency of streams. It's always preferable to load and consume
    the data in chunks, and you could do so by using native streams such as `fs.createReadStream`,
    by building a custom stream, or by using `Readable.from` with lazy iterables such
    as generators, iterators, or async iterators. We will see some examples of the
    latter approach in *Chapter 9*, *Behavioral Design Patterns*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽量不要在内存中实例化大型数组。想象一下，在前面的例子中，如果我们想列出世界上所有的山脉。大约有100万座山脉，如果我们一开始就将它们全部加载到数组中，我们会分配相当大的内存。即使我们通过一个`Readable`流消耗数组中的数据，所有数据已经被预先加载，所以我们实际上是在浪费流的内存效率。始终最好是分块加载和消耗数据，你可以通过使用原生的流，如`fs.createReadStream`，通过构建自定义流，或者使用`Readable.from`与懒迭代器，如生成器、迭代器或异步迭代器来实现。我们将在第9章，*行为设计模式*中看到后者的示例。
- en: Writable streams
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可写流
- en: A `Writable` stream represents a data destination. Imagine, for instance, a
    file on the filesystem, a database table, a socket, the standard error, or the
    standard output interface. In Node.js, it's implemented using the `Writable` abstract
    class, which is available in the `stream` module.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`Writable`流表示一个数据目的地。例如，想象一下文件系统上的一个文件、一个数据库表、一个套接字、标准错误或标准输出接口。在Node.js中，它使用`stream`模块中的`Writable`抽象类实现。'
- en: Writing to a stream
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向流中写入
- en: 'Pushing some data down a `Writable` stream is a straightforward business; all
    we have to do is use the `write()` method, which has the following signature:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 将一些数据推送到`Writable`流是一个简单的过程；我们只需要使用`write()`方法，该方法具有以下签名：
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `encoding` argument is optional and can be specified if `chunk` is a string
    (it defaults to `utf8`, and it's ignored if `chunk` is a buffer). The `callback`
    function, on the other hand, is called when the chunk is flushed into the underlying
    resource and is optional as well.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`encoding`参数是可选的，如果`chunk`是一个字符串（默认为`utf8`，如果`chunk`是缓冲区则忽略），则可以指定。另一方面，`callback`函数在数据块被刷新到底层资源时被调用，也是可选的。'
- en: 'To signal that no more data will be written to the stream, we have to use the
    `end()` method:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要表示不再向流中写入数据，我们必须使用`end()`方法：
- en: '[PRE26]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can provide a final chunk of data through the `end()` method; in this case,
    the `callback` function is equivalent to registering a listener to the `finish`
    event, which is fired when all the data written in the stream has been flushed
    into the underlying resource.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`end()`方法提供一个数据块；在这种情况下，`callback`函数相当于注册了一个监听器到`finish`事件，该事件在流中写入的所有数据都已刷新到底层资源时触发。
- en: 'Now, let''s show how this works by creating a small HTTP server that outputs
    a random sequence of strings:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过创建一个小型HTTP服务器来演示它是如何工作的，该服务器输出一个随机字符串序列：
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The HTTP server that we created writes into the `res` object, which is an instance
    of `http.ServerResponse` and also a `Writable` stream. What happens is explained
    as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的HTTP服务器写入到`res`对象中，它是`http.ServerResponse`的一个实例，也是一个`Writable`流。以下是解释发生的情况：
- en: We first write the head of the HTTP response. Note that `writeHead()` is not
    a part of the `Writable` interface; in fact, it's an auxiliary method exposed
    by the `http.ServerResponse` class and is specific to the HTTP protocol.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先写入HTTP响应的头部。请注意，`writeHead()`不是`Writable`接口的一部分；实际上，它是`http.ServerResponse`类暴露的一个辅助方法，并且特定于HTTP协议。
- en: We start a loop that terminates with a likelihood of 5% (we instruct `chance.bool()`
    to return `true` 95% of the time).
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动一个循环，其终止概率为5%（我们指示`chance.bool()`返回`true` 95%的时间）。
- en: Inside the loop, we write a random string into the stream.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环内部，我们将一个随机字符串写入流中。
- en: Once we are out of the loop, we call `end()` on the stream, indicating that
    no more data will be written. Also, we provide a final string containing two new
    line characters to be written into the stream before ending it.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们退出循环，我们就在流上调用`end()`，表示不再写入更多数据。此外，我们还提供了一个包含两个换行符的最终字符串，在结束之前写入流中。
- en: Finally, we register a listener for the `finish` event, which will be fired
    when all the data has been flushed into the underlying socket.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们为`finish`事件注册了一个监听器，该事件将在所有数据都已刷新到底层套接字时触发。
- en: 'To test the server, we can open a browser at the address `http://localhost:8080`
    or use `curl` from the terminal as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试服务器，我们可以打开地址为`http://localhost:8080`的浏览器，或者从终端使用以下`curl`命令：
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: At this point, the server should start sending random strings to the HTTP client
    that you chose (please bear in mind that some browsers might buffer the data,
    and the streaming behavior might not be apparent).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，服务器应该开始向您选择的HTTP客户端发送随机字符串（请记住，一些浏览器可能会缓冲数据，流式传输的行为可能不明显）。
- en: Backpressure
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 背压
- en: Similar to a liquid flowing in a real piping system, Node.js streams can also
    suffer from bottlenecks, where data is written faster than the stream can consume
    it. The mechanism to cope with this problem involves buffering the incoming data;
    however, if the stream doesn't give any feedback to the writer, we may incur a situation
    where more and more data is accumulated in the internal buffer, leading to undesired
    levels of memory usage.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与真实管道系统中流动的液体类似，Node.js流也可能遭受瓶颈，即数据写入速度超过流可以消耗的速度。处理此问题的机制涉及缓冲传入的数据；然而，如果流不对写入者提供任何反馈，我们可能会遇到越来越多的数据积累在内部缓冲区中的情况，导致不希望的内存使用水平。
- en: To prevent this from happening, `writable.write()` will return `false` when
    the internal buffer exceeds the `highWaterMark` limit. In `Writable` streams,
    the `highWaterMark` property is the limit of the internal buffer size, beyond
    which the `write()` method starts returning `false`, indicating that the application
    should now stop writing. When the buffer is emptied, the `drain` event is emitted,
    communicating that it's safe to start writing again. This mechanism is called
    **backpressure**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况发生，当内部缓冲区超过`highWaterMark`限制时，`writable.write()`将返回`false`。在`Writable`流中，`highWaterMark`属性是内部缓冲区大小的限制，超过这个限制，`write()`方法开始返回`false`，表示应用程序现在应该停止写入。当缓冲区清空时，会触发`drain`事件，表示现在可以安全地开始写入。这种机制称为**背压**。
- en: Backpressure is an advisory mechanism. Even if `write()` returns `false`, we
    could ignore this signal and continue writing, making the buffer grow indefinitely.
    The stream won't be blocked automatically when the `highWaterMark` threshold is
    reached; therefore, it is recommended to always be mindful and respect the backpressure.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 背压是一种建议机制。即使`write()`返回`false`，我们也可以忽略这个信号并继续写入，使缓冲区无限增长。当达到`highWaterMark`阈值时，流不会自动阻塞；因此，建议始终保持警觉并尊重背压。
- en: The mechanism described in this section is similarly applicable to `Readable`
    streams. In fact, backpressure exists in `Readable` streams too, and it's triggered
    when the `push()` method, which is invoked inside `_read()`, returns `false`.
    However, that's a problem specific to stream implementers, so we usually have
    to deal with it less frequently.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中描述的机制同样适用于`Readable`流。事实上，`Readable`流也存在背压，它是在`_read()`内部调用的`push()`方法返回`false`时触发的。然而，那是一个特定于流实现者的问题，所以我们通常不太经常处理它。
- en: 'We can quickly demonstrate how to take into account the backpressure of a `Writable`
    stream by modifying the `entropy-server.js` module that we created previously:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过修改之前创建的`entropy-server.js`模块来快速演示如何考虑`Writable`流的背压：
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The most important steps of the previous code can be summarized as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 之前代码最重要的步骤可以总结如下：
- en: We wrapped the main logic in a function called `generateMore()`.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将主要逻辑封装在一个名为`generateMore()`的函数中。
- en: To increase the chances of receiving some backpressure, we increased the size
    of the data chunk to 16 KB minus 1 byte, which is very close to the default `highWaterMark`
    limit.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了增加接收一些背压的机会，我们将数据块的大小增加到16 KB减去1字节，这非常接近默认的`highWaterMark`限制。
- en: After writing a chunk of data, we check the return value of `res.write()`. If
    we receive `false`, it means that the internal buffer is full and we should stop
    sending more data. When this happens, we exit the function and register another
    cycle of writes using `generateMore()` for when the `drain` event is emitted.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在写入一块数据后，我们检查`res.write()`的返回值。如果我们收到`false`，这意味着内部缓冲区已满，我们应该停止发送更多数据。当这种情况发生时，我们退出函数，并使用`generateMore()`注册另一个写入周期，以便在`drain`事件触发时进行。
- en: If we now try to run the server again, and then generate a client request with
    `curl`, there is a high probability that there will be some backpressure, as the
    server produces data at a very high rate, faster than the underlying socket can
    handle.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在再次尝试运行服务器，然后使用`curl`生成客户端请求，有很大可能性会出现一些背压，因为服务器以非常高的速率产生数据，这比底层套接字可以处理的速度要快。
- en: Implementing Writable streams
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现Writable流
- en: We can implement a new `Writable` stream by inheriting the class `Writable`
    and providing an implementation for the `_write()` method. Let's try to do it
    immediately while discussing the details along the way.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过继承`Writable`类并提供`_write()`方法的实现来创建一个新的`Writable`流。让我们立即尝试，同时在讨论过程中详细说明。
- en: 'Let''s build a `Writable` stream that receives objects in the following format:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个接收以下格式的对象的`Writable`流：
- en: '[PRE30]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: For each one of these objects, our stream has to save the `content` property
    into a file created at the given `path`. We can immediately see that the inputs
    of our stream are objects, and not strings or buffers. This means that our stream
    has to work in object mode.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些对象中的每一个，我们的流必须将`content`属性保存到在给定`path`处创建的文件中。我们可以立即看出，我们的流输入是对象，而不是字符串或缓冲区。这意味着我们的流必须工作在对象模式下。
- en: 'Let''s call the module `to-file-stream.js`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称这个模块为`to-file-stream.js`：
- en: '[PRE31]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We created a new class for our new stream, which extends `Writable` from the
    `stream` module.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为我们的新流创建了一个新类，该类从`stream`模块扩展了`Writable`。
- en: 'We had to invoke the parent constructor to initialize its internal state; we
    also needed to make sure that the `options` object specifies that the stream works
    in object mode (`objectMode: true`). Other options accepted by `Writable` are
    as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '我们必须调用父构造函数来初始化其内部状态；我们还需要确保`options`对象指定流在对象模式下工作（`objectMode: true`）。`Writable`接受的其它选项如下：'
- en: '`highWaterMark` (the default is 16 KB): This controls the backpressure limit.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`highWaterMark`（默认为16 KB）：这控制了背压限制。'
- en: '`decodeStrings` (defaults to `true`): This enables the automatic decoding of
    strings into binary buffers before passing them to the `_write()` method. This
    option is ignored in object mode.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decodeStrings`（默认为`true`）：这启用了在将字符串传递给`_write()`方法之前自动将字符串解码为二进制缓冲区的功能。在对象模式下，此选项被忽略。'
- en: Finally, we provided an implementation for the `_write()` method. As you can
    see, the method accepts a data chunk and an encoding (which makes sense only if
    we are in binary mode and the stream option `decodeStrings` is set to `false`).
    Also, the method accepts a callback function (`cb`), which needs to be invoked
    when the operation completes; it's not necessary to pass the result of the operation
    but, if needed, we can still pass an error that will cause the stream to emit
    an `error` event.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为`_write()`方法提供了一个实现。正如你所见，该方法接受一个数据块和一个编码（仅在二进制模式下且流选项`decodeStrings`设置为`false`时才有意义）。此外，该方法还接受一个回调函数（`cb`），当操作完成时需要调用该函数；不需要传递操作的结果，但如果需要，我们仍然可以传递一个错误，这将导致流发出一个`error`事件。
- en: 'Now, to try the stream that we just built, we can create a new module and perform
    some write operations against the stream:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了尝试我们刚刚构建的流，我们可以创建一个新的模块并对该流执行一些写操作：
- en: '[PRE32]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, we created and used our first custom `Writable` stream. Run the new module
    as usual and check its output. You will see that after the execution, three new
    files will be created within a new folder called `files`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建并使用了我们的第一个自定义`Writable`流。像往常一样运行新模块并检查其输出。你将看到执行后，在名为`files`的新文件夹内将创建三个新的文件。
- en: Simplified construction
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简化构建
- en: 'As we saw for `Readable` streams, `Writable` streams also offer a simplified
    construction mechanism. If we were to rewrite our `ToFileStream` using the simplified
    construction for `Writable` streams, it would look like this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在`Readable`流中看到的，`Writable`流也提供了一个简化的构建机制。如果我们使用`Writable`流的简化构建来重写我们的`ToFileStream`，它将看起来像这样：
- en: '[PRE33]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: With this approach, we are simply using the `Writable` constructor and passing
    a `write()` function that implements the custom logic of our `Writable` instance.
    Note that with this approach, the `write()` function doesn't have an underscore
    in the name. We can also pass other construction options like `objectMode`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们只是使用`Writable`构造函数并传递一个实现我们`Writable`实例自定义逻辑的`write()`函数。请注意，使用这种方法，`write()`函数的名称中没有下划线。我们还可以传递其他构造选项，如`objectMode`。
- en: Duplex streams
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Duplex流
- en: A `Duplex` stream is a stream that is both `Readable` and `Writable`. It is
    useful when we want to describe an entity that is both a data source and a data
    destination, such as network sockets, for example. `Duplex` streams inherit the
    methods of both `stream.Readable` and `stream.Writable`, so this is nothing new
    to us. This means that we can `read()` or `write()` data, or listen for both `readable`
    and `drain` events.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `Duplex` 流是一个既是 `Readable` 又是 `Writable` 的流。当我们需要描述一个既是数据源又是数据目的地的实体时，它非常有用，例如网络套接字。`Duplex`
    流继承了 `stream.Readable` 和 `stream.Writable` 的方法，所以这对我们来说并不陌生。这意味着我们可以 `read()`
    或 `write()` 数据，或者监听 `readable` 和 `drain` 事件。
- en: To create a custom `Duplex` stream, we have to provide an implementation for
    both `_read()` and `_write()`. The `options` object passed to the `Duplex()` constructor
    is internally forwarded to both the `Readable` and `Writable` constructors. The
    options are the same as those we already discussed in the previous sections, with
    the addition of a new one called `allowHalfOpen` (defaults to `true`) that, if
    set to `false`, will cause both parts (`Readable` and `Writable`) of the stream
    to end if only one of them does.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个自定义的 `Duplex` 流，我们必须为 `_read()` 和 `_write()` 提供实现。传递给 `Duplex()` 构造函数的
    `options` 对象内部转发到 `Readable` 和 `Writable` 构造函数。这些选项与我们在前面的章节中讨论的选项相同，新增了一个名为 `allowHalfOpen`（默认为
    `true`）的选项，如果设置为 `false`，则当只有其中一部分（`Readable` 或 `Writable`）结束时，两部分都会结束。
- en: If we need to have a `Duplex` stream working in object mode on one side and
    binary mode on the other, we can use the options `readableObjectMode` and `writableObjectMode`
    independently.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在一边使用对象模式，在另一边使用二进制模式，我们可以独立使用 `readableObjectMode` 和 `writableObjectMode`
    选项。
- en: Transform streams
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换流
- en: '`Transform` streams are a special kind of `Duplex` stream that are specifically
    designed to handle data transformations. Just to give you a few concrete examples,
    the functions `zlib.createGzip()` and `crypto.createCipheriv()` that we discussed
    at the beginning of this chapter create `Transform` streams for compression and
    encryption, respectively.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transform` 流是一种特殊的 `Duplex` 流，专门设计用来处理数据转换。仅举几个具体的例子，我们在本章开头讨论的 `zlib.createGzip()`
    和 `crypto.createCipheriv()` 函数分别创建用于压缩和加密的 `Transform` 流。'
- en: 'In a simple `Duplex` stream, there is no immediate relationship between the
    data read from the stream and the data written into it (at least, the stream is
    agnostic to such a relationship). Think about a TCP socket, which just sends and
    receives data to and from the remote peer; the socket is not aware of any relationship
    between the input and output. *Figure 6.4* illustrates the data flow in a `Duplex`
    stream:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个简单的 `Duplex` 流中，从流中读取的数据和写入流中的数据之间没有直接的关系（至少，流对此类关系是无关紧要的）。想想 TCP 套接字，它只是向远程对等方发送和接收数据；套接字并不了解输入和输出之间的任何关系。*图
    6.4* 阐述了 `Duplex` 流中的数据流：
- en: '![](img/B15729_06_04.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_06_04.png)'
- en: 'Figure 6.4: Duplex stream schematic representation'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：`Duplex` 流示意图
- en: 'On the other hand, `Transform` streams apply some kind of transformation to
    each chunk of data that they receive from their `Writable` side, and then make
    the transformed data available on their `Readable` side. *Figure 6.5* shows how
    the data flows in a `Transform` stream:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`Transform` 流会对它们从 `Writable` 端接收到的每个数据块应用某种转换，然后在其 `Readable` 端提供转换后的数据。*图
    6.5* 展示了 `Transform` 流中的数据流：
- en: '![](img/B15729_06_05.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_06_05.png)'
- en: 'Figure 6.5: Transform stream schematic representation'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：`Transform` 流示意图
- en: 'From the outside, the interface of a `Transform` stream is exactly like that
    of a `Duplex` stream. However, when we want to build a new `Duplex` stream, we
    have to provide both the `_read()` and `_write()` methods, while for implementing
    a new `Transform` stream, we have to fill in another pair of methods: `_transform()`
    and `_flush()`.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 从外部看，`Transform` 流的接口与 `Duplex` 流的接口完全相同。然而，当我们想要构建一个新的 `Duplex` 流时，我们必须提供 `_read()`
    和 `_write()` 方法，而对于实现一个新的 `Transform` 流，我们必须填写另一对方法：`_transform()` 和 `_flush()`。
- en: Let's see how to create a new `Transform` stream with an example.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子看看如何创建一个新的 `Transform` 流。
- en: Implementing Transform streams
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现转换流
- en: 'Let''s implement a `Transform` stream that replaces all the occurrences of
    a given string. To do this, we have to create a new module named `replaceStream.js`.
    Let''s jump directly to the implementation:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个替换给定字符串出现的 `Transform` 流。为此，我们必须创建一个名为 `replaceStream.js` 的新模块。让我们直接进入实现：
- en: '[PRE34]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In this example, we created a new class extending the `Transform` base class.
    The constructor of the class accepts three arguments: `searchStr`, `replaceStr`,
    and `options`. As you can imagine, they allow us to define the text to match and
    the string to use as a replacement, plus an `options` object for advanced configuration
    of the underlying `Transform` stream. We also initialize an internal `tail` variable,
    which will be used later by the `_transform()` method.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了一个新的类，它扩展了 `Transform` 基类。该类的构造函数接受三个参数：`searchStr`、`replaceStr`
    和 `options`。正如你所想象的那样，它们允许我们定义要匹配的文本和用作替换的字符串，以及一个用于对底层 `Transform` 流进行高级配置的 `options`
    对象。我们还初始化了一个内部 `tail` 变量，它将被 `_transform()` 方法稍后使用。
- en: Now, let's analyze the `_transform()` method, which is the core of our new class.
    The `_transform()` method has practically the same signature as the `_write()`
    method of the `Writable` stream, but instead of writing data into an underlying
    resource, it pushes it into the internal read buffer using `this.push()`, exactly
    as we would do in the `_read()` method of a `Readable` stream. This shows how
    the two sides of a `Transform` stream are connected.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们分析 `_transform()` 方法，这是我们新类中的核心。`_transform()` 方法几乎与 `Writable` 流的 `_write()`
    方法的签名相同，但它不是将数据写入底层资源，而是使用 `this.push()` 将其推入内部读取缓冲区，这与我们在 `Readable` 流的 `_read()`
    方法中所做的一样。这展示了 `Transform` 流的两端是如何连接的。
- en: 'The `_transform()` method of `ReplaceStream` implements the core of our algorithm.
    To search for and replace a string in a buffer is an easy task; however, it''s
    a totally different story when the data is streaming, and possible matches might
    be distributed across multiple chunks. The procedure followed by the code can
    be explained as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReplaceStream` 的 `_transform()` 方法实现了我们算法的核心。在缓冲区中搜索和替换字符串是一个简单的任务；然而，当数据是流式传输的，并且可能的匹配项可能分布在多个块中时，这完全是另一回事。代码遵循的步骤可以解释如下：'
- en: Our algorithm splits the data in memory (`tail` data and the current `chunk`)
    using `searchStr` as a separator.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的算法使用 `searchStr` 作为分隔符，在内存中分割数据（`tail` 数据和当前 `chunk`）。
- en: Then, it takes the last item of the array generated by the operation and extracts
    the last `searchString.length - 1` characters. The result is saved in the `tail`
    variable and will be prepended to the next chunk of data.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它取操作生成的数组中的最后一个元素，并提取最后 `searchString.length - 1` 个字符。结果保存在 `tail` 变量中，并将被添加到下一个数据块的开头。
- en: Finally, all the pieces resulting from `split()` are joined together using `replaceStr`
    as a separator and pushed into the internal buffer.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用 `replaceStr` 作为分隔符将 `split()` 生成的所有片段连接起来，并推入内部缓冲区。
- en: When the stream ends, we might still have some content in the `tail` variable
    not pushed into the internal buffer. That's exactly what the `_flush()` method
    is for; it is invoked just before the stream is ended, and this is where we have
    one final chance to finalize the stream or push any remaining data before completely
    ending the stream.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 当流结束时，我们可能在 `tail` 变量中还有一些内容尚未推入内部缓冲区。这正是 `_flush()` 方法的作用；它在流结束前被调用，这是我们最后一次机会来最终化流或推入任何剩余的数据，以完全结束流。
- en: The `_flush()` method only takes in a callback, which we have to make sure to
    invoke when all the operations are complete, causing the stream to be terminated.
    With this, we have completed our `ReplaceStream` class.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`_flush()` 方法只接受一个回调函数，我们必须确保在所有操作完成后调用它，这将导致流被终止。这样，我们就完成了 `ReplaceStream`
    类。'
- en: 'Now, it''s time to try the new stream. Let''s create a script that writes some
    data into the stream and then reads the transformed result:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候尝试新的流了。让我们创建一个脚本，将一些数据写入流，然后读取转换后的结果：
- en: '[PRE35]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To make life a little bit harder for our stream, we spread the search term
    (which is `World`) across two different chunks, then, using the flowing mode,
    we read from the same stream, logging each transformed chunk. Running the preceding
    program should produce the following output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的流更难一些，我们将搜索词（即 `World`）分散到两个不同的块中，然后，使用流动模式，我们从同一个流中读取，记录每个转换后的块。运行前面的程序应该会产生以下输出：
- en: '[PRE36]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Please note that the preceding output is broken into multiple lines because
    we are using `console.log()` to print it out. This allows us to demonstrate that
    our implementation is able to replace string matches correctly, even when the
    matching text spans multiple chunks of data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的输出被分成多行，因为我们正在使用 `console.log()` 打印它。这使我们能够展示我们的实现能够正确地替换字符串匹配，即使匹配文本跨越多个数据块。
- en: Simplified construction
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简化构建
- en: 'Unsurprisingly, even `Transform` streams support simplified construction. At
    this point, we should have developed an instinct for how this API might look,
    so let''s get our hands dirty straight away and rewrite the previous example using
    this approach:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 不足为奇，即使是 `Transform` 流也支持简化构建。到目前为止，我们应该已经对这种 API 的外观有了直觉，所以让我们直接动手重写之前的例子，使用这种方法：
- en: '[PRE37]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As expected, simplified construction works by directly instantiating a new `Transform`
    object and passing our specific transformation logic through the `transform()`
    and `flush()` functions directly through the `options` object. Note that `transform()`
    and `flush()` don't have a prepended underscore here.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，简化构建通过直接实例化一个新的 `Transform` 对象，并通过 `options` 对象直接通过 `transform()` 和 `flush()`
    函数传递我们的特定转换逻辑来实现。请注意，这里的 `transform()` 和 `flush()` 没有前置下划线。
- en: Filtering and aggregating data with Transform streams
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Transform 流过滤和聚合数据
- en: As we mentioned in the previous section, `Transform` streams are the perfect
    building blocks for implementing data transformation pipelines. In the previous
    section, we illustrated an example of a `Transform` stream that can replace words
    in a stream of text. But `Transform` streams can be used to implement other types
    of data transformation as well. For instance, it's quite common to use `Transform`
    streams to implement data filtering and data aggregation.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中提到的，`Transform` 流是实现数据转换管道的完美构建块。在上一节中，我们展示了 `Transform` 流可以替换文本流中的单词的例子。但
    `Transform` 流也可以用来实现其他类型的数据转换。例如，使用 `Transform` 流来实现数据过滤和数据聚合是非常常见的。
- en: Just to make a practical example, let's imagine we are asked by a Fortune 500
    company to analyze a big file containing all the sales for the year 2020\. The
    company wants us to use `data.csv`, a sales report in CSV format, to calculate
    the total profit for the sales made in Italy.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 只为了做一个实际例子，让我们假设我们被一家财富 500 强公司要求分析一个包含 2020 年所有销售的大文件。公司希望我们使用 `data.csv`，这是一个
    CSV 格式的销售报告，来计算在意大利进行的销售的总利润。
- en: 'For simplicity, let''s imagine the sales data that is stored in the CSV file
    contains three fields per line: item type, country of sale, and profit. So, such
    a file could look like this:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们假设存储在 CSV 文件中的销售数据每行包含三个字段：项目类型、销售国家和利润。因此，这样的文件可能看起来像这样：
- en: '[PRE38]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, it's clear that we have to find all the records that have "Italy" as `country`
    and, in the process, sum up the `profit` value of the matching lines into a single
    number.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很明显，我们必须找到所有 `country` 字段为 "Italy" 的记录，并在过程中将匹配行的 `profit` 值累加到一个单独的数字中。
- en: In order to process a CSV file in a streaming fashion, we can use the excellent
    `csv-parse` module ([nodejsdp.link/csv-parse](http://nodejsdp.link/csv-parse)).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以流式处理方式处理 CSV 文件，我们可以使用出色的 `csv-parse` 模块 ([nodejsdp.link/csv-parse](http://nodejsdp.link/csv-parse))。
- en: 'If we assume for a moment that we have already implemented our custom streams
    to filter and aggregate the data, a possible solution to this task might look
    like this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们暂时已经实现了我们的自定义流来过滤和聚合数据，这个任务的可能的解决方案可能看起来像这样：
- en: '[PRE39]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The streaming pipeline proposed here consists of five steps:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提出的流式管道由五个步骤组成：
- en: We read the source CSV file as a stream.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将源 CSV 文件作为流读取。
- en: We use the `csv-parse` library to parse every line of the document as a CSV
    record. For every line, this stream will emit an object containing the properties
    `type`, `country`, and `profit`.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `csv-parse` 库将文档的每一行解析为 CSV 记录。对于每一行，这个流将发出一个包含 `type`、`country` 和 `profit`
    属性的对象。
- en: We filter all the records by country, retaining only the records that match
    the country "Italy." All the records that don't match "Italy" are dropped, which
    means that they will not be passed to the other steps in the pipeline. Note that
    this is one of the custom `Transform` streams that we have to implement.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过国家过滤所有记录，仅保留匹配国家 "Italy" 的记录。所有不匹配 "Italy" 的记录将被丢弃，这意味着它们将不会传递到管道中的其他步骤。请注意，这是我们必须实现的自定义
    `Transform` 流之一。
- en: For every record, we accumulate the profit. This stream will eventually emit
    one single string, which represents the value of the total profit for products
    sold in Italy. This value will be emitted by the stream only when all the data
    from the original file has been completely processed. Note that this is the second
    custom `Transform` stream that we have to implement to complete this project.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每条记录，我们累积利润。这个流最终会发出一个字符串，表示在意大利销售的产品总利润值。这个值只有在原始文件的所有数据都完全处理完毕后才会由流发出。请注意，这是我们必须实现的第二个自定义`Transform`流，以完成此项目。
- en: Finally, the data emitted from the previous step is displayed in the standard
    output.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，前一步骤发出的数据将在标准输出中显示。
- en: 'Now, let''s implement the `FilterByCountry` stream:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现`FilterByCountry`流：
- en: '[PRE40]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`FilterByCountry` is a custom `Transform` stream. We can see that the constructor
    accepts an argument called `country`, which allows us to specify the country name
    we want to filter on. In the constructor, we also set the stream to run in `objectMode`
    because we know it will be used to process objects (records from the CSV file).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`FilterByCountry`是一个自定义的`Transform`流。我们可以看到构造函数接受一个名为`country`的参数，这允许我们指定要过滤的国家名称。在构造函数中，我们还设置了流以`objectMode`模式运行，因为我们知道它将被用于处理对象（CSV文件的记录）。'
- en: In the `_transform` method, we check if the country of the current record matches
    the country specified at construction time. If it's a match, then we pass the
    record on to the next stage of the pipeline by calling `this.push()`. Regardless
    of whether the record matches or not, we need to invoke `cb()` to indicate that
    the current record has been successfully processed and that the stream is ready
    to receive another record.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在`_transform`方法中，我们检查当前记录的国家是否与构建时指定的国家匹配。如果匹配，则通过调用`this.push()`将记录传递到管道的下一阶段。无论记录是否匹配，我们都需要调用`cb()`来表示当前记录已成功处理，并且流已准备好接收另一个记录。
- en: '**Pattern: Transform filter**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式：转换过滤器**'
- en: Invoke `this.push()` in a conditional way to allow only some data to reach the
    next stage of the pipeline.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 以条件方式调用`this.push()`，以允许只有一些数据达到管道的下一阶段。
- en: 'Finally, let''s implement the `SumProfit` filter:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们实现`SumProfit`过滤器：
- en: '[PRE41]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This stream needs to run in `objectMode` as well, because it will receive objects
    representing records from the CSV file. Note that, in the constructor, we also
    initialize an instance variable called `total` and we set its value to `0`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 此流也需要以`objectMode`模式运行，因为它将接收代表CSV文件记录的对象。请注意，在构造函数中，我们还初始化了一个名为`total`的实例变量，并将其值设置为`0`。
- en: In the `_transform()` method, we process every record and use the current `profit`
    value to increase the `total`. It's important to note that this time, we are not
    calling `this.push()`. This means that no value is emitted while the data is flowing
    through the stream. We still need to call `cb()`, though, to indicate that the
    current record has been processed and the stream is ready to receive another one.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在`_transform()`方法中，我们处理每条记录，并使用当前的`profit`值来增加`total`。重要的是要注意，这次我们没有调用`this.push()`。这意味着在数据通过流流动时不会发出任何值。尽管如此，我们仍然需要调用`cb()`，以表示当前记录已被处理，并且流已准备好接收另一个记录。
- en: In order to emit the final result when all the data has been processed, we have
    to define a custom flush behavior using the `_flush()` method. Here, we finally
    call `this.push()` to emit a string representation of the resulting `total` value.
    Remember that `_flush()` is automatically invoked before the stream is closed.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在所有数据都处理完毕时发出最终结果，我们必须使用`_flush()`方法定义一个自定义的刷新行为。在这里，我们最终调用`this.push()`来发出结果`total`值的字符串表示。请记住，`_flush()`是在流关闭之前自动调用的。
- en: '**Pattern: Streaming aggregation**'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式：流聚合**'
- en: Use `_transform()` to process the data and accumulate the partial result, then
    call `this.push()` only in the `_flush()` method to emit the result when all the
    data has been processed.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`_transform()`处理数据并累积部分结果，然后在`_flush()`方法中调用`this.push()`以在所有数据都处理完毕时发出结果。
- en: This completes our example. Now, you can grab the CSV file from the code repository
    and execute this program to see what the total profit for Italy is. No surprise
    it's going to be a lot of money since we are talking about the profit of a Fortune
    500 company!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们的示例。现在，你可以从代码仓库中获取CSV文件并执行此程序，以查看意大利的总利润是多少。毫不奇怪，这将是一大笔钱，因为我们谈论的是一家《财富》500强公司的利润！
- en: PassThrough streams
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 透传流
- en: 'There is a fifth type of stream that is worth mentioning: `PassThrough`. This
    type of stream is a special type of `Transform` that outputs every data chunk
    without applying any transformation.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，还有一种第五种类型的流，那就是`PassThrough`。这种类型的流是一种特殊的`Transform`流，它输出每个数据块而不应用任何转换。
- en: '`PassThrough` is possibly the most underrated type of stream, but there are
    actually several circumstances in which it can be a very valuable tool in our
    toolbelt. For instance, `PassThrough` streams can be useful for observability
    or to implement late piping and lazy stream patterns.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`PassThrough`可能是最被低估的流类型，但实际上，在几种情况下，它可以成为我们工具箱中非常有价值的工具。例如，`PassThrough`流对于可观察性或实现延迟管道和懒流模式非常有用。'
- en: Observability
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可观察性
- en: 'If we want to observe how much data is flowing through one or more streams,
    we could do so by attaching a `data` event listener to a `PassThrough` instance
    and then piping this instance in a given point of a stream pipeline. Let''s a
    see a simplified example to be able to appreciate this concept:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要观察通过一个或多个流流动的数据量，我们可以通过将`data`事件监听器附加到`PassThrough`实例，然后在流管道的某个点上管道此实例来实现。让我们看看一个简化的例子，以便能够欣赏这个概念：
- en: '[PRE42]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In this example, we are creating a new instance of `PassThrough` and using
    the `data` event to count how many bytes are flowing through the stream. We also
    use the `finish` event to dump the total amount to the console. Finally we write
    some data directly into the stream using `write()` and `end()`. This is just an
    illustrative example; in a more realistic scenario, we would be piping our `monitor`
    instance in a given point of a stream pipeline. For instance, if we wanted to
    monitor how many bytes are written to disk in our first file compression example
    of this chapter, we could easily achieve that by doing something like this:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了一个新的`PassThrough`实例，并使用`data`事件来计算通过流流动的字节数。我们还使用`finish`事件将总量输出到控制台。最后，我们使用`write()`和`end()`直接将一些数据写入流。这只是一个说明性的例子；在更现实的场景中，我们会在流管道的某个点上管道我们的`monitor`实例。例如，如果我们想要监控在章节的第一个文件压缩示例中写入磁盘的字节数，我们可以轻松地通过这样做来实现：
- en: '[PRE43]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The beauty of this approach is that we didn't have to touch any of the other
    existing streams in the pipeline, so if we need to observe other parts of the
    pipeline (for instance, imagine we want to know the number of bytes of the uncompressed
    data), we can move `monitor` around with very little effort.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是，我们不需要触及管道中其他现有的任何流，因此如果我们需要观察管道的其他部分（例如，假设我们想知道未压缩数据的字节数），我们可以轻松地将`monitor`移动到管道的某个点上。
- en: Note that you could implement an alternative version of the `monitor` stream
    by using a custom transform stream instead. In such a case, you would have to
    make sure that the received chunks are pushed without any modification or delay,
    which is something that a `PassThrough` stream would do automatically for you.
    Both approaches are equally valid, so pick the approach that feels more natural
    to you.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您可以通过使用自定义转换流来实现`monitor`流的替代版本。在这种情况下，您必须确保接收到的块被无修改或延迟地推送到，这正是`PassThrough`流会自动为您做的。两种方法都是同样有效的，所以选择您觉得更自然的方法。
- en: Late piping
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟管道
- en: In some circumstances, we might have to use APIs that accept a stream as an
    input parameter. This is generally not a big deal because we already know how
    to create and use streams. However, it may get a little bit more complicated if
    the data we want to read or write through the stream is available after we've
    called the given API.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能需要使用接受流作为输入参数的API。这通常不是什么大问题，因为我们已经知道如何创建和使用流。然而，如果我们想要通过流读取或写入的数据在调用给定的API之后才可用，这可能会变得稍微复杂一些。
- en: 'To visualize this scenario in more concrete terms, let''s imagine that we have
    to use an API that gives us the following function to upload a file to a data
    storage service:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地可视化这个场景，让我们想象一下，我们必须使用一个API，该API提供了一个函数来将文件上传到数据存储服务：
- en: '[PRE44]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This function is effectively a simplified version of what is commonly available
    in the SDK of file storage services like Amazon Simple Storage Service (S3) or
    Azure Blob Storage service. Often, those libraries will provide the user with
    a more flexible function that can receive the content data in different formats
    (for example, a string, a buffer, or a `Readable` stream).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数实际上是一个简化版本，通常在像亚马逊简单存储服务（Amazon Simple Storage Service, S3）或Azure Blob存储服务这样的文件存储服务的SDK中可用。通常，这些库会向用户提供一个更灵活的函数，该函数可以接收不同格式的数据内容（例如，一个字符串、一个缓冲区或一个`Readable`流）。
- en: 'Now, if we want to upload a file from the filesystem, this is a trivial operation,
    and we can do something like this:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想从文件系统上传文件，这是一个简单的操作，我们可以这样做：
- en: '[PRE45]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: But what if we want to do some processing to the file stream before the upload.
    For instance, let's say we want to compress or encrypt the data? Also, what if
    we have to do this transformation asynchronously after the `upload` function has
    been called?
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们想在上传之前对文件流进行一些处理怎么办。例如，假设我们想压缩或加密数据？此外，如果我们必须在`upload`函数调用之后异步执行此转换怎么办？
- en: In such cases, we can provide a `PassThrough` stream to the `upload()` function,
    which will effectively act as a placeholder. The internal implementation of `upload()`
    will immediately try to consume data from it, but there will be no data available
    in the stream until we actually write to it. Also, the stream won't be considered
    complete until we close it, so the `upload()` function will have to wait for data
    to flow through the `PassThrough` instance to initiate the upload.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以向`upload()`函数提供一个`PassThrough`流，它将有效地充当占位符。`upload()`函数的内部实现将立即尝试从其中消耗数据，但在我们实际写入之前，流中不会有任何数据。此外，直到我们关闭它，流才被认为是完整的，因此`upload()`函数将不得不等待数据通过`PassThrough`实例流动以启动上传。
- en: 'Let''s see a possible command-line script that uses this approach to upload
    a file from the filesystem and also compresses it using the **Brotli** compression.
    We are going to assume that the third-party `upload()` function is provided in
    a file called `upload.js`:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个可能的命令行脚本，该脚本使用这种方法从文件系统上传文件，并使用**Brotli**压缩进行压缩。我们将假设第三方`upload()`函数在一个名为`upload.js`的文件中提供：
- en: '[PRE46]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In this book's repository, you will find a complete implementation of this example
    that allows you to upload files to an HTTP server that you can run locally.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的存储库中，您将找到一个完整的实现示例，允许您将文件上传到您可以在本地运行的HTTP服务器。
- en: 'Let''s review what''s happening in the previous example:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下上一个例子中发生的事情：
- en: We get the path to the file we want to upload from the first command-line argument
    and use `basename` to extrapolate the filename from the given path.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从第一个命令行参数中获取我们想要上传的文件的路径，并使用`basename`从给定的路径中推断出文件名。
- en: We create a placeholder for our content stream as a `PassThrough` instance.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个内容流的占位符作为`PassThrough`实例。
- en: Now, we invoke the `upload` function by passing our filename (with the added
    `.br` suffix, indicating that it is using the Brotli compression) and the placeholder
    content stream.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们通过传递我们的文件名（添加了`.br`后缀，表示使用Brotli压缩）和占位符内容流来调用`upload`函数。
- en: Finally, we create a pipeline by chaining a filesystem `Readable` stream, a
    Brotli compression `Transform` stream, and finally our content stream as the destination.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过链式连接一个文件系统的`Readable`流、一个Brotli压缩`Transform`流，最后将我们的内容流作为目的地来创建一个管道。
- en: When this code is executed, the upload will start as soon as we invoke the `upload()`
    function (possibly establishing a connection to the remote server), but the data
    will start to flow only later, when our pipeline is initialized. Note that our
    pipeline will also close the `contentStream` when the processing completes, which
    will indicate to the `upload()` function that all the content has been fully consumed.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 当此代码执行时，上传将在我们调用`upload()`函数时开始（可能建立与远程服务器的连接），但数据将在稍后开始流动，即当我们的管道初始化时。请注意，我们的管道在处理完成后也会关闭`contentStream`，这将向`upload()`函数表明所有内容都已完全消耗。
- en: '**Pattern**'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式**'
- en: Use a `PassThrough` stream when you need to provide a placeholder for data that
    will be read or written in the future.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要为将来要读取或写入的数据提供一个占位符时，请使用`PassThrough`流。
- en: 'We can also use this pattern to transform the interface of the `upload()` function.
    Instead of accepting a `Readable` stream as input, we can make it return a Writeable
    stream, which can then be used to provide the data we want to upload:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用这种模式来转换 `upload()` 函数的接口。而不是接受一个 `Readable` 流作为输入，我们可以让它返回一个 `Writeable`
    流，然后可以使用它来提供我们想要上传的数据：
- en: '[PRE47]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'If we were tasked to implement this function, we could achieve that in a very
    elegant way by using a `PassThrough` instance, as in the following example implementation:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们被要求实现这个函数，我们可以通过使用 `PassThrough` 实例以非常优雅的方式实现它，如下面的示例实现所示：
- en: '[PRE48]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding code, we are using a `PassThrough` stream as a connector. This
    stream becomes a perfect abstraction for a case where the consumer of the library
    can write data at any later stage.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `PassThrough` 流作为连接器。这个流成为了一个完美的抽象，在这种情况下，库的消费者可以在任何后续阶段写入数据。
- en: 'The `createUploadStream()` function can then be used as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用 `createUploadStream()` 函数如下所示：
- en: '[PRE49]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This book's repository also contains an HTTP upload example that adopts this
    alternative pattern.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的仓库还包含一个采用这种替代模式的 HTTP 上传示例。
- en: Lazy streams
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 懒流
- en: Sometimes, we need to create a large number of streams at the same time, for
    example, to pass them to a function for further processing. A typical example
    is when using `archiver` ([nodejsdp.link/archiver](http://nodejsdp.link/archiver)),
    a package for creating archives such as TAR and ZIP. The `archiver` package allows
    you to create an archive from a set of streams, representing the files to add.
    The problem is that if we want to pass a large number of streams, such as from
    files on the filesystem, we would likely get an `EMFILE, too many open files`
    error. This is because functions like `createReadStream()` from the `fs` module
    will actually open a file descriptor every time a new stream is created, even
    before you start to read from those streams.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要同时创建大量流，例如，将它们传递给一个函数进行进一步处理。一个典型的例子是在使用 `archiver` ([nodejsdp.link/archiver](http://nodejsdp.link/archiver))
    包时，这是一个用于创建归档文件如 TAR 和 ZIP 的包。`archiver` 包允许你从一组流中创建一个归档，这些流代表要添加的文件。问题是如果我们想传递大量流，比如来自文件系统的文件，我们可能会遇到
    `EMFILE, too many open files` 错误。这是因为 `fs` 模块中的 `createReadStream()` 函数实际上会在创建每个新流时打开一个文件描述符，即使在你开始从这些流中读取之前。
- en: In more generic terms, creating a stream instance might initialize expensive
    operations straight away (for example, open a file or a socket, initialize a connection
    to a database, and so on), even before we actually start to use such a stream.
    This might not be desirable if you are creating a large number of stream instances
    for later consumption.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 用更通用的术语来说，创建流实例可能会立即初始化昂贵的操作（例如，打开文件或套接字，初始化与数据库的连接等），即使在我们实际上开始使用这样的流之前。如果你正在创建大量流实例以供以后使用，这可能不是你所希望的。
- en: In these cases, you might want to delay the expensive initialization until you
    actually need to consume data from the stream.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你可能希望延迟昂贵的初始化，直到你实际上需要从流中消耗数据。
- en: It is possible to achieve this by using a library like `lazystream` ([nodejsdp.link/lazystream](http://nodejsdp.link/lazystream)).
    This library allows you to effectively create proxies for actual stream instances,
    where the proxied instance is not created until some piece of code is actually
    starting to consume data from the proxy.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用像 `lazystream` ([nodejsdp.link/lazystream](http://nodejsdp.link/lazystream))
    这样的库，我们可以实现这一点。这个库允许你有效地为实际的流实例创建代理，其中代理实例是在实际开始从代理中消耗数据之前不会创建的。
- en: 'In the following example, `lazystream` allows us to create a lazy `Readable`
    stream for the special Unix file `/dev/urandom`:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，`lazystream` 允许我们为特殊的 Unix 文件 `/dev/urandom` 创建一个懒 `Readable` 流：
- en: '[PRE50]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The function we pass as a parameter to `new lazystream.Readable()` is effectively
    a factory function that generates the proxied stream when necessary.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递给 `new lazystream.Readable()` 的函数实际上是一个工厂函数，它在必要时生成代理流。
- en: Behind the scenes, `lazystream` is implemented using a `PassThrough` stream
    that, only when its `_read()` method is invoked for the first time, creates the
    proxied instance by invoking the factory function, and pipes the generated stream
    into the `PassThrough` itself. The code that consumes the stream is totally agnostic
    of the proxying that is happening here, and it will consume the data as if it
    was flowing directly from the `PassThrough` stream. `lazystream` implements a
    similar utility to create lazy `Writable` streams as well.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，`lazystream` 是通过使用一个 `PassThrough` 流实现的，该流仅在第一次调用其 `_read()` 方法时，通过调用工厂函数创建代理实例，并将生成的流管道连接到
    `PassThrough` 本身。消费流的代码对这里发生的代理行为一无所知，并且它将数据消费得就像它直接从 `PassThrough` 流中流出一样。`lazystream`
    实现了类似的实用工具，用于创建懒 `Writable` 流。
- en: Creating lazy `Readable` and `Writable` streams from scratch could be an interesting
    exercise that is left to you. If you get stuck, have a look at the source code
    of `lazystream` for inspiration on how to implement this pattern.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始创建懒 `Readable` 和 `Writable` 流可能是一个有趣的练习，留给你来完成。如果你卡住了，可以查看 `lazystream`
    的源代码，从中获得如何实现此模式的灵感。
- en: In the next section, we will discuss the `.pipe()` method in greater detail
    and also see other ways to connect different streams to form a processing pipeline.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地讨论 `.pipe()` 方法，并查看连接不同流以形成处理管道的其他方法。
- en: Connecting streams using pipes
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用管道连接流
- en: 'The concept of Unix pipes was invented by Douglas Mcllroy. This enabled the
    output of a program to be connected to the input of the next. Take a look at the
    following command:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Unix 管道的概念是由 Douglas Mcllroy 发明的。这使得一个程序的输出可以连接到下一个程序的输入。看看以下命令：
- en: '[PRE51]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the preceding command, `echo` will write `Hello World!` to its standard output,
    which is then redirected to the standard input of the `sed` command (thanks to
    the pipe `|` operator). Then, `sed` replaces any occurrence of `World` with `Node.js`
    and prints the result to its standard output (which, this time, is the console).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，`echo` 将 `Hello World!` 写入其标准输出，然后通过管道 `|` 操作符重定向到 `sed` 命令的标准输入（感谢管道操作符）。然后，`sed`
    将 `World` 的任何出现替换为 `Node.js` 并将结果打印到其标准输出（这次是控制台）。
- en: 'In a similar way, Node.js streams can be connected using the `pipe()` method
    of the `Readable` stream, which has the following interface:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，Node.js 流可以使用 `Readable` 流的 `pipe()` 方法连接，该方法的接口如下：
- en: '[PRE52]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Very intuitively, the `pipe()` method takes the data that is emitted from the
    `readable` stream and pumps it into the provided `writable` stream. Also, the
    `writable` stream is ended automatically when the `readable` stream emits an `end`
    event (unless we specify `{end: false}` as `options`). The `pipe()` method returns
    the `writable` stream passed in the first argument, allowing us to create chained
    invocations if such a stream is also `Readable` (such as a `Duplex` or `Transform`
    stream).'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '非常直观地，`pipe()` 方法将来自 `readable` 流的数据泵入提供的 `writable` 流。此外，当 `readable` 流发出
    `end` 事件时，`writable` 流会自动结束（除非我们指定 `{end: false}` 作为 `options`）。`pipe()` 方法返回第一个参数中传入的
    `writable` 流，允许我们创建链式调用，如果这样的流也是 `Readable`（例如 `Duplex` 或 `Transform` 流）。'
- en: Piping two streams together will create *suction*, which allows the data to
    flow automatically to the `writable` stream, so there is no need to call `read()`
    or `write()`, but most importantly, there is no need to control the backpressure
    anymore, because it's automatically taken care of.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个流一起管道化将创建 *吸力*，允许数据自动流向 `writable` 流，因此不需要调用 `read()` 或 `write()`，但更重要的是，不再需要控制背压，因为它会自动处理。
- en: 'To provide a quick example, we can create a new module that takes a text stream
    from the standard input, applies the *replace* transformation discussed earlier
    when we built our custom `ReplaceStream`, and then pushes the data back to the
    standard output:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个快速示例，我们可以创建一个新的模块，该模块从标准输入获取文本流，在构建我们的自定义 `ReplaceStream` 时应用之前讨论的 *replace*
    转换，然后将数据推回标准输出：
- en: '[PRE53]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The preceding program pipes the data that comes from the standard input into
    an instance of `ReplaceStream` and then back to the standard output. Now, to try
    this small application, we can leverage a Unix pipe to redirect some data into
    its standard input, as shown in the following example:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的程序将来自标准输入的数据管道连接到 `ReplaceStream` 的一个实例，然后将其返回到标准输出。现在，为了尝试这个小应用程序，我们可以利用
    Unix 管道将一些数据重定向到其标准输入，如下面的示例所示：
- en: '[PRE54]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This should produce the following output:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '[PRE55]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This simple example demonstrates that streams (and in particular, text streams)
    are a universal interface and that pipes are the way to compose and interconnect
    all these interfaces almost magically.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子表明，流（特别是文本流）是一个通用的接口，而管道是几乎神奇地组合和互联所有这些接口的方式。
- en: Pipes and error handling
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管道和错误处理
- en: 'The `error` events are not propagated automatically through the pipeline when
    using `pipe()`. Take, for example, this code fragment:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `pipe()` 时，`error` 事件不会自动通过管道传播。例如，看看这个代码片段：
- en: '[PRE56]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In the preceding pipeline, we will catch only the errors coming from `stream2`,
    which is the stream that we attached the listener to. This means that, if we want
    to catch any error generated from `stream1`, we have to attach another error listener
    directly to it, which will make our example look like this:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的管道中，我们将只捕获来自 `stream2` 的错误，这是我们附加监听器的流。这意味着，如果我们想捕获 `stream1` 产生的任何错误，我们必须直接将其附加另一个错误监听器，这将使我们的例子看起来像这样：
- en: '[PRE57]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This is clearly not ideal, especially when dealing with pipelines with a significant
    number of steps. To make this worse, in the event of an error, the failing stream
    is only unpiped from the pipeline. The failing stream is not properly destroyed,
    which might leave dangling resources (for example, file descriptors, connections,
    and so on) and leak memory. A more robust (yet inelegant) implementation of the
    preceding snippet might look like this:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然不是理想的情况，尤其是在处理具有大量步骤的管道时。更糟糕的是，在发生错误的情况下，失败的流仅从管道中取消管道连接。失败的流没有被正确销毁，这可能会留下悬挂的资源（例如，文件描述符、连接等）并导致内存泄漏。前面代码片段的更健壮（但不够优雅）的实现可能如下所示：
- en: '[PRE58]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In this example, we registered a handler for the `error` event for both `stream1`
    and `stream2`. When an error happens, our `handleError()` function is invoked,
    and we can log the error and destroy every stream in the pipeline. This allows
    us to ensure that all the allocated resources are properly released, and the error
    is handled gracefully.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们为 `stream1` 和 `stream2` 注册了 `error` 事件的处理器。当发生错误时，我们的 `handleError()`
    函数会被调用，我们可以记录错误并销毁管道中的每个流。这确保了所有分配的资源都得到了适当的释放，并且错误得到了优雅的处理。
- en: Better error handling with pipeline()
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `pipeline()` 函数进行更好的错误处理
- en: Handling errors manually in a pipeline is not just cumbersome, but also error-prone—definitely
    something we should avoid if we can!
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道中手动处理错误不仅麻烦，而且容易出错——如果我们能避免的话，这绝对是我们应该避免的事情！
- en: Luckily, the core `stream` package offers us an excellent utility function that
    can make building pipelines a much safer and more enjoyable practice, which is
    the `pipeline()` helper function.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，核心的 `stream` 包提供了一个出色的实用函数，可以使构建管道变得更加安全和愉快，这个实用函数就是 `pipeline()` 辅助函数。
- en: 'In a nutshell, you can use the `pipeline()` function as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你可以如下使用 `pipeline()` 函数：
- en: '[PRE59]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This helper pipes every stream passed in the arguments list to the next one.
    For each stream, it will also register a proper `error` and `close` listeners.
    This way, all the streams are properly destroyed when the pipeline completes successfully
    or when it's interrupted by an error. The last argument is an optional callback
    that will be called when the stream finishes. If it finishes because of an error,
    the callback will be invoked with the given error as the first argument.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这个辅助函数将所有传递给参数列表的流管道连接到下一个流。对于每个流，它还会注册适当的 `error` 和 `close` 监听器。这样，当管道成功完成或因错误中断时，所有流都会被正确销毁。最后一个参数是一个可选的回调函数，当流完成时将被调用。如果是因为错误而完成，回调函数将使用给定的错误作为第一个参数调用。
- en: 'In order to build up some practice with this helper, let''s write a simple
    command-line script that implements the following pipeline:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这个辅助函数积累一些实践经验，让我们编写一个简单的命令行脚本，实现以下管道：
- en: Reads a Gzip data stream from the standard input
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从标准输入读取Gzip数据流
- en: Decompresses the data
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解压缩数据
- en: Makes all the text uppercase
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有文本转换为大写
- en: Gzips the resulting data
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对结果数据进行gzip压缩
- en: Sends the data back to the standard output
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据发送回标准输出
- en: 'Let''s call this module `uppercasify-gzipped.js`:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称这个模块为 `uppercasify-gzipped.js`：
- en: '[PRE60]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In this example:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中：
- en: We are importing the necessary dependencies from `zlib` and the `stream` modules.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 `zlib` 和 `stream` 模块导入必要的依赖。
- en: We create a simple `Transform` stream that makes every chunk uppercase.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个简单的 `Transform` 流，它将每个数据块转换为大写。
- en: We define our pipeline, where we list all the stream instances in order.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义我们的管道，按顺序列出所有流实例。
- en: We add a callback to monitor the completion of the stream. In the event of an
    error, we print the error in the standard error interface, and we exit with error
    code `1`.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加一个回调来监控流的完成。在发生错误的情况下，我们在标准错误接口中打印错误，并以错误代码 `1` 退出。
- en: The pipeline will start automatically by consuming data from the standard input
    and producing data for the standard output.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 管道将自动启动，通过读取标准输入并生成标准输出数据来启动。
- en: 'We could test our script with the following command:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令测试我们的脚本：
- en: '[PRE61]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This should produce the following output:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '[PRE62]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'If we try to remove the `gzip` step from the preceding sequence of commands,
    our script will fail with an error similar to the following:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试从前面的命令序列中移除 `gzip` 步骤，我们的脚本将失败，并出现类似于以下错误的错误：
- en: '[PRE63]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This error is raised by the stream created with the `createGunzip()` function,
    which is responsible for decompressing the data. If the data is not actually gzipped,
    the decompression algorithm won't be able to process the data and it will fail.
    In such a case, `pipeline()` will take care of cleaning up after the error and
    destroy all the streams in the pipeline.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误是由使用 `createGunzip()` 函数创建的流引发的，该函数负责解压缩数据。如果数据实际上没有被压缩，解压缩算法将无法处理数据，并会失败。在这种情况下，`pipeline()`
    将负责清理错误并销毁管道中的所有流。
- en: The `pipeline()` function can be easily *promisified* by using the `promisify()`
    helper from the core `util` module.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipeline()` 函数可以通过使用核心 `util` 模块中的 `promisify()` 辅助函数轻松地进行 `promisified`。'
- en: Now that we have built a solid understanding of Node.js streams, we are ready
    to move into some more involved stream patterns like control flow and advanced
    piping patterns.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 Node.js 流有了坚实的理解，我们准备进入一些更复杂的流模式，如控制流和高级管道模式。
- en: Asynchronous control flow patterns with streams
  id: totrans-388
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流的异步控制流模式
- en: Going through the examples that we have presented so far, it should be clear
    that streams can be useful not only to handle I/O, but also as an elegant programming
    pattern that can be used to process any kind of data. But the advantages do not
    end at its simple appearance; streams can also be leveraged to turn "asynchronous
    control flow" into "**flow control**," as we will see in this section.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们迄今为止提供的示例，应该很清楚，流不仅可以用于处理 I/O，还可以作为一种优雅的编程模式，可以用于处理任何类型的数据。但优势不仅限于其简单的外观；流还可以被利用将“异步控制流”转变为“**流控制**”，正如我们将在本节中看到的那样。
- en: Sequential execution
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序执行
- en: By default, streams will handle data in sequence. For example, the `_transform()`
    function of a `Transform` stream will never be invoked with the next chunk of
    data until the previous invocation completes by calling `callback()`. This is
    an important property of streams, crucial for processing each chunk in the right
    order, but it can also be exploited to turn streams into an elegant alternative
    to the traditional control flow patterns.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，流将按顺序处理数据。例如，`Transform` 流的 `_transform()` 函数只有在通过调用 `callback()` 完成前一次调用后，才会被用于处理下一块数据。这是流的一个重要特性，对于按正确顺序处理每一块数据至关重要，但也可以被利用将流转变为传统控制流模式的优雅替代方案。
- en: 'Some code is always better than too much explanation, so let''s work on an
    example to demonstrate how we can use streams to execute asynchronous tasks in
    sequence. Let''s create a function that concatenates a set of files received as
    input, making sure to honor the order in which they are provided. Let''s create
    a new module called `concat-files.js` and define its contents as follows:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 代码总是比过多的解释要好，所以让我们通过一个示例来展示如何使用流按顺序执行异步任务。让我们创建一个函数，将作为输入接收的一组文件连接起来，确保按照它们提供的顺序执行。让我们创建一个新的模块，名为
    `concat-files.js`，并定义其内容如下：
- en: '[PRE64]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The preceding function implements a sequential iteration over the `files` array
    by transforming it into a stream. The algorithm can be explained as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数通过将 `files` 数组转换为流来实现对 `files` 数组的顺序迭代。算法可以解释如下：
- en: 'First, we use `Readable.from()` to create a `Readable` stream from the `files`
    array. This stream operates in object mode (the default setting for streams created
    with `Readable.from()`) and it will emit filenames: every chunk is a string indicating
    the path to a file. The order of the chunks respects the order of the files in
    the `files` array.'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用 `Readable.from()` 将 `files` 数组转换为 `Readable` 流。这个流以对象模式运行（这是使用 `Readable.from()`
    创建的流的默认设置），并且它将发出文件名：每个块都是一个表示文件路径的字符串。块顺序尊重 `files` 数组中文件的顺序。
- en: 'Next, we create a custom `Transform` stream to handle each file in the sequence.
    Since we are receiving strings, we set the option `objectMode` to `true`. In our
    transformation logic, for each file, we create a `Readable` stream to read the
    file content and pipe it into `destStream` (a `Writable` stream for the destination
    file). We make sure not to close `destStream` after the source file finishes reading
    by specifying `{ end: false }` in the `pipe()` options.'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '接下来，我们创建一个自定义的 `Transform` 流来处理序列中的每个文件。由于我们接收字符串，我们将选项 `objectMode` 设置为 `true`。在我们的转换逻辑中，对于每个文件，我们创建一个
    `Readable` 流来读取文件内容并将其管道传输到 `destStream`（目标文件的 `Writable` 流）。我们确保在指定 `{ end: false
    }` 选项后不关闭 `destStream`，这样在源文件读取完成后就不会关闭。'
- en: When all the contents of the source file have been piped into `destStream`,
    we invoke the `done` function to communicate the completion of the current processing,
    which is necessary to trigger the processing of the next file.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当源文件的所有内容都通过 `destStream` 管道传输后，我们调用 `done` 函数来传达当前处理的完成，这是触发下一个文件处理的必要条件。
- en: When all the files have been processed, the `finish` event is fired; we can
    finally end `destStream` and invoke the `cb()` function of `concatFiles()`, which signals
    the completion of the whole operation.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当所有文件都处理完毕后，将触发 `finish` 事件；我们最终可以结束 `destStream` 并调用 `concatFiles()` 的 `cb()`
    函数，这标志着整个操作完成。
- en: 'We can now try to use the little module we just created. Let''s do that in
    a new file, called `concat.js`:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以尝试使用我们刚刚创建的小模块。让我们在一个新文件中这样做，文件名为 `concat.js`：
- en: '[PRE65]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We can now run the preceding program by passing the destination file as the
    first command-line argument, followed by a list of files to concatenate; for example:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过将目标文件作为第一个命令行参数传递，然后跟上一系列要连接的文件来运行前面的程序；例如：
- en: '[PRE66]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: This should create a new file called `all-together.txt` containing, in order,
    the contents of `file1.txt` and `file2.txt`.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会创建一个名为 `all-together.txt` 的新文件，其中按顺序包含 `file1.txt` 和 `file2.txt` 的内容。
- en: With the `concatFiles()` function, we were able to obtain an asynchronous sequential
    iteration using only streams. This is an elegant and compact solution that enriches
    our toolbelt, along with the techniques we already explored in *Chapter 4*, *Asynchronous
    Control Flow Patterns with Callbacks*, and *Chapter 5*, *Asynchronous Control
    Flow Patterns with Promises and Async/Await*.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `concatFiles()` 函数，我们能够仅使用流实现异步顺序迭代。这是一个优雅且紧凑的解决方案，丰富了我们的工具集，包括我们在 *第 4 章*
    中探索的 *使用回调的异步控制流模式*，以及 *第 5 章* 中的 *使用 Promises 和 Async/Await 的异步控制流模式*。
- en: '**Pattern**'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式**'
- en: Use a stream, or combination of streams, to easily iterate over a set of asynchronous
    tasks in sequence.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流或流组合，可以轻松按顺序迭代一组异步任务。
- en: In the next section, we will discover how to use Node.js streams to implement
    unordered parallel task execution.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何使用 Node.js 流来实现无序并行任务执行。
- en: Unordered parallel execution
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无序并行执行
- en: We just saw that streams process each data chunk in sequence, but sometimes,
    this can be a bottleneck as we would not make the most of the concurrency of Node.js.
    If we have to execute a slow asynchronous operation for every data chunk, it can
    be advantageous to parallelize the execution and speed up the overall process.
    Of course, this pattern can only be applied if there is no relationship between
    each chunk of data, which might happen frequently for object streams, but very
    rarely for binary streams.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到流按顺序处理每个数据块，但有时这可能会成为瓶颈，因为我们没有充分利用 Node.js 的并发性。如果我们必须为每个数据块执行慢速的异步操作，那么并行化执行并加快整体过程可能是有利的。当然，只有当数据块之间没有关系时，这种模式才能应用，对于对象流这种情况可能经常发生，但对于二进制流则很少发生。
- en: '**Caution**'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Unordered parallel streams cannot be used when the order in which the data is
    processed is important.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据处理的顺序很重要时，不能使用无序并行流。
- en: To parallelize the execution of a `Transform` stream, we can apply the same
    patterns that we learned in *Chapter 4*, *Asynchronous Control Flow Patterns with
    Callbacks*, but with some adaptations to get them working with streams. Let's
    see how this works.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行化 `Transform` 流的执行，我们可以应用我们在 *第 4 章* 中学到的相同模式，即 *使用回调的异步控制流模式*，但需要对它们进行一些调整以使其与流一起工作。让我们看看这是如何工作的。
- en: Implementing an unordered parallel stream
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现无序并行流
- en: 'Let''s immediately demonstrate how to implement an unordered parallel stream
    with an example. Let''s create a module called `parallel-stream.js` and define
    a generic `Transform` stream that executes a given transform function in parallel:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即通过一个示例演示如何实现一个无序的并行流。让我们创建一个名为 `parallel-stream.js` 的模块，并定义一个通用的 `Transform`
    流，该流并行执行给定的转换函数：
- en: '[PRE67]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let''s analyze this new class step by step:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步分析这个新类：
- en: As you can see, the constructor accepts a `userTransform()` function, which
    is then saved as an instance variable. We invoke the parent constructor and for
    convenience, we enable the object mode by default.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，构造函数接受一个 `userTransform()` 函数，然后将其保存为实例变量。我们调用父构造函数，并且为了方便，我们默认启用对象模式。
- en: Next, it is the `_transform()` method's turn. In this method, we execute the
    `userTransform()` function and then increment the count of running tasks. Finally,
    we notify the `Transform` stream that the current transformation step is complete
    by invoking `done()`. The trick for triggering the processing of another item
    in parallel is exactly this. We are not waiting for the `userTransform()` function
    to complete before invoking `done()`; instead, we do it immediately. On the other
    hand, we provide a special callback to `userTransform()`, which is the `this._onComplete()`
    method. This allows us to get notified when the execution of `userTransform()`
    completes.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，轮到 `_transform()` 方法。在这个方法中，我们执行 `userTransform()` 函数，然后增加正在运行的任务计数。最后，我们通过调用
    `done()` 通知 `Transform` 流当前转换步骤已完成。触发并行处理另一个项目的技巧正是这个。我们不是等待 `userTransform()`
    函数完成后再调用 `done()`；相反，我们立即这样做。另一方面，我们向 `userTransform()` 提供一个特殊的回调，即 `this._onComplete()`
    方法。这允许我们在 `userTransform()` 执行完成后得到通知。
- en: The `_flush()` method is invoked just before the stream terminates, so if there
    are still tasks running, we can put the release of the `finish` event on hold
    by not invoking the `done()` callback immediately. Instead, we assign it to the
    `this.terminateCallback` variable.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_flush()` 方法在流终止之前被调用，所以如果还有任务正在运行，我们可以通过不立即调用 `done()` 回调来挂起释放 `finish` 事件。相反，我们将其分配给
    `this.terminateCallback` 变量。'
- en: To understand how the stream is then properly terminated, we have to look into
    the `_onComplete()` method. This last method is invoked every time an asynchronous
    task completes. It checks whether there are any more tasks running and, if there
    are none, it invokes the `this.terminateCallback()` function, which will cause
    the stream to end, releasing the `finish` event that was put on hold in the `_flush()`
    method.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要理解流是如何正确终止的，我们必须查看 `_onComplete()` 方法。这个最后的方法在每次异步任务完成时被调用。它检查是否还有更多任务正在运行，如果没有，它将调用
    `this.terminateCallback()` 函数，这将导致流结束，释放 `_flush()` 方法中挂起的 `finish` 事件。
- en: 'The `ParallelStream` class we just built allows us to easily create a `Transform`
    stream that executes its tasks in parallel, but there is a caveat: it does not
    preserve the order of the items as they are received. In fact, asynchronous operations
    can complete and push data at any time, regardless of when they are started. We
    immediately understand that this property does not play well with binary streams
    where the order of data usually matters, but it can surely be useful with some
    types of object streams.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚构建的 `ParallelStream` 类允许我们轻松创建一个并行执行其任务的 `Transform` 流，但有一个注意事项：它不会保留接收到的项目顺序。事实上，异步操作可以在任何时间完成并推送数据，无论它们何时开始。我们立即理解，这个属性与通常数据顺序很重要的二进制流不太兼容，但它肯定可以用于某些类型的对象流。
- en: Implementing a URL status monitoring application
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现URL状态监控应用程序
- en: Now, let's apply our `ParallelStream` to a concrete example. Let's imagine that
    we want to build a simple service to monitor the status of a big list of URLs.
    Let's imagine all these URLs are contained in a single file and are newline separated.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们的 `ParallelStream` 应用到一个具体的例子中。让我们想象我们想要构建一个简单的服务来监控大量 URL 的状态。让我们想象所有这些
    URL 都包含在一个单独的文件中，并且以换行符分隔。
- en: Streams can offer a very efficient and elegant solution to this problem, especially
    if we use our `ParallelStream` class to parallelize the checking of the URLs.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 流可以提供一种非常高效且优雅的解决方案，特别是如果我们使用我们的 `ParallelStream` 类来并行化 URL 的检查。
- en: 'Let''s build this simple application immediately in a new module called `check-urls.js`:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 立即在一个名为 `check-urls.js` 的新模块中构建这个简单的应用程序：
- en: '[PRE68]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'As we can see, with streams, our code looks very elegant and straightforward:
    everything is contained in a single streaming pipeline. Let''s see how it works:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用流，我们的代码看起来非常优雅和直接：所有内容都包含在一个单一的流管道中。让我们看看它是如何工作的：
- en: First, we create a `Readable` stream from the file given as input.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从提供的文件创建一个`Readable`流。
- en: We pipe the contents of the input file through `split` ([nodejsdp.link/split](http://nodejsdp.link/split)),
    a `Transform` stream that ensures each line is emitted in a different chunk.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将输入文件的 内容通过`split` ([nodejsdp.link/split](http://nodejsdp.link/split)) 流，这是一个`Transform`流，确保每行数据都在不同的数据块中发出。
- en: Then, it's time to use our `ParallelStream` to check the URL. We do this by
    sending a `head` request and waiting for a response. When the operation completes,
    we push the result down the stream.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，是时候使用我们的`ParallelStream`来检查URL了。我们通过发送一个`head`请求并等待响应来完成这个操作。当操作完成后，我们将结果推送到流中。
- en: Finally, all the results are piped into a file, `results.txt`.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，所有结果都会被导入到一个文件中，名为`results.txt`。
- en: 'Now, we can run the `check-urls.js` module with a command such as this:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用如下命令运行`check-urls.js`模块：
- en: '[PRE69]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Here, the file `urls.txt` contains a list of URLs (one per line); for example:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，文件`urls.txt`包含一个URL列表（每行一个）；例如：
- en: '[PRE70]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'When the command finishes running, we will see that a file, `results.txt`,
    was created. This contains the results of the operation; for example:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 当命令运行完成后，我们将看到创建了一个文件，名为`results.txt`。这个文件包含了操作的结果；例如：
- en: '[PRE71]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: There is a good probability that the order in which the results are written
    is different from the order in which the URLs were specified in the input file.
    This is clear evidence that our stream executes its tasks in parallel, and it
    does not enforce any order between the various data chunks in the stream.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的写入顺序很可能与输入文件中指定的URL顺序不同。这是我们的流并行执行任务的明确证据，并且它不会在流中的各种数据块之间强制执行任何顺序。
- en: For the sake of curiosity, we might want to try replacing `ParallelStream` with
    a normal `Transform` stream and compare the behavior and performance of the two
    (you might want to do this as an exercise). Using `Transform` directly is way
    slower, because each URL is checked in sequence, but on the other hand the order
    of the results in the file `results.txt` is preserved.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，我们可能想尝试用普通的`Transform`流替换`ParallelStream`，并比较两种流的行为和性能（你可能想将此作为练习）。直接使用`Transform`要慢得多，因为每个URL都是按顺序检查的，但另一方面，文件`results.txt`中结果的顺序得到了保留。
- en: In the next section, we will see how to extend this pattern to limit the number
    of parallel tasks running at a given time.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何扩展这个模式以限制在给定时间内运行的并行任务的数量。
- en: Unordered limited parallel execution
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无序限制的并行执行
- en: If we try to run the `check-urls.js` application against a file that contains
    thousands or millions of URLs, we will surely run into issues. Our application
    will create an uncontrolled number of connections all at once, sending a considerable
    amount of data in parallel, and potentially undermining the stability of the application
    and the availability of the entire system. As we already know, the solution to
    keep the load and resource usage under control is to limit the concurrency of
    the parallel tasks.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试运行`check-urls.js`应用程序来检查包含数千或数百万个URL的文件，我们肯定会遇到问题。我们的应用程序将一次性创建大量连接，并行发送大量数据，这可能会破坏应用程序的稳定性和整个系统的可用性。正如我们所知，保持负载和资源使用在控制之下的解决方案是限制并行任务的并发性。
- en: Let's see how this works with streams by creating a `limited-parallel-stream.js`
    module, which is an adaptation of `parallel-stream.js` we created in the previous section.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过创建一个`limited-parallel-stream.js`模块来查看这是如何通过流实现的，这个模块是我们在上一节中创建的`parallel-stream.js`的改编。
- en: 'Let''s see what it looks like, starting from its constructor (we will highlight
    the changed parts):'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的样子，从其构造函数开始（我们将突出显示更改的部分）：
- en: '[PRE72]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: We need a `concurrency` limit to be taken as input, and this time, we are going
    to save two callbacks, one for any pending `_transform` method (`continueCb`—more
    on this next) and another one for the callback of the `_flush` method (`terminateCb`).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个`concurrency`限制作为输入，这次我们将保存两个回调，一个用于任何挂起的`_transform`方法（`continueCb`——关于这一点我们将在下一节中详细说明）和另一个用于`_flush`方法的回调（`terminateCb`）。
- en: 'Next is the `_transform()` method:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`_transform()`方法：
- en: '[PRE73]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: This time, in the `_transform()` method, we have to check whether we have any
    free execution slots before we can invoke `done()` and trigger the processing
    of the next item. If we have already reached the maximum number of concurrently
    running streams, we can simply save the `done()` callback in the `continueCb`
    variable so that it can be invoked as soon as a task finishes.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，在 `_transform()` 方法中，在我们调用 `done()` 并触发下一项的处理之前，我们必须检查是否有任何空闲的执行槽位。如果我们已经达到了并发运行的流的最大数量，我们只需将
    `done()` 回调保存到 `continueCb` 变量中，以便在任务完成时立即调用它。
- en: 'The `_flush()` method remains exactly the same as in the `ParallelStream` class,
    so let''s move directly to implementing the `_onComplete()` method:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '`_flush()` 方法与 `ParallelStream` 类中的方法完全相同，所以让我们直接实现 `_onComplete()` 方法：'
- en: '[PRE74]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Every time a task completes, we invoke any saved `continueCb()` that will cause
    the stream to unblock, triggering the processing of the next item.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 每当任务完成时，我们调用任何保存的 `continueCb()`，这将导致流解除阻塞，触发下一项的处理。
- en: That's it for the `LimitedParallelStream` class. We can now use it in the `check-urls.js`
    module in place of `ParallelStream` and have the concurrency of our tasks limited
    to the value that we set.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '`LimitedParallelStream` 类就到这里。我们现在可以在 `check-urls.js` 模块中使用它来代替 `ParallelStream`，并且我们的任务并发性将限制为我们设置的值。'
- en: Ordered parallel execution
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有序并行执行
- en: 'The parallel streams that we created previously may shuffle the order of the
    emitted data, but there are situations where this is not acceptable. Sometimes,
    in fact, it is necessary to have each chunk emitted in the same order in which
    it was received. However, not all hope is lost: we can still run the transform
    function in parallel; all we have to do is to sort the data emitted by each task
    so that it follows the same order in which the data was received.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前创建的并行流可能会打乱发出的数据的顺序，但有些情况下这是不可接受的。事实上，有时我们需要每个块以接收到的相同顺序发出。然而，并非所有的希望都破灭了：我们仍然可以并行运行转换函数；我们唯一要做的就是排序每个任务发出的数据，使其遵循接收到的数据相同的顺序。
- en: This technique involves the use of a buffer to reorder the chunks while they
    are emitted by each running task. For brevity, we are not going to provide an
    implementation of such a stream, as it's quite verbose for the scope of this book.
    What we are going to do instead is reuse one of the available packages on npm
    built for this specific purpose, that is, `parallel-transform` ([nodejsdp.link/parallel-transform](http://nodejsdp.link/parallel-transform)).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术涉及使用缓冲区在运行的任务发出块时重新排序块。为了简洁，我们不会提供这样一个流的实现，因为它在这个书籍的范围内容易变得冗长。我们将要做的是重用
    npm 上为这个特定目的构建的一个可用包，即 `parallel-transform` ([nodejsdp.link/parallel-transform](http://nodejsdp.link/parallel-transform))。
- en: 'We can quickly check the behavior of an ordered parallel execution by modifying
    our existing `check-urls` module. Let''s say that we want our results to be written
    in the same order as the URLs in the input file, while executing our checks in
    parallel. We can do this using `parallel-transform`:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过修改现有的 `check-urls` 模块来快速检查有序并行执行的行为。假设我们希望我们的结果以与输入文件中的 URL 相同的顺序写入，同时在并行执行我们的检查。我们可以使用
    `parallel-transform` 来实现这一点：
- en: '[PRE75]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: In the example here, `parallelTransform()` creates a `Transform` stream in object
    mode that executes our transformation logic with a maximum concurrency of 4\.
    If we try to run this new version of `check-urls.js`, we will now see that the
    `results.txt` file lists the results in the same order as the URLs appear in the
    input file. It is important to see that, even though the order of the output is
    the same as the input, the asynchronous tasks still run in parallel and can possibly
    complete in any order.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`parallelTransform()` 创建了一个对象模式的 `Transform` 流，以最大并发性 4 执行我们的转换逻辑。如果我们尝试运行这个新的
    `check-urls.js` 版本，我们现在会看到 `results.txt` 文件以输入文件中 URL 出现的顺序列出结果。重要的是要注意，尽管输出顺序与输入相同，但异步任务仍然并行运行，并且可能以任何顺序完成。
- en: When using the ordered parallel execution pattern, we need to be aware of slow
    items blocking the pipeline or growing the memory indefinitely. In fact, if there
    is an item that requires a very long time to complete, depending on the implementation
    of the pattern, it will either cause the buffer containing the pending ordered
    results to grow indefinitely or the entire processing to block until the slow
    item completes. With the first strategy, we are optimizing for performance, while
    with the second, we get predictable memory usage. `parallel-transform` implementation
    opts for predictable memory utilization and maintains an internal buffer that
    will not grow more than the specified maximum concurrency.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用有序并行执行模式时，我们需要注意慢速项目可能会阻塞管道或无限期地增长内存。实际上，如果有一个项目需要非常长的时间才能完成，根据模式的实现，它可能会导致包含待处理有序结果的缓冲区无限期地增长，或者整个处理过程会阻塞，直到慢速项目完成。采用第一种策略，我们是在优化性能，而采用第二种策略，我们得到可预测的内存使用。`parallel-transform`
    实现选择可预测的内存利用，并维护一个内部缓冲区，其大小不会超过指定的最大并发数。
- en: With this, we conclude our analysis of the asynchronous control flow patterns
    with streams. Next, we are going to focus on some piping patterns.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们完成了对使用流的异步控制流模式的分析。接下来，我们将关注一些管道模式。
- en: Piping patterns
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道模式
- en: As in real-life plumbing, Node.js streams can also be piped together by following
    different patterns. We can, in fact, merge the flow of two different streams into
    one, split the flow of one stream into two or more pipes, or redirect the flow
    based on a condition. In this section, we are going to explore the most important
    plumbing patterns that can be applied to Node.js streams.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 就像现实生活中的管道工一样，Node.js 流也可以通过遵循不同的模式相互连接。实际上，我们可以将两个不同流的流程合并为一个，将一个流的流程分割成两个或更多管道，或者根据条件重定向流程。在本节中，我们将探讨可以应用于
    Node.js 流的最重要管道模式。
- en: Combining streams
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组合流
- en: 'In this chapter, we have stressed the fact that streams provide a simple infrastructure
    to modularize and reuse our code, but there is one last piece missing in this
    puzzle: what if we want to modularize and reuse an entire pipeline? What if we
    want to combine multiple streams so that they look like one from the outside?
    The following figure shows what this means:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们强调了流提供了一个简单的基础设施来模块化和重用我们的代码，但在这个谜题中还有一个缺失的部分：如果我们想模块化和重用整个管道怎么办？如果我们想将多个流组合起来，使其从外部看起来像一个怎么办？以下图示展示了这意味着什么：
- en: '![](img/B15729_06_06.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_06_06.png)'
- en: 'Figure 6.6: Combining streams'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：组合流
- en: 'From *Figure 6.6*, we should already get a hint of how this works:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 6.6* 中，我们应该已经得到了一些关于它是如何工作的提示：
- en: When we write into the combined stream, we are actually writing into the first
    stream of the pipeline.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们向组合流写入时，实际上是在向管道的第一个流写入。
- en: When we read from the combined stream, we are actually reading from the last
    stream of the pipeline.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们从组合流读取时，实际上是在从管道的最后一个流读取。
- en: A combined stream is usually a `Duplex` stream, which is built by connecting
    the first stream to its `Writable` side and the last stream to its `Readable`
    side.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 组合流通常是一个 `Duplex` 流，它通过将第一个流连接到其 `Writable` 端，并将最后一个流连接到其 `Readable` 端来构建。
- en: To create a `Duplex` stream out of two different streams, one `Writable` and
    one `Readable`, we can use an npm module such as `duplexer2` ([nodejsdp.link/duplexer2](http://nodejsdp.link/duplexer2))
    or `duplexify` ([nodejsdp.link/duplexify](http://nodejsdp.link/duplexify)).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 要从两个不同的流中创建一个 `Duplex` 流，一个 `Writable` 流和一个 `Readable` 流，我们可以使用一个 npm 模块，例如
    `duplexer2` ([nodejsdp.link/duplexer2](http://nodejsdp.link/duplexer2)) 或 `duplexify`
    ([nodejsdp.link/duplexify](http://nodejsdp.link/duplexify))。
- en: But that's not enough. In fact, another important characteristic of a combined
    stream is that it has to capture and propagate all the errors that are emitted
    from any stream inside the pipeline. As we already mentioned, any error event
    is not automatically propagated down the pipeline when we use `pipe()`, and we
    should explicitly attach an error listener to each stream. We saw that we could
    use the `pipeline()` helper function to overcome the limitations of `pipe()` with
    error management, but the issue with both `pipe()` and the `pipeline()` helper
    is that the two functions return only the last stream of the pipeline, so we only
    get the (last) `Readable` component and not the (first) `Writable` component.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还不够。事实上，组合流的一个重要特性是它必须捕获并传播管道中任何流发出的所有错误。正如我们之前提到的，当我们使用`pipe()`时，任何错误事件都不会自动沿着管道向下传播，我们应该为每个流显式地附加一个错误监听器。我们看到我们可以使用`pipeline()`辅助函数来克服`pipe()`在错误管理方面的限制，但`pipe()`和`pipeline()`辅助函数的问题在于，这两个函数只返回管道中的最后一个流，所以我们只得到（最后一个）`Readable`组件，而不是（第一个）`Writable`组件。
- en: 'We can verify this very easily with the following snippet of code:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码片段非常容易地验证这一点：
- en: '[PRE76]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: From the preceding code, it should be clear that with just `pipe()` or `pipeline()`,
    it would not be trivial to construct a combined stream.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，应该很明显，仅使用`pipe()`或`pipeline()`，构建组合流并不是一件简单的事情。
- en: 'To recap, a combined stream has two major advantages:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，组合流有两个主要优势：
- en: We can redistribute it as a black box by hiding its internal pipeline.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过隐藏其内部管道将其作为一个黑盒重新分发。
- en: We have simplified error management, as we don't have to attach an error listener
    to each stream in the pipeline, but just to the combined stream itself.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们简化了错误管理，因为我们不需要为管道中的每个流附加错误监听器，只需为组合流本身附加即可。
- en: Combining streams is a pretty common practice, so if we don't have any particular
    need, we might just want to reuse an existing library such as `pumpify` ([nodejsdp.link/pumpify](http://nodejsdp.link/pumpify)).
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 组合流是一种相当常见的做法，所以如果我们没有特别的需求，我们可能只想重用现有的库，如`pumpify`（[nodejsdp.link/pumpify](http://nodejsdp.link/pumpify)）。
- en: 'This library offers a very simple interface. In fact, all you have to do to
    obtain a combined stream is to call `pumpify()`, passing all the streams you want
    in your pipeline. This is very similar to the signature of `pipeline()`, except
    that there''s no callback:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库提供了一个非常简单的接口。实际上，你只需要调用`pumpify()`，传递你管道中想要的所有流，就可以获得一个组合流。这与`pipeline()`的签名非常相似，只是没有回调：
- en: '[PRE77]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: When we do something like this, `pumpify` will create a pipeline out of our
    streams, return a new combined stream that abstracts away the complexity of our
    pipeline, and provide the advantages discussed previously.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们这样做时，`pumpify`将创建一个由我们的流组成的管道，返回一个新的组合流，该流抽象了我们的管道的复杂性，并提供了之前讨论的优势。
- en: If you are curious to see what it takes to build a library like `pumpify`, you
    should check its source code on GitHub ([nodejsdp.link/pumpify-gh](http://nodejsdp.link/pumpify-gh)).
    One interesting fact is that, internally, `pumpify` uses `pump` ([nodejsdp.link/pump](http://nodejsdp.link/pump)),
    a module that was born before the Node.js `pipeline()` helper. `pump` is effectively
    the module that inspired the development of `pipeline()`. If you compare their
    source code, you will find out that, unsurprisingly, the two modules have a lot
    in common.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你好奇构建像`pumpify`这样的库需要什么，你应该查看其在GitHub上的源代码（[nodejsdp.link/pumpify-gh](http://nodejsdp.link/pumpify-gh)）。一个有趣的事实是，内部上，`pumpify`使用了`pump`（[nodejsdp.link/pump](http://nodejsdp.link/pump)），这是一个在Node.js的`pipeline()`辅助函数之前诞生的模块。`pump`实际上是启发`pipeline()`开发的模块。如果你比较它们的源代码，你会发现，不出所料，这两个模块有很多共同之处。
- en: Implementing a combined stream
  id: totrans-485
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现组合流
- en: 'To illustrate a simple example of combining streams, let''s consider the case
    of the following two `Transform` streams:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明组合流的简单示例，让我们考虑以下两个`Transform`流的情况：
- en: One that both compresses and encrypts the data
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种既能压缩又能加密数据的方法
- en: One that both decrypts and decompresses the data
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种既能解密又能解压缩数据的方法
- en: 'Using a library such as `pumpify`, we can easily build these streams (in a
    file called `combined-streams.js`) by combining some of the streams that we already
    have available from the core libraries:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像`pumpify`这样的库，我们可以轻松地构建这些流（在一个名为`combined-streams.js`的文件中）通过组合一些我们从核心库中已经可用的流：
- en: '[PRE78]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We can now use these combined streams as if they were black boxes, for example,
    to create a small application that archives a file by compressing and encrypting
    it. Let''s do that in a new module named `archive.js`:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这些组合流用作黑盒，例如，创建一个通过压缩和加密文件来归档文件的小型应用程序。让我们在名为 `archive.js` 的新模块中这样做：
- en: '[PRE79]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Note how we don't have to worry about how many steps there are within `archiveFile`.
    In fact, we just treat it as a single stream within our current pipeline. This
    makes our combined stream easily reusable in other contexts.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不必担心 `archiveFile` 中有多少步骤。实际上，我们只是将其视为当前管道中的一个单独的流。这使得我们的组合流在其它上下文中易于重用。
- en: 'Now, to run the `archive` module, simply specify a password and a file in the
    command-line arguments:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要运行 `archive` 模块，只需在命令行参数中指定一个密码和一个文件：
- en: '[PRE80]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: This command will create a file called `/path/to/a/file.txt.gz.enc` and it will
    print the generated initialization vector to the console.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将创建一个名为 `/path/to/a/file.txt.gz.enc` 的文件，并且会将生成的初始化向量打印到控制台。
- en: Now, as an exercise, you could use the `createDecryptAndDecompress()` function
    to create a similar script that takes a password, an initialization vector, and
    an archived file and unarchives it.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，作为一个练习，你可以使用 `createDecryptAndDecompress()` 函数创建一个类似的脚本，该脚本接受一个密码、一个初始化向量和归档文件，然后解压缩它。
- en: In real-life applications, it is generally preferable to include the initialization
    vector as part of the encrypted data, rather than requiring the user to pass it
    around. One way to implement this is by having the first 16 bytes emitted by the
    archive stream to be representing the initialization vector. The unarchive utility
    would need to be updated accordingly to consume the first 16 bytes before starting
    to process the data in a streaming fashion. This approach would add some additional
    complexity, which is outside the scope of this example, therefore we opted for
    a simpler solution. Once you feel comfortable with streams, we encourage you to
    try to implement as an exercise a solution where the initialization vector doesn't
    have to be passed around by the user.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，通常更倾向于将初始化向量作为加密数据的一部分包含在内，而不是要求用户传递它。实现这一点的其中一种方法是通过让归档流发出的前16个字节代表初始化向量。解压缩实用程序需要相应更新，以便在以流式方式处理数据之前消耗前16个字节。这种方法会添加一些额外的复杂性，这超出了本示例的范围，因此我们选择了更简单的解决方案。一旦你对流感到舒适，我们鼓励你尝试实现一个作为练习的解决方案，其中初始化向量不需要由用户传递。
- en: With this example, we have clearly demonstrated how important it is to combine
    streams. From one side, it allows us to create reusable compositions of streams,
    and from the other, it simplifies the error management of a pipeline.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个示例，我们清楚地展示了结合流的重要性。一方面，它允许我们创建可重用的流组合，另一方面，它简化了管道的错误管理。
- en: Forking streams
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分叉流
- en: We can perform a *fork* of a stream by piping a single `Readable` stream into
    multiple `Writable` streams. This is useful when we want to send the same data
    to different destinations; for example, two different sockets or two different
    files. It can also be used when we want to perform different transformations on
    the same data, or when we want to split the data based on some criteria. If you
    are familiar with the Unix command `tee` ([nodejsdp.link/tee](http://nodejsdp.link/tee)),
    this is exactly the same concept applied to Node.js streams!
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将单个 `Readable` 流管道到多个 `Writable` 流中来执行流的分叉。当我们想要将相同的数据发送到不同的目的地时，这很有用；例如，两个不同的套接字或两个不同的文件。它也可以用于我们想要对相同的数据执行不同的转换，或者当我们想要根据某些标准分割数据时。如果你熟悉Unix命令
    `tee` ([nodejsdp.link/tee](http://nodejsdp.link/tee))，这正是在Node.js流中应用了相同的概念！
- en: '*Figure 6.7* gives us a graphical representation of this pattern:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.7* 给出了此模式的图形表示：'
- en: '![](img/B15729_06_07.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7](img/B15729_06_07.png)'
- en: 'Figure 6.7: Forking a stream'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：分叉流
- en: Forking a stream in Node.js is quite easy, but there are a few caveats to keep
    in mind. Let's start by discussing this pattern with an example. It will be easier
    to appreciate the caveats of this pattern once we have an example at hand.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在Node.js中分叉一个流相当简单，但有一些注意事项需要记住。让我们从一个示例开始讨论这个模式。一旦我们有了示例，我们就能更容易地欣赏这个模式的注意事项。
- en: Implementing a multiple checksum generator
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现一个多校验和生成器
- en: 'Let''s create a small utility that outputs both the `sha1` and `md5` hashes
    of a given file. Let''s call this new module `generate-hashes.js`:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个小型实用程序，输出给定文件的 `sha1` 和 `md5` 哈希。让我们把这个新模块叫做 `generate-hashes.js`：
- en: '[PRE81]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Very simple, right? The `inputStream` variable is piped into `sha1Stream` on
    one side and `md5Stream` on the other. There are a few things to note that happen
    behind the scenes:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单，对吧？`inputStream`变量在一侧被管道导入到`sha1Stream`，在另一侧导入到`md5Stream`。有一些幕后发生的事情需要注意：
- en: 'Both `md5Stream` and `sha1Stream` will be ended automatically when `inputStream`
    ends, unless we specify `{ end: false }` as an option when invoking `pipe()`.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '当`inputStream`结束时，`md5Stream`和`sha1Stream`将自动结束，除非我们在调用`pipe()`时指定了`{ end:
    false }`作为选项。'
- en: The two forks of the stream will receive the same data chunks, so we must be
    very careful when performing side-effect operations on the data, as that would
    affect every stream that we are sending data to.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流的两个分支将接收相同的数据块，因此当我们对数据进行副作用操作时必须非常小心，因为这会影响我们发送数据的每个流。
- en: Backpressure will work out of the box; the flow coming from `inputStream` will
    go as fast as the slowest branch of the fork. In other words, if one destination
    pauses the source stream to handle backpressure for a long time, all the other
    destinations will be waiting as well. Also, one destination blocking indefinitely
    will block the entire pipeline!
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背压将自动工作；来自`inputStream`的流量将像分叉的最慢分支一样快。换句话说，如果一个目的地暂停源流以处理长时间的背压，所有其他目的地也将等待。此外，一个目的地无限期地阻塞将阻塞整个管道！
- en: If we pipe to an additional stream after we've started consuming the data at
    source (async piping), the new stream will only receive new chunks of data. In
    those cases, we can use a `PassThrough` instance as a placeholder to collect all
    the data from the moment we start consuming the stream. Then, the `PassThrough`
    stream can be read at any future time without the risk of losing any data. Just
    be aware that this approach might generate backpressure and block the entire pipeline,
    as discussed in the previous point.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在开始从源消费数据（异步管道）之后将数据管道到另一个流，新的流将只会接收新的数据块。在这些情况下，我们可以使用`PassThrough`实例作为占位符来收集从开始消费流的那一刻起的所有数据。然后，`PassThrough`流可以在任何未来的时间点读取，而不会丢失任何数据。只是要注意，这种方法可能会产生背压并阻塞整个管道，正如前一点所讨论的。
- en: Merging streams
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并流
- en: 'Merging is the opposite operation to forking and involves piping a set of `Readable`
    streams into a single `Writable` stream, as shown in *Figure 6.8*:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 合并操作是分叉操作的相反操作，它涉及将一组`Readable`流通过管道导入一个单一的`Writable`流，如图*6.8*所示：
- en: '![](img/B15729_06_08.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15729_06_08.png)'
- en: 'Figure 6.8: Merging streams'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '图6.8: 合并流'
- en: 'Merging multiple streams into one is, in general, a simple operation; however,
    we have to pay attention to the way we handle the `end` event, as piping using
    the default options (whereby `{ end: true }`) causes the destination stream to
    end as soon as one of the sources ends. This can often lead to an error, as the
    other active sources continue to write to an already terminated stream.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '将多个流合并为一个通常是一个简单的操作；然而，我们必须注意我们处理`end`事件的方式，因为使用默认选项（其中`{ end: true }`）会导致目的地流在任何一个源流结束时立即结束。这通常会导致错误，因为其他活跃的源流会继续向已经终止的流写入数据。'
- en: 'The solution to this problem is to use the option `{ end: false }` when piping
    multiple sources to a single destination and then invoke `end()` on the destination
    only when all the sources have completed reading.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '解决这个问题的方法是，在将多个源流合并到单个目的地时使用`{ end: false }`选项，然后在所有源流完成读取后，仅对目的地调用`end()`。'
- en: Merging text files
  id: totrans-520
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合并文本文件
- en: 'To make a simple example, let''s implement a small program that takes an output
    path and an arbitrary number of text files, and then merges the lines of every
    file into the destination file. Our new module is going to be called `merge-lines.js`.
    Let''s define its contents, starting from some initialization steps:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做一个简单的例子，让我们实现一个小程序，它接受一个输出路径和任意数量的文本文件，然后将每个文件的行合并到目的地文件中。我们的新模块将被命名为`merge-lines.js`。让我们从一些初始化步骤开始定义其内容：
- en: '[PRE82]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: In the preceding code, we are just loading all the dependencies and initializing
    the variables that contain the name of the destination (`dest`) file and all the
    source files (`sources`).
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们只是在加载所有依赖项并初始化包含目的地文件名（`dest`）和所有源文件名（`sources`）的变量。
- en: 'Next, we will create the destination stream:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建目的地流：
- en: '[PRE83]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now, it''s time to initialize the source streams:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候初始化源流了：
- en: '[PRE84]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: In the preceding code, we created a `Readable` stream for every source file.
    Then, for each source stream, we attached an `end` listener, which will terminate
    the destination stream only when all the files have been read completely. Finally,
    we piped every source stream to `split()`, a `Transform` stream that makes sure
    that we produce a chunk for every line of text, and finally, we piped the results
    to our destination stream. This is when the real merge happens. We are piping
    multiple source streams into one single destination.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们为每个源文件创建了一个`Readable`流。然后，对于每个源流，我们附加了一个`end`监听器，它只会在所有文件都完全读取完毕时终止目标流。最后，我们将每个源流通过`split()`管道传输，这是一个`Transform`流，确保我们为每一行文本生成一个块，并将结果最终通过管道传输到目标流。这就是真正的合并发生的时候。我们将多个源流通过管道传输到一个单一的目标流中。
- en: 'We can now execute this code with the following command:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下命令执行此代码：
- en: '[PRE85]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: If you run this code with large enough files, you will notice that the destination
    file will contain lines that are randomly intermingled from all the source files
    (a low `highWaterMark` of 16 bytes makes this property even more apparent). This
    kind of behavior can be acceptable in some types of object streams and some text
    streams split by line (as in our current example), but it is often undesirable
    when dealing with most binary streams.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用足够大的文件运行此代码，你会注意到目标文件将包含来自所有源文件的随机交织的行（一个低至16字节的`highWaterMark`使得这一特性更加明显）。在某些类型的对象流和某些按行分割的文本流（如我们当前的例子）中，这种行为可能是可接受的，但处理大多数二进制流时通常是不希望的。
- en: There is one variation of the pattern that allows us to merge streams in order;
    it consists of consuming the source streams one after the other. When the previous
    one ends, the next one starts emitting chunks (it is like *concatenating* the
    output of all the sources). As always, on npm, we can find some packages that
    also deal with this situation. One of them is `multistream` ([https://npmjs.org/package/multistream](https://npmjs.org/package/multistream)).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种模式的变体允许我们按顺序合并流；它包括依次消费源流。当前一个流结束时，下一个流开始发出块（就像*连接*所有源输出的结果）。像往常一样，在npm上，我们可以找到一些也处理这种情况的包。其中之一是`multistream`([https://npmjs.org/package/multistream](https://npmjs.org/package/multistream))。
- en: Multiplexing and demultiplexing
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复用和解复用
- en: 'There is a particular variation of the merge stream pattern in which we don''t
    really want to just join multiple streams together but, instead, use a shared
    channel to deliver the data of a set of streams. This is a conceptually different
    operation because the source streams remain logically separated inside the shared
    channel, which allows us to split the stream again once the data reaches the other
    end of the shared channel. *Figure 6.9* clarifies this situation:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 在合并流模式中有一个特定的变体，我们并不真的想要仅仅将多个流合并在一起，而是使用一个共享通道来传递一组流的 数据。这是一个概念上不同的操作，因为源流在共享通道内部保持逻辑上的分离，这允许我们在数据达到共享通道的另一端后再次拆分流。*图6.9*清晰地说明了这种情况：
- en: '![](img/B15729_06_09.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15729_06_09.png)'
- en: 'Figure 6.9: Multiplexing and demultiplexing streams'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：复用和解复用流
- en: The operation of combining multiple streams (in this case, also known as channels)
    to allow transmission over a single stream is called **multiplexing**, while the
    opposite operation, namely reconstructing the original streams from the data received
    from a shared stream, is called **demultiplexing**. The *devices* that perform
    these operations are called **multiplexer** (or **mux**) and **demultiplexer**
    (or **demux**), respectively. This is a widely studied area in computer science
    and telecommunications in general, as it is one of the foundations of almost any
    type of communication media such as telephony, radio, TV, and, of course, the
    Internet itself. For the scope of this book, we will not go too far with the explanations,
    as this is a vast topic.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个流（在这种情况下也称为通道）组合在一起以允许通过单个流传输的操作称为**复用**，而相反的操作，即从从共享流接收到的数据中重建原始流，称为**解复用**。执行这些操作的*设备*分别称为**复用器**（或**mux**）和**解复用器**（或**demux**）。这在计算机科学和电信领域是一个广泛研究的话题，因为它是几乎所有类型通信媒体的基础，如电话、无线电、电视，当然还有互联网本身。对于本书的范围，我们将不会深入解释，因为这个主题非常广泛。
- en: What we want to demonstrate in this section is how it's possible to use a shared
    Node.js stream to transmit multiple logically separated streams that are then
    separated again at the other end of the shared stream.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们想要展示的是如何使用共享的Node.js流来传输多个逻辑上分离的流，然后在共享流的另一端再次分离。
- en: Building a remote logger
  id: totrans-539
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建远程日志记录器
- en: 'Let''s use an example to drive our discussion. We want a small program that
    starts a child process and redirects both its standard output and standard error
    to a remote server, which, in turn, saves the two streams in two separate files.
    So, in this case, the shared medium is a TCP connection, while the two channels
    to be multiplexed are the `stdout` and `stderr` of a child process. We will leverage
    a technique called **packet switching**, the same technique that is used by protocols
    such as IP, TCP, and UDP. Packet switching involves wrapping the data into *packets*,
    allowing us to specify various meta information that''s useful for multiplexing,
    routing, controlling the flow, checking for corrupted data, and so on. The protocol
    that we are implementing in our example is very minimalist. We wrap our data into
    simple packets, as illustrated in *Figure 6.10*:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来引导我们的讨论。我们想要一个小的程序，该程序启动一个子进程，并将它的标准输出和标准错误重定向到一个远程服务器，该服务器反过来将两个流保存到两个单独的文件中。因此，在这种情况下，共享介质是一个TCP连接，而要复用的两个通道是子进程的`stdout`和`stderr`。我们将利用一种称为**分组交换**的技术，这种技术与IP、TCP和UDP等协议中使用的技术相同。分组交换涉及将数据封装成*数据包*，允许我们指定对复用、路由、控制流量、检查损坏数据等有用的各种元信息。我们在示例中实现的协议非常简约。我们像*图6.10*所示那样将数据封装成简单的数据包：
- en: '![](img/B15729_06_10.png)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_06_10.png)'
- en: 'Figure 6.10: Bytes structure of the data packet for our remote logger'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：远程日志记录器数据包的字节结构
- en: As shown in *Figure 6.10*, the packet contains the actual data, but also a header
    (*Channel ID* + *Data length*), which will make it possible to differentiate the
    data of each stream and enable the demultiplexer to route the packet to the right
    channel.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图6.10*所示，数据包包含实际数据，同时也包含一个头部（*通道ID* + *数据长度*），这使得能够区分每个流的数据，并使解复用器能够将数据包路由到正确的通道。
- en: Client side – multiplexing
  id: totrans-544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 客户端 – 复用
- en: Let's start to build our application from the client side. With a lot of creativity,
    we will call the module `client.js.` This represents the part of the application
    that is responsible for starting a child process and multiplexing its streams.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从客户端开始构建我们的应用程序。通过大量的创意，我们将模块命名为`client.js`。这代表应用程序负责启动子进程并复用其流的那个部分。
- en: 'So, let''s start by defining the module. First, we need some dependencies:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们首先定义模块。首先，我们需要一些依赖项：
- en: '[PRE86]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Now, let''s implement a function that performs the multiplexing of a list of
    sources:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个执行多个源复用的函数：
- en: '[PRE87]'
  id: totrans-549
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The `multiplexChannels()` function takes in, as input, the source streams to
    be multiplexed and the destination channel, and then it performs the following
    steps:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '`multiplexChannels()`函数接受要复用的源流和目标通道作为输入，然后执行以下步骤：'
- en: For each source stream, it registers a listener for the `readable` event, where
    we read the data from the stream using the non-flowing mode.
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个源流，它注册一个监听`readable`事件的监听器，在那里我们使用非流动模式从流中读取数据。
- en: When a chunk is read, we wrap it into a packet that contains, in order, 1 byte
    (`UInt8`) for the channel ID, 4 bytes (`UInt32BE`) for the packet size, and then
    the actual data.
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当读取一个块时，我们将它封装成一个包含以下内容的包：1个字节（`UInt8`）用于通道ID，4个字节（`UInt32BE`）用于包大小，然后是实际数据。
- en: When the packet is ready, we write it into the destination stream.
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当数据包准备好后，我们将其写入目标流。
- en: Finally, we register a listener for the `end` event so that we can terminate
    the destination stream when all the source streams have ended.
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们注册一个监听`end`事件的监听器，以便在所有源流结束时终止目标流。
- en: Our protocol is to be able to multiplex up to 256 different source streams because
    we only have 1 byte to identify the channel.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的协议能够复用多达256个不同的源流，因为我们只有1个字节用于标识通道。
- en: 'Now, the last part of our client becomes very easy:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们客户端的最后部分变得非常简单：
- en: '[PRE88]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'In this last code fragment, we perform the following operations:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最后的代码片段中，我们执行以下操作：
- en: We create a new TCP client connection to the address `localhost:3000`.
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个新的TCP客户端连接到地址`localhost:3000`。
- en: 'We start the child process by using the first command-line argument as the
    path, while we provide the rest of the `process.argv` array as arguments for the
    child process. We specify the option `{silent: true}` so that the child process
    does not inherit `stdout` and `stderr` of the parent.'
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '我们通过使用第一个命令行参数作为路径来启动子进程，同时我们将`process.argv`数组中的其余部分作为子进程的参数。我们指定选项`{silent:
    true}`，这样子进程就不会继承父进程的`stdout`和`stderr`。'
- en: Finally, we take `stdout` and `stderr` of the child process and we multiplex
    them into the socket's `Writable` stream using the `mutiplexChannels()` function.
  id: totrans-561
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们获取子进程的`stdout`和`stderr`，并使用`mutiplexChannels()`函数将它们多路复用到套接字的`Writable`流。
- en: Server side – demultiplexing
  id: totrans-562
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 服务器端 – 解复用
- en: Now, we can take care of creating the server side of the application (`server.js`),
    where we demultiplex the streams from the remote connection and pipe them into
    two different files.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以处理创建应用程序的服务器端（`server.js`），在那里我们从远程连接解复用流并将它们管道传输到两个不同的文件。
- en: 'Let''s start by creating a function called `demultiplexChannel()`:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建一个名为`demultiplexChannel()`的函数：
- en: '[PRE89]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The preceding code might look complicated, but it is not. Thanks to the features
    of Node.js `Readable` streams, we can easily implement the demultiplexing of our
    little protocol as follows:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码可能看起来很复杂，但实际上并不复杂。多亏了Node.js `Readable`流的功能，我们可以轻松实现我们小协议的解复用，如下所示：
- en: We start reading from the stream using the non-flowing mode.
  id: totrans-567
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们以非流动模式从流中开始读取。
- en: First, if we have not read the channel ID yet, we try to read 1 byte from the
    stream and then transform it into a number.
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，如果我们还没有读取通道ID，我们尝试从流中读取1个字节并将其转换为数字。
- en: The next step is to read the length of the data. We need 4 bytes for that, so
    it's possible (even if unlikely) that we don't have enough data in the internal
    buffer, which will cause the `this.read()` invocation to return `null`. In such
    a case, we simply interrupt the parsing and retry at the next `readable` event.
  id: totrans-569
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是读取数据长度。我们需要4个字节，所以即使不太可能，我们可能没有足够的内部缓冲区数据，这将导致`this.read()`调用返回`null`。在这种情况下，我们简单地中断解析，并在下一个`readable`事件时重试。
- en: When we can finally also read the data size, we know how much data to pull from
    the internal buffer, so we try to read it all.
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们最终也能读取数据大小，我们就知道要从内部缓冲区中提取多少数据，所以我们尝试读取所有数据。
- en: When we read all the data, we can write it to the right destination channel,
    making sure that we reset the `currentChannel` and `currentLength` variables (these
    will be used to parse the next packet).
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们读取所有数据后，我们可以将其写入正确的目标通道，确保重置`currentChannel`和`currentLength`变量（这些将用于解析下一个数据包）。
- en: Lastly, we make sure to end all the destination channels when the source channel
    ends.
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们确保在源通道结束时结束所有目标通道。
- en: 'Now that we can demultiplex the source stream, let''s put our new function
    to work:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够解复用源流，让我们将我们的新函数投入使用：
- en: '[PRE90]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'In the preceding code, we first start a TCP server on port `3000`; then, for
    each connection that we receive, we create two `Writable` streams pointing to
    two different files: one for the standard output and the other for the standard
    error. These are our destination channels. Finally, we use `demultiplexChannel()`
    to demultiplex the `socket` stream into `stdoutStream` and `stderrStream`.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先在端口`3000`上启动一个TCP服务器；然后，对于每个接收到的连接，我们创建两个指向两个不同文件的`Writable`流：一个用于标准输出，另一个用于标准错误。这些是我们的目标通道。最后，我们使用`demultiplexChannel()`将`socket`流解复用到`stdoutStream`和`stderrStream`。
- en: Running the mux/demux application
  id: totrans-576
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 运行复用/解复用应用
- en: 'Now, we are ready to try our new mux/demux application, but first, let''s create a small
    Node.js program to produce some sample output; let''s call it `generate-data.js`:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备尝试我们的新复用/解复用应用，但首先，让我们创建一个小的Node.js程序来生成一些示例输出；让我们称它为`generate-data.js`：
- en: '[PRE91]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Okay; now, we are ready to try our remote logging application. First, let''s
    start the server:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 好的；现在，我们准备尝试我们的远程日志应用。首先，让我们启动服务器：
- en: '[PRE92]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Then, we''ll start the client by providing the file that we want to start as
    a child process:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过提供我们想要启动的文件作为子进程来启动客户端：
- en: '[PRE93]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: The client will run almost immediately, but at the end of the process, the standard
    input and standard output of the `generate-data.js` application will have traveled
    through one single TCP connection and then, on the server, be demultiplexed into
    two separate files.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端将几乎立即运行，但在进程结束时，`generate-data.js`应用程序的标准输入和标准输出将通过一个单一的TCP连接传输，然后在服务器上被解复用为两个单独的文件。
- en: Please make a note that, as we are using `child_process.fork()` ([nodejsdp.link/fork](http://nodejsdp.link/fork)),
    our client will only be able to launch other Node.js modules.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们正在使用`child_process.fork()`([nodejsdp.link/fork](http://nodejsdp.link/fork))，我们的客户端将只能启动其他Node.js模块。
- en: Multiplexing and demultiplexing object streams
  id: totrans-585
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多路复用和解复用对象流
- en: The example that we have just shown demonstrates how to multiplex and demultiplex
    a binary/text stream, but it's worth mentioning that the same rules apply to object
    streams. The biggest difference is that when using objects, we already have a
    way to transmit the data using atomic messages (the objects), so multiplexing
    would be as easy as setting a `channelID` property in each object. Demultiplexing
    would simply involve reading the `channelID` property and routing each object
    toward the right destination stream.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚展示的示例演示了如何多路复用和解复用二进制/文本流，但值得一提的是，相同的规则也适用于对象流。最大的区别在于，当使用对象时，我们已经有了一种使用原子消息（对象）传输数据的方式，因此多路复用就像在每个对象中设置一个`channelID`属性一样简单。解复用只需简单地读取`channelID`属性，并将每个对象路由到正确的目标流。
- en: 'Another pattern involving only demultiplexing is routing the data coming from
    a source depending on some condition. With this pattern, we can implement complex
    flows, such as the one shown in *Figure 6.11*:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种仅涉及解复用的模式是根据某些条件路由来自源的数据。使用这种模式，我们可以实现复杂的流程，例如*图6.11*中所示：
- en: '![](img/B15729_06_11.png)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B15729_06_11.png)'
- en: 'Figure 6.11: Demultiplexing an object stream'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：解复用对象流
- en: 'The demultiplexer used in the system in *Figure 6.11* takes a stream of objects
    representing *animals* and distributes each of them to the right destination stream
    based on the class of the animal: *reptiles*, *amphibians*, or *mammals*.'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.11*中使用的解复用器接收代表*动物*的对象流，并根据动物的类别：*爬行动物*、*两栖动物*或*哺乳动物*，将每个对象分配到正确的目标流。
- en: Using the same principle, we can also implement an `if...else` statement for
    streams. For some inspiration, take a look at the `ternary-stream` package ([nodejsdp.link/ternary-stream](http://nodejsdp.link/ternary-stream)),
    which allows us to do exactly that.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的原理，我们也可以为流实现一个`if...else`语句。为了获得一些灵感，请查看`ternary-stream`包([nodejsdp.link/ternary-stream](http://nodejsdp.link/ternary-stream))，它允许我们做到这一点。
- en: Summary
  id: totrans-592
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we have shed some light on Node.js streams and some of their
    most common use cases. We learned why streams are so acclaimed by the Node.js
    community and we mastered their basic functionality, enabling us to discover more
    and navigate comfortably in this new world. We analyzed some advanced patterns
    and started to understand how to connect streams in different configurations,
    grasping the importance of interoperability, which is what makes streams so versatile
    and powerful.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要介绍了Node.js流及其一些最常见用例。我们了解了为什么流在Node.js社区中受到如此赞誉，并掌握了它们的基本功能，使我们能够在这个新世界中探索更多并轻松导航。我们分析了某些高级模式，并开始理解如何以不同的配置连接流，掌握了互操作性的重要性，这也是流如此灵活和强大的原因。
- en: If we can't do something with one stream, we can probably do it by connecting
    other streams together, and this works great with the *one thing per module* philosophy.
    At this point, it should be clear that streams are not just a *good to know* feature
    of Node.js; they are an essential part—a crucial pattern to handle binary data,
    strings, and objects. It's not by chance that we dedicated an entire chapter to
    them.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们无法对单个流进行操作，我们可能通过连接其他流来实现，这与*每个模块一件事*的哲学非常契合。此时，应该很清楚，流不仅仅是Node.js的一个*值得了解*的功能；它们是一个基本的部分——处理二进制数据、字符串和对象的关键模式。我们之所以专门用一整章来介绍它们，并非偶然。
- en: In the next few chapters, we will focus on the traditional object-oriented design
    patterns. But don't be fooled; even though JavaScript is, to some extent, an object-oriented
    language, in Node.js, the functional or hybrid approach is often preferred. Get
    rid of every prejudice before reading the next chapters.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将专注于传统的面向对象设计模式。但不要被误导；尽管JavaScript在某种程度上是一种面向对象的语言，但在Node.js中，函数式或混合方法通常更受欢迎。在阅读下一章之前，摒弃所有偏见。
- en: Exercises
  id: totrans-596
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: '**6.1 Data compression efficiency**: Write a command-line script that takes
    a file as input and compresses it using the different algorithms available in
    the `zlib` module (Brotli, Deflate, Gzip). You want to produce a summary table
    that compares the algorithm''s compression time and compression efficiency on
    the given file. Hint: This could be a good use case for the fork pattern, but
    remember that we made some important performance considerations when we discussed
    it earlier in this chapter.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6.1 数据压缩效率**：编写一个命令行脚本，该脚本接受一个文件作为输入，并使用`zlib`模块（Brotli、Deflate、Gzip）中可用的不同算法对其进行压缩。你希望生成一个比较表，比较算法在给定文件上的压缩时间和压缩效率。提示：这可能是fork模式的一个很好的用例，但请记住，我们在本章前面讨论它时已经做了一些重要的性能考虑。'
- en: '**6.2 Stream data processing**: On Kaggle, you can find a lot of interesting
    data sets, such as the London Crime Data ([nodejsdp.link/london-crime](http://nodejsdp.link/london-crime)).
    You can download the data in CSV format and build a stream processing script that
    analyzes the data and tries to answer the following questions:'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6.2 流数据处理**：在Kaggle上，你可以找到许多有趣的数据集，例如伦敦犯罪数据([nodejsdp.link/london-crime](http://nodejsdp.link/london-crime))。你可以以CSV格式下载数据，并构建一个流处理脚本，分析数据并尝试回答以下问题：'
- en: Did the number of crimes go up or down over the years?
  id: totrans-599
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近年来犯罪数量是增加还是减少了？
- en: What are the most dangerous areas of London?
  id: totrans-600
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伦敦最危险的区域是哪些？
- en: What is the most common crime per area?
  id: totrans-601
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个区域最常见的犯罪是什么？
- en: What is the least common crime?
  id: totrans-602
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常见的犯罪是什么？
- en: 'Hint: You can use a combination of `Transform` streams and `PassThrough` streams
    to parse and observe the data as it is flowing. Then, you can build in-memory
    aggregations for the data, which can help you answer the preceding questions.
    Also, you don''t need to do everything in one pipeline; you could build very specialized
    pipelines (for example, one per question) and use the fork pattern to distribute
    the parsed data across them.'
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示：你可以使用`Transform`流和`PassThrough`流的组合来解析和观察数据流。然后，你可以为数据构建内存聚合，这有助于你回答前面的问题。此外，你不需要在一个管道中完成所有操作；你可以构建非常专业的管道（例如，每个问题一个），并使用fork模式将解析后的数据分配到它们中。
- en: '**6.3 File share over TCP**: Build a client and a server to transfer files
    over TCP. Extra points if you add a layer of encryption on top of that and if
    you can transfer multiple files at once. Once you have your implementation ready,
    give the client code and your IP address to a friend or a colleague, then ask
    them to send you some files! Hint: You could use mux/demux to receive multiple
    files at once.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6.3 通过TCP进行文件共享**：构建一个客户端和一个服务器，以通过TCP传输文件。如果你在之上添加了一层加密，并且能够一次性传输多个文件，那么你会得到额外的分数。一旦你的实现准备就绪，将客户端代码和你的IP地址给一个朋友或同事，然后请他们给你发送一些文件！提示：你可以使用多路复用/解复用功能一次性接收多个文件。'
- en: '**6.4 Animations with** **Readable** **streams**: Did you know you can create
    amazing terminal animations with just `Readable` streams? Well, to understand
    what we are talking about here, try to run `curl parrot.live` in your terminal
    and see what happens! If you think that this is cool, why don''t you try to create
    something similar? Hint: If you need some help with figuring out how to implement
    this, you can check out the actual source code of `parrot.live` by simply accessing
    its URL through your browser.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6.4 使用可读流进行动画**：你知道你可以只用`Readable`流来创建令人惊叹的终端动画吗？好吧，为了理解我们在这里谈论的内容，试着在你的终端中运行`curl
    parrot.live`并看看会发生什么！如果你认为这很酷，为什么不尝试创建一些类似的东西呢？提示：如果你需要一些帮助来弄清楚如何实现这个功能，你可以通过浏览器访问其URL来查看`parrot.live`的实际源代码。'
