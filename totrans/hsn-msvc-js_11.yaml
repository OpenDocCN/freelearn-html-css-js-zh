- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world of software development is constantly evolving. As applications grow
    in complexity, traditional monolithic architectures struggle to keep pace. This
    chapter dives into some key design patterns that empower developers to build scalable
    and resilient systems – an API gateway, **Command Query Responsibility Segregation**
    (**CQRS**), event sourcing, and Service Registry and discovery.
  prefs: []
  type: TYPE_NORMAL
- en: These patterns, particularly when used together within a microservices architecture,
    offer numerous benefits. They promote loose coupling between services, making
    them easier to develop, maintain, and deploy independently. They also enhance
    scalability by allowing individual services to be scaled based on specific needs.
    Additionally, these patterns contribute to improved fault tolerance and resilience,
    ensuring that your application remains robust even if individual services encounter
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will provide a comprehensive introduction to each of these patterns,
    outlining their core concepts, benefits, and use cases. We will explore how to
    apply some of them to create a robust foundation for building modern, scalable
    applications. By understanding these patterns, you’ll be equipped to design and
    develop applications that can thrive in the ever-changing landscape of software
    development.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with an API gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CQRS and event sourcing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service Registry and discovery in microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get into the chapter!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow us along the chapter, we need an IDE (we prefer Visual Studio Code),
    Postman, Docker, and a browser of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: It is preferable to download the repository from [https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript)
    and open the `Ch11` folder to easily follow our code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with an API gateway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An API Gateway integrates with a microservice architecture by acting as a central
    hub, managing communication between client applications and the distributed microservices.
    When we build our microservices, we want them independently developed, deployed,
    and scaled without affecting client applications. Clients only interact with the
    API gateway, which shields them from the complexities of the underlying microservice
    network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1: A simple API Gateway](img/B09148_11_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: A simple API Gateway'
  prefs: []
  type: TYPE_NORMAL
- en: 'The API Gateway receives requests from clients and intelligently routes them
    to the appropriate microservice(s), based on the request content or URL. It can
    handle simple routing or complex scenarios, involving multiple microservices working
    together to fulfill a request. Let’s explore the importance of integrating an
    API Gateway into our microservice architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplified client interaction**: Clients have a centralized entry point/single
    point of contact (the API gateway) to interact with an application, regardless
    of how many microservices are involved. This reduces development complexity on
    the client side.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved scalability**: An API Gateway can be independently scaled to handle
    increasing traffic volumes without impacting the individual microservices. Microservices
    can also be scaled independently based on their specific workloads, highlighting
    the importance of API gateways.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced security**: Centralized security management of an API Gateway strengthens
    overall application security. The API Gateway can implement authentication, authorization,
    and other security policies to protect microservices from unauthorized access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced development complexity**: Developers don’t need to implement functionalities
    such as routing, security, and monitoring logic within each microservice. An API
    Gateway handles these cross-cutting concerns centrally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now see how an API Gateway might work.
  prefs: []
  type: TYPE_NORMAL
- en: How an API Gateway works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a microservice architecture, an API Gateway acts as the central entry point
    for all client requests. It plays a crucial role in managing and optimizing the
    flow of communication between clients and backend services. By handling authentication,
    routing, load balancing, and other vital functions, the API Gateway ensures that
    the microservices remain loosely coupled and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a step-by-step breakdown of how an API Gateway typically processes client
    requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Client request**: A client (e.g., a web or mobile app) sends a request to
    an API Gateway. The request includes details such as the HTTP method, URL path,
    headers, and possibly a body.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Request handling**: The API Gateway receives the request and examines its
    contents. Based on the URL path or other routing rules, the Gateway determines
    which backend service(s) should handle the request.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Authentication and authorization**: The API Gateway checks the request for
    authentication tokens (e.g., JWT or OAuth tokens). It verifies the token’s validity
    and checks whether the client has the necessary permissions to access the requested
    resource.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Request transformation**: The API Gateway may modify the request to fit the
    requirements of the backend service. This might include changing the protocol,
    altering headers, or modifying the request body.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Routing and aggregation**: The Gateway routes the request to the appropriate
    backend service(s). If the request involves multiple services, the Gateway will
    handle communication with each service and aggregate their responses into a single
    response for the client.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Caching and load balancing**: The Gateway checks whether the response is
    cached to serve it quickly without hitting the backend service. It also distributes
    the request load among multiple instances of the backend service to balance traffic
    and improve performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Rate limiting and throttling**: The API Gateway enforces rate limits to control
    the number of requests a client can make within a specified period. It may throttle
    requests if a client exceeds the allowed request rate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Response handling**: Once the backend service(s) respond, the Gateway may
    modify the response before sending it back to the client. This could include adding
    or removing headers, transforming data formats, or aggregating multiple responses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Logging and monitoring**: The API Gateway logs details of the request and
    response for monitoring and analysis. Metrics such as request counts, response
    times, and error rates are tracked to monitor the health and performance of the
    services.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we know how an API Gateway works, let’s see what the better choice
    in a given situation is – single or multiple API gateways.
  prefs: []
  type: TYPE_NORMAL
- en: Single versus multiple API gateways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can implement multiple API gateways in a microservice architecture, but
    it’s not always the most straightforward or recommended approach. There are situations
    where it might be beneficial, but generally, a single API Gateway is preferred
    for simplicity and maintainability.
  prefs: []
  type: TYPE_NORMAL
- en: A single API Gateway is ideal when you want centralized management, a consistent
    client experience, and simplified scalability – all of which streamline API operations
    and reduce complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'While a single Gateway is often preferred, there are some situations where
    multiple gateways might be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Heterogeneous client types**: If you have clients using vastly different
    protocols or communication styles (e.g., mobile apps, web applications, and legacy
    systems), separate API gateways could be used to cater to these specific needs
    with custom protocols or functionalities. This approach can be complex to maintain
    in the long run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Physical separation**: If your microservices are geographically distributed
    across different data centers or cloud regions, you might consider placing an
    API Gateway in each location for performance reasons. However, this introduces
    additional management overhead to maintain consistency across gateways.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security segmentation**: In very specific security-sensitive scenarios, you
    might implement separate API gateways for different security zones within your
    application. This allows for stricter control over access to certain microservices.
    However, this requires careful design and expertise to avoid creating unnecessary
    complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, the benefits of a single API Gateway outweigh the potential advantages
    of using multiple gateways, as the former promotes simplicity, maintainability,
    and a consistent client experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to reap the benefits of multiple API gateways without the complexity,
    here are some alternatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**An API Gateway with routing by client type**: Consider using a single API
    Gateway with routing logic that can differentiate between different client types
    and tailor responses accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microservice facades**: Implement a **facade** pattern (more on this shortly)
    within some microservices to handle specific client interactions, potentially
    reducing the need for multiple gateways.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should carefully consider your specific needs before implementing multiple
    API gateways. In most cases, a well-designed single API Gateway will provide the
    optimal solution for your microservice architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The facade pattern
  prefs: []
  type: TYPE_NORMAL
- en: A facade in this context refers to implementing a layer within some microservices
    that specifically handles interactions with clients. Instead of introducing multiple
    API gateways, which can add complexity, a microservice facade acts as a simplified
    interface or *front* that abstracts the internal workings of the microservice
    for the client.
  prefs: []
  type: TYPE_NORMAL
- en: It is time to implement and see the power of an API Gateway in practice. The
    next section will dive into the details of the practical implementation of an
    API gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing microservices using an API gateway
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is possible to implement the API Gateway pattern using different forms with
    different libraries. In this context, `Ch11`/`ApiGateway` folder in our repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the real value of the API Gateway pattern, we need to have at
    least two microservices. The reason for needing *at least two* microservices to
    show the true value of the API Gateway pattern is that the pattern is designed
    to handle multiple services and consolidate their functionality for clients. We
    will use the following two microservices in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The post-microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing post microservice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our first microservice, the *post-microservice*, acts as a wrapper/abstraction
    over the `jsonplaceholder` service. `jsonplaceholder` is a free online service
    that provides a REST API with fake data. It’s often used by developers to easily
    access and utilize realistic-looking sample data (users, posts, comments, etc.)
    without having to set up their databases. This allows them to quickly test API
    endpoints, frontend functionality, and user interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new folder (a `post-microservice` folder in our case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `npm install express axios` to install the required packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is what your `package.json` should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For all chapters, you don’t need to install the exact package versions listed.
    While our focus is on using the packages themselves rather than specific versions,
    if there are major changes or breaking differences in newer versions, refer to
    the official documentation for updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create a new file called `server.js` in the folder we created (i.e.,
    `post-microservices`) with the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet uses the Express framework to create a simple web server that
    listens on port `3001`. It imports the `axios` library to make HTTP requests.
    The server has a single route, `/posts/:id`, which responds to `GET` requests.
    When a request is made to this route, it extracts the `id` parameter from the
    URL. The server then makes an asynchronous request to `https://jsonplaceholder.typicode.com/posts/${postId}`
    to fetch a specific post. If the post is found, it sends the post data as a JSON
    response. If the post is not found, it responds with a `404` status code. If there
    are any errors during the request, it logs them and responds with a `500`-status
    code, indicating an internal server error.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run our microservice using the `node server.js` command and test whether
    everything is working. Open your favorite browser and navigate to `localhost:3001/posts/1`
    (*Figure 11**.2*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2: A post-microservice response](img/B09148_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: A post-microservice response'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing user microsevice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our second microservice is called a *user microservice*. It has approximately
    the same implementation as our post microservice, with a different port (`3002`)
    and different service abstraction (a GitHub service abstraction):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s run our microservice using the `node server.js` command and test whether
    everything is working. Open your favorite browser and navigate to `localhost:3002/users/1`
    (*Figure 11**.3*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3: A user microservice response](img/B09148_11_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: A user microservice response'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build our API Gateway as a third microservice and combine the post and
    user microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Developing an API gateway
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After implementing two microservices, we’re ready to show the value and power
    of an API gateway. We plan to implement rate limit, cache, and response aggregation
    functionalities for the API gateway. You can add more features such as logging,
    appropriate exception handling, monitoring, and other interesting behaviors after
    understanding the essentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first – you need to understand that an API Gateway by itself acts
    as a separate microservice. So, create a new folder for it (it is called `api-``g``ateway`
    in our GitHub repo). We have `package.json` with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We will use an `express-rate-limit` package to implement rate-limit functionality
    in our microservice. In a microservice architecture, where applications are broken
    down into smaller, independent services, **rate limiting** is a technique used
    to control the number of requests that a service can receive within a specific
    timeframe. It acts like a traffic controller, preventing a service from being
    overloaded by a surge of requests.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, `apicache` is used to implement cache behavior for an API gateway.
    **Caching** refers to a functionality that allows you to store responses from
    your backend services for a specific time. This cached data can then be served
    to subsequent requests, improving performance and reducing load on your backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a `server.js` file to implement an API gateway. Our imported packages
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let’s configure our rate limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `express-rate-limit` to control how many times users can access your
    API Gateway in a minute. It acts like a gatekeeper. If a user makes fewer than
    a hundred requests within a minute, they get through. If they go over a hundred,
    they’ll be blocked with a `Too many requests, please slow down` message. This
    protects our API from overload and ensures a good user experience for everyone.
    We will use this `limiter` object later when we specify routing for our endpoint.
    Let’s move on and implement data aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This function, `getAggregatedData`, retrieves data from two different microservices
    to build a combined response. It takes an ID as input:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it makes two separate asynchronous calls using `axios.get`. One fetches
    post data from the post microservice at port `3001`, and the other fetches user
    data from the user microservice at port `3002`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it combines the data into a single object, named `aggregatedData`. User
    data such as location, the followers’ URLs, and the person followed by the URL
    are included. Additionally, the post data retrieved from the first call is added
    under the key post.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the function returns the `aggregatedData` object, containing all the
    relevant information about the user and their posts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By aggregating data in an API gateway, we present a simplified API to client
    applications. They only need to call a single endpoint (within the gateway) to
    receive the combined user and post data, instead of making separate calls to each
    microservice.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when requesting `localhost:3000/users/1`, we should get user information
    from both the post and user microservices. Here is how we get aggregated data
    from more than one microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a route handler for the API Gateway using Express.js. It handles
    `GET` requests to the `/users/:id` URL path, where `:id` is a dynamic parameter
    representing the user ID. The `limiter` middleware is applied before the route
    handler function, which ensures that only allowed requests (typically, a hundred
    per minute based on the previous code) can proceed. Inside the function, the API
    extracts the ID from the request parameters. It then calls the `getAggregatedData`
    function to asynchronously retrieve and combine user and post data. If successful,
    the function sends a JSON response with the retrieved aggregated data. If there
    are errors during data fetching, it sends a response with a status code of `400`
    (bad request) and a generic error message.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last functionality in our API Gateway is caching. We need to add the following
    code snippet to the `server.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Using this code, we apply caching for five minutes for all types of endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: We’re done with our infrastructure (the post microservice, API Gateway, and
    user microservice); it is time to test all of them together.
  prefs: []
  type: TYPE_NORMAL
- en: Testing an API Gateway in Docker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To test an API Gateway, you can run every microservice separately, but as you
    know, we have different names for microservices in our `getAggregatedData` function
    – `http://post-microservice:3001` and `http://user-microservice:3002`. To make
    these microservices work properly and not run every microservice every time, we
    will containerize them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For every microservice, we have `Dockerfile`, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4: An API Gateway project structure](img/B09148_11_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: An API Gateway project structure'
  prefs: []
  type: TYPE_NORMAL
- en: A `Dockerfile` is a text file that contains instructions to build a Docker image.
    It acts like a recipe that tells Docker what steps to take to create a self-contained
    environment for your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'All three Docker files are completely the same, with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This `Dockerfile` creates an image for a Node.js application. It starts with
    a lightweight Node.js base image, installs dependencies, copies your entire project,
    and then runs your server code upon startup.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a `docker-compose.yml` file in our root folder that will combine all
    these three `Dockerfile` files and compose them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This `docker-compose.yml` file defines a multi-container application. It creates
    three services – `post-microservice`, `user-microservice`, and `api-``g``ateway`.
    Each builds its own image from a separate directory (for example, `./post-microservice`)
    using a common `Dockerfile`.
  prefs: []
  type: TYPE_NORMAL
- en: Each service gets exposed on a specific port (`3001` for posts, `3002` for users,
    and `3000` for the Gateway).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `api-Gateway` relies on both `post-microservice` and `user-microservice`
    to be active before starting itself, ensuring that the dependencies are available.
    To compose these microservices’ Docker files, navigate to the folder where we
    have the `docker-compose.yml` file and run the `docker-compose up -d` command.
    It should build and run composed services together. Here is what running all required
    services together via Docker looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5: An API Gateway in Docker](img/B09148_11_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: An API Gateway in Docker'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to `localhost:3000/users/1` from your browser, and you should get
    the following aggregated data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6: An API Gateway in action](img/B09148_11_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: An API Gateway in action'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have explored the role of an API Gateway in a microservices architecture,
    emphasizing how it simplifies client interactions by acting as a central entry
    point for routing, security, and load balancing. We learned how the API Gateway
    aggregates data from multiple microservices, applies caching and rate limiting,
    and enhances scalability. By integrating it into our architecture, we improve
    both performance and security while maintaining the flexibility and independence
    of individual microservices. Finally, we containerized the microservices and API
    Gateway using Docker for efficient testing and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In our next section, we’re going to explore other interesting patterns such
    as CQRS and event sourcing. First, we will learn what are they and why we use
    them.
  prefs: []
  type: TYPE_NORMAL
- en: CQRS and event sourcing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CQRS is a software design pattern used in distributed systems (often microservices)
    to separate read and write operations. This separation offers several advantages,
    particularly when dealing with applications with high read/write disparities or
    complex data models.
  prefs: []
  type: TYPE_NORMAL
- en: When you apply for jobs that use distributed architecture in their applications,
    you often hear about CQRS and, most probably, will be asked about its usage. First
    things first – we need to understand that CQRS is not an architecture style; it
    is neither an architecture nor architectural principle. It is just a design pattern
    that has no wide usage. So, what is CQRS? Before answering this question, let’s
    understand the problem that CQRS seeks to resolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional monolithic applications typically use a single database to both
    read and write data. This approach can lead to the following challenges as an
    application grows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling bottlenecks**: When read traffic spikes, it can impact write performance
    (and vice versa).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data model mismatch**: Optimal read and write models may differ. Reads might
    benefit from denormalized data for faster retrieval, while writes might require
    a normalized structure for data integrity. This mismatch creates inefficiencies
    or duplication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transaction conflicts**: Updates and reads can compete for resources, potentially
    blocking each other or causing inconsistencies (violations of **ACID** (**Atomicity,
    Consistency, Isolation,** **Durability**) principles).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization challenges**: Optimizing for reads might hinder write performance,
    and vice versa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we work with monolithic applications, we often use one single data store.
    This means we have multiple read and write instructions in the same database.
    We use the same data store model, and everything is simple when it comes to working
    with only one single storage in terms of development. But is that all? Well, not
    everything is okay when we have only one data store. Depending on our requirements,
    we may need to separate our database into read and write databases.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CQRS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CQRS helps us to separate data stores into read and write data stores. Why?
    One reason is that we need to optimize our read and write operations. Using CQRS,
    we can optimize our read data store to read data effectively. We can also configure
    our schema to optimize reading operations. The same is applicable for writing
    data stores.
  prefs: []
  type: TYPE_NORMAL
- en: When we have separate data storages, depending on loading, we can scale them
    independently. When we have separate data stores for reading and writing, we can
    scale them independently, based on the specific load requirements of each. This
    is particularly useful in applications that experience high demand for read operations.
    By decoupling the read and write operations, we can scale the read data store
    to handle the load without affecting the performance of the write data store,
    or vice versa. This approach allows more efficient resource allocation, ensuring
    that each data store is optimized for its specific role.
  prefs: []
  type: TYPE_NORMAL
- en: With CQRS, read and write are separated storages, and we have two different
    data models. We can now focus on optimizing and building them to support only
    one operation – either read or write.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, here are the benefits of CQRS:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved performance**: Optimized read and write models can significantly
    enhance performance for both read and write operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced scalability**: You can scale read and write models independently
    based on their access patterns. This allows you to handle fluctuating read/write
    loads more effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility in data modeling**: Each model can be designed for its specific
    purpose, improving overall data management and reducing complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Is CQRS a silver bullet? Of course not. You should consider the following when
    you integrate CQRS into your projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Added complexity**: Implementing CQRS introduces additional complexity compared
    to a single store. Careful design and trade-off analysis are necessary for successful
    implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency**: Maintaining consistency across read and write models
    requires careful consideration. Strategies such as eventual consistency or materialized
    views can be employed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CQRS is a valuable pattern for applications with *high read/write disparities*
    (e.g., e-commerce with frequent product views and infrequent purchases), *complex
    data models* with different requirements for reads and writes, and scenarios that
    require *independent scaling* of read and write operations.
  prefs: []
  type: TYPE_NORMAL
- en: Before adopting CQRS, carefully analyze your application’s needs. While it offers
    significant benefits in specific scenarios, the added complexity might not be
    necessary for simpler applications.
  prefs: []
  type: TYPE_NORMAL
- en: When discussing CQRS, it is also important to discuss event sourcing. They are
    complementary patterns that work well together, but they address different aspects
    of an application’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Event sourcing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Event sourcing** is a design pattern to persist data as a sequence of events.
    Instead of storing the current state of an entity (such as a user account), you
    record each action that modifies that entity. This creates an immutable history
    of changes, allowing you to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Replay events to rebuild state at any point in time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gain deep insights into an application’s history for auditing and debugging
    purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplify data evolution, as new events can be added without modifying existing
    ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These events represent what happened, not the current state of the data. By
    replaying an event stream, you can reconstruct the state at any point in time.
    Traditional databases in CQRS can be used to write models (i.e., store commands).
    Event sourcing shines on the read model side of CQRS. The event stream from event
    sourcing serves as the source of truth for read models. Read models are materialized
    projections built by replaying relevant events.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is very important to note that CQRS can be implemented without
    event sourcing. Event sourcing often benefits from CQRS when managing read models,
    as the two patterns work well together in many scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: CQRS handles the high volume of reads efficiently by using optimized read models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event sourcing provides a complete history to build these read models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates to an event stream automatically trigger updates in the read models,
    ensuring consistency (although eventual consistency might apply).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event sourcing versus event streaming
  prefs: []
  type: TYPE_NORMAL
- en: Event streaming is not the same as event sourcing, although they are closely
    related and often used together. The key difference is that event streaming is
    a mechanism for transmitting a sequence of events between different parts of a
    system, or even between different systems. Event streaming focuses on the delivery
    of events, ensuring that they are received by interested parties. It can be used
    for various purposes, such as real-time notifications, data pipelines, or triggering
    actions in other microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, event sourcing is a data persistence pattern where the entire history
    of changes to an entity is stored as a sequence of events. It focuses on the storage
    and utilization of events as a system’s source of truth. These events are used
    to replay the history and rebuild the current state of data if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an analogy for better understanding. Imagine event streaming as a live
    stream – it continuously delivers updates (events) to anyone subscribed. Event
    sourcing is like a detailed log – it keeps a permanent record of all past updates
    (events) for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: But how are these two connected? Event sourcing often leverages event streaming
    to efficiently store and transmit the sequence of events. The event stream from
    event sourcing can be used by other systems or services subscribed to it. Some
    event stores (to be discussed further shortly), which are specialized databases
    for event sourcing, might have built-in functionalities for event streaming. In
    essence, event streaming is a broader concept for data in motion. Event sourcing
    utilizes event streaming to preserve its event history.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a quick look at an event store next.
  prefs: []
  type: TYPE_NORMAL
- en: Event store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The other element we need to consider in our CQRS, and event-sourcing ecosystem
    is an **event store**. This is a specialized type of database designed specifically
    to store sequences of events. Unlike traditional relational databases that focus
    on the current state of data, event stores record every change made to an entity
    as a unique event. This creates an immutable history of all actions that have
    occurred, allowing several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Auditability and debugging**: You can easily track changes and identify issues
    by reviewing the sequence of events. This provides a detailed log of what happened,
    when, and why.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data evolution**: As your application evolves, new events can be added to
    a store without modifying existing logic. This makes it easier to adapt to changing
    requirements without breaking existing functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replayability**: By replaying an event stream in a specific order, you can
    reconstruct the state of an entity at any point in time. This is useful for various
    purposes, such as rebuilding materialized views or disaster recovery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Event stores are often optimized to handle high volumes of
    writes, making them well-suited for event-driven architectures with frequent data
    changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, an event store not only captures a complete and immutable history
    of changes but also enhances flexibility and scalability. By preserving every
    event that modifies the state of an entity, the event store provides a foundation
    for reliable audit trails, effortless adaptation to new business requirements,
    and the ability to reconstruct the state as needed. These features make it a vital
    component in modern architectures, especially where high data throughput and accountability
    are important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how event stores typically work:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Events**: Each action or change to an entity is represented as an event.
    These events contain relevant data about the change, such as timestamps, user
    IDs, and the specific modifications made.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Append-only**: Events are stored in an append-only fashion, meaning they
    cannot be modified or deleted after being added. This ensures the immutability
    of the event history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event stream**: Each entity typically has its event stream, which is a sequence
    of all events related to that entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event stores typically work by representing each action or change to an entity
    as an event. These events capture relevant information about the change, such
    as the time it occurred, the user responsible, and the specific details of the
    modification. Once an event is recorded, it is stored in an append-only fashion,
    meaning that it cannot be altered or deleted after being added. This ensures that
    the event history remains immutable, providing a reliable audit trail. Additionally,
    each entity is associated with its own event stream, which is a chronological
    sequence of all the events related to that specific entity. This stream allows
    you to trace the life cycle of the entity from its initial state to its current
    form, based entirely on the sequence of events recorded in a store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Event stores offer the following significant benefits that make them highly
    suitable for modern architectures, especially those driven by events:'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages is the creation of an **immutable history**. Every
    change to a system is stored as an event, ensuring that past actions cannot be
    tampered with or altered. This creates a reliable, tamper-proof audit trail that
    allows you to track the complete life cycle of an entity, making it particularly
    useful for debugging, compliance, and historical analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of **scalability**, event stores are designed to handle high volumes
    of writes efficiently. Since events are appended to a store rather than modifying
    existing records, they can support applications with frequent data changes and
    ensure that performance remains consistent, even as the volume of data grows.
    This makes them an excellent choice for systems that need to process large amounts
    of data or handle real-time event streams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important benefit is **data evolution**. As applications evolve and
    new business requirements emerge, event stores allow you to adapt without disrupting
    existing functionality. New events can be added to reflect changes in a system,
    while the old event data remains intact, preserving the full history. This flexibility
    simplifies the process of evolving your application over time while maintaining
    backward compatibility with previous versions of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replayability** is another important feature of event stores. By replaying
    the event stream, you can reconstruct the state of an entity at any point in time.
    This capability is invaluable for disaster recovery, rebuilding materialized views,
    or even simulating past system states for analysis or testing. It gives you the
    power to revisit the past and see exactly how an entity reached its current state,
    something that’s not possible with traditional databases that only store the latest
    state of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These benefits make event stores a powerful tool for building scalable, flexible,
    and resilient systems, particularly in event-driven architectures where maintaining
    a detailed history of changes is critical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the challenges of using event stores:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Querying**: Traditional relational database querying techniques might not
    be directly applicable. Designing efficient queries on event streams can require
    different approaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased complexity**: Event stores require a different data management
    mindset compared to traditional databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, let’s look at some popular event store options, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**EventStoreDB**: A leading dedicated event store solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kafka**: A distributed streaming platform that can be used for event
    storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traditional databases (with modifications)**: Relational databases such as
    PostgreSQL can be configured for append-only functionality to act as basic event
    stores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, event stores are a valuable tool for building event-driven architectures
    and applications that require a detailed history of changes, data evolution capabilities,
    and resilience.
  prefs: []
  type: TYPE_NORMAL
- en: That’s enough theory; it’s time to put CQRS and event sourcing into practice.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing CQRS and event sourcing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a simple application that uses CQRS and event-sourcing. Our application
    will allow us to attach multiple payment mechanisms to our account. It will be
    possible to register payment mechanisms to accounts, disable them, and enable
    them. We will use NestJS, but you can use any other framework. In `Ch11`, in the
    `CQRS_EventSourcing` folder, we have the `cqrs_app` folder in our Git repository.
    You can download it to properly follow throughout the chapter, but the other option
    is to implement everything from scratch, as we plan to do here:'
  prefs: []
  type: TYPE_NORMAL
- en: Create any folder and open your favorite IDE. Load the empty folder to your
    IDE, and from the command line, type `npx @nestjs/cli new cqrs_app` or `npm i
    -g @nestjs/cli` with `nest` `new cqrs_app`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This should install the NestJS template in the folder. Now, let’s install the
    required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'services:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'eventstore.db:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'image: eventstore/eventstore:24.2.0-jammy'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'environment:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- EVENTSTORE_CLUSTER_SIZE=1'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- EVENTSTORE_RUN_PROJECTIONS=All'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- EVENTSTORE_START_STANDARD_PROJECTIONS=true'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- EVENTSTORE_HTTP_PORT=2113'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- EVENTSTORE_INSECURE=true'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- EVENTSTORE_ENABLE_ATOM_PUB_OVER_HTTP=true'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'ports:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- ''2113:2113'''
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'volumes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- type: volume'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'source: eventstore-volume-data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'target: /var/lib/eventstore'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- type: volume'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'source: eventstore-volume-logs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'target: /var/log/eventstore'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'volumes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'eventstore-volume-data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: eventstore.db. It uses the eventstore/eventstore:24.2.0-jammy image, which is
    a specific version of EventStoreDB. You can use any other versions with a bit
    different configuration. The service runs with several environment variables to
    configure EventStore, including starting all projections and enabling insecure
    connections (which is not recommended for production). The service maps port 2113
    on the host machine to port 2113 within the container, allowing access to the
    EventStoreDB instance. Finally, it defines persistent volumes for data and logs
    to ensure that information is preserved even if the container restarts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run `docker-compose up -d` command to run it. After a successful run, you can
    navigate to `localhost:2213` for the `EventStoreDB` dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.7: The event store dashboard](img/B09148_11_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: The event store dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in our `src` folder, create an `eventstore.ts` file with the following
    content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The provided `connect` function attempts to read a single event (`maxCount:
    1`) from the beginning (`direction: FORWARDS, fromPosition: START`) of the event
    stream. Any errors encountered during this read operation are caught and logged
    to the console. Finally, both the client connection and the connect function are
    exported for potential use in other parts of the code.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will store account-based elements such as events, commands, and aggregates
    together. Storing account-based elements such as events, commands, and aggregates
    together helps maintain consistency and clarity within the domain model. These
    elements are tightly interconnected commands that initiate actions that change
    the state of an aggregate, and these changes are captured as events. Keeping them
    together simplifies the logical flow of operations, ensuring that all related
    components are easily accessible and organized. That is why we need to create
    a folder called `account` under `src`. After creating a folder, create a new file
    called `account.commands.ts` under `src` / `account` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code defines three commands for an account unit system in a NestJS application,
    using CQRS:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`RegisterAccountUnitCommand`: This command takes an `aggregateId` (a unique
    identifier for the account unit) and a `paymentmechanismCount` (the number of
    payment methods associated). It’s used to create a new account unit.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DisableAccountUnitCommand`: This command simply takes `aggregateId` and presumably
    disables the account unit.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EnableAccountUnitCommand`: Similar to the disabling command, this takes `aggregateId`
    and typically reenables a previously disabled account unit.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These commands represent different actions that users might take on account
    units, and they follow the CQRS pattern by focusing on modifying the system state
    (i.e., creating, disabling, or enabling).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Instead of calling the required functionalities directly, we will encapsulate
    them using commands. Our commands work based on a command design pattern. Using
    a command pattern, it is possible to encapsulate every action/request as an object.
    This encapsulation brings a lot of additional features, depending on the context;
    you can implement late execution, redo, undo, transactional operations, and so
    on. The `ICommand` interface helps us to achieve this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The other contracts we need to implement to cover CQRS with event sourcing
    are events. In the `src/account` folder, create a new file called `account.events.ts`
    with the following content:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In CQRS, events are used to communicate changes that occur in a system. By
    inheriting from `IEvent` (provided by the `@nestjs/cqrs` package), we ensure that
    `AccountEvent` and its subclasses conform to the expected event structure within
    the CQRS framework. This allows the framework to handle these events appropriately,
    such as publishing them to an event bus or persisting them for eventual consistency:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AccountEvent` (the base class): Acts as a base for all account events. It
    inherits from `IEvent` (from `@nestjs/cqrs`) and holds common properties such
    as `aggregateId` and `paymentmechanismCount`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AccountRegisteredEvent` inherit from `AccountEvent`, customizing it for specific
    actions (i.e., registration, disabling, and enabling) with potentially additional
    properties if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach promotes code reuse and keeps event data consistent across different
    account unit events.
  prefs: []
  type: TYPE_NORMAL
- en: We have specified our commands and events, but we haven’t used them. The purpose
    of the `account.aggregate.ts` file under `src` | `account` is exactly for that.
    We need first to specify our command handler. If you have a command, there should
    be a handler to handle it.
  prefs: []
  type: TYPE_NORMAL
- en: Commands and handlers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Commands represent the actions that users or external systems want to perform
    on the domain model. They encapsulate the data needed to execute the action. In
    our example, `RegisterAccountUnitCommand`, `DisableAccountUnitCommand`, and `EnableAccountUnitCommand`
    are all commands that represent actions on account units.
  prefs: []
  type: TYPE_NORMAL
- en: Commands are typically defined as interfaces or classes. They often include
    properties that specify an action and any necessary data (e.g., `aggregateId`
    in our commands). Conversely, **command handlers** (also referred to as handlers
    in this chapter) are responsible for receiving commands, executing the necessary
    logic to modify the system state, and potentially producing events that reflect
    the changes. They act as the bridge between commands and the domain model.
  prefs: []
  type: TYPE_NORMAL
- en: Each command typically has a corresponding command handler. The handler receives
    the command, interacts with the domain logic (i.e., aggregate root, entities and
    services), and updates the system state accordingly. It might also trigger the
    creation of events to communicate the changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `account.aggregate.ts` contains `AggregateRoot`, `CommandHandler`, and
    `EventHandler` implementations. First, we will look at the command handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This NestJS code defines a command handler to register account units using CQRS.
    The `@CommandHandler` decorator associates it with the `RegisterAccountUnitCommand`.
    It injects `EventPublisher` (for event sourcing). In the `execute` method, it
    creates an `AccountAggregate` instance, calls its `registerAccount` method with
    command data, and potentially commits the changes. This demonstrates processing
    a command by interacting with the domain model and potentially publishing events.
    We will discuss `AggregateRoot` a bit later. For now, we will just focus on the
    base idea behind the commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two more commands that have approximately the same implementation,
    with different method calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`DisableAccountUnitHandler` retrieves the `AccountAggregate` instance associated
    with the `command.aggregateId`, using `AccountAggregate.loadAggregate`.'
  prefs: []
  type: TYPE_NORMAL
- en: It verifies whether the account is already disabled, using `!aggregate.disabled`.
    If not disabled, it calls `aggregate.disableAccount` to perform the disabling
    logic and then `aggregate.commit` to potentially persist the change as an event.
  prefs: []
  type: TYPE_NORMAL
- en: 'This handler ensures that an account unit is only disabled once and triggers
    event publication (if applicable) upon successful disabling. The last handler
    is `EnableAccountHandler`, which is a counterpart of `DisableAccountUnitHandler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We’re done with handlers. It is time to explore `IEventHandler<T>` interface
    from the `@nestjs/cqrs` package. These handlers respond to specific domain events
    that are emitted by the aggregate.
  prefs: []
  type: TYPE_NORMAL
- en: An event handler in the context of CQRS is responsible for handling the domain
    events that occur within a system. The events represent significant state changes
    within your aggregates, and the event handlers respond to these changes by performing
    side effects or additional logic outside the aggregate itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same file (`account.aggregate.ts`), we have three event handlers (`AccountRegisteredEventHandler`,
    `AccountDisabledEventHandler`, and `AccountEnabledEventHandler`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: All event handlers have the same contract, and that is why we use the `AccountEvent`
    interface. It then implements a function, `handleAccountEvent`, that takes an
    event type and an event object as arguments. The function prepares data in a JSON-compatible
    format and uses an event store service to persist the event information, under
    a stream specific to the involved account aggregate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at concrete event handler implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we define event handlers for account registration, disabling,
    and enabling. When an account is registered, the `AccountRegisteredEventHandler`
    triggers logic related to account creation. Similarly, `AccountDisabledEventHandler`
    and `AccountEnabledEventHandler` handle account disabling and enabling events,
    respectively. These handlers leverage the `handleAccountEvent` function for centralized
    event processing.
  prefs: []
  type: TYPE_NORMAL
- en: That is great, but how do these commands interact with events? To demonstrate
    this, we need to discuss one more concept, called an aggregate root, a popular
    pattern in **Domain-Driven** **Design** (**DDD**).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an aggregate root
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In DDD, an **aggregate root** is a fundamental concept for modeling complex
    domains. It acts as the central entity within a cluster of related objects, also
    known as an **aggregate**.
  prefs: []
  type: TYPE_NORMAL
- en: An aggregate root encapsulates the core data and logic associated with a particular
    domain concept. In our example, `AccountAggregate` will hold all the essential
    information about an account (i.e., ID, payment mechanism count, and disabled
    status). This centralizes the account’s state and promotes data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: An aggregate root plays a crucial role in event sourcing, a technique for persisting
    domain object changes as a sequence of events. In our code, `AccountAggregate`
    methods such as `registerAccount` apply events to the aggregate, reflecting state
    changes. By reconstructing the state from the event stream, the aggregate root
    becomes the central source of truth for the account’s history.
  prefs: []
  type: TYPE_NORMAL
- en: An aggregate root defines the transactional boundaries within our domain. Within
    an aggregate, changes to the state of all related entities (including the root
    itself) must happen atomically. This ensures data consistency within the aggregate.
  prefs: []
  type: TYPE_NORMAL
- en: An aggregate root also serves as the sole entry point for external interactions
    with the aggregate. This means other parts of your application (or other aggregates)
    should interact with the domain through the aggregate root’s methods. This promotes
    loose coupling and simplifies reasoning about domain logic.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate roots promote data consistency and integrity by centralizing state
    management and defining transactional boundaries. They simplify domain logic by
    providing a clear entry point for interactions. They also improve code maintainability
    by encapsulating related entities and their behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'By effectively utilizing aggregate roots in DDD, we can build robust and maintainable
    domain models that accurately reflect your business processes. Now, let’s see
    how it is possible to rebuild the state of `AccountAggregate` by reading its event
    stream from the event store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This NestJS code defines an asynchronous function named `loadAggregate` that
    takes an aggregate ID as input. It retrieves a `stream` of events related to that
    ID from the event store. The function then iterates through each event and applies
    the changes it describes to an `AccountAggregate` object. There are cases for
    handling different event types, such as `AccountUnitCreated`, `AccountUnitDisabled`,
    and `AccountUnitEnabled`. If an event type isn’t recognized, it’s skipped. If
    there are errors processing an event, it logs an error message but keeps iterating.
    Finally, the function returns the populated `AccountAggregate` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download our Git repository for a more complete example of implementing an
    aggregate root. Here is a snippet from an aggregate root that handles the operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you might guess, commands interact with events using an aggregate root, and
    the latter encapsulates the logic that triggers events.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing projection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In CQRS and event sourcing architectures, `Account Created` or `Account Disabled`).
  prefs: []
  type: TYPE_NORMAL
- en: Projections are like the projection booth in a movie theater. They take the
    event stream (the film reel) and *project* it into a specific format, suitable
    for reading. This format, called a **read model**, is optimized to query data
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, let’s understand why projections are important:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Read efficiency**: Projections help rebuild the entire system state from
    the event stream, as doing so for every read query would be slow. Projections
    pre-process the event stream, creating a separate, optimized data structure for
    frequently accessed information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: We can create multiple projections tailored to different reading
    needs with projections. One projection might focus on account details, while another
    might analyze purchase history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let’s see how projections work:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event listeners**: Projections act as event listeners, subscribing to the
    event stream.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Processing events**: As new events arrive, a projection processes them one
    by one, updating its internal read model accordingly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Read model access**: When a read query arrives, a system retrieves the relevant
    data from a projection’s read model instead of the entire event stream.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Projections are not a replacement for an event store. The event store remains
    the single source of truth for all historical events. Projections simply offer
    a way to efficiently access specific data from that history. Having said that,
    let’s look at some of the benefits of projections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faster reads**: Queries run against read models are significantly faster
    than replaying an entire event stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Projections can be scaled independently to handle increasing
    read traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Different projections cater to diverse read needs without
    impacting write performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We plan to implement a simple projection that demonstrates the usage of projection
    in CQRS and event sourcing architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the `src` /`paymentmechanism` folder, create a `paymentmechanism-total.projection.ts`
    file with the following functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This code defines an event handler class named `PaymentMechanismProjection`
    in a CQRS architecture with event sourcing. It listens for three specific events
    related to account management:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AccountRegisteredEvent`: Triggers when a new account is created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AccountDisabledEvent`: Triggers when an account is deactivated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AccountEnabledEvent`: Triggers when a deactivated account is reactivated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class keeps track of the total number of payment mechanisms (`currentPayment`
    **MechanismTotal**), but its initial value is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `handle` method is the core functionality. It checks the type of the incoming
    event and calls a specific handler function, based on the event type:'
  prefs: []
  type: TYPE_NORMAL
- en: '`handleAccountRegistered`: Handles `AccountRegisteredEvent` by incrementing
    `currentPaymentMechanismTotal`, based on information in the event data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handleAccountDisabled`: Handles `AccountDisabledEvent` and decrements `currentPaymentMechanismTotal`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handleAccountEnabled`: Handles the `AccountEnabledEvent` and applies the opposite
    operation of `handleAccountDisabled`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a simplified example, but it demonstrates how an event handler projection
    can listen for specific events and update its internal state accordingly, maintaining
    a view of the data optimized for a particular purpose (e.g., tracking total payment
    mechanisms). Here are our detailed handler methods in this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Our handlers simply interact with `currentPaymentMechanismTotal` and build logic
    around it. The idea is simple, but you can implement more complex logic based
    on this knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing API functionalities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use controllers as an entry point to our request flow. In a classical flow,
    controllers accept requests and forward them to the related services. When we
    apply CQRS and event sourcing, we usually use the same controllers, but instead
    of specifying direct services, we apply a command pattern to provide commands
    and their handlers. Controllers serve as the intermediary between a client and
    the backend logic, determining how an application should respond to various requests.
    Controllers map specific routes to corresponding methods that contain business
    logic. By organizing request handling within controllers, the application maintains
    a clear separation of concerns, making it more structured, scalable, and easier
    to manage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new folder called `api` under the `src` folder. Then, create a new
    file called `account.controller.ts` under `src` / `api`, with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This NestJS controller handles account management. It’s named `AccountUnitController`
    and is mapped to the `/Account` route. The controller uses a command bus to send
    commands. There are three functionalities exposed through `POST` requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`registerAccount` allows you to create a new account with a payment mechanism
    count, by sending `RegisterAccountUnitCommand`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disableAccount` deactivates an account by ID using `DisableAccountUnitCommand`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enableAccount` reactivates an account using an `EnableAccountUnitCommand`,
    based on its ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All successful requests return a message indicating the command was received
    and the aggregate ID (for registration).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to enable a controller’s functionality, we need to import several
    essential elements. `Controller`, `Param`, `Post`, and `Query` from `@nestjs/common`
    are necessary to define the controller, handle route parameters, and process HTTP
    `POST` requests with query parameters. `CommandBus` from `@nestjs/cqrs` allows
    us to dispatch commands, following the CQRS pattern. We import the specific commands
    (`DisableAccountUnitCommand`, `EnableAccountUnitCommand`, and `RegisterAccountUnitCommand`)
    from the `account.commands` file to perform specific operations on the account
    unit. Finally, we import the `uuid` package to generate unique IDs for these operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our controller doesn’t know about events. It only interacts with commands. The
    request will flow to command handlers, and they will trigger our events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the controller, we have the `account.module.ts` file, which contains
    `AccountModule`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: For a complete example with imported functionalities, check out our repository.
  prefs: []
  type: TYPE_NORMAL
- en: This code defines `AccountModule` used in a CQRS architecture with event sourcing.
    It implements the `OnModuleInit` life cycle hook, which gets called after the
    module is initialized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of the functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '`onModuleInit`: This method is called when the module is ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`startSubscription (private)`: This private method initiates a subscription
    to an event stream. It uses an **Immediately Invoked Function Expression** (**IIFE**)
    to encapsulate the asynchronous logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will take a look at `subscribeToAll(private, async)`; this private
    asynchronous method does the actual subscription work. It uses `eventStore.subscribeToAll`
    to subscribe to all event streams that start with the `Account-unit-stream-` prefix.
    This method typically captures all events related to account management. It iterates
    through the subscription using `for await...` of the loop. For each event received,
    it logs the event revision number and stream ID, extracts the event data, and
    logs it as well. The `AccountModule` subscribes to a specific category of events
    in the event store (events related to accounts). Whenever a new event related
    to accounts arrives, it logs details about the event and its data for potential
    processing or monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Testing an application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before running our application, you should run the provided `docker-compose`
    file via the `docker-compose up -d` command. It ensures that we already have `EventStoreDB`
    as a data store. To make sure if data store is running, just navigate to `localhost:2113`,
    and you should see the `EventStoreDB`’s dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: To run our application, execute the `nest start` command from the command line.
    Open your Postman application, and create a new tab. Select the `paymentmechanismcount`
    set to `67`. Then, click the **Send** button.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8: Account registration](img/B09148_11_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: Account registration'
  prefs: []
  type: TYPE_NORMAL
- en: After successful operation, you should get the following message to your VS
    Code console.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9: Account registration logs](img/B09148_11_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: Account registration logs'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ID will be different in your case because it is automatically generated
    by the system. After running the same command with a different payment mechanism
    count (it is twenty-three in our case), you should get the following message with
    `currentPaymentMechanismCount=90`. The ID is different again, but if you use the
    same payment mechanism count, the values should be totaled based on the `currentPaymentMechanismTotal
    = currentPaymentMechanismTotal +` `paymentMechanismCount` formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10: Account registration calculation](img/B09148_11_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: Account registration calculation'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have two different IDs (aggregate IDs), and we can use any of them to
    enable and disable requests.
  prefs: []
  type: TYPE_NORMAL
- en: Open a new tab on Postman and send a POST request to `http://localhost:8080/account/YOUR_AGGREGATE_ID/disable`.
    The last aggregate ID stores the value of `paymentmechanismCount`, which is twenty-three.
    So, disabling the endpoint should end up making a value of `currentPaymentMechanismTotal
    = 67`. The logic is ninety minus twenty-three equals to sixty-seven.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the `http://localhost:8080/account/90f80d89-4620-4526-ae3e-02a8156df9a1/disable`
    and click **Send**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11: The disabled account response](img/B09148_11_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11: The disabled account response'
  prefs: []
  type: TYPE_NORMAL
- en: To enable the account, just replace `disable` with `enable` and run the command
    again. It should restore `currentPaymentMechanismTotal` to `90`.
  prefs: []
  type: TYPE_NORMAL
- en: Besides CQRS and event sourcing, we have a Service Registry and discovery for
    microservices. The next section will help us to understand them in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Service Registry and discovery in microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices development by itself consists of huge amounts of patterns and
    best practices. It is indeed not possible to cover all of them in one book. In
    this section, we will provide popular patterns and techniques used in microservices
    development.
  prefs: []
  type: TYPE_NORMAL
- en: In a microservice architecture, applications are built as a collection of small,
    independent services. These services need to communicate with each other to cover
    user requests. Service Registry and discovery is a mechanism that simplifies this
    communication by enabling services to find each other dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Service Registry and discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine a central database. This database, called the **Service Registry**,
    acts as a directory of all the microservices in your system. Each service instance
    (i.e., an individual running a copy of a microservice) registers itself with the
    registry. During registration, the service provides details such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network location**: The address (IP address and port) where the service can
    be found.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capabilities**: What the service can do (e.g., processes payments or provides
    user data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health Information**: Status details such as whether the service is currently
    healthy and available to handle requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use tools such as Consul, ZooKeeper, and Eureka Server (as used by Netflix)
    for real-world service registries.
  prefs: []
  type: TYPE_NORMAL
- en: Service Registry often integrates with API gateways, which are a single-entry
    point for external clients to access microservices. An API Gateway might leverage
    the Service Registry to discover the latest locations of the microservices it
    needs to route requests to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, **Service** **d****iscovery** is the process where microservices
    find the location of other services they need to interact with. There are two
    main approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Client-side discovery**: The service that needs another service (the client)
    directly queries the registry to find the address of the target service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server-side discovery**: A separate component, such as a load balancer, sits
    in front of the services. This component retrieves service locations from the
    registry and routes requests to the appropriate service instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at some benefits of Service Registry and discovery:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic service location**: Services don’t need to be hardcoded with the
    addresses of other services. They can discover them on-demand from the registry,
    making the system more adaptable to changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and elasticity**: As you add or remove service instances, the
    registry automatically reflects the changes. This ensures that clients always
    interact with available services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loose coupling**: Services become loosely coupled, as they rely on the registry
    for communication. This promotes independent development and deployment of microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using a central registry and enabling dynamic discovery, Service Registry
    and discovery simplify communication and promote flexibility in a microservice
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches for implementing Service Registry and Discovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main approaches to implementing Service Registry and discovery
    in Node.js microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: The first option is using a dedicated Service Registry tool. This approach leverages
    a separate service specifically designed for Service Registry and discovery functionalities.
    We can use popular options such as Consul, ZooKeeper, and Eureka Server (Netflix).
    These tools offer robust features for registration, discovery, health checks,
    and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option is Node.js client libraries. Each registry tool typically
    provides a Node.js client library that simplifies interaction with the registry.
    The library allows your microservices to register themselves, discover other services,
    and monitor their health.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, let us look at how we can implement a Service Registry before wrapping
    up this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Service Registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s take a brief look at implementing a Service Registry:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose a Service Registry tool, and install its Node.js client library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During startup, each microservice registers itself with a registry using the
    library. It provides its network location, capabilities, and health information.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In client-side discovery, the service needing another service uses the library
    to query the registry for the target service’s address.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In server-side discovery, a separate component, such as a load balancer, retrieves
    service locations from the registry and routes requests accordingly.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s move on to building a simple registry with Node.js: For smaller
    deployments or learning purposes, you can implement a basic Service Registry using
    Node.js itself. Here’s a simplified example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**For data storage**: Use a lightweight in-memory data store, such as Redis,
    or a simple Node.js object to store service information'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Registration**: During startup, each microservice registers itself with the
    registry by sending a message containing its details'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discovery**: Services can query the registry to retrieve a list of available
    services and their addresses'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we end this section, let’s look at some important considerations of Service
    Registry and discovery, the first being **security**. When implementing your own
    registry, ensure proper authentication and authorization mechanisms to control
    access to registration and discovery functionalities. Next is **scalability**.
    A homegrown registry might not scale well for large deployments. Consider a dedicated
    tool for production environments. Finally, **health checks** are very important.
    Regularly check the health of registered services to ensure that they are available.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered everything about microservice architecture in this chapter. It’s
    now time to wrap up.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dived into the building blocks of a strong microservice architecture.
    It covered API gateways, explaining their purpose, use cases, and how to implement
    them for optimal performance, with caching, rate limiting, and response aggregation.
    The chapter then explored CQRS and event sourcing patterns, along with event streaming,
    a technology that makes them work. Finally, it discussed Service Registry and
    discovery, essential for microservices to communicate with each other. This chapter
    provided the knowledge and practical examples to build a well-designed and scalable
    microservice infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore testing strategies in depth and cover how
    to write effective unit and integration tests for your microservices.
  prefs: []
  type: TYPE_NORMAL
