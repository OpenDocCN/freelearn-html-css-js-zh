<html><head></head><body>
        

                            
                    <h1 class="header-title">Applying the Event Sourcing and CQRS Patterns</h1>
                
            
            
                
<p>In this chapter, the following recipes will be covered:</p>
<ul>
<li class="mce-root">Creating a data lake</li>
<li class="mce-root">Applying the event-first variant of the Event Sourcing pattern</li>
<li class="mce-root">Creating a micro event store</li>
<li class="mce-root">Applying the database-first variant of the Event Sourcing pattern with DynamoDB</li>
<li class="mce-root">Applying the database-first variant of the Event Sourcing pattern with Cognito datasets</li>
<li class="mce-root">Creating a materialized view in DynamoDB</li>
<li class="mce-root">Creating a materialized view in S3</li>
<li class="mce-root">Creating a materialized view in Elasticsearch</li>
<li class="mce-root">Creating a materialized view in a Cognito dataset</li>
<li class="mce-root">Replaying events</li>
<li class="mce-root">Indexing the data lake</li>
<li class="mce-root">Implementing bi-directional synchronization</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction</h1>
                
            
            
                
<p>Cloud-native is autonomous. It empowers self-sufficient, full-stack teams to rapidly perform lean experiments and continuously deliver innovation with confidence. The operative word here is <em>confidence</em>. We leverage fully managed cloud services, such as function-as-a-service, cloud-native databases, and event streaming to decrease the risk of running these advanced technologies. However, at this rapid pace of change, we cannot completely eliminate the potential for human error. To remain stable despite the pace of change, cloud-native systems are composed of bounded, isolated, and autonomous services that are separated by bulkheads to minimize the blast radius when any given service experiences a failure. Each service is completely self-sufficient and stands on its own, even when related services are unavailable.</p>
<p>Following reactive principles, these autonomous services leverage event streaming for all inter-service communication. Event streaming turns the database inside out by replicating data across services in the form of materialized views stored in cloud-native databases. This cloud-native data forms a bulkhead between services and effectively turns the cloud into the database to maximize responsiveness, resilience, and elasticity. The <strong>Event Sourcing</strong> and <strong>Command Query Responsibility Segregation</strong> (<strong>CQRS</strong>) patterns are fundamental to creating autonomous services. This chapter contains recipes that demonstrate how to use fully managed, serverless cloud services to apply these patterns.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a data lake</h1>
                
            
            
                
<p>One of the major benefits of the Event Sourcing pattern is that it results in an audit trail of all the state changes within a system. This store of events can also be leveraged to replay events to repair broken services and seed new components. A cloud-native event stream, such as <strong>AWS Kinesis</strong>, only stores events for a short period of time, ranging from 24 hours to 7 days. An <strong>event stream</strong> can be thought of as a temporary or temporal event store that is used for normal, near real-time operation. In the <em>Creating a micro event store</em> recipe, we will discuss how to create specialized event stores that are dedicated to a single service. In this recipe, we will create a data lake in S3. A <strong>data lake</strong> is a perpetual event store that collects and stores all events in their raw format in perpetuity with complete fidelity and high durability to support auditing and replay.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>Before starting this recipe, you will need an <strong>AWS Kinesis Stream</strong>, such as the one created in the <em>Creating an event stream</em> recipe.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/data-lake-s3 --path cncb-data-lake-s3</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-data-lake-s3</kbd> directory with <kbd>cd cncb-data-lake-s3</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-data-lake-s3<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/><br/>functions:<br/>  <strong>transformer</strong>:<br/>    handler: handler.transform<br/>    timeout: 120<br/><br/>resources:<br/>  Resources:<br/>    <strong>Bucket</strong>:<br/>      Type: AWS::S3::Bucket<br/>      <strong>DeletionPolicy</strong>: Retain<br/>    <strong>DeliveryStream</strong>:<br/>      Type: AWS::<strong>KinesisFirehose</strong>::DeliveryStream<br/>      Properties:<br/>        DeliveryStreamType: KinesisStreamAsSource      <br/>        KinesisStreamSourceConfiguration: <br/>          <strong>KinesisStreamARN</strong>: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>        ExtendedS3DestinationConfiguration:<br/>          BucketARN:<br/>            Fn::GetAtt: [ Bucket, Arn ]<br/>          <strong>Prefix</strong>: ${cf:cncb-event-stream-${opt:stage}.streamName}/<br/>          ...<br/><br/>  Outputs:<br/>    <strong>DataLakeBucketName</strong>:<br/>      Value:<br/>        Ref: Bucket</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">exports.<strong>transform</strong> = (event, context, callback) =&gt; {<br/>  const output = event.records.map((record, i) =&gt; {<br/>    // store all available data<br/>    const uow = {<br/>      event: JSON.parse((Buffer.from(record.data, 'base64')).toString('utf8')),<br/>      kinesisRecordMetadata: record.kinesisRecordMetadata,<br/>      firehoseRecordMetadata: {<br/>        deliveryStreamArn: event.deliveryStreamArn,<br/>        region: event.region,<br/>        invocationId: event.invocationId,<br/>        recordId: record.recordId,<br/>        approximateArrivalTimestamp: record.approximateArrivalTimestamp,<br/>      }<br/>    };<br/><br/>    return {<br/>      recordId: record.recordId,<br/>      result: 'Ok',<br/>      data: Buffer.from(JSON.stringify(uow) + <strong>'\n'</strong>, 'utf-8').toString('base64'),<br/>    };<br/>  });<br/><br/>  callback(null, { records: output });<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-data-lake-s3@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-data-lake-s3<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>Stack Outputs<br/><strong>DataLakeBucketName</strong>: cncb-data-lake-s3-john-bucket-1851i1c16lnha<br/>...</pre>
<ol start="9">
<li>Review the stack, data lake bucket, and Firehose delivery stream in the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"thing-created"}'</strong><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49582906351415672136958521360120605392824155736450793474"<br/>}</pre>
<ol start="11">
<li>Allow the Firehose buffer time to process and then review the data lake contents created in the S3 bucket.</li>
</ol>
<ol start="12">
<li>Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>
<p>Remove the data lake stack after you have worked through all the other recipes. This will allow you to watch the data lake accumulating all the other events.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>The most important characteristic of a data lake is that it stores data in perpetuity. The only way to really meet this requirement is to use object storage, such as AWS S3. S3 provides 11 nines of durability. Said another way, S3 provides 99.999999999% durability of objects over a given year. It is also fully managed and provides life cycle management features to age objects into cold storage. Note that the bucket is defined with the <kbd>DeletionPolicy</kbd> set to <kbd>Retain</kbd>. This highlights that even if the stack is deleted, we still want to ensure that we are not inappropriately deleting this valuable data.</p>
<p>We are using Kinesis Firehose because it performs the heavy lifting of writing the events to the bucket. It provides a buffer based on the time and size, compression, encryption, and error handling. To simplify this recipe, I did not use compression or encryption, but it is recommended that you use these features.</p>
<p>This recipe defines one delivery stream, because in this cookbook, our stream topology consists of only one stream with <kbd>${cf:cncb-event-stream-${opt:stage}.streamArn}</kbd>. In practice, your topology will consist of multiple streams, and you will define one Firehose delivery stream per Kinesis stream to ensure that the data lake is capturing all events. We set <kbd>prefix</kbd> to <kbd>${cf:cncb-event-stream-${opt:stage}.streamName}/</kbd> so that we can easily distinguish the events in the data lake by their stream.</p>
<p>Another important characteristic of a data lake is that the data is stored in its raw format, without modification. To this end, the <kbd>transformer</kbd> function adorns all available metadata about the specific Kinesis stream and Firehose delivery stream, to ensure that all available information is collected. In the <em>Replaying events</em> recipe, we will see how this metadata can be leveraged. Also, note that <kbd>transformer</kbd> adds the end-of-line character (<kbd>\n</kbd>) to facilitate future processing of the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Applying the event-first variant of the Event Sourcing pattern</h1>
                
            
            
                
<p>Event sourcing is a key pattern for designing eventually consistent cloud-native systems. Upstream services produce events as their state changes, and downstream services consume these events and produce their own events as needed. This results in a chain of events whereby services collaborate to produce a business process that results in an eventual consistency solution. Each step in this chain must be implemented as an atomic unit of work. Cloud-native systems do not support distributed transactions, because they do not scale horizontally in a cost-effective manner. Therefore, each step must update one, and only one, system. If multiple systems must be updated, then each is updated in a series of steps. In this recipe, we leverage the event-first variant of the Event Sourcing pattern where the atomic unit of work is writing to the event stream. The ultimate persistence of the data is delegated to downstream components.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/event-first --path cncb-event-first</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-event-first</kbd> directory with <kbd>cd cncb-event-first</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-event-first<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    - Effect: Allow<br/>      Action:<br/>        - kinesis:PutRecord<br/>      Resource: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/><br/>functions:<br/>  <strong>submit</strong>:<br/>    handler: handler.<strong>submit</strong><br/>    environment:<br/>      <strong>STREAM_NAME</strong>: ${cf:cncb-event-stream-${opt:stage}.streamName}</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>submit</strong> = (thing, context, callback) =&gt; {<br/>  thing.id = thing.id || uuid.v4();<br/><br/>  const event = {<br/>    <strong>type</strong>: 'thing-submitted',<br/>    id: uuid.v1(),<br/>    <strong>partitionKey</strong>: thing.id,<br/>    timestamp: Date.now(),<br/>    <strong>tags</strong>: {<br/>      region: process.env.AWS_REGION,<br/>      kind: thing.kind,<br/>    },<br/>    thing: thing,<br/>  };<br/><br/>  const params = {<br/>    StreamName: process.env.<strong>STREAM_NAME</strong>,<br/>    PartitionKey: event.partitionKey,<br/>    Data: Buffer.from(JSON.stringify(event)),<br/>  };<br/><br/>  const kinesis = new aws.Kinesis();<br/><br/>  kinesis.<strong>putRecord</strong>(params, (err, resp) =&gt; {<br/>    callback(err, event);<br/>  });<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-event-first@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-event-first<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  submit: cncb-event-first-john-submit<br/>...</pre>
<ol start="9">
<li>Review the stack and function in the AWS Console.</li>
<li>Invoke the <kbd>submit</kbd> function with the following command:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -f submit -r us-east-1 -s $MY_STAGE -d '{"id":"11111111-1111-1111-1111-111111111111","name":"thing one","kind":"other"}'</strong><br/><br/>{<br/>    "type": "thing-submitted",<br/>    "id": "2a1f5290-42c0-11e8-a06b-33908b837f8c",<br/>    "partitionKey": "11111111-1111-1111-1111-111111111111",<br/>    "timestamp": 1524025374265,<br/>    "tags": {<br/>        "region": "us-east-1",<br/>        "kind": "other"<br/>    },<br/>    "thing": {<br/>        "id": "11111111-1111-1111-1111-111111111111",<br/>        "name": "thing one",<br/>        "kind": "other"<br/>    }<br/>}</pre>
<ol start="11">
<li>Take a look at the logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f submit -r us-east-1 -s $MY_STAGE</strong><br/><br/>START ...<br/>2018-04-18 00:22:54 ... params: {"StreamName":"john-cncb-event-stream-s1","PartitionKey":"11111111-1111-1111-1111-111111111111","Data":{"type":"Buffer","data":[...]}}<br/>2018-04-18 00:22:54 ... response: {"ShardId":"shardId-000000000000","SequenceNumber":"4958...2466"}<br/>END ...<br/>REPORT ... Duration: 381.21 ms    Billed Duration: 400 ms ... Max Memory Used: 34 MB    </pre>
<ol start="12">
<li>Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we implement a command function called <kbd>submit</kbd> that would be part of a Backend For Frontend service. Following the Event Sourcing pattern, we make this command atomic by only writing to a single resource. In some scenarios, such as initiating a long-lived business process or tracking user clicks, we only need to <strong>fire-and-forget</strong>. In these cases, the event-first variant is most appropriate. The command just needs to execute quickly and leave as little to chance as possible. We write the event to the highly available, fully-managed cloud-native event stream and trust that the downstream services will eventually consume the event.</p>
<p>The logic wraps the domain object in the standard event format, as discussed in the <em>Creating an event stream and publishing an event</em> recipe in <a href="a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml">Chapter 1</a>, <em>Getting Started with Cloud-Native</em>. The event <kbd>type</kbd> is specified, the domain object ID is used as the <kbd>partitionKey</kbd>, and useful <kbd>tags</kbd> are adorned. Finally, the event is written to the stream specified by the <kbd>STREAM_NAME</kbd> environment variable.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a micro event store</h1>
                
            
            
                
<p>In the <em>Creating a data lake</em> recipe, we will discuss how the Event Sourcing pattern provides the system with an audit trail of all the state-change events in the system. An event stream essentially provides a temporal event store that feeds downstream event processors in near real-time. The data lake provides a high durability, perpetual event store that is the official source of record. However, we have a need for a middle ground. Individual stream processors need the ability to source specific events that support their processing requirement. In this recipe, we will implement a micro event store in <strong>AWS DynamoDB</strong> that is owned by and tailored to the needs of a specific service.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/micro-event-store --path cncb-micro-event-store</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-micro-event-store</kbd> directory with <kbd>cd cncb-micro-event-store</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-micro-event-store<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    - Effect: Allow<br/>      Action:<br/>        - dynamodb:PutItem<br/>        - dynamodb:Query<br/>      Resource:<br/>        Fn::GetAtt: [ Table, Arn ]<br/>  environment:<br/>    <strong>TABLE_NAME</strong>:<br/>      Ref: Table<br/><br/>functions:<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: <strong>kinesis</strong><br/>          ...<br/>  <strong>trigger</strong>:<br/>    handler: handler.<strong>trigger</strong><br/>    events:<br/>      - stream:<br/>          type: <strong>dynamodb</strong><br/>          arn:<br/>            Fn::GetAtt: [ Table, StreamArn ]<br/>          batchSize: 100<br/>          startingPosition: TRIM_HORIZON<br/><br/>resources:<br/>  Resources:<br/>    Table:<br/>      Type: AWS::DynamoDB::Table<br/>      Properties:<br/>        TableName: ${opt:stage}-${self:service}-<strong>events</strong><br/>        AttributeDefinitions:<br/>          ...<br/>        KeySchema:<br/>          - AttributeName: <strong>partitionKey</strong><br/>            KeyType: <strong>HASH</strong><br/>          - AttributeName: <strong>eventId</strong><br/>            KeyType: <strong>RANGE</strong><br/>        ...<br/>        <strong>StreamSpecification</strong>:<br/>          StreamViewType: NEW_AND_OLD_IMAGES</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>byType</strong>)<br/>    .flatMap(<strong>put</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>byType</strong> = event =&gt; event.type.matches(/thing-.+/);<br/><br/>const <strong>put</strong> = event =&gt; {<br/>  const params = {<br/>    TableName: process.env.<strong>TABLE_NAME</strong>,<br/>    Item: {<br/>      <strong>partitionKey</strong>: event.partitionKey,<br/>      <strong>eventId</strong>: event.id,<br/>      <strong>event</strong>: event,<br/>    }<br/>  };<br/><br/>  const db = new aws.DynamoDB.DocumentClient();<br/>  return _(db.<strong>put</strong>(params).promise());<br/>};<br/><br/>module.exports.<strong>trigger</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .flatMap(<strong>getMicroEventStore</strong>)<br/>    .tap(events =&gt; console.log('<strong>events</strong>: %j', events))<br/>    .collect().toCallback(cb);<br/>};<br/><br/>const <strong>getMicroEventStore</strong> = (record) =&gt; {<br/>  const params = {<br/>    <strong>TableName</strong>: process.env.<strong>TABLE_NAME</strong>,<br/>    KeyConditionExpression: '#partitionKey = :partitionKey',<br/>    ExpressionAttributeNames: {<br/>      '#partitionKey': 'partitionKey'<br/>    },<br/>    ExpressionAttributeValues: {<br/>      ':<strong>partitionKey</strong>': record.dynamodb.Keys.partitionKey.S<br/>    }<br/>  };<br/><br/>  const db = new aws.DynamoDB.DocumentClient();<br/>  return _(db.<strong>query</strong>(params).promise());<br/>}</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-micro-event-store@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-micro-event-store<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  listener: cncb-micro-event-store-john-listener<br/>  trigger: cncb-micro-event-store-john-trigger<br/>...</pre>
<ol start="9">
<li>Review the stack, functions, and table in the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"thing-updated","partitionKey":"11111111-1111-1111-1111-111111111111","thing":{"new":{"name":"thing one","id":"11111111-1111-1111-1111-111111111111"}}}'</strong><br/><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49583553996455686705785668952922460091805481438885707778"<br/>}</pre>
<ol start="11">
<li>Take a look at the logs for the <kbd>listener</kbd> function:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong><br/><br/>START ...<br/>2018-04-18 01:18:55... {"type":"thing-updated","partitionKey":"11111111-1111-1111-1111-111111111111","thing":{"new":{"name":"thing one","id":"11111111-1111-1111-1111-111111111111"}},"id":"fcc03460-42c7-11e8-8756-f75e650b2731","timestamp":1524028734118,"tags":{"region":"us-east-1"}}<br/>2018-04-18 01:18:55.394 (-04:00)    b42aaa92-8a9a-418d-8e22-ecc54e9966f6    <strong>params</strong>: {"<strong>TableName</strong>":"john-cncb-micro-event-store-events","Item":{"<strong>partitionKey</strong>":"11111111-1111-1111-1111-111111111111","<strong>eventId</strong>":"fcc03460-42c7-11e8-8756-f75e650b2731","<strong>event</strong>":{"type":"thing-updated","partitionKey":"11111111-1111-1111-1111-111111111111","thing":{"new":{"name":"thing one","id":"11111111-1111-1111-1111-111111111111"}},"id":"fcc03460-42c7-11e8-8756-f75e650b2731","timestamp":1524028734118,"tags":{"region":"us-east-1"}}}}<br/>END ...<br/>REPORT ... Duration: 149.24 ms    Billed Duration: 200 ms ... Max Memory Used: 35 MB    </pre>
<p class="mce-root"/>
<ol start="12">
<li>Take a look at the logs for the <kbd>trigger</kbd> function: </li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong><br/><br/>START ...<br/>2018-04-18 01:18:56 ... event: {"Records":[{"eventID":"b8dbee2d9f49ee05609a7e930ac204e7","eventName":"INSERT",...,"Keys":{"<strong>eventId</strong>":{"S":"<strong>fcc03460-42c7-11e8-8756-f75e650b2731</strong>"},"<strong>partitionKey</strong>":{"S":"11111111-1111-1111-1111-111111111111"}},...}]}<br/>2018-04-18 01:18:56 ... <strong>params</strong>: {"<strong>TableName</strong>":"john-cncb-micro-event-store-events",...,"ExpressionAttributeValues":{":<strong>partitionKey</strong>":"11111111-1111-1111-1111-111111111111"}}<br/>2018-04-18 01:18:56 ... <strong>events</strong>: {"Items":[{"<strong>eventId</strong>":"<strong>2a1f5290-42c0-11e8-a06b-33908b837f8c</strong>","<strong>partitionKey</strong>":"11111111-1111-1111-1111-111111111111","<strong>event</strong>":{"id":"2a1f5290-42c0-11e8-a06b-33908b837f8c","type":"thing-submitted","partitionKey":"11111111-1111-1111-1111-111111111111","thing":{"name":"thing one","kind":"other","id":"11111111-1111-1111-1111-111111111111"},"timestamp":1524025374265,"tags":{"region":"us-east-1","kind":"other"}}},{"<strong>eventId</strong>":"<strong>fcc03460-42c7-11e8-8756-f75e650b2731</strong>","<strong>partitionKey</strong>":"11111111-1111-1111-1111-111111111111","<strong>event</strong>":{"id":"fcc03460-42c7-11e8-8756-f75e650b2731","type":"thing-updated","partitionKey":"11111111-1111-1111-1111-111111111111","thing":{"new":{"name":"thing one","id":"11111111-1111-1111-1111-111111111111"}},"timestamp":1524028734118,"tags":{"region":"us-east-1"}}}],"<strong>Count</strong>":2,"ScannedCount":2}<br/>END ...<br/>REPORT ... Duration: 70.88 ms    Billed Duration: 100 ms     Memory Size: 1024 MB    Max Memory Used: 42 MB    </pre>
<ol start="13">
<li>Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>When implementing a stream processor function, we often need more information than is available in the current event object. It is a best practice when publishing events to include all the relevant data that is available in the publishing context so that each event represents a micro snapshot of the system at the time of publishing. When this data is not enough, we need to retrieve more data; however, in cloud-native systems, we strive to eliminate all synchronous inter-service communication because it reduces the <em>autonomy</em> of the services. Instead, we create a micro event store that is tailored to the needs of the specific service.</p>
<p>First, we implement a <kbd>listener</kbd> function and <kbd>filter</kbd> for the desired events from the stream. Each event is stored in a DynamoDB table. You can store the entire event or just the information that is needed. When storing these events, we need to collate related events by carefully defining the <kbd>HASH</kbd> and <kbd>RANGE</kbd> keys. For example, we might want to collate all events for a specific domain object ID or all events from a specific user ID. In this example, we use <kbd>event.partitionKey</kbd> as the hash key, but you can calculate the hash key from any of the available data. For the range key, we need a value that is unique within the hash key. The <kbd>event.id</kbd> is a good choice if it is implemented with a V1 UUID because they are time-based. The Kinesis sequence number is another good choice. The <kbd>event.timestamp</kbd> is another alternative, but there could be a potential that events are created at the exact same time within a hash key.</p>
<p>The <kbd>trigger</kbd> function, which is attached to the DynamoDB stream, takes over after the <kbd>listener</kbd> has saved an event. The trigger calls <kbd>getMicroEventStore</kbd> to retrieve the micro event store based on the hash key calculated for the current event. At this point, the stream processor has all the relevant data available in memory. The events in the micro event store are in historical order, based on the value used for the range key. The stream processor can use this data however it sees fit to implement its business logic.</p>
<p>Use the DynamoDB TTL feature to keep the micro event store from growing unbounded.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Applying the database-first variant of the Event Sourcing pattern with DynamoDB</h1>
                
            
            
                
<p>In the previous recipe, <em>Applying the event-first variant of the Event Sourcing pattern</em>, we discussed how the Event Sourcing pattern allows us to design eventually consistent systems that are composed of a chain of atomic steps. Distributed transactions are not supported in cloud-native systems, because they do not scale effectively. Therefore, each step must update one, and only one, system. In this recipe, we will leverage the <strong>database-first</strong> variant of the Event Sourcing pattern, where the atomic unit of work is writing to a single cloud-native database. A cloud-native database provides a <strong>change data capture</strong> mechanism that allows further logic to be atomically triggered that publishes an appropriate <strong>domain event</strong> to the event stream for further downstream processing. In this recipe, we will demonstrate implementing this pattern with <strong>AWS DynamoDB</strong> and <strong>DynamoDB Streams</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/db-first-dynamodb --path cncb-db-first-dynamodb</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-db-first-dynamodb</kbd> directory with <kbd>cd cncb-db-first-dynamodb</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-db-first-dynamodb<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    - Effect: Allow<br/>      Action:<br/>        - dynamodb:PutItem<br/>      Resource:<br/>        Fn::GetAtt: [ Table, Arn ]<br/>    - Effect: Allow<br/>      Action:<br/>        - kinesis:PutRecord<br/>      Resource: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/><br/>functions:<br/>  <strong>command</strong>:<br/>    handler: handler.<strong>command</strong><br/>    environment:<br/>      <strong>TABLE_NAME</strong>:<br/>        Ref: Table<br/>  <strong>trigger</strong>:<br/>    handler: handler.<strong>trigger</strong><br/>    events:<br/>      - stream:<br/>          type: <strong>dynamodb</strong><br/>          arn:<br/>            Fn::GetAtt: [ Table, StreamArn ]<br/>          ...<br/>    environment:<br/>      <strong>STREAM_NAME</strong>: ${cf:cncb-event-stream-${opt:stage}.streamName}<br/><br/>resources:<br/>  Resources:<br/>    <strong>Table</strong>:<br/>      Type: AWS::DynamoDB::Table<br/>      Properties:<br/>        TableName: ${opt:stage}-${self:service}-things<br/>        AttributeDefinitions:<br/>          ...<br/>        KeySchema:<br/>          - AttributeName: <strong>id</strong><br/>            KeyType: <strong>HASH</strong><br/>        ...<br/>        <strong>StreamSpecification</strong>:<br/>          StreamViewType: NEW_AND_OLD_IMAGES</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>command</strong> = (request, context, callback) =&gt; {<br/>  const thing = {<br/>    id: uuid.v4(),<br/>    ...request,<br/>  };<br/><br/>  const params = {<br/>    TableName: process.env.<strong>TABLE_NAME</strong>,<br/>    Item: thing,<br/>  };<br/><br/>  const db = new aws.DynamoDB.DocumentClient();<br/>  db.<strong>put</strong>(params, callback);<br/>};<br/><br/>module.exports.<strong>trigger</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(<strong>toEvent</strong>)<br/>    .flatMap(<strong>publish</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>const <strong>toEvent</strong> = record =&gt; ({<br/>  id: record.eventID,<br/>  type: `thing-${<strong>EVENT_NAME_MAPPING</strong>[record.eventName]}`,<br/>  timestamp: record.dynamodb.ApproximateCreationDateTime * 1000,<br/>  partitionKey: record.dynamodb.Keys.id.S,<br/>  tags: {<br/>    region: record.awsRegion,<br/>  },<br/>  thing: {<br/>    <strong>old</strong>: record.dynamodb.OldImage ?<br/>      aws.DynamoDB.Converter.unmarshall(record.dynamodb.OldImage) :<br/>      undefined,<br/>    <strong>new</strong>: record.dynamodb.NewImage ?<br/>      aws.DynamoDB.Converter.unmarshall(record.dynamodb.NewImage) :<br/>      undefined,<br/>  },<br/>});<br/><br/>const <strong>EVENT_NAME_MAPPING</strong> = {<br/>  INSERT: 'created',<br/>  MODIFY: 'updated',<br/>  REMOVE: 'deleted',<br/>};<br/><br/>const <strong>publish</strong> = event =&gt; {<br/>  const params = {<br/>    StreamName: process.env.<strong>STREAM_NAME</strong>,<br/>    PartitionKey: event.partitionKey,<br/>    Data: Buffer.from(JSON.stringify(<strong>event</strong>)),<br/>  };<br/><br/>  const kinesis = new aws.Kinesis();<br/>  return _(kinesis.<strong>putRecord</strong>(params).promise());<br/>}</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-db-first-dynamodb@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-db-first-dynamodb<br/>&gt; sls deploy -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  command: cncb-db-first-dynamodb-john-command<br/>  trigger: cncb-db-first-dynamodb-john-trigger</pre>
<ol start="9">
<li>Review the stack, functions, and table in the AWS Console.</li>
<li>Invoke the function with the following command:</li>
</ol>
<pre style="padding-left: 30px">$ sls invoke -r us-east-1 -f command -s $MY_STAGE -d '{"name":"thing one"}'</pre>
<ol start="11">
<li>Take a look at the <kbd>command</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f command -r us-east-1 -s $MY_STAGE</strong><br/>START ...<br/>2018-04-17 00:29:13 ... request: {"name":"thing one"}<br/>2018-04-17 00:29:13 ... params: {"TableName":"john-cncb-db-first-dynamodb-things","Item":{"id":"4297c253-f512-443d-baaf-65f0a36aaaa3","name":"thing one"}}<br/>END ...<br/>REPORT ... Duration: 136.99 ms    Billed Duration: 200 ms     Memory Size: 1024 MB    Max Memory Used: 35 MB    </pre>
<ol start="12">
<li>Take a look at the trigger function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong><br/>START ...<br/>2018-04-17 00:29:15 ... <strong>event</strong>: {"Records":[{"<strong>eventID</strong>":"<strong>39070dc13de0eb76548506a977d4134c</strong>","<strong>eventName</strong>":"<strong>INSERT</strong>",...,"dynamodb":{"ApproximateCreationDateTime":1523939340,"Keys":{"id":{"S":"4297c253-f512-443d-baaf-65f0a36aaaa3"}},"NewImage":{"<strong>name</strong>":{"S":"<strong>thing one</strong>"},"id":{"S":"4297c253-f512-443d-baaf-65f0a36aaaa3"}},"SequenceNumber":"100000000006513931753",...},...}]}<br/>2018-04-17 00:29:15 ... {"<strong>id</strong>":"<strong>39070dc13de0eb76548506a977d4134c</strong>","<strong>type</strong>":"<strong>thing-created</strong>","<strong>timestamp</strong>":1523939340000,"<strong>partitionKey</strong>":"4297c253-f512-443d-baaf-65f0a36aaaa3","<strong>tags</strong>":{"region":"us-east-1"},"<strong>thing</strong>":{"<strong>new</strong>":{"name":"<strong>thing one</strong>","id":"4297c253-f512-443d-baaf-65f0a36aaaa3"}}}<br/>2018-04-17 00:29:15 ... params: {"StreamName":"john-cncb-event-stream-s1","PartitionKey":"4297c253-f512-443d-baaf-65f0a36aaaa3","Data":{"type":"Buffer","data":[...]}}<br/>2018-04-17 00:29:15 ... {"ShardId":"shardId-000000000000","SequenceNumber":"4958...3778"}<br/>END ...<br/>REPORT ... Duration: 326.99 ms    Billed Duration: 400 ms ... Max Memory Used: 40 MB    </pre>
<ol start="13">
<li>Review the events collected in the data lake bucket.</li>
<li>Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we implement a <kbd>command</kbd> function that would be part of a Backend For Frontend service. Following the Event Sourcing pattern, we make this command atomic by only writing to a single resource. In many scenarios, such as the authoring of data, we need to write data and make sure it's immediately available for reading. In these cases, the <em>database-first</em> variant is most appropriate. The command just needs to execute quickly and leave as little to chance as possible. We write the <strong>domain object</strong> to the highly available, fully-managed cloud-native database and trust that the database's <strong>change data capture</strong> mechanism will handle the next step.</p>
<p>In this recipe, the database is DynamoDB and the change data capture mechanism is DynamoDB Streams. The <kbd>trigger</kbd> function is a stream processor that is consuming events from the specified DynamoDB stream. We enable the stream by adding the <kbd>StreamSpecification</kbd> to the definition of the table.</p>
<p>The stream processor logic wraps the domain object in the standard event format, as discussed in the <em>Creating an event stream and publishing an event</em> recipe in <a href="a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml">Chapter 1</a>, <em>Getting Started with Cloud-Native</em>. The <kbd>record.eventID</kbd> generated by DynamoDB is reused as the domain event ID, the database trigger's <kbd>record.eventName</kbd> is translated into the domain event type, the domain object ID is used as <kbd>partitionKey</kbd>, and useful <kbd>tags</kbd> are adorned. The <kbd>old</kbd> and <kbd>new</kbd> values of the domain object are included in the event so that downstream services can calculate a delta however they see fit.</p>
<p>Finally, the event is written to the stream specified by the <kbd>STREAM_NAME</kbd> environment variable. Note that the trigger function is similar to the <em>event-first</em> variant. It just needs to execute quickly and leave as little to chance as possible. We write the event to a single resource, the highly available, fully-managed cloud-native event stream, and trust that the downstream services will eventually consume the event.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Applying the database-first variant of the Event Sourcing pattern with Cognito datasets</h1>
                
            
            
                
<p>In the <em>Applying the event-first variant of the Event Sourcing pattern</em> recipe, we discussed how the Event Sourcing pattern allows us to design eventually consistent systems that are composed of a chain of atomic steps. Distributed transactions are not supported in cloud-native systems, because they do not scale effectively. Therefore, each step must update one, and only one, system. In this recipe, we leverage the <em>database-first</em> variant of the Event Sourcing pattern, where the atomic unit of work is writing to a single cloud-native database. A cloud-native database provides a change data capture mechanism that allows further logic to be atomically triggered that publishes an appropriate domain event to the event stream for further downstream processing. In the recipe, we demonstrate an <strong>offline-first</strong> implementation of this pattern with <strong>AWS Cognito datasets</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/db-first-cognito --path cncb-db-first-cognito</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-db-first-cognito</kbd> directory with <kbd>cd cncb-db-first-cognito</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-db-first-cognito<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  ...<br/><br/>functions:<br/>  <strong>trigger</strong>:<br/>    handler: handler.<strong>trigger</strong><br/>    events:<br/>      - stream:<br/>          type: <strong>kinesis</strong><br/>          arn:<br/>            Fn::GetAtt: [ <strong>CognitoStream</strong>, Arn ]<br/>          ...<br/>    environment:<br/>      <strong>STREAM_NAME</strong>: ${cf:cncb-event-stream-${opt:stage}.streamName}<br/><br/>resources:<br/>  Resources:<br/>    <strong>CognitoStream</strong>:<br/>      Type: AWS::Kinesis::Stream<br/>      Properties:<br/>          ShardCount: 1<br/><br/>    <strong>IdentityPool</strong>:<br/>      Type: AWS::Cognito::IdentityPool<br/>      Properties:<br/>        CognitoStreams:<br/>          StreamName:<br/>            Ref: <strong>CognitoStream</strong><br/>    ...<br/><br/>  Outputs:<br/>    <strong>identityPoolId</strong>:<br/>      Value: <br/>        Ref: IdentityPool<br/>    <strong>identityPoolName</strong>:<br/>      Value: <br/>        Fn::GetAtt: [ IdentityPool, Name ]</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>trigger</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .flatMap(<strong>recordToSync</strong>)<br/>    .map(<strong>toEvent</strong>)<br/>    .flatMap(publish)<br/>    .collect().toCallback(cb);<br/>};<br/><br/>const <strong>recordToSync</strong> = r =&gt; {<br/>  const data = JSON.parse(Buffer.from(r.kinesis.data, 'base64'));<br/>  return _(data.kinesisSyncRecords.map(sync =&gt; ({<br/>    <strong>record</strong>: r,<br/>    <strong>data</strong>: data,<br/>    <strong>sync</strong>: sync,<br/>    <strong>thing</strong>: JSON.parse(sync.value)<br/>  })));<br/>}<br/><br/>const <strong>toEvent</strong> = uow =&gt; ({<br/>  id: uuid.v1(),<br/>  <strong>type</strong>: `<strong>thing-created</strong>`,<br/>  timestamp: uow.sync.lastModifiedDate,<br/>  partitionKey: uow.thing.id,<br/>  <strong>tags</strong>: {<br/>    region: uow.record.awsRegion,<br/>    identityPoolId: uow.data.identityPoolId,<br/>    datasetName: uow.data.datasetName<br/>  },<br/>  <strong>thing</strong>: {<br/>      identityId: uow.data.identityId, // the end user<br/>      ...uow.thing,<br/>  },<br/>  <strong>raw</strong>: uow.sync<br/>});<br/><br/>...</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-db-first-cognito@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-db-first-cognito<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  trigger: cncb-db-first-cognito-john-trigger<br/><br/>Stack Outputs<br/><strong>identityPoolName</strong>: IdentityPool_P6awUWzjQH0y<br/><strong>identityPoolId</strong>: us-east-1:e51ba12c-75c2-4548-868d-2d023eb9398b<br/>...</pre>
<ol start="9">
<li>Review the stack, function, and identity pool in the AWS Console.</li>
<li>
<div><p>Update the file named <kbd>./index.html</kbd> with the  <kbd>identityPoolId</kbd> from the previous output.</p>
</div>
</li>
</ol>
<ol start="11">
<li>Open the file named <kbd>./index.html</kbd> in a browser, enter a <kbd>name</kbd> and <kbd>description</kbd>, and press <kbd>Save</kbd> and then <kbd>Synchronize</kbd>, as shown in the following screenshot:</li>
</ol>
<div><img src="img/9b194c69-5f43-45c1-95cc-69277df5207e.png"/></div>
<ol start="12">
<li>Take a look at the logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong><br/><br/>START ...<br/>2018-04-19 00:50:15 ... {"id":"2714e290-438d-11e8-b3de-2bf7e0b964a2","<strong>type</strong>":"<strong>thing-created</strong>","timestamp":1524113413268,"partitionKey":"fd398c3b-8199-fd26-8c3c-156bb7ae8feb","<strong>tags</strong>":{"region":"us-east-1","<strong>identityPoolId</strong>":"us-east-1:e51ba12c-75c2-4548-868d-2d023eb9398b","<strong>datasetName</strong>":"things"},"<strong>thing</strong>":{"<strong>identityId</strong>":"us-east-1:28a2c685-2822-472e-b42a-f7bd1f02545a","<strong>id</strong>":"fd398c3b-8199-fd26-8c3c-156bb7ae8feb","<strong>name</strong>":"<strong>thing six</strong>","description":"the sixth thing"},"<strong>raw</strong>":{"key":"thing","value":"{\"id\":\"fd398c3b-8199-fd26-8c3c-156bb7ae8feb\",\"name\":\"thing six\",\"description\":\"the sixth thing\"}","syncCount":1,"lastModifiedDate":1524113413268,"deviceLastModifiedDate":1524113410528,"op":"replace"}}<br/>2018-04-19 00:50:15 ... params: {"StreamName":"john-cncb-event-stream-s1","PartitionKey":"fd398c3b-8199-fd26-8c3c-156bb7ae8feb","Data":{"type":"Buffer","data":[...]}}<br/>END ...<br/>REPORT ... Duration: 217.22 ms    Billed Duration: 300 ms ... Max Memory Used: 40 MB    </pre>
<ol start="13">
<li>Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd></li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we are implementing an offline-first solution where the ReactJS client application stores all changes in local storage and then synchronizes the data to a Cognito dataset in the cloud when connectivity is available. This scenario is very common in mobile applications where the mobile application may not always be connected. An AWS Cognito dataset is associated with a specific user in an <strong>AWS Identity Pool</strong>. In this recipe, the identity pool supports unauthenticated users. Anonymous access is another common characteristic of mobile applications.</p>
<p>The bare bones ReactJS application is implemented in the <kbd>./index.html</kbd> file. It contains a form for the user to enter data. The <kbd>Save</kbd> button saves the form's data to local storage via the Cognito SDK. The <kbd>Synchronize</kbd> button uses the SDK to send the local data to the cloud. In a typical application, this synchronization would be triggered behind the scenes by events in the normal flow of the application, such as on save, on load, and before exit. In the <em>Creating a materialized view in a Cognito Dataset</em> recipe, we show how synchronizing will also retrieve data from the cloud.</p>
<p>Cognito's change data capture feature is implemented via <strong>AWS Kinesis</strong>. Therefore, we create a Kinesis stream called <kbd>CognitoStream</kbd> that is dedicated to our Cognito datasets. The <kbd>trigger</kbd> function is a stream processor that is consuming sync records from this stream. The stream processor's <kbd>recordToSync</kbd> step extracts the domain object from each sync record, where it is stored as a JSON string. The <kbd>toEvent</kbd> step wraps the domain object in the standard event format, as discussed in the <em>Creating an event stream and publishing an event</em> recipe in <a href="a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml">Chapter 1</a>, <em>Getting Started with Cloud-Native</em>. Finally, the event is written to the stream specified by the <kbd>STREAM_NAME</kbd> environment variable. Note that the trigger function is similar to the event-first variant. It just needs to execute quickly and leave as little to chance as possible. We write the event to a single resource, the highly available, fully managed cloud-native event stream, and trust that the downstream services will eventually consume the event.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a materialized view in DynamoDB</h1>
                
            
            
                
<p>The <strong>Command Query Responsibility Segregation</strong> (<strong>CQRS</strong>) pattern is critical for designing cloud-native systems that are composed of bounded, isolated, and autonomous services with appropriate bulkheads to limit the blast radius when a service experiences an outage. These bulkheads are implemented by creating materialized views in downstream services.</p>
<p>Upstream services are responsible for the commands that write data using the Event Sourcing pattern. Downstream services take responsibility for their own queries by creating materialized views that are specifically tailored to their needs. This <strong>replication</strong> of data increases scalability, reduces latency, and allows services to be completely <strong>autonomous</strong> and function even when upstream source services are unavailable. In this recipe, we will implement a materialized view in <strong>AWS DynamoDB</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/materialized-view-dynamodb --path cncb-materialized-view-dynamodb</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-materialized-view-dynamodb</kbd> directory with <kbd>cd cncb-materialized-view-dynamodb</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-materialized-view-dynamodb<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/>  environment:<br/>    <strong>TABLE_NAME</strong>:<br/>      Ref: Table<br/><br/>functions:<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>  <strong>query</strong>:<br/>    handler: handler.<strong>query</strong><br/><br/>resources:<br/>  Resources:<br/>    <strong>Table</strong>:<br/>      Type: AWS::DynamoDB::Table<br/>      Properties:<br/>        TableName: ${opt:stage}-${self:service}-things<br/>        AttributeDefinitions:<br/>          ...<br/>        KeySchema:<br/>          - AttributeName: id<br/>            KeyType: HASH<br/>        ...</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>forThingCreated</strong>)<br/>    .map(<strong>toThing</strong>)<br/>    .flatMap(put)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>forThingCreated</strong> = e =&gt; e.type === 'thing-created';<br/><br/>const <strong>toThing</strong> = event =&gt; ({<br/>  id: event.thing.new.id,<br/>  name: event.thing.new.name,<br/>  description: event.thing.new.description,<br/>  <strong>asOf</strong>: event.timestamp,<br/>});<br/><br/>const <strong>put</strong> = thing =&gt; {<br/>  const params = {<br/>    TableName: process.env.<strong>TABLE_NAME</strong>,<br/>    Item: thing,<br/>  };<br/><br/>  const db = new aws.DynamoDB.DocumentClient();<br/>  return _(db.put(params).promise());<br/>};<br/><br/>module.exports.<strong>query</strong> = (id, context, callback) =&gt; {<br/>  const params = {<br/>    TableName: process.env.<strong>TABLE_NAME</strong>,<br/>    <strong>Key</strong>: {<br/>      id: id,<br/>    },<br/>  };<br/><br/>  const db = new aws.DynamoDB.DocumentClient();<br/>  db.get(params, callback);<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-materialized-view-dynamodb@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-materialized-view-dynamodb<br/>&gt; sls deploy -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  listener: cncb-materialized-view-dynamodb-john-listener<br/>  query: cncb-materialized-view-dynamodb-john-query</pre>
<ol start="9">
<li>Review the stack, functions, and table in the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"thing-created","thing":{"new":{"name":"thing two","id":"22222222-2222-2222-2222-222222222222"}}}'</strong><br/>{<br/> "ShardId": "shardId-000000000000",<br/> "SequenceNumber": "49583553996455686705785668952916415462701426537440215042"<br/>}</pre>
<ol start="11">
<li>Take a look at the <kbd>listener</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong><br/>START ...<br/>2018-04-17 00:54:48 ... event: {"Records":[...]}<br/>2018-04-17 00:54:48 ... {"id":"39070dc13de0eb76548506a977d4134c","type":"<strong>thing-created</strong>","timestamp":<strong>1523939340000</strong>,"tags":{"region":"us-east-1"},"thing":{"new":{"name":"<strong>thing two</strong>","id":"22222222-2222-2222-2222-222222222222"}}}<br/>2018-04-17 00:54:48 ... <strong>params</strong>: {"<strong>TableName</strong>":"john-cncb-materialized-view-dynamodb-things","<strong>Item</strong>":{"id":"22222222-2222-2222-2222-222222222222","name":"<strong>thing two</strong>","<strong>asOf</strong>":<strong>1523939340000</strong>}}<br/>END ...<br/>REPORT ... Duration: 306.17 ms    Billed Duration: 400 ms ... Max Memory Used: 36 MB    </pre>
<ol start="12">
<li>Invoke the <kbd>query</kbd> command:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -r us-east-1 -f query -s $MY_STAGE -d 22222222-2222-2222-2222-222222222222</strong><br/>{<br/>    "Item": {<br/>        "id": "22222222-2222-2222-2222-222222222222",<br/>        "name": "thing two",<br/>        "<strong>asOf</strong>": 1523939340000<br/>    }<br/>}</pre>
<ol start="13">
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we implemented a <kbd>listener</kbd> function that consumes upstream events and populates a materialized view that is used by a <strong>Backend For Frontend</strong> (<strong>BFF</strong>) service. This function is a <em>stream processor,</em> such as the one we discussed in the <em>Creating a stream processor</em> recipe in <a href="a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml">Chapter 1</a>, <em>Getting Started with Cloud-Native</em>. The function performs a <kbd>filter</kbd> for the desired events and then transforms the data in a <kbd>map</kbd> step to the desired materialized view. The materialized view is optimized to support the requirements of the query needed by the BFF. Only the minimum necessary data is stored and the optimal database type is used. In this recipe, the database type is DynamoDB. DynamoDB is a good choice for a materialized view when the data changes frequently.</p>
<p>Note that the <kbd>asOf</kbd> timestamp is included in the record. In an eventually consistent system, it is important to provide the user with the <kbd>asOf</kbd> value so that he or she can access the latency of the data. Finally, the data is stored in the highly available, fully managed, cloud-native database.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a materialized view in S3</h1>
                
            
            
                
<p>In the <em>Creating a materialized view in</em> <em>DynamoDB</em> recipe, we discussed how the CQRS pattern allows us to design services that are bounded, isolated, and autonomous. This allows services to operate, even when their upstream dependencies are unavailable, because we have eliminated all synchronous inter-service communication in favor of replicating and caching the required data locally in dedicated materialized views. In this recipe, we will implement a materialized view in AWS S3.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/materialized-view-s3 --path cncb-materialized-view-s3</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-materialized-view-s3</kbd> directory with <kbd>cd cncb-materialized-view-s3</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-materialized-view-s3<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/><br/>functions:<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>    environment:<br/>      <strong>BUCKET_NAME</strong>:<br/>        Ref: Bucket<br/><br/>resources:<br/>  Resources:<br/>    <strong>Bucket</strong>:<br/>      Type: AWS::S3::Bucket<br/><br/>  Outputs:<br/>    BucketName:<br/>      Value:<br/>        Ref: Bucket<br/>    BucketDomainName:<br/>      Value:<br/>        Fn::GetAtt: [ Bucket, DomainName ]</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.listener = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>forThingCreated</strong>)<br/>    .map(<strong>toThing</strong>)<br/>    .flatMap(<strong>put</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>forThingCreated</strong> = e =&gt; e.type === 'thing-created';<br/><br/>const <strong>toThing</strong> = event =&gt; ({<br/>  id: event.thing.new.id,<br/>  name: event.thing.new.name,<br/>  description: event.thing.new.description,<br/>  <strong>asOf</strong>: event.timestamp,<br/>});<br/><br/>const <strong>put</strong> = thing =&gt; {<br/>  const params = {<br/>    Bucket: process.env.<strong>BUCKET_NAME</strong>,<br/>    <strong>Key</strong>: `things/${thing.id}`,<br/>    ACL: 'public-read',<br/>    ContentType: 'application/json',<br/>    CacheControl: 'max-age=300',<br/>    Body: JSON.stringify(<strong>thing</strong>),<br/>  };<br/><br/>  const s3 = new aws.S3();<br/>  return _(s3.<strong>putObject</strong>(params).promise());<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-materialized-view-s3@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-materialized-view-s3<br/>&gt; sls deploy -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  listener: cncb-materialized-view-s3-john-listener<br/><br/>Stack Outputs<br/><strong>BucketName</strong>: cncb-materialized-view-s3-john-bucket-<strong>1pp3d4c2z99kt</strong><br/>BucketDomainName: cncb-materialized-view-s3-john-bucket-1pp3d4c2z99kt.s3.amazonaws.com<br/>...</pre>
<ol start="9">
<li>Review the stack, function, and bucket from the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"thing-created","thing":{"new":{"name":"thing three","id":"33333333-3333-3333-3333-333333333333"}}}'</strong><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49583553996455686705785668952918833314346020725338406914"<br/>}</pre>
<ol start="11">
<li>Take a look at the logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$</strong> <strong>sls logs -f listener -r us-east-1 -s $MY_STAGE</strong><br/>START ...<br/>2018-04-17 22:49:20 ... event: {"Records":[...]}<br/>2018-04-17 22:49:20 ... {"type":"<strong>thing-created</strong>","thing":{"new":{"name":"<strong>thing three</strong>","id":"33333333-3333-3333-3333-333333333333"}},"id":"16a7b930-42b3-11e8-8700-a918e007d88a","partitionKey":"3de89e9d-c48d-4255-84fc-6c1b7e3f8b90","timestamp":<strong>1524019758148</strong>,"tags":{"region":"us-east-1"}}<br/>2018-04-17 22:49:20 ... params: {"Bucket":"cncb-materialized-view-s3-john-bucket-1pp3d4c2z99kt","Key":"things/33333333-3333-3333-3333-333333333333","ACL":"public-read","ContentType":"application/json","CacheControl":"max-age=300","Body":"{\"id\":\"33333333-3333-3333-3333-333333333333\",\"name\":\"thing three\",\"<strong>asOf</strong>\":<strong>1524019758148</strong>}"}<br/>2018-04-17 22:49:20 ... {"ETag":"\"edfee997659a520994ed18b82255be2a\""}<br/>END ...<br/>REPORT ... Duration: 167.66 ms    Billed Duration: 200 ms ... Max Memory Used: 36 MB    </pre>
<ol start="12">
<li>Invoke the following command, after updating the <kbd>bucket-suffix</kbd>, to get the data from S3:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl https://s3.amazonaws.com/cncb-materialized-view-s3-$MY_STAGE-bucket-&lt;bucket-suffix&gt;/things/33333333-3333-3333-3333-333333333333 | json_pp</strong><br/>{<br/> "<strong>asOf</strong>" : 1524019758148,<br/> "name" : "thing three",<br/> "id" : "33333333-3333-3333-3333-333333333333"<br/>}</pre>
<ol start="13">
<li>Use the console to delete the objects from the bucket before removing the stack.</li>
<li>Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we implement a <kbd>listener</kbd> function that consumes upstream events and populates a materialized view that is used by a Backend For Frontend service. This function is a stream processor, such as the one we discussed in the <em>Creating a stream processor</em> recipe in <a href="a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml">Chapter 1</a>, <em>Getting Started with Cloud-Native</em>. The function performs a <kbd>filter</kbd> for the desired events and then transforms the data in a <kbd>map</kbd> step to the desired materialized view. The materialized view is optimized to support the requirements of the query needed by the BFF. Only the minimum necessary data is stored, and the optimal database type is used.</p>
<p>In this recipe, the database type is S3. S3 is a good choice for a materialized view when the data changes infrequently, and it can be cached in the CDN. Note that the <kbd>asOf</kbd> timestamp is included in the record so that the user can access the latency of the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a materialized view in Elasticsearch</h1>
                
            
            
                
<p>In the <em>Creating a materialized view in DynamoDB</em> recipe, we discussed how the CQRS pattern allows us to design services that are bounded, isolated, and autonomous. This allows services to operate, even when their upstream dependencies are unavailable, because we have eliminated all synchronous inter-service communication in favor of replicating and caching the required data locally in dedicated materialized views. In this recipe, we will implement a materialized view in AWS Elasticsearch.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/materialized-view-es --path cncb-materialized-view-es</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-materialized-view-es</kbd> directory with <kbd>cd cncb-materialized-view-es</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-materialized-view-es<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/>  environment:<br/>    <strong>DOMAIN_ENDPOINT</strong>:<br/>      Fn::GetAtt: [ <strong>Domain</strong>, DomainEndpoint ]<br/><br/>...<br/><br/>functions:<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>  <strong>search</strong>:<br/>    handler: handler.<strong>search</strong><br/><br/>resources:<br/>  Resources:<br/>    <strong>Domain</strong>:<br/>      Type: AWS::Elasticsearch::Domain<br/>      Properties:<br/>        ...<br/><br/>  Outputs:<br/>    DomainName:<br/>      Value:<br/>        Ref: Domain<br/>    <strong>DomainEndpoint</strong>:<br/>      Value:<br/>        Fn::GetAtt: [ Domain, DomainEndpoint ]</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">const client = require('elasticsearch').Client({<br/>  hosts: [`https://${process.env.<strong>DOMAIN_ENDPOINT</strong>}`],<br/>  connectionClass: require('http-aws-es'),<br/>  log: 'trace',<br/>});<br/><br/>module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>forThingCreated</strong>)<br/>    .map(<strong>toThing</strong>)<br/>    .flatMap(<strong>index</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>forThingCreated</strong> = e =&gt; e.type === 'thing-created';<br/><br/>const <strong>toThing</strong> = event =&gt; ({<br/>  id: event.thing.new.id,<br/>  name: event.thing.new.name,<br/>  description: event.thing.new.description,<br/>  <strong>asOf</strong>: event.timestamp,<br/>});<br/><br/>const <strong>index</strong> = thing =&gt; {<br/>  const params = {<br/>    index: 'things',<br/>    type: 'thing',<br/>    id: thing.id,<br/>    body: <strong>thing</strong>,<br/>  };<br/><br/>  return _(client.<strong>index</strong>(params));<br/>};<br/><br/>module.exports.<strong>search</strong> = (query, context, callback) =&gt; {<br/>  const params = {<br/>    index: 'things',<br/>    q: <strong>query</strong>,<br/>  };<br/><br/>  client.<strong>search</strong>(params, callback);<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<p>Deploying an Elasticsearch domain can take upwards of 1-20 minutes.</p>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-materialized-view-es@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-materialized-view-es<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  listener: cncb-materialized-view-es-john-listener<br/>  search: cncb-materialized-view-es-john-search<br/><br/>Stack Outputs<br/>...<br/><strong>DomainEndpoint</strong>: search-cncb-ma-domain-gw419rzj26hz-p2g37av7sdlltosbqhag3qhwnq.us-east-1.es.amazonaws.com<br/>DomainName: cncb-ma-domain-gw419rzj26hz<br/>...</pre>
<ol start="9">
<li>Review the stack, function, and Elasticsearch domain in the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ $ sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"thing-created","thing":{"new":{"name":"thing four","id":"44444444-4444-4444-4444-444444444444"}}}'</strong><br/><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49583655996852917476267785004768832452002332571160543234"<br/>}</pre>
<ol start="11">
<li>Take a look at the <kbd>listener</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong><br/><br/>START ...<br/>2018-04-19 01:54:33 ... {"type":"thing-created","thing":{"new":{"name":"<strong>thing four</strong>","id":"44444444-4444-4444-4444-444444444444"}},"id":"0e1a68c0-4395-11e8-b455-8144cebc5972","partitionKey":"8082a69c-00ee-4388-9697-c590c523c061","timestamp":<strong>1524116810060</strong>,"tags":{"region":"us-east-1"}}<br/>2018-04-19 01:54:33 ... params: {"index":"things","type":"thing","id":"44444444-4444-4444-4444-444444444444","body":{"id":"44444444-4444-4444-4444-444444444444","name":"<strong>thing four</strong>","<strong>asOf</strong>":<strong>1524116810060</strong>}}<br/>2018-04-19 01:54:33 ... {"_index":"things","_type":"thing","_id":"44444444-4444-4444-4444-444444444444","_version":1,"result":"created","_shards":{"total":2,"successful":1,"failed":0},"_seq_no":0,"_primary_term":1}<br/>END ...<br/>REPORT ... Duration: 31.00 ms    Billed Duration: 100 ms ... Max Memory Used: 42 MB    </pre>
<p class="mce-root"/>
<ol start="12">
<li>Invoke the <kbd>search</kbd> command:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -r us-east-1 -f search -s $MY_STAGE -d four</strong><br/><br/>{<br/>    ...<br/>    "hits": {<br/>        "total": 1,<br/>        "max_score": 0.2876821,<br/>        "hits": [<br/>            {<br/>                "_index": "things",<br/>                "_type": "thing",<br/>                "_id": "44444444-4444-4444-4444-444444444444",<br/>                "_score": 0.2876821,<br/>                "_source": {<br/>                    "id": "44444444-4444-4444-4444-444444444444",<br/>                    "name": "thing four",<br/>                    "<strong>asOf</strong>": 1524116810060<br/>                }<br/>            }<br/>        ]<br/>    }<br/>}</pre>
<ol start="13">
<li>Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we implement a <kbd>listener</kbd> function that consumes upstream events and populates a materialized view that is used by a Backend For Frontend service. This function is a stream processor, such as the one we discussed in the <em>Creating a stream processor</em> recipe in <a href="a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml">Chapter 1</a>, <em>Getting Started with Cloud-Native</em>. The function performs a <kbd>filter</kbd> for the desired events and then transforms the data in a <kbd>map</kbd> step to the desired materialized view. The materialized view is optimized to support the requirements of the query needed by the BFF. Only the minimum necessary data is stored, and the optimal database type is used. In this recipe, the database type is Elasticsearch. Elasticsearch is a good choice for a materialized view when the data must be searched and filtered. Note that the <kbd>asOf</kbd> timestamp is included in the record so that the user can access the latency of the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a materialized view in a Cognito dataset</h1>
                
            
            
                
<p>In the <em>Creating a materialized view in DynamoDB</em> recipe, we discussed how the CQRS pattern allows us to design services that are bounded, isolated, and autonomous. This allows services to operate, even when their upstream dependencies are unavailable, because we have eliminated all synchronous inter-service communication in favor of replicating and caching the required data locally in materialized views. In this recipe, we will implement an offline-first materialized view in an AWS Cognito dataset.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/materialized-view-cognito --path cncb-materialized-view-cognito</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-materialized-view-cognito</kbd> directory with <kbd>cd cncb-materialized-view-cognito</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-materialized-view-cognito<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/><br/>functions:<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>    environment:<br/>      <strong>IDENTITY_POOL_ID</strong>:<br/>        Ref: IdentityPool<br/><br/>resources:<br/>  Resources:<br/>    <strong>IdentityPool</strong>:<br/>      Type: AWS::Cognito::IdentityPool<br/>      ...<br/><br/>  Outputs:<br/>    <strong>identityPoolId</strong>:<br/>      Value: <br/>        Ref: IdentityPool<br/>    <strong>identityPoolName</strong>:<br/>      Value: <br/>        Fn::GetAtt: [ IdentityPool, Name ]</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>forThingCreated</strong>)<br/>    .map(<strong>toThing</strong>)<br/>    .flatMap(<strong>put</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>forThingCreated</strong> = e =&gt; e.type === 'thing-created';<br/><br/>const <strong>toThing</strong> = event =&gt; ({<br/>  id: event.thing.new.id,<br/>  name: event.thing.new.name,<br/>  description: event.thing.new.description,<br/>  <strong>identityId</strong>: event.thing.new.identityId, // the end user<br/>  <strong>asOf</strong>: event.timestamp,<br/>});<br/><br/>const <strong>put</strong> = thing =&gt; {<br/>  const params = {<br/>    IdentityPoolId: process.env.<strong>IDENTITY_POOL_ID</strong>,<br/>    <strong>IdentityId</strong>: thing.identityId,<br/>    DatasetName: 'things',<br/>  };<br/><br/>  const cognitosync = new aws.CognitoSync();<br/><br/>  return _(<br/>    cognitosync.listRecords(params).promise()<br/>      .then(data =&gt; {<br/>        params.SyncSessionToken = data.SyncSessionToken;<br/>        params.RecordPatches = [{<br/>          Key: 'thing',<br/>          Value: JSON.stringify(<strong>thing</strong>),<br/>          Op: 'replace',<br/>          SyncCount: data.DatasetSyncCount,<br/>        }];<br/><br/>        return cognitosync.<strong>updateRecords</strong>(params).promise()<br/>      })<br/>  );<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-materialized-view-cognito@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-materialized-view-cognito<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  listener: cncb-materialized-view-cognito-john-listener<br/><br/>Stack Outputs<br/><strong>identityPoolName</strong>: IdentityPool_c0GbzyVSh3Ws<br/><strong>identityPoolId</strong>: us-east-1:3a07e120-f1d8-4c85-9c34-0f908f2a21a1<br/>...</pre>
<ol start="9">
<li>Review the stack, function, and identity pool in the AWS Console.</li>
<li>
<div><p>Update the file named <kbd>index.html</kbd> file with the <kbd>identityPoolId</kbd> from previous output.</p>
</div>
</li>
<li>
<div><p>Open the file named <kbd>index.html</kbd> in a browser and copy the identity ID for use in the next step.<br/></p>
</div>
</li>
</ol>
<ol start="12">
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"thing-created","thing":{"new":{"name":"thing five","id":"55555555-5555-5555-5555-555555555555", "identityId":"&lt;identityId from previous step&gt;"}}}'</strong><br/><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49583655996852917476267784847452524471369889169788633090"<br/>}</pre>
<ol start="13">
<li>Take a look at the logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong><br/><br/>START ...<br/>2018-04-19 00:18:42 ... {"type":"thing-created","thing":{"new":{"name":"<strong>thing five</strong>","id":"55555555-5555-5555-5555-555555555555","<strong>identityId</strong>":"us-east-1:ee319396-fec4-424d-aa19-71ee751624d1"}},"id":"bda76e80-4388-11e8-a845-5902692b9264","partitionKey":"c9d4e9e5-d33f-4907-9a7a-af03710fa50f","timestamp":<strong>1524111521129</strong>,"tags":{"region":"us-east-1"}}<br/>2018-04-19 00:18:42 ... params: {"IdentityPoolId":"us-east-1:3a07e120-f1d8-4c85-9c34-0f908f2a21a1","<strong>IdentityId</strong>":"us-east-1:ee319396-fec4-424d-aa19-71ee751624d1","DatasetName":"things"}<br/>2018-04-19 00:18:43 ... {"Records":[{"Key":"thing","Value":"{\"id\":\"55555555-5555-5555-5555-555555555555\",\"name\":\"<strong>thing five</strong>\",\"<strong>asOf</strong>\":<strong>1524111521129</strong>,\"<strong>identityId</strong>\":\"us-east-1:ee319396-fec4-424d-aa19-71ee751624d1\"}","SyncCount":1,"LastModifiedDate":"2018-04-19T04:18:42.978Z","LastModifiedBy":"123456789012","DeviceLastModifiedDate":"2018-04-19T04:18:42.978Z"}]}<br/>END ...<br/>REPORT ... Duration: 340.94 ms    Billed Duration: 400 ms ... Max Memory Used: 33 MB    </pre>
<ol start="14">
<li>
<p class="CDPAlignLeft CDPAlign">Open the file named <kbd>index.html</kbd> in a browser and press the Synchronize button to retrieve the data from the materialized view:</p>
</li>
</ol>
<div><img src="img/16110567-c727-48a5-83cc-3657233b5f86.png"/></div>
<ol start="15">
<li class="CDPAlignLeft CDPAlign">Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we implement a <kbd>listener</kbd> function that consumes upstream events and populates a materialized view that is used by a Backend For Frontend service. This function is a stream processor, such as the one we discussed in the <em>Creating a stream processor</em> recipe in <a href="a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml">Chapter 1</a>, <em>Getting Started with Cloud-Native</em>. The function performs a <kbd>filter</kbd> for the desired events and then transforms the data in a <kbd>map</kbd> step to the desired materialized view. The materialized view is optimized to support the requirements of the query needed by the BFF. Only the minimum necessary data is stored, and the optimal database type is used.</p>
<p>In this recipe, the database type is a Cognito dataset. A Cognito dataset is a good choice for a materialized view when network availability is intermittent, and thus an offline-first approach is needed to synchronize data to a user's devices. The data must also be specific to a user so that it can be targeted to the user based on the user's <kbd>identityId</kbd>.  Due to the intermittent nature of connectivity, the <kbd>asOf</kbd> timestamp is included in the record so that the user can access the latency of the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Replaying events</h1>
                
            
            
                
<p>One of the advantages of the Event Sourcing and data lake patterns is that they allow us to replay events when necessary to repair broken services and seed new services, and even new versions of a service. In this recipe, we will implement a utility that reads selected events from the data lake and applies them to a specified Lambda function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>Before starting this recipe, you will need the data lake that was created in the <em>Creating a data lake</em> recipe in this chapter. The data lake should contain events that were generated by working through the other recipes in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/replaying-events --path cncb-replaying-events</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-replaying-events</kbd> directory with <kbd>cd cncb-replaying-events</kbd>.</li>
<li>Review the file named <kbd>./lib/replay.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">exports.command = 'replay [bucket] [prefix]'<br/>exports.desc = 'Replay the events in [bucket] for [prefix]'<br/><br/>const _ = require('highland');<br/>const lodash = require('lodash');<br/>const aws = require('aws-sdk');<br/>aws.config.setPromisesDependency(require('bluebird'));<br/><br/>exports.builder = {<br/>    <strong>bucket</strong>: {<br/>        alias: 'b',<br/>    },<br/>    <strong>prefix</strong>: {<br/>        alias: 'p',<br/>    },<br/>    <strong>function</strong>: {<br/>        alias: 'f',<br/>    },<br/>    dry: {<br/>        alias: 'd',<br/>        default: true,<br/>        type: 'boolean'<br/>    },<br/>    region: {<br/>        alias: 'r',<br/>        default: 'us-east-1'<br/>    },<br/>}<br/><br/>exports.handler = (argv) =&gt; {<br/>    aws.config.logger = process.stdout;<br/>    aws.config.region = argv.region;<br/><br/>    const s3 = new aws.S3();<br/>    const lambda = new aws.Lambda();<br/><br/>    paginate(s3, argv)<br/>        .flatMap(obj =&gt; <strong>get</strong>(s3, argv, obj))<br/>        .flatMap(event =&gt; <strong>invoke</strong>(lambda, argv, event))<br/>        .collect()<br/>        .each(list =&gt; console.log('count:', list.length));<br/>}<br/><br/>const paginate = (s3, options) =&gt; {<br/>    let marker = undefined;<br/><br/>    return _((push, next) =&gt; {<br/>        const params = {<br/>            Bucket: options.bucket,<br/>            Prefix: options.prefix,<br/>            Marker: marker // paging indicator<br/>        };<br/><br/>        s3.<strong>listObjects</strong>(params).promise()<br/>            .then(data =&gt; {<br/>                if (data.<strong>IsTruncated</strong>) {<br/>                    marker = lodash.last(data.Contents)['Key'];<br/>                } else {<br/>                    marker = undefined;<br/>                }<br/><br/>                data.Contents.forEach(obj =&gt; {<br/>                    <strong>push</strong>(null, <strong>obj</strong>);<br/>                })<br/>            })<br/>            .catch(err =&gt; {<br/>                <strong>push</strong>(<strong>err</strong>, null);<br/>            })<br/>            .finally(() =&gt; {<br/>                if (marker) { // indicates more pages<br/>                    <strong>next</strong>();<br/>                } else {<br/>                    <strong>push</strong>(null, _.<strong>nil</strong>);<br/>                }<br/>            })<br/>    });<br/>}<br/><br/>const <strong>get</strong> = (s3, options, obj) =&gt; {<br/>    const params = {<br/>        Bucket: options.b,<br/>        Key: obj.Key<br/>    };<br/><br/>    return _(<br/>        s3.<strong>getObject</strong>(params).promise()<br/>            .then(data =&gt; Buffer.from(data.<strong>Body</strong>).toString())<br/>    )<br/>        .<strong>split</strong>() // EOL we added in data lake recipe transformer<br/>        .filter(line =&gt; line.length != 0)<br/>        .map(JSON.parse);<br/>}<br/><br/>const <strong>invoke</strong> = (lambda, options, event) =&gt; {<br/>    let payload = {<br/>        Records: [<br/>            {<br/>                kinesis: {<br/>                    partitionKey: event.kinesisRecordMetadata.partitionKey,<br/>                    sequenceNumber: event.kinesisRecordMetadata.sequenceNumber,<br/>                    data: Buffer.from(JSON.stringify(event.event)).toString('base64'),<br/>                    kinesisSchemaVersion: '1.0',<br/>                },<br/>                eventID: `${event.kinesisRecordMetadata.shardId}:${event.kinesisRecordMetadata.sequenceNumber}`,<br/>                eventName: 'aws:kinesis:record',<br/>                eventSourceARN: event.firehoseRecordMetadata.deliveryStreamArn,<br/>                eventSource: 'aws:kinesis',<br/>                eventVersion: '1.0',<br/>                awsRegion: event.firehoseRecordMetadata.region,<br/>            }<br/>        ]<br/>    };<br/><br/>    payload = Buffer.from(JSON.stringify(payload));<br/><br/>    const params = {<br/>        FunctionName: options.function,<br/>        InvocationType: options.dry ? '<strong>DryRun</strong>' : <br/>           payload.length &lt;= 100000 ? '<strong>Event</strong>' : '<strong>RequestResponse</strong>',<br/>        Payload: payload,<br/>    };<br/><br/>    return _(<br/>        lambda.<strong>invoke</strong>(params).promise()<br/>    );<br/>}</pre>
<ol start="4">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Deploy the stack with <kbd>npm run dp:lcl -- -s $MY_STAGE</kbd>.</li>
<li>Replay events with the following command:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ node index.js replay -b cncb-data-lake-s3-john-bucket-396po814rlai -p john-cncb-event-stream-s1 -f cncb-replaying-events-john-listener -dry false</strong><br/>[AWS s3 200 0.288s 0 retries] <strong>listObjects</strong>({ Bucket: 'cncb-data-lake-s3-john-bucket-396po814rlai',<br/>  Prefix: 'john-cncb-event-stream-s1',<br/>  Marker: undefined })<br/>[AWS s3 200 0.199s 0 retries] <strong>getObject</strong>({ Bucket: 'cncb-data-lake-s3-john-bucket-396po814rlai',<br/>  Key: 'john-cncb-event-stream-s1/2018/04/08/03/cncb-data-lake-s3-john-DeliveryStream-13N6LEC9XJ6DZ-3-2018-04-08-03-53-28-d79d6893-aa4c-4845-8964-61256ffc6496' })<br/>[AWS lambda 202 0.199s 0 retries] <strong>invoke</strong>({ FunctionName: 'cncb-replaying-events-john-listener',<br/>  InvocationType: 'Event',<br/>  Payload: '***SensitiveInformation***' })<br/>[AWS lambda 202 0.151s 0 retries] <strong>invoke</strong>({ FunctionName: 'cncb-replaying-events-john-listener',<br/>  InvocationType: 'Event',<br/>  Payload: '***SensitiveInformation***' })<br/>[AWS lambda 202 0.146s 0 retries] <strong>invoke</strong>({ FunctionName: 'cncb-replaying-events-john-listener',<br/>  InvocationType: 'Event',<br/>  Payload: '***SensitiveInformation***' })<br/>count: 3</pre>
<ol start="7">
<li>Take a look at the logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong><br/>START ...<br/>2018-04-17 23:43:14 ... event: {"<strong>Records</strong>":[{"kinesis":{"partitionKey":"ccfd67c3-a266-4dec-9576-ae5ea228a79c","sequenceNumber":"49583337208235522365774435506752843085880683263405588482","data":"...","kinesisSchemaVersion":"1.0"},"eventID":"shardId-000000000000:49583337208235522365774435506752843085880683263405588482","eventName":"aws:kinesis:record","eventSourceARN":"arn:aws:firehose:us-east-1:123456789012:deliverystream/cncb-data-lake-s3-john-DeliveryStream-13N6LEC9XJ6DZ","eventSource":"aws:kinesis","eventVersion":"1.0","awsRegion":"us-east-1"}]}<br/>END ...<br/>REPORT ... Duration: 10.03 ms    Billed Duration: 100 ms ... Max Memory Used: 20 MB    <br/><br/>...  </pre>
<ol start="8">
<li>Remove the stack once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we implement a <strong>Command-Line Interface</strong> (<strong>CLI</strong>) program that reads events from the data lake S3 bucket and sends them to a specific AWS Lambda function. When replaying events, we do not re-publish the events because this would broadcast the events to all subscribers. Instead, we want to replay events to a specific function to either repair the specific service or seed a new service.</p>
<p>When executing the program, we provide the name of the data lake <kbd>bucket</kbd> and the specific path <kbd>prefix</kbd> as arguments. The <kbd>prefix</kbd> allows us to replay only a portion of the events, such as a specific month, day, or hour. The program uses functional reactive programming with the <kbd>Highland.js</kbd> library. We use a <kbd>generator</kbd> function to page through the objects in the bucket and <kbd>push</kbd> each object down the stream. <strong>Backpressure</strong> is a major advantage of this programming approach, as we will discuss in <a href="5c400ff6-91da-4782-9369-549622d4a0d1.xhtml" target="_blank">Chapter 8</a>, <em>Designing for Failure</em>. If we retrieved all the data from the bucket in a loop, as we would in the imperative programming style, then we would likely run out of memory and/or overwhelm the Lambda function and receive throttling errors.</p>
<p>Instead, we pull data through the stream. When downstream steps are ready for more work they pull the <kbd>next</kbd> piece of data. This triggers the generator function to paginate data from S3 when the program is ready for more data.</p>
<p>When storing events in the data lake bucket, Kinesis Firehose buffers the events until a maximum amount of time is reached or a maximum file size is reached. This buffering maximizes the write performance when saving the events. When transforming the data for these files, we delimited the events with an EOL character. Therefore, when we <kbd>get</kbd> a specific file, we leverage the Highland.js <kbd>split</kbd> function to stream each row in the file one at a time. The split function also supports <strong>backpressure</strong>.</p>
<p>For each event, we <kbd>invoke</kbd> the <kbd>function</kbd> specified in the command-line arguments. These functions are designed to listen for events from a Kinesis stream. Therefore, we must wrap each event in the Kinesis input format that these functions are expecting. This is one reason why we included the Kinesis metadata when saving the events to the data lake in the <em>Creating a data lake</em> recipe. To maximize throughput, we invoke the Lambda <em>asynchronously</em> with the <kbd>Event</kbd> InvocationType, provided that the payload size is within the limits. Otherwise, we invoke the Lambda <em>synchronously</em> with the <kbd>RequestReponse</kbd> InvocationType. We also leverage the Lambda <kbd>DryRun</kbd> feature so that we can see what events might be replayed before actually effecting the change.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Indexing the data lake</h1>
                
            
            
                
<p>A data lake is a crucial design pattern for providing cloud-native systems with an audit trail of all the events in a system and for supporting the ability to replay events. In the <em>Creating a data lake</em> recipe, we implemented the S3 component of the data lake that provides high durability. However, a data lake is only useful if we can find the relevant data. In this recipe, we will index all the events in Elasticsearch so that we can search events for troubleshooting and business analytics.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/data-lake-es --path cncb-data-lake-es</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-data-lake-es</kbd> directory with <kbd>cd cncb-data-lake-es</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-data-lake-es<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/><br/>plugins:<br/>  - <strong>elasticsearch</strong><br/><br/>functions:<br/>  <strong>transformer</strong>:<br/>    handler: handler.transform<br/>    timeout: 120<br/><br/>resources:<br/>  Resources:<br/>    <strong>Domain</strong>:<br/>      Type: AWS::Elasticsearch::Domain<br/>      Properties:<br/>        ...<br/><br/>    <strong>DeliveryStream</strong>:<br/>      Type: AWS::KinesisFirehose::DeliveryStream<br/>      Properties: <br/>        DeliveryStreamType: KinesisStreamAsSource      <br/>        KinesisStreamSourceConfiguration: <br/>          KinesisStreamARN: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>        ElasticsearchDestinationConfiguration: <br/>          DomainARN: <br/>            Fn::GetAtt: [ Domain, DomainArn ]<br/>          IndexName: events<br/>          IndexRotationPeriod: OneDay<br/>          TypeName: event<br/>          BufferingHints: <br/>            IntervalInSeconds: 60<br/>            SizeInMBs: 50<br/>          RetryOptions: <br/>            DurationInSeconds: 60<br/>          ...<br/>          ProcessingConfiguration: ${file(includes.yml):ProcessingConfiguration}<br/><br/>    Bucket:<br/>      Type: AWS::S3::Bucket<br/><br/>    ...<br/><br/>  Outputs:<br/>    ...<br/>    DomainEndpoint:<br/>      Value:<br/>        Fn::GetAtt: [ Domain, DomainEndpoint ]<br/>    <strong>KibanaEndpoint</strong>:<br/>      Value:<br/>        Fn::Join:<br/>          - ''<br/>          - - Fn::GetAtt: [ Domain, DomainEndpoint ]<br/>            - '/_plugin/kibana'<br/>    ...</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">exports.<strong>transform</strong> = (event, context, callback) =&gt; {<br/>  const output = event.records.map((record, i) =&gt; {<br/>    // store all available data<br/>    const uow = {<br/>      event: JSON.parse((Buffer.from(record.data, 'base64')).toString('utf8')),<br/>      kinesisRecordMetadata: record.kinesisRecordMetadata,<br/>      firehoseRecordMetadata: {<br/>        deliveryStreamArn: event.deliveryStreamArn,<br/>        region: event.region,<br/>        invocationId: event.invocationId,<br/>        recordId: record.recordId,<br/>        approximateArrivalTimestamp: record.approximateArrivalTimestamp,<br/>      }<br/>    };<br/><br/>    return {<br/>      recordId: record.recordId,<br/>      result: 'Ok',<br/>      data: Buffer.from(JSON.stringify(uow), 'utf-8').toString('base64'),<br/>    };<br/>  });<br/><br/>  callback(null, { records: output });<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack: </li>
</ol>
<p>Deploying an Elasticsearch domain can take upwards of 20 minutes.</p>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-data-lake-es@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-data-lake-es<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  transformer: cncb-data-lake-es-john-transformer<br/><br/>Stack Outputs<br/>DeliveryStream: cncb-data-lake-es-john-DeliveryStream-1ME9ZI78H3347<br/>DomainEndpoint: search-cncb-da-domain-5qx46izjweyq-oehy3i3euztbnog4juse3cmrs4.us-east-1.es.amazonaws.com<br/>DeliveryStreamArn: arn:aws:firehose:us-east-1:123456789012:deliverystream/cncb-data-lake-es-john-DeliveryStream-1ME9ZI78H3347<br/><strong>KibanaEndpoint</strong>: search-cncb-da-domain-5qx46izjweyq-oehy3i3euztbnog4juse3cmrs4.us-east-1.es.amazonaws.com/_plugin/kibana<br/>DomainArn: arn:aws:es:us-east-1:123456789012:domain/cncb-da-domain-5qx46izjweyq<br/>...</pre>
<ol start="9">
<li>Review the stack, function, and Elasticsearch domain in the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"thing-created"}'</strong><br/><br/>{<br/> "ShardId": "shardId-000000000000",<br/> "SequenceNumber": "49583655996852917476267785049074754815059037929823272962"<br/>}</pre>
<p>Allow the Firehose buffer time to process, as the buffer interval is 60 seconds.</p>
<ol start="11">
<li>Take a look at the <kbd>transformer</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px">$ sls logs -f transformer -r us-east-1 -s $MY_STAGE   </pre>
<ol start="12">
<li>Open Kibana using the preceding <kbd>KibanaEndpoint</kbd> output with protocol <kbd>https</kbd>.</li>
<li>Select the <kbd>Management</kbd> menu, set the index pattern to <kbd>events-*</kbd>, and press the <kbd>Next step</kbd> button.</li>
<li>Select <kbd>timestamp</kbd> as the <kbd>Time Filter field name</kbd>, and press <kbd>Create Index pattern</kbd>.</li>
<li>Select the <kbd>Discover</kbd> menu to view the current events in the index.</li>
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>The data lake is a valuable source of information. Elasticsearch is uniquely suited for indexing this coarse-grained time series information. <strong>Kibana</strong> is the data visualization plugin for Elasticsearch. Kibana is a great tool for creating dashboards containing statistics about the events in the data lake and to perform ad hoc searches to troubleshoot system problems based on the contents of the events.</p>
<p>In this recipe, we are using Kinesis Firehose because it performs the heavy lifting of writing the events to Elasticsearch. It provides buffering based on time and size, hides the complexity of the Elasticsearch bulk index API, provides error handling, and supports index rotation. In the custom <kbd>elasticsearch</kbd> Serverless plugin, we create the index template that defines the <kbd>index_patterns</kbd> and the <kbd>timestamp</kbd> field used to affect the index rotation.</p>
<p>This recipe defines one delivery stream, because in this cookbook, our stream topology consists of only one stream with <kbd>${cf:cncb-event-stream-${opt:stage}.streamArn}</kbd>. In practice, your topology will consist of multiple streams and you will define one Firehose delivery stream per Kinesis stream to ensure that all events are indexed.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing bi-directional synchronization</h1>
                
            
            
                
<p>Cloud-native systems are architectured to support the continuous evolution of the system. Upstream and downstream services are designed to be pluggable. New service implementations can be added without impacting related services. Furthermore, continuous deployment and delivery necessitate the ability to run multiple versions of a service side by side and synchronize data between the different versions. The old version is simply removed when the new version is complete and the feature is flipped on. In this recipe, we will enhance the <em>database-first</em> variant of the Event Sourcing pattern with the <em>latching</em> pattern to facilitate bi-directional synchronization without causing an infinite loop of events.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create two projects from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/bi-directional-sync --path cncb-<strong>1</strong>-bi-directional-sync<br/><br/>$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch2/bi-directional-sync --path cncb-<strong>2</strong>-bi-directional-sync</pre>
<ol start="2">
<li>Review the file named <kbd>serverless.yml</kbd> with the following content in each project:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-<strong>1</strong>-bi-directional-sync<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/>  environment:<br/>    <strong>SERVERLESS_PROJECT</strong>: ${self:service}<br/>    ...<br/><br/>functions:<br/>  <strong>command</strong>:<br/>    handler: handler.<strong>command</strong><br/>  <strong>trigger</strong>:<br/>    handler: handler.<strong>trigger</strong><br/>    events:<br/>      - stream:<br/>          type: dynamodb<br/>          ...<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          ...<br/>  <strong>query</strong>:<br/>    handler: handler.<strong>query</strong><br/><br/>resources:<br/>  Resources:<br/>    Table:<br/>      ...</pre>
<ol start="3">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>command</strong> = (request, context, callback) =&gt; {<br/>  const thing = {<br/>    id: uuid.v4(),<br/>    <strong>latch</strong>: '<strong>open</strong>',<br/>    ...request,<br/>  };<br/><br/>  ...<br/>  db.<strong>put</strong>(params, callback);<br/>};<br/><br/>module.exports.<strong>trigger</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .filter(<strong>forLatchOpen</strong>)<br/>    .map(<strong>toEvent</strong>)<br/>    .flatMap(publish)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>const <strong>forLatchOpen</strong> = e =&gt; e.dynamodb.NewImage.<strong>latch</strong>.S === '<strong>open</strong>';<br/><br/>const <strong>toEvent</strong> = record =&gt; ({<br/>  id: record.eventID,<br/>  ...<br/>  tags: {<br/>    region: record.awsRegion,<br/>    <strong>source</strong>: process.env.<strong>SERVERLESS_PROJECT</strong><br/>  },<br/>  thing: ...,<br/>});<br/><br/>...<br/><br/>module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>forSourceNotSelf</strong>)<br/>    .filter(forThingCrud)<br/>    .map(<strong>toThing</strong>)<br/>    .flatMap(put)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>forSourceNotSelf</strong> = e =&gt; e.tags.<strong>source</strong> != process.env.<strong>SERVERLESS_PROJECT</strong>;<br/>...<br/><br/>const <strong>toThing</strong> = event =&gt; ({<br/>  id: event.thing.new.id,<br/>  ...<br/>  <strong>latch</strong>: '<strong>closed</strong>',<br/>});<br/><br/>...</pre>
<ol start="4">
<li>Navigate to the <kbd>cncb-1-bi-directional-sync</kbd> directory with<kbd>cd cncb-1-bi-directional-sync</kbd>.</li>
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-1-bi-directional-sync@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-1-bi-directional-sync<br/>&gt; sls deploy -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  command: cncb-1-bi-directional-sync-john-command<br/>  trigger: cncb-1-bi-directional-sync-john-trigger<br/>  listener: cncb-1-bi-directional-sync-john-listener<br/>  query: cncb-1-bi-directional-sync-john-query</pre>
<ol start="9">
<li>Review the stack, functions, and table in the AWS Console.</li>
<li>Navigate to the <kbd>cncb-2-bi-directional-sync</kbd> directory with <kbd>cd cncb-2-bi-directional-sync</kbd>.</li>
<li>Repeat steps 5-9 for the <kbd>cncb-2-bi-directional-sync</kbd> project.</li>
<li>Navigate back to the <kbd>cncb-1-bi-directional-sync</kbd> directory with <kbd>cd cncb-1-bi-directional-sync</kbd>.</li>
<li>Invoke the <kbd>command</kbd> function to save data to the first service:</li>
</ol>
<pre style="padding-left: 30px">$ sls invoke -r us-east-1 -f command -s $MY_STAGE -d '{"id":"77777777-7777-7777-7777-777777777777","name":"thing seven"}'</pre>
<ol start="14">
<li>Take a look at the logs for the <kbd>command</kbd> and <kbd>trigger</kbd> functions: </li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f command -r us-east-1 -s $MY_STAGE</strong><br/><br/>START ...<br/>2018-04-24 02:02:11 ... event: {"id":"77777777-7777-7777-7777-777777777777","name":"thing seven"}<br/>2018-04-24 02:02:11 ... params: {"TableName":"john-cncb-1-bi-directional-sync-things","Item":{"id":"77777777-7777-7777-7777-777777777777","<strong>latch</strong>":"<strong>open</strong>","name":"thing seven"}}<br/>END ...<br/>REPORT ... Duration: 146.90 ms    Billed Duration: 200 ms ... Max Memory Used: 40 MB    <br/><br/><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong><br/>START ...<br/>2018-04-24 02:02:13 ... event: {"Records":[{"eventID":"494ec22686941c0d5ff56dee86df47dd","eventName":"INSERT",...,"Keys":{"id":{"S":"77777777-7777-7777-7777-777777777777"}},"NewImage":{"name":{"S":"thing seven"},"id":{"S":"77777777-7777-7777-7777-777777777777"},"<strong>latch</strong>":{"S":"<strong>open</strong>"}},...},...}]}<br/>2018-04-24 02:02:13 ... {"id":"494ec22686941c0d5ff56dee86df47dd","type":"thing-created",...,"tags":{"region":"us-east-1","<strong>source</strong>":"<strong>cncb-1-bi-directional-sync</strong>"},"thing":{"new":{"name":"thing seven","id":"77777777-7777-7777-7777-777777777777","<strong>latch</strong>":"<strong>open</strong>"}}}<br/>...<br/>END ...<br/>REPORT ... Duration: 140.20 ms    Billed Duration: 200 ms ... Max Memory Used: 35 MB    </pre>
<ol start="15">
<li>Navigate to the <kbd>cncb-2-bi-directional-sync</kbd> directory with <kbd>cd cncb-2-bi-directional-sync</kbd></li>
<li>Take a look at the logs for the <kbd>listener</kbd> and <kbd>trigger</kbd> functions: </li>
</ol>
<pre style="padding-left: 30px">$ sls logs -f listener -r us-east-1 -s $MY_STAGE<br/><br/>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</pre>
<ol start="17">
<li>Invoke the <kbd>query</kbd> function to retrieve the synchronized data to the second service:</li>
</ol>
<pre style="padding-left: 30px">$ sls invoke -r us-east-1 -f query -s $MY_STAGE -d 77777777-7777-7777-7777-777777777777</pre>
<ol start="18">
<li>Remove both stacks once you have finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>Cloud-native systems are architected to evolve. Over time, the functional requirements will change and the technology options will improve. However, some changes are not incremental and/or do not support an immediate switch from one implementation to another. In these cases, it is necessary to have multiple versions of the same functionality running simultaneously. If these services produce data, then it is necessary to synchronize data changes between the services. This bi-directional synchronization will produce an infinite messaging loop if an appropriate <strong>latching</strong> mechanism is not employed.</p>
<p>This recipe builds on the database-first variant of the Event Sourcing pattern. A user of service one invokes the command function. The <kbd>command</kbd> function opens the <kbd>latch</kbd> by setting the latch on the domain object to <kbd>open</kbd>. The <kbd>trigger</kbd> function's <kbd>forLatchOpen</kbd> filter will only allow publishing an event when the latch is <kbd>open</kbd>, because the open latch indicates that the change originated in service one. The <kbd>listener</kbd> function's <kbd>forSourceNotSelf</kbd> filter in service one ignores the event because the <kbd>source</kbd> tag indicates that the event originates from service one. The <kbd>listener</kbd> function in service two closes the <kbd>latch</kbd> before saving the data by setting the latch on the domain object to <kbd>closed</kbd>. The <kbd>trigger</kbd> function in service two does not publish an event, because the <kbd>closed</kbd> latch indicates that the change did not originate in service two.</p>
<p>This same flow unfolds when the command originates in service two. You can add a third and fourth service and more, and all the services will remain in sync.</p>


            

            
        
    </body></html>