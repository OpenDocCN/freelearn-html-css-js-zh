<html><head></head><body>
        

                            
                    <h1 class="header-title">Continuous Integration</h1>
                
            
            
                
<p>In the previous chapters, we adopted a <strong>Test-Driven Development</strong> (<strong>TDD</strong>) approach to developing a backend API server, which exposes a user directory platform. However, there are still many areas for improvement in our workflow:</p>
<ul>
<li>We are running tests on our local, development environment, which may contain artifacts that lead to inaccurate test results</li>
<li>Carrying out all these steps manually is slow and error-prone</li>
</ul>
<p>In this chapter, we are going to eliminate these two issues by integrating with a <strong>Continuous Integration</strong> server. In essence, a CI server is a service that watches for changes in your repository, and then automatically runs the test suite inside a clean environment. This ensures the test results are more deterministic and repeatable. In other words, it prevents situations where something works on one person's machine but not another's.</p>
<p>By following this chapter, you will:</p>
<ul>
<li>Understand what CI is</li>
<li>Integrate our GitHub repository with a hosted CI platform called <strong>Travis</strong></li>
<li>Set up a self-hosted <strong>Jenkins</strong> server</li>
<li>Set up our test suite to run whenever a new change is pushed to GitHub</li>
<li>Understand <strong>pipelines,</strong> especially the difference between <strong>declarative</strong> and <strong>scripted pipelines</strong></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Continuous Integration (CI)</h1>
                
            
            
                
<p class="mce-root">On a large-scale project, you're going to have many developers working on many features, releases, hotfixes, and so on, at the same time. CI is the practice of <em>integrating</em> work from different developers continuously. This means merging code from feature branches into the <kbd>dev</kbd> branch, or from a release branch into <kbd>master</kbd>. At every integration point, there's a chance that the integration would cause something to break. Therefore, we must perform tests at these integration points, and only carry through with the integration if all tests pass.</p>
<p>We already do this in our current workflow, but it is done manually. By having automated builds and tests that detect errors in these integration points, it allows members of a software development team to integrate their work frequently.</p>
<p class="mce-root">By practicing CI, we can abide by the "test early, test often" mantra, and ensure bugs are identified and fixed as early as possible. It also means that at any point, we will always have a fully functional codebase that can be deployed.</p>
<p>We have already laid the groundwork for following this practice by using a robust Git workflow and having a comprehensive test suite. The next step is to introduce a CI server into the mix.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Picking a CI server</h1>
                
            
            
                
<p>There are many online CI services (such as <strong>Travis</strong>, <strong>CircleCI</strong>, <strong>Bamboo</strong>, and <strong>Shippable</strong>) as well as self-hosted CI-capable platforms (such as <strong>Jenkins</strong>, <strong>TeamCity</strong>, <strong>CruiseControl</strong>, and <strong>BuildBot</strong>). For CI, they pretty much have the same set of capabilities, and can perform the following tasks:</p>
<ul>
<li>Hook onto events and perform predefined tasks when triggered. For instance, when a new Git commit is pushed to the repository, the CI server would build the application and run the tests.</li>
<li>Run tasks in a clean, standalone environment.</li>
<li>Chain tasks together so that some tasks are triggered on the completion of the previous tasks. For instance, after the tests have finished, email the build and tests results to all developers.</li>
<li>Store history of the builds and test results.</li>
</ul>
<p>Since each CI platform is able to fulfill our requirements, our decision of which CI server to pick boils down to whether to use a hosted or self-hosted solution. As always, there're pros and cons to each approach:</p>
<ul>
<li><strong>Costs</strong>: Most hosted CI services are free for open source projects but require a paid plan for private repositories. However, hosting your own CI server also incurs costs of running the server.</li>
<li><strong>Self-Reliance</strong>: Relying on external services for your workflow means if the external service is down, your workflow will be broken. However, most hosted CI services have very good uptime, so availability should not be a huge issue.</li>
<li><strong>Flexibility</strong>: With a self-hosted solution, you have complete control over the CI server, and can extend the code and feature set with plugins or packages. On the other hand, if you require a feature that is not supported in a hosted CI server, you'd have to raise a support ticket/feature request and hope it will get implemented.</li>
</ul>
<p>In this chapter, we are going to demonstrate both the hosted and self-hosted solutions, using Travis and Jenkins, respectively. The majority of the chapter, however, will focus on Jenkins, as it is a much more powerful and generic automation server than Travis.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Integrating with Travis CI</h1>
                
            
            
                
<p>Travis is an online CI service that installs, builds, and tests our project. Travis is free for open source projects and integrates well with other popular services such as GitHub. There's also nothing to install—all we have to do is include a <kbd>.travis.yml</kbd> configuration file at the root of our repository, and configure the repository in Travis's web application. Travis has a very shallow learning curve and can save us a lot of time. To get started, go to <a href="https://travis-ci.org/">travis-ci.org</a> and sign in using your GitHub account.</p>
<p>Travis has two URLs <a href="https://travis-ci.org/">travis-ci.org</a>, which is used for open source projects, and <a href="https://travis-ci.com/">travis-ci.com</a>, which is used for private projects. Make sure you're using the right one.</p>
<p>It will ask you for many permissions; these permissions are required for Travis to do the following:</p>
<ul>
<li><strong>Read the contents of all repositories associated with your account</strong>: This allows Travis to view the content of the <kbd>.travis.yml</kbd> file, as well as to be able to clone your repository in order to build/test it.</li>
<li><strong>Install webhooks and services</strong>: This allows Travis to add hooks into your repositories, so that when any changes are pushed to your repository, GitHub can inform Travis and execute the instructions defined in the <kbd>.travis.yml</kbd> file.</li>
<li><strong>Register with the Commit Status API</strong>: This allows Travis to inform GitHub of the result of a build/test, so that GitHub can update its UI.</li>
</ul>
<p>After you've reviewed these permissions, click Authorize travis-ci:</p>
<div><img src="img/1e2d927e-fad4-4539-bde3-3bb2083570a7.png" style="width:31.58em;height:52.83em;"/></div>
<p>After the authorization step, you'll be brought back to the main Travis dashboard, where you can see every repository under your control:</p>
<div><img src="img/efc2755d-dff6-4c83-baa3-74532477e8f8.png"/></div>
<p>Here, we only have one project, which we should enable by clicking on the toggle button. This will make Travis install a GitHub service hook for that repository. Once installed, GitHub will send a message to Travis whenever changes are pushed to that repository.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring Travis CI</h1>
                
            
            
                
<p>Travis will now be notified of any changes in the repository, but we haven't provided it with instructions to execute once a change is detected. Therefore, at the root of our project directory, create a configuration file named <kbd>.travis.yml</kbd>.</p>
<p>Note the period (<kbd>.</kbd>) before <kbd>travis</kbd>, and also that the file extension is <kbd>yml</kbd>, not <kbd>yaml</kbd>.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Specifying the language</h1>
                
            
            
                
<p>In <kbd>.travis.yml</kbd>, we must first specify the primary language our project is written in. This allows Travis to install the required dependencies, and use appropriate default settings and configurations. For example, if we specify that our project is written in Node.js, Travis will, by default, configure itself to install dependencies by running <kbd>npm install</kbd>, and test the application by running <kbd>npm test</kbd>. It'll also look for a <kbd>yarn.lock</kbd> file at the root directory, and if it's present, use the <kbd>yarn install</kbd> and <kbd>yarn run test</kbd> commands instead.</p>
<p>Therefore, add the following line inside our <kbd>.travis.yml</kbd> file to inform Travis that this project uses Node.js:</p>
<pre>language: node_js</pre>
<p>With Node.js, you can also specify which version of Node.js (or io.js) you want the build and tests to run on. You can specify Node versions by their major, minor, and patch versions, and it will get the latest version that satisfies that criteria. You can also use the string <kbd>"node"</kbd> to get the latest stable Node.js release, or <kbd>"lts/*"</kbd> for the latest LTS Node.js release.</p>
<p>Since this is a server-side application, we have control over the environment our application is run in. Therefore, if we want to, we can run our test only against the Node.js version specified in the <kbd>.nvmrc</kbd> file (<kbd>8.11.4</kbd>). However, since this process is automated, and Travis can run these tests in parallel, the cost of running additional tests is very low. Therefore, we should run our tests against future Node.js versions; doing so will prevent deprecated syntax from being introduced into our project.</p>
<p>Therefore, update our <kbd>.travis.yml</kbd> to the following:</p>
<pre>language: node_js<br/>node_js:<br/>  - "node"<br/>  - "lts/*"<br/>  - "8"<br/>  - "8.11.4"</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up databases</h1>
                
            
            
                
<p>Our code also depends on a running instance of Elasticsearch; therefore, we need to specify this requirement in the <kbd>.travis.yml</kbd> file by adding a <kbd>services</kbd> property:</p>
<pre>services:<br/>  - elasticsearch</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This will install and start Elasticsearch on the Travis server instance using the default configuration (namely, port <kbd>9200</kbd>). However, it is advisable to run a specific version of Elasticsearch—the same version we are running locally—to ensure we get results that are consistent with our development environment. Therefore, below the <kbd>services</kbd> block, add the following <kbd>before_install</kbd> block:</p>
<pre>before_install:<br/>  - <strong>curl -O</strong> https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.deb<br/>  - <strong>sudo dpkg -i --force-confnew elasticsearch-6.3.2.deb</strong><br/>  -<strong> sudo service elasticsearch restart</strong></pre>
<p>The Elasticsearch service may take some time to start; therefore, we should also tell Travis to wait a few seconds before attempting to run the tests.</p>
<pre>before_script:<br/>  - <strong>sleep 10</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting environment variables</h1>
                
            
            
                
<p>Lastly, our application reads variables from the environment. Since the <kbd>.env</kbd> and <kbd>test.env</kbd> files are not included as part of our repository, we need to manually provide them to Travis. We can do this by adding an <kbd>env.global</kbd> block:</p>
<pre>env:<br/>  global:<br/>    - NODE_ENV=test<br/>    - SERVER_PROTOCOL=http<br/>    - SERVER_HOSTNAME=localhost<br/>    - SERVER_PORT=8888<br/>    - ELASTICSEARCH_PROTOCOL=http<br/>    - ELASTICSEARCH_HOSTNAME=localhost<br/>    - ELASTICSEARCH_PORT=9200<br/>    - ELASTICSEARCH_INDEX=test</pre>
<p>Our final <kbd>.travis.yml</kbd> should look like this:</p>
<pre>language: node_js<br/>node_js:<br/>  - "node"<br/>  - "lts/*"<br/>  - "8"<br/>  - "8.11.4"<br/>services:<br/>  - elasticsearch<br/>before_install:<br/>  - <strong>curl -O</strong> https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.deb<br/>  -<strong> sudo dpkg -i --force-confnew elasticsearch-6.3.2.deb</strong><br/>  -<strong> sudo service elasticsearch restart</strong><br/>before_script:<br/>  - <strong>sleep 10</strong><br/>env:<br/>  global:<br/>    - NODE_ENV=test<br/>    - SERVER_PROTOCOL=http<br/>    - SERVER_HOSTNAME=localhost<br/>    - SERVER_PORT=8888<br/>    - ELASTICSEARCH_PROTOCOL=http<br/>    - ELASTICSEARCH_HOSTNAME=localhost<br/>    - ELASTICSEARCH_PORT=9200<br/>    - ELASTICSEARCH_INDEX=test</pre>
<p>For more information about different fields in the <kbd>.travis.yml</kbd> file, check out <a href="https://docs.travis-ci.com/user/customizing-the-build/">docs.travis-ci.com/user/customizing-the-build/</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Activating our project</h1>
                
            
            
                
<p>Next, go to <a href="https://travis-ci.org">travis-ci.org</a> to ensure your project is activated:</p>
<div><img src="img/cd405fdf-8dd6-46bd-91ba-79e6123f3704.png" style="width:34.08em;height:5.42em;"/></div>
<p>Now, commit <kbd>.travis.yml</kbd> to the root of the project directory, and push the change to GitHub. The GitHub service hook will now notify Travis of the change, and Travis will clone the repository, build the application, and run the tests. After the tests are complete (or aborted in cases of error), it will show a report on the Travis dashboard. The results will also be shared with GitHub so that it can update its UI.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Examining Travis CI results</h1>
                
            
            
                
<p class="mce-root CDPAlignLeft CDPAlign">A Travis build will either pass or fail. If the build fails, it will be accompanied by a red cross:</p>
<div><img src="img/24ebb3ff-c1bd-4347-9f2b-528a3688de68.png" style="font-size: 1em;"/></div>
<p>By default, Travis will also send an email notifying us of the result of the build and tests:</p>
<div><img src="img/d98e8647-f081-4847-8cec-172c573eed1c.jpg"/></div>
<p class="mce-root"/>
<p>Travis also integrates with GitHub's Commit Status API (<a href="https://developer.github.com/v3/repos/statuses/">developer.github.com/v3/repos/statuses/</a>), which allows third parties to attach a status to commits. Here, Travis is attaching the failure state to the comment, which shows up as a red cross indicator next to the commit time:</p>
<div><img src="img/f8b3b2ad-3496-4c7c-887a-6c5b3db10eed.png" style="width:40.75em;height:4.00em;"/></div>
<p>But, most importantly, each run also saves the history of the logs, so in cases of error, the developers are able to pinpoint the issue and fix it quickly:</p>
<div><img src="img/7537f940-ef7b-4c27-8de0-441761843896.png" style="width:87.75em;height:54.75em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Continuous Integration with Jenkins</h1>
                
            
            
                
<p>Now you know how to integrate with Travis CI, and know what you can expect from a CI server, let's try to replicate the same results using Jenkins, a self-hosted alternative. We have chosen Jenkins here because, at the time of writing, it is the most popular CI tool, with over 1 million users and 150,000 installs.</p>
<p>First, we will give you a brief introduction to Jenkins, and then we'll install and integrate it with our repository.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction to Jenkins</h1>
                
            
            
                
<p>While Travis is purely a CI server, Jenkins is much more powerful. Generally speaking, Jenkins is an open source <strong>automation server</strong>. This means it can automate any processes that are tricky to do by hand, either because it is repetitive, time-consuming, prone to human errors, or all of the above. For example, we can use Jenkins for the following:</p>
<ul>
<li>Building/packaging applications</li>
<li>Dynamically generating documentation</li>
<li>Running pre-deployment E2E/integration/unit/UI tests</li>
<li>Deployment onto various testing environments (for example, development, staging)</li>
<li>Running post-deployment tests</li>
<li>Deployment onto the production environment</li>
</ul>
<p>Furthermore, these processes can be chained together to form workflows, where the execution of one process depends on the result of the previous process. There are two ways of configuring these automated workflows—as <strong>freestyle projects</strong>, or as <strong>pipelines</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Freestyle projects</h1>
                
            
            
                
<p>Freestyle projects (a.k.a. <strong>jobs</strong>, or simply <strong>projects</strong>) were the original method for which all automated tasks must be defined in Jenkins. A freestyle project is simply a set of user-defined tasks that Jenkins should perform. For example, a project may involve building an application from a Git repository, while another project is used to run tests on this built application.</p>
<p>The terms <strong>freestyle project</strong>, <strong>project</strong>, and <strong>job</strong> are synonymous with each other. The term <strong>job</strong> is commonly used in the UI of the web interface, but it has been deprecated and we will use the term <strong>project</strong> in this book.</p>
<p>You can configure a freestyle project using the web interface, which allows you to define the following:</p>
<ul>
<li><strong>Source Code Management</strong> (<strong>SCM</strong>): Specifies how Jenkins can obtain the starting source code for it to build/test.</li>
<li><strong>Build triggers</strong>: Specifies when this project should execute. For example, you may want to trigger a build when a new commit is pushed to the repository; or build the project every night at 00:00 to produce the nightly build.</li>
<li><strong>Build environment</strong>.</li>
<li><strong>Build</strong>: Allows you to specify build steps. Despite its name, you can actually run any shell command, such as test runners, as a build step.</li>
<li><strong>Post-build action</strong>: Allows you to specify commands to execute after the build steps have been completed. You can, for instance, send the test results to the system administrator via email. Furthermore, you can use the post-build action to trigger another project to execute. This way, you can form a chain of projects that run one after another.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Pipeline</h1>
                
            
            
                
<p>Freestyle projects are powerful and have been the status quo for many years. However, it is found lacking in several areas:</p>
<ul>
<li>When Hudson, Jenkins' predecessor, was written, using a UI for configuration was the norm. However, in the last few years, the ecosystem has moved towards <strong>Configuration-as-Code</strong> (<strong>CaC</strong>), where the configuration can be tracked in source control.</li>
<li>Jenkins saves the configurations files for freestyle projects on the Jenkins server under <kbd>/var/lib/jenkins/jobs/</kbd>. This means if the Jenkins server is destroyed, all the configuration settings would be lost. Furthermore, the configuration file is written in XML and is hard to read.</li>
<li>While it is possible to chain multiple freestyle projects together using post-build actions, you are likely to end up with a lot of duplicate projects, each with different post-build action steps.</li>
</ul>
<p>To address these issues, Jenkins 2.0 came with a feature called <strong>P</strong><strong>ipeline</strong>, which allow you to do the following:</p>
<ul>
<li>Instead of linking multiple freestyle projects together via post-action build steps, you can, with Pipeline, specify many sequential <strong>steps</strong>, which can optionally be grouped into <strong>stages</strong>. In a Pipeline, the execution of a downstream step/stage depends on the outcome of the previous step/stage in the chain. Only when the previous steps are successful will the subsequent steps be run. For example, if the tests did not pass, then the deployment step would not run.</li>
<li>Allows you to specify steps using a <kbd>Jenkinsfile</kbd>: A configuration file that is part of your codebase. This CaC (or "pipeline as code") approach means all changes made to the pipeline can be tracked in Git, Pipelines can be branched and merged, and any broken pipelines can be reverted back to the last-known-good version. Furthermore, even if the Jenkins server is corrupt, the configuration will still survive as the <kbd>Jenkinsfile</kbd> is stored in the repository, not the Jenkins server; this also means that you can build the project using any Jenkins server that has access to the repository.</li>
</ul>
<p>Note that you can still define your Pipeline using the Jenkins web UI, although using a Jenkinsfile checked into your Git repository is the recommended approach.</p>
<p>The pipeline feature is enabled by the pipeline plugin, which is installed by default. To define a pipeline, you have to write in a pipeline <strong>Domain Specific Language</strong> (<strong>DSL</strong>) syntax and save it inside a text file named <kbd>Jenkinsfile</kbd>. A simple <kbd>Jenkinsfile</kbd> looks like this:</p>
<pre class="mce-root">pipeline {<br/>  agent { docker 'node:6.3' }<br/>  stages {<br/>    stage('build') {<br/>      steps {<br/>        sh 'npm --version'<br/>      }<br/>    }<br/>  }<br/>}</pre>
<p class="mce-root"/>
<p>For the remainder of this chapter, we will focus on using Jenkins to replicate the functions of Travis CI, specifically the following:</p>
<ul>
<li>Integrate with GitHub so that a message will be sent to our Jenkins server whenever changes are pushed to our project repository</li>
<li>Whenever Jenkins receives that message, it will check out the source code and run the tests inside a clean and isolated environment</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up a new Jenkins server</h1>
                
            
            
                
<p>With the Travis-GitHub integration, when GitHub detects a change on any branches in the repository, it will send a message to Travis's server(s), which will clone the repository, build it, and run the tests. Therefore, to replicate this behavior with Jenkins, we must set up a Jenkins CI service to receive GitHub's messages and run our tests.</p>
<p>We can run our Jenkins server on the same machine as our API server. However, if our Jenkins job somehow crashes the machine, it will cause our API server to go down as well. Therefore, it's much safer to deploy Jenkins CI on its own separate server.</p>
<p>Therefore, go to your VPS provider (we'll use DigitalOcean here) and provision a new VPS server. The Jenkins server uses around 600 MB of memory when idle; therefore, choose a VPS with at least 2 GB of memory.</p>
<p>If you forgot how to set up and provision a new VPS, refer back to <a href="673a49d6-f4c5-47b4-afec-af3ff031f150.xhtml" target="_blank">Chapter 10</a>, <em>Deploying Your Application on a VPS</em>.<br/>
<br/>
Also, since we have an SSH key pair already, we can simply select that SSH key to be used for this VPS, without having to manually upload our SSH key onto the server.</p>
<div><h1 class="header-title">Creating the jenkins user</h1>
                
            
            
                
<p>Once you have a VPS running, create a user called <kbd>jenkins</kbd> with <kbd>sudo</kbd> privileges:</p>
<pre>root@ci:# <strong>adduser jenkins</strong><br/>root@ci:# <strong>usermod -aG sudo jenkins</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Then, to allow us to log into the server as the power-restricted user <kbd>jenkins</kbd> and not <kbd>root</kbd>, we must first add the public key of our development machine to <kbd>/home/jenkins/.ssh/authorized_keys</kbd>; the easiest way to do that is to copy the <kbd>/root/.ssh/</kbd> directory and change its owner:</p>
<pre>root@ci:# <strong>cp -R /root/.ssh/ /home/jenkins/</strong><br/>root@ci:# <strong>chown -R jenkins:jenkins /home/jenkins/.ssh/</strong></pre>
<p class="mce-root">Then, disable password authentication and root login by editing <kbd>/etc/ssh/sshd_config</kbd>:</p>
<pre>PermitRootLogin <strong>no</strong><br/>PasswordAuthentication <strong>no</strong></pre>
<p>Reload the SSH daemon for the new settings to take effect:</p>
<pre>root@ci:# <strong>systemctl reload ssh.service</strong></pre>
<p>On a new Terminal, try logging in using the <kbd>jenkins</kbd> user. Once that's done, continue the rest of the setup as <kbd>jenkins</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring time</h1>
                
            
            
                
<p>Next, let's configure the timezone and NTP synchronization:</p>
<pre>jenkins@ci:# <strong>sudo dpkg-reconfigure tzdata</strong><br/>jenkins@ci:# <strong>sudo apt update</strong><br/>jenkins@ci:# <strong>sudo apt install ntp</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Java</h1>
                
            
            
                
<p>Then, we need to install and configure Java (replace <kbd>java-8-openjdk-amd64</kbd> with your version of Java):</p>
<pre>jenkins@ci:# <strong>sudo apt update &amp;&amp; sudo apt install -y openjdk-8-jdk</strong><br/>jenkins@ci:# <strong>echo 'JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"' | sudo tee --append /etc/environment &gt; /dev/null</strong></pre>
<p>At the time of this writing, Jenkins work best with Java 8. Java 10 and 11 support are still experimental (see <a href="https://jenkins.io/blog/2018/06/17/running-jenkins-with-java10-11/">jenkins.io/blog/2018/06/17/running-jenkins-with-java10-11/</a>).  This is why we are using the <kbd>openjdk-8-jdk</kbd> package instead of <kbd>default-jdk</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Jenkins</h1>
                
            
            
                
<p class="mce-root">There are two main versions of Jenkins:</p>
<ul>
<li class="mce-root"><strong>Weekly</strong>: Released every week.</li>
<li class="mce-root"><strong>Long-term Support</strong> (<strong>LTS</strong>): Released every 12 weeks. The Jenkins team picks the most stable release from the last time an LTS was released, and designates that as the next LTS version.</li>
</ul>
<p class="mce-root">For enterprise platforms, we want the most recent and stable version; therefore, we will install the latest LTS version, which is currently <kbd>2.138.1</kbd>.</p>
<p class="mce-root">There are many ways to install Jenkins, listed as follows:</p>
<ul>
<li>It is distributed as a <strong>Web Application ARchive</strong> (<strong>WAR</strong>), or <kbd>.war</kbd>, file, which is simply a collection of resources that, together, constitute a web application. A WAR file is how web applications written in Java are distributed; any operating system that supports Java would be able to run the WAR file. You can download it from <a href="http://mirrors.jenkins.io/war-stable/latest/jenkins.war">mirrors.jenkins.io/war-stable/latest/jenkins.war</a> and run it directly with <kbd>java -jar jenkins.war --httpPort=8765</kbd>. It'll then be available on port <kbd>8765</kbd>.</li>
<li>As a Docker container, which you can download from <a href="https://hub.docker.com/r/jenkins/jenkins/">hub.docker.com/r/jenkins/jenkins/.</a></li>
<li>As a distribution-specific package—different operating systems also maintain their own Jenkins package in their repository. Jenkins packages from the most common systems, including Ubuntu/Debian, Red Hat/Fedora/CentOS, Windows, and macOS, are maintained by the Jenkins team.</li>
</ul>
<p class="mce-root">Ideally, we would run our Jenkins server (and everything else for that matter) inside isolated Docker containers, however, that requires an understanding of containers and Docker, which will be overwhelming to learn alongside Jenkins. Therefore, in this chapter, we will use the Jenkins package provided by the APT repositories, and you can migrate to using Docker after reading <a href="d336b08b-f67f-4789-8194-c65d7aa3decc.xhtml" target="_blank">Chapter 17</a>, <em>Migrating to Docker</em>.</p>
<p class="mce-root">First, get the public key for the Jenkins repository and add it to APT; this allows APT to verify the authenticity of the package:</p>
<pre class="mce-root">jenkins@ci:$ <strong>wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add -</strong></pre>
<p class="mce-root"/>
<p class="mce-root">Next, we need to add the Jenkins repository to the list of repositories that APT will search for. This list is stored at <kbd>/etc/apt/sources.list</kbd>, as well as in files within the <kbd>/etc/apt/sources.list.d/</kbd> directory. Therefore, run the following command, which will create a new <kbd>jenkins.list</kbd> file and store the repository address inside:</p>
<pre class="mce-root">jenkins@ci:$ <strong>echo 'deb https://pkg.jenkins.io/debian-stable binary/' | sudo tee /etc/apt/sources.list.d/jenkins.list</strong></pre>
<p class="mce-root">Lastly, update our local package index and install Jenkins:</p>
<pre class="mce-root">jenkins@ci:$ <strong>sudo apt update &amp;&amp; sudo apt -y install jenkins</strong></pre>
<p class="mce-root">The installation will do several things, as follows:</p>
<ol>
<li class="mce-root">Download the WAR file and place it at <kbd>/usr/share/jenkins</kbd></li>
<li>Create a new user called <kbd>jenkins</kbd> which will run the service</li>
<li class="mce-root">Set up Jenkins as a service/daemon that runs when the system first starts</li>
<li>Create a <kbd>/var/log/jenkins/jenkins.log</kbd> file and direct all output from Jenkins to this file</li>
</ol>
<p>You can check the status of the Jenkins service by running <kbd>sudo systemctl status jenkins.service</kbd>.</p>
<p>Jenkins runs as a service in the background. It utilizes the Jetty server (<a href="http://www.eclipse.org/jetty/">eclipse.org/jetty/</a>) to provide a web interface for users to interact with. By default, this server will bind to port <kbd>8080</kbd>.</p>
<div><kbd>8080</kbd> is a very common port. If you're running Jenkins on an existing server where port <kbd>8080</kbd> is bound by another process, you can change Jenkins' default port by editing the <kbd>HTTP_PORT</kbd> entry inside Jenkins' configuration file —<kbd>/etc/default/jenkins</kbd>. To put this change into effect, make sure you run <kbd>sudo systemctl restart jenkins.service</kbd>.</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing NGINX as a reverse proxy</h1>
                
            
            
                
<p>Now, if you go to <kbd>http://&lt;server-ip&gt;:8080</kbd> on your browser, you'll see the Jenkins setup screen. But ideally, we want to use a human-friendly hostname. So, just as we did with our API server, let's install NGINX to reverse proxy requests from <kbd>jenkins.hobnob.social</kbd> to <kbd>http://localhost:8080</kbd>:</p>
<pre>jenkins@ci:$ <strong>echo "deb http://nginx.org/packages/ubuntu/ bionic nginx" | sudo tee -a /etc/apt/sources.list.d/nginx.list</strong><br/>jenkins@ci:$ <strong>echo "deb-src http://nginx.org/packages/ubuntu/ bionic nginx" | sudo tee -a /etc/apt/sources.list.d/nginx.list</strong><br/>jenkins@ci:$ <strong>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys ABF5BD827BD9BF62</strong><br/>jenkins@ci:$ <strong>sudo apt update</strong><br/>jenkins@ci:$ <strong>sudo apt install nginx</strong><br/>jenkins@ci:$ <strong>sudo mkdir /etc/nginx/sites-available /etc/nginx/sites-enabled</strong></pre>
<p>Then, in the <kbd>/etc/nginx/nginx.conf</kbd> file, add a line after <kbd>include /etc/nginx/conf.d/*.conf;</kbd>:</p>
<pre>include /etc/nginx/conf.d/*.conf;<br/><strong>include /etc/nginx/sites-enabled/*;</strong></pre>
<p>Next, create a configuration file for Jenkins at <kbd>/etc/nginx/sites-available/jenkins.hobnob.social</kbd> and paste in the following content:</p>
<pre>server {<br/>    listen 80 default_server;<br/>    <strong>server_name jenkins.hobnob.social;</strong><br/>    root /var/cache/jenkins/war;<br/>    access_log /var/log/nginx/jenkins/access.log;<br/>    error_log /var/log/nginx/jenkins/error.log;<br/>    ignore_invalid_headers off;<br/>    location ~ "^/static/[0-9a-fA-F]{8}\/(.*)$" {<br/>      rewrite "^/static/[0-9a-fA-F]{8}\/(.*)" /$1 last;<br/>    }<br/>    location /userContent {<br/>      root /var/lib/jenkins/;<br/>      if (!-f $request_filename){<br/>        rewrite (.*) /$1 last;<br/>        break;<br/>      }<br/>      sendfile on;<br/>    }<br/>    <strong>location @jenkins</strong> {<br/>      sendfile off;<br/>      <strong>proxy_pass http://localhost:8080;</strong><br/><strong>      proxy_redirect default;</strong><br/>      proxy_http_version 1.1;<br/><br/>      proxy_set_header Host $host;<br/>      proxy_set_header X-Real-IP $remote_addr;<br/>      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;<br/>      proxy_set_header X-Forwarded-Proto $scheme;<br/>      proxy_max_temp_file_size 0;<br/><br/>      client_max_body_size 10m;<br/>      client_body_buffer_size 128k;<br/><br/>      proxy_connect_timeout 90;<br/>      proxy_send_timeout 90;<br/>      proxy_read_timeout 90;<br/>      proxy_buffering off;<br/>      proxy_request_buffering off;<br/>      proxy_set_header Connection "";<br/>    }<br/>    <strong>location /</strong> {<br/>      try_files $uri <strong>@jenkins</strong>;<br/>    }<br/>}</pre>
<p>This configuration is taken from <a href="https://wiki.jenkins.io/display/JENKINS/Running+Jenkins+behind+Nginx">wiki.jenkins.io/display/JENKINS/Running+Jenkins+behind+Nginx</a>. The most pertinent parts are highlighted in preceding bold.</p>
<p>When a request comes in for <kbd>jenkins.hobnob.social</kbd>, it will match the <kbd>location /</kbd> block, which then proxies the request to the service running at the <kbd>proxy_pass</kbd> directive (<kbd>http://localhost:8080</kbd>). Likewise, when the internal service returns with a response, the <kbd>proxy_redirect</kbd> directive will rewrite the <kbd>Location</kbd> header of the response and replace <kbd>http://localhost:8080</kbd> with <kbd>http://jenkins.hobnob.social</kbd>.</p>
<p>Now that our server block is ready, add it to the <kbd>/etc/nginx/sites-enabled/</kbd> directory using a symbolic link:</p>
<pre>jenkins@ci:$ <strong>sudo ln -s /etc/nginx/sites-available/jenkins.hobnob.social /etc/nginx/sites-enabled/</strong></pre>
<p>Lastly, make sure our NGINX configuration does not contain any syntax errors, and start it:</p>
<pre>jenkins@ci:$ <strong>sudo nginx -t</strong><br/>jenkins@ci:$ <strong>sudo systemctl start nginx.service</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring the firewall</h1>
                
            
            
                
<p>To complete our NGINX setup, configure the firewall to ensure traffic can reach port <kbd>80</kbd>:</p>
<pre>jenkins@ci:$ <strong>sudo ufw allow OpenSSH</strong><br/>jenkins@ci:$ <strong>sudo ufw allow 80/tcp</strong><br/>jenkins@ci:$ <strong>sudo ufw enable</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Updating our DNS records</h1>
                
            
            
                
<p>Now, our Jenkins server should be available on port <kbd>80</kbd>, but we are still accessing our server via an IP address. Therefore, the next step is to configure our DNS records to direct traffic destined for <kbd>jenkins.hobnob.social</kbd> to our VPS.</p>
<p>On DigitalOcean, go to the Networking tab at the top and add a new <kbd>A</kbd> record pointing the hostname <kbd>jenkins.hobnob.social</kbd> to our VPS instance:</p>
<div><img src="img/0b410a9c-3ad0-4986-b9ce-ff1810a8b961.png"/></div>
<p>Now, our Jenkins server instance should be available at <kbd>jenkins.hobnob.social</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Configuring Jenkins</h1>
                
            
            
                
<p class="mce-root">Now, we are ready to configure Jenkins. Navigate to <kbd>jenkins.hobnob.social</kbd> on your browser; there you'll see a setup wizard.</p>
<p class="mce-root">When Jenkins was installed, a password was written to a file at <kbd>/var/lib/jenkins/secrets/initialAdminPassword</kbd>, which only the system administrator (or users with <kbd>sudo</kbd> privileges) will have access to. This is to ensure that the person accessing the setup wizard is the system administrator and not some malicious party.</p>
<p class="mce-root">Therefore, the first step is to copy the contents of the <kbd>/var/lib/jenkins/secrets/initialAdminPassword</kbd> file and paste it into the wizard:</p>
<div><img src="img/99437f2c-3e0b-4a05-a581-42e5dce10684.png"/></div>
<p class="mce-root"/>
<p class="mce-root">On the next screen, you'll be presented with the Customize Jenkins screen, where you can choose to install <strong>plugins</strong>. Jenkins, on its own, is just a platform that enables automation and has few features itself. Its functionalities are modularized into plugins. There are over 1,300 plugins, including integration with the following:</p>
<ul>
<li>Version control systems</li>
<li>Bug databases</li>
<li>Build tools</li>
<li>Testing frameworks</li>
</ul>
<p class="mce-root">Pick Install suggested plugins to install the most commonly used plugins, including the Git and GitHub plugins we will use later. You can track the progress of the installation on the next screen:</p>
<div><img src="img/58b808c2-b01c-4fa7-9d6b-9d787011c4b1.png"/></div>
<p class="mce-root">Lastly, you'll be prompted to create an administrative user for the web interface, which you'll use to continue the setup process (so remember your username and password!).</p>
<p class="mce-root">Great, now we will have successfully installed Jenkins and have it running on a public URL:</p>
<div><img src="img/3cbb07f9-d80f-408a-853f-07e1585365f5.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Composing a Jenkinsfile</h1>
                
            
            
                
<p>Now that we have set up our Jenkins instance, we are ready to define our Pipeline using the Pipeline DSL. Let's take a look at the Pipeline DSL syntax.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Pipeline DSL syntax</h1>
                
            
            
                
<p>There are a number of global variables, keywords, and directives that can be used inside any Pipeline, for example:</p>
<ul>
<li><kbd>env</kbd>: Environment variables</li>
<li><kbd>params</kbd>: Parameters set when configuring the pipeline</li>
<li><kbd>currentBuild</kbd>: Information about the current build, such as results, display name, and so on</li>
</ul>
<p>The complete Global Variables list can be found at <kbd>/pipeline-syntax/globals</kbd>.</p>
<p>There are keywords which are available only inside steps. For example, the <kbd>sh</kbd> keyword allows you to specify some arbitrary shell command to run, and you can use <kbd>echo</kbd> to print something into the console output.</p>
<p>The DSL syntax can also be extended. For example, the JUnit plugin adds the <kbd>junit</kbd> step to the Pipeline vocabulary, which allows your step to aggregate test reports. In this chapter, we will use the Docker Pipeline plugin, which adds a <kbd>docker</kbd> keyword to run our tests inside a Docker container. More on this later.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Declarative versus scripted pipelines</h1>
                
            
            
                
<p class="mce-root">There are two syntaxes to define Pipelines—declarative and scripted. Originally, the Pipeline plugin supported only scripted pipelines, but Declarative Pipeline syntax 1.0 was added in February 2017 with Pipeline 2.5. Both of these syntaxes use the same underlying execution engine to execute instructions.</p>
<p>A Scripted Pipeline allows you to define your instructions using a full-featured programming language called <strong>Groovy</strong>; because of this, you are able to be very expressive. The downside is that the code may be less understandable, and thus less maintainable.</p>
<p>The Declarative Pipeline syntax brings structure to the Pipeline, which means it's easier to check the file for syntax errors, provide linting help. But with Declarative Pipelines, you can only define instructions that are supported by the syntax.</p>
<p>Therefore, you should use the Declarative Pipeline syntax <em>wherever possible</em>, and fall back to the Scripted Pipelines only when there are instructions that cannot be achieved using a Declarative Pipeline.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The declarative pipeline</h1>
                
            
            
                
<p>Every declarative pipeline must start with the <kbd>pipeline</kbd> directive. Within the <kbd>pipeline</kbd> directive are, usually, the <kbd>agent</kbd>, <kbd>stages</kbd>, and <kbd>step</kbd> directives.</p>
<p>The <kbd>agent</kbd> directive tells Jenkins to allocate an executor and workspace for this part of the Pipeline. A workspace is simply a directory in the filesystem where Jenkins can work with the files to run the build, and an executor is simply a thread that executes the task. When you use the <kbd>agent</kbd> directive, it will also download the source repository and save it to the workspace, so that the code is available for subsequent stages.</p>
<p>A typical declarative pipeline might look like this:</p>
<pre>#!/usr/bin/env groovy<br/><br/>pipeline {<br/>    agent {<br/>        docker {<br/>            image 'node'<br/>            args '-u root'<br/>        }<br/>    }<br/><br/>    stages {<br/>        stage('Build') {<br/>            steps {<br/>                echo 'Building...'<br/>                sh 'npm install'<br/>            }<br/>        }<br/>        stage('Test') {<br/>            steps {<br/>                echo 'Testing...'<br/>                sh 'npm test'<br/>            }<br/>        }<br/>    }<br/>}</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">The scripted pipeline</h1>
                
            
            
                
<p>A declarative pipeline must be defined within a <kbd>pipeline</kbd> directive that includes an <kbd>agent</kbd> directive; for scripted pipelines, the Pipeline must be enclosed within the <kbd>node</kbd> directive.</p>
<p class="mce-root"/>
<p>The <kbd>node</kbd> directive in a Scripted Pipeline is similar to the <kbd>agent</kbd> directive in the Declarative Pipeline, and allocates an executor and workspace for the pipeline. Unlike the <kbd>agent</kbd> directive, the node will not automatically download the source repository and save it to your workspace; instead, you have to specify that manually using the <kbd>checkout scm</kbd> step:</p>
<pre>node {<br/>    checkout scm<br/>}</pre>
<div><kbd>scm</kbd> is a special variable that represents the version of the repository that triggered the build.</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up the environment</h1>
                
            
            
                
<p>To ensure that the build and test steps execute consistently, we should run them inside a <strong>container</strong>, which is an ephemeral, pre-configured, isolated environment.</p>
<p>A container is similar to a virtual machine, but uses fewer resources and is quicker to provision. Creating containers is cheap; this allows us to create containers, run the tests, and then discard them afterward.</p>
<p>We will dive more in-depth into Docker in <a href="d336b08b-f67f-4789-8194-c65d7aa3decc.xhtml" target="_blank">Chapter 17</a>, <em>Migrating to Docker</em>; for now, it's sufficient to understand that a Docker container provides an isolated and consistent environment for us to run our builds and tests.</p>
<p>Docker is the most popular container framework out there, and we will run our builds and tests inside Docker containers. In your repository, add the following Scripted Pipeline into a <kbd>Jenkinsfile</kbd> file:</p>
<pre>node {<br/>    checkout scm<br/>    docker.image('docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2').withRun('-e "discovery.type=single-node"') { c -&gt;<br/>        docker.image('node:8.11.4').inside("--link ${c.id}:db") {<br/>            withEnv(['SERVER_HOSTNAME=db',<br/>                     'JENKINS=true',<br/>                     'NODE_ENV=test',<br/>                     'SERVER_PROTOCOL=http',<br/>                     'SERVER_HOSTNAME=localhost',<br/>                     'SERVER_PORT=8888',<br/>                     'ELASTICSEARCH_PROTOCOL=http',<br/>                     'ELASTICSEARCH_HOSTNAME=localhost',<br/>                     'ELASTICSEARCH_PORT=9200',<br/>                     'ELASTICSEARCH_INDEX=test']) {<br/>              stage('Waiting') {<br/>                sh 'until curl --silent $DB_PORT_9200_TCP_ADDR:$ELASTICSEARCH_PORT -w "" -o /dev/null; do sleep 1; done'<br/>              }<br/>              stage('Unit Tests') {<br/>                sh 'ELASTICSEARCH_HOSTNAME=$DB_PORT_9200_TCP_ADDR npm run test:unit'<br/>              }<br/>              stage('Integration Tests') {<br/>                sh 'ELASTICSEARCH_HOSTNAME=$DB_PORT_9200_TCP_ADDR npm run test:integration'<br/>              }<br/>              stage('End-to-End (E2E) Tests') {<br/>                sh 'ELASTICSEARCH_HOSTNAME=$DB_PORT_9200_TCP_ADDR npm run test:e2e'<br/>              }<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>The <kbd>docker</kbd> variable is provided by the Docker Pipeline plugin, and allows you to run Docker-related functions within a Pipeline. Here, we are using <kbd>docker.image()</kbd> to pull in an image. The image's <kbd>withRun</kbd> method will use <kbd>docker run</kbd> to run the image on the host.</p>
<p>Here, we are running the <kbd>elasticsearch-oss</kbd> image, and passing in the <kbd>discovery.type</kbd> flag—the same one that we've been using in previous chapters. Once the container is running, Jenkins will execute all the commands specified within the <kbd>withRun</kbd> block <em>on the host</em>, and then automatically exit once all the commands inside the body have finished.</p>
<p>Within the <kbd>withRun</kbd> block, we are specifying a <kbd>docker.image().inside()</kbd> block. Similar to <kbd>withRun</kbd>, commands inside the <kbd>inside</kbd> block will run once the container is up, but these instructions will run <em>inside the container</em>, instead of on the host. This is where we will run our tests.</p>
<p class="mce-root">Finally, we are passing in a <kbd>--link</kbd> flag to <kbd>inside</kbd>. This uses legacy Docker container links to provide our <kbd>node:8.11.4</kbd> container with information about the <kbd>elasticsearch-oss</kbd> container, such as its address and ports. This allows our API application to connect to the database.</p>
<p class="mce-root">The <kbd>--link</kbd> flag has the following syntax:</p>
<pre>--link &lt;name or id&gt;:alias</pre>
<p>Here, <kbd>&lt;name or id&gt;</kbd> is the name or ID of the container we want to link to, and <kbd>alias</kbd> is a string that allows us to refer to this link by name. After <kbd>withRun</kbd> has successfully run a container, it will provide the body with a container object, <kbd>c</kbd>, which has an <kbd>id</kbd> property we can use in the link.</p>
<p>Once a container is linked, Docker will set several environment variables to provide information about the linked container. For instance, we can find out the IP address of the linked container by referring to the value of <kbd>DB_PORT_9200_TCP_ADDR</kbd>.</p>
<p>You can see a full list of environment variables set by Docker at <a href="https://docs.docker.com/network/links/#environment-variables">docs.docker.com/network/links/#environment-variables</a>.</p>
<p>Save this <kbd>Jenkinsfile</kbd> and push it to the remote repository.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Docker</h1>
                
            
            
                
<p>Since Jenkins now relies on Docker, we must install Docker on this Jenkins server:</p>
<pre>jenkins@ci:$ <strong>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</strong><br/>jenkins@ci:$ <strong>sudo add-apt-repository "deb [arch=amd64] \</strong><br/>             <strong>https://download.docker.com/linux/ubuntu \</strong><br/>             <strong>$(lsb_release -cs) stable"</strong><br/>jenkins@ci:$ <strong>sudo apt update</strong><br/>jenkins@ci:$ <strong>sudo apt install -y docker-ce</strong></pre>
<p>This installation will do a few things, as follows:</p>
<ul>
<li>Install the Docker Engine, which runs as a daemon in the background</li>
<li>Install the Docker client, which is a command-line tool (<kbd>docker</kbd>) we can run in our Terminal</li>
<li>Create a user on our machine called <kbd>docker</kbd>, and assign it to the <kbd>docker</kbd> group</li>
</ul>
<p>To check Docker is installed properly, you can check its status by running <kbd>sudo systemctl status docker</kbd>.</p>
<p>By default, the docker command must be invoked with root privileges. The exception to this rule is if the user is <kbd>docker</kbd>, or if the user is in the <kbd>docker</kbd> group. We are running our Jenkins server under the user <kbd>jenkins</kbd>; therefore, to allow our Jenkins server to spawn new Docker containers, we must add the <kbd>jenkins</kbd> user to the <kbd>docker</kbd> group:</p>
<pre>jenkins@ci:$ <strong>sudo usermod -aG docker jenkins</strong></pre>
<p>To check that this is successful, run the following:</p>
<pre>jenkins@ci:$ <strong>grep docker /etc/group</strong><br/>docker:x:999:jenkins</pre>
<p>Lastly, restart the Jenkins service for this change to take effect:</p>
<pre>jenkins@ci:$ <strong>sudo systemctl restart jenkins</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Integration with GitHub</h1>
                
            
            
                
<p>We now have a <kbd>Jenkinsfile</kbd> that provides instructions on how to run the tests, and a Jenkins server to run them; the only thing left is to set up a service hook with GitHub, so that it will trigger the Jenkins Pipeline whenever changes are pushed to the repository.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Providing access to the repository</h1>
                
            
            
                
<p>First and foremost, we must provide our Jenkins server with permissions to access our repository, as well as to set up service hooks. There are several ways to do this:</p>
<ol>
<li>Create a Personal Access (OAuth) Token on GitHub, which essentially allows your Jenkins server to masquerade as you. The benefits of this approach are that you can have one token that can be used everywhere to access all repositories under your control. However, although the scope of the token can be restricted, these permissions are applicable for all repositories under your account. Thus, this approach does not allow you to set granular permissions for each repository.</li>
<li>Create a new user on GitHub that represents the Jenkins server, and add that user as a collaborator into your repository. After creating the account, you'll need to create a new SSH key pair on your Jenkins host machine, and add the public key to GitHub (just as you would for a normal user). Then, configure your Jenkins server to use this SSH key to communicate with GitHub.<br/>
The benefits of this approach are that it allows you to separate your identity from the Jenkin servers, and you can simply add the Jenkin GitHub user to any other repository that you wish to grant access to the Jenkins server.</li>
<li>As in Step 2, create a new user on GitHub that represents the Jenkin server, and set up a new SSH key pair. Then, go to your repository and click on the Settings tab. In the sidebar, click Deploy Keys, then click Add deploy key. Now, paste your SSH key into the text area and save.<br/>
The benefits of this approach are that you can grant access to only a single repository. They are called <strong>Deploy keys</strong> precisely because this method is used a lot for automated deployments. You can set the permission for the deploy key to be read-only (so they only clone, build, and deploy), or both read and write permissions (so they can also push changes back to the repository).</li>
</ol>
<p>To keep it simple, we are going to use the Personal Access Token method, as outlined in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Personal Access (OAuth) Token</h1>
                
            
            
                
<p>Go to <a href="https://github.com/settings/tokens">github.com/settings/tokens</a> and click on Generate new token. Select the repo, admin:repo_hook, and admin:org_hook scopes:</p>
<div><img src="img/b6630e9c-2586-4332-bb2c-3bfa4c3e062e.png" style="width:37.92em;height:42.08em;"/></div>
<p>Now, a new token is generated:</p>
<div><img src="img/4d55b352-2dc2-4aaa-9f89-22069cce546b.png"/></div>
<p>Next, we can add this token to the <strong>Credentials</strong> store in Jenkins, which is like a password manager and stores our credentials for us to reference inside our configuration. Click on the Credentials entry from the sidebar on the left of the Jenkins UI.</p>
<p>Next, under Stores scoped to Jenkins, click on the arrow next to the (global) link and then Add credentials. This will allow you to add your Personal Access Token to be available to the whole of Jenkins server:</p>
<div><img src="img/2dfe208a-9f81-4da1-ba9b-0b6315bb73cb.png"/></div>
<p>Then, in the form that appears, input the following values:</p>
<div><img src="img/f3b2b491-6639-422a-a13d-b073605f2a82.png" style="width:34.08em;height:22.75em;"/></div>
<p>There are two options available for the Scope field—system or global. A system-scoped credential can be used by the Jenkins instance itself, but not in freestyle projects or pipelines. Global-scoped credentials can be used by all. The ID is an internal unique ID that is used to identify this credential; if left blank, an ID will be generated automatically.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the GitHub plugin</h1>
                
            
            
                
<p>To integrate with GitHub so that changes pushed to the repository will trigger a build on Jenkins, we will need to use two plugins:</p>
<ul>
<li><strong>Git plugin</strong> (<a href="https://plugins.jenkins.io/git">plugins.jenkins.io/git</a>): Enables Jenkins to clone and pull from any Git repository that it has access to. Also adds Git-specific environment variables to the build environment so you can use it during any build steps.</li>
<li><strong>GitHub plugin</strong> (<a href="https://plugins.jenkins.io/github">plugins.jenkins.io/github</a>): Allows you to set up a service hook on GitHub that will send a message to our Jenkins instance each time a change is pushed to GitHub. The GitHub plugin also depends on the Git plugin.</li>
</ul>
<p>These two plugins should be installed if you followed the standard installation; otherwise, install them before moving forward.</p>
<p>For the GitHub plugin to automatically set up service hooks for us, we must provide it with the credentials we stored earlier. Go to Manage Jenkins | Configure Systems and under the GitHub section, add a new GitHub Server:</p>
<div><img src="img/82ad4aaa-335e-485e-aa5e-8c08fb15ae02.png"/></div>
<p>In the Credentials field, select the credential we stored in the previous name. Then, click Test connection so Jenkins can send a dummy request to GitHub to ensure the token is a valid one. Now, our GitHub plugin will be able to perform actions on our behalf.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up GitHub service hooks manually</h1>
                
            
            
                
<p>Next, go to our <kbd>Hobnob</kbd> repository on GitHub and select Settings | Integrations &amp; services. You should see a list of services that hook onto events on GitHub, including the Travis service we added at the beginning of this chapter:</p>
<div><img src="img/19fa6e47-56af-4b77-b168-7ea84c454df2.png"/></div>
<p>Next, we need to add the Jenkins (GitHub plugin) to the list of services:</p>
<div><img src="img/7b33b7da-dcb8-4ecc-899f-390a90efd838.png"/></div>
<p>On the next screen, GitHub will ask you to specify the Jenkins hook url; this is the URL that GitHub uses to inform our Jenkins instance of a change in the repository. Jenkins uses a single post-commit hook URL for all the repositories; by default, this has the format of <kbd>http://&lt;ip-or-hostname&gt;/github-webhook/</kbd>. So for us, we will use <kbd>http://jenkins.hobnob.social/github-webhook/</kbd>:</p>
<p class="mce-root"/>
<div><img src="img/e7163bf8-54c9-4b64-a8e1-3ea4dcd34786.png"/></div>
<p class="mce-root"/>
<p>This will add the Service Hook to GitHub, but it'll also indicate that it has never been triggered:</p>
<div><img src="img/c359edd8-701c-47bf-81b0-ad1b425dbd9b.png"/></div>
<p>Next, we need to create the pipeline on Jenkins so that when the service hook is triggered, we can run our pipeline as defined in the <kbd>Jenkinsfile</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a new folder</h1>
                
            
            
                
<p>But before we create a pipeline, we know that our application will consist of two parts—a backend API server and a frontend web interface. We will eventually use Jenkins to build both of these applications, and so it would be wise to separate the pipelines into two separate groups, which in the context of Jenkins, is a folder.</p>
<p>To create a new folder, click on the New Item link found on the left-hand side of the interface. You'll then be presented with the following screen:</p>
<div><img src="img/0eb17864-b848-4298-9787-45d23d9f727a.png"/></div>
<p>Under the Name parameter, enter a name that identifies this folder and acts as a namespace for all projects grouped under this folder. This name will also be used in the URL path, as well as the directory name in the filesystem, therefore, you should pick a name that does not contain spaces or special characters (especially slashes).</p>
<p>You may optionally specify a Display Name and Description. Click Save and the folder will be created and can be accessed through the URL, <a href="http://jenkins.hobnob.social/job/backend/" target="_blank">http://jenkins.hobnob.social/job/backend/</a>. Next, we are going to create a new pipeline under this folder.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a new pipeline</h1>
                
            
            
                
<p>Navigate to <kbd>http://jenkins.hobnob.social/job/backend/</kbd> and click on the New Item link again, but this time select the Pipeline option:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/21ebfb29-eac9-4615-b902-b7e1537d2aee.png"/></p>
<p>In the General section, check the GitHub project checkbox and paste in the URL to your GitHub project:</p>
<div><img src="img/19b30e70-8b79-40d1-a0f9-b1566f6900ec.png"/></div>
<p>Then, in the Build Triggers section, check the option for GitHub hook trigger for GITScm polling. This means this pipeline will be executed every time our webhook endpoint (<kbd>http://jenkins.hobnob.social/github-webhook/</kbd>) receives a message from GitHub related to this GitHub project:</p>
<div><img src="img/440e7c87-f61c-4454-b69a-f03c52f9c483.png"/></div>
<p>Next, under the Pipeline section, select Pipeline script from SCM and make sure Script Path is set to Jenkinsfile. This will tell Jenkins to use the Jenkins file from the repository. Then, click on Add repository and paste in the repository URL. Lastly, in Branches to build, enter the value <kbd>*/*</kbd> so that the pipeline will trigger based on changes on any branch:</p>
<div><img src="img/5a8c6f08-6b59-4750-a2dd-4fd488d6a3a6.png"/></div>
<p class="mce-root"/>
<p>Save the pipeline, and move on to running our first build!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the first build</h1>
                
            
            
                
<p>Now, run <kbd>git commit --amend</kbd> to change the commit hash; this will be sufficient to constitute a change. Push this change to the remote repository. Fingers crossed, this should trigger the build on our Jenkins server.</p>
<p>First, it will download the repository and it into a workspace located at <kbd>/var/lib/jenkins/jobs</kbd>, then, it will run the instructions specified in our <kbd>Jenkinsfile</kbd>. </p>
<p>When a build (freestyle project or pipeline) is triggered, it will be added to the list of builds in the Build History sidebar on the left. The indicator to the left of the build shows the status of the build. Initially, it will be flashing blue, indicating it is running but not yet complete. Once the pipeline has completed execution, the indicator will change to a non-flashing blue or red, representing a successful or failed build:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b0510767-7849-4803-b9ce-e34c6fce0040.png" style="width:25.08em;height:24.42em;"/></p>
<p class="mce-root"/>
<p>You can keep track of the progress of the pipeline by going to the Console Output tab and reading the <kbd>stdout</kbd> produced. However, if you prefer a visual representation, you can look at the <strong>Stage View</strong>, which displays a table with colored blocks, where green represents a passing stage, and red represents a failed stage. This is provided by the pipeline stage view plugin (<a href="https://plugins.jenkins.io/pipeline-stage-view">plugins.jenkins.io/pipeline-stage-view</a>), which is installed by default:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/04f1e1f2-72cf-43de-93b6-045657b25561.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we have integrated our project with two CI services—Travis and Jenkins. With CI, we are able to trigger tests to run after certain events and automate the testing of our app. We have also used Docker to provide an isolated environment for our tests, ensuring our tests remain reliable and repeatable. In <a href="d336b08b-f67f-4789-8194-c65d7aa3decc.xhtml" target="_blank">Chapter 17</a>, <em>Migrating to Docker</em>, we will even migrate our entire deployment to using Docker.</p>
<p>In the next chapter, we will learn how to secure our application by implement authentication and authorization checks in our API.</p>


            

            
        
    </body></html>