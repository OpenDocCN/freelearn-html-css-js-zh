- en: Chapter 9. Advanced NodeJS Concurrency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章。高级 NodeJS 并发
- en: In [Chapter 8](ch08.html "Chapter 8. Evented IO with NodeJS"), *Evented IO with
    NodeJS*, you learned about the concurrency mechanism that's central to NodeJS
    applications—the IO event loop. In this chapter, we'll dig into some more advanced
    topics that are both—complimentary to the event loop and contrary to the event
    loop.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 8 章](ch08.html "第 8 章。使用 NodeJS 的事件驱动 IO") 中，*使用 NodeJS 的事件驱动 IO*，你学习了
    NodeJS 应用程序中核心的并发机制——IO 事件循环。在本章中，我们将深入研究一些既补充事件循环又与事件循环相反的更高级的主题。
- en: Kicking us off is a discussion on implementing coroutines in Node using the
    `Co` library. Next, we'll look at creating subprocesses and communicating with
    these processes. After this, we'll dig into Node's built-in capability to create
    a process cluster, each with their own event loop. We'll close this chapter with
    a look at creating clusters at large-scale clusters of Node servers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从讨论在 Node 中使用 `Co` 库实现协程开始。接下来，我们将探讨创建子进程以及与这些进程通信。之后，我们将深入研究 Node 内置的创建进程集群的能力，每个集群都有自己的事件循环。我们将以查看创建大规模
    Node 服务器集群的集群结束本章。
- en: Coroutines with Co
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Co 的协程
- en: We've already seen one approach to implement coroutines in the front-end using
    generators, in [Chapter 4](ch04.html "Chapter 4. Lazy Evaluation with Generators"),
    *Lazy Evaluation with Generators*. In this section, we'll use the `Co` library
    ([https://github.com/tj/co](https://github.com/tj/co)) to implement coroutines.
    This library also relies on generators and promises.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [第 4 章](ch04.html "第 4 章。使用生成器的懒加载评估") 中看到了一种在前端使用生成器实现协程的方法，*使用生成器的懒加载评估*。在本节中，我们将使用
    `Co` 库 ([https://github.com/tj/co](https://github.com/tj/co)) 来实现协程。这个库也依赖于生成器和承诺。
- en: We'll start by walking through the general premise of `Co`, and then, we'll
    write some code that waits for asynchronous values using promises. We'll then
    look into the mechanics of transferring resolved values from a promise to our
    coroutine function, asynchronous dependencies, and creating coroutine utility
    functions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先概述 `Co` 的一般原理，然后编写一些使用承诺等待异步值的代码。接下来，我们将探讨如何将已解析的值从承诺传递到我们的协程函数中，异步依赖关系，以及创建协程实用函数。
- en: Generating promises
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成承诺
- en: 'At its core, the `Co` library uses a `co()` function to create a coroutine.
    In fact, its basic usage looks familiar to the coroutine function that we created
    earlier in this book. Here''s what it looks like:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，`Co` 库使用一个 `co()` 函数来创建协程。实际上，其基本用法与我们在本书早期创建的协程函数看起来很相似。下面是它的样子：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Another similarity between the `Co` library and our earlier coroutine implementation
    is that values are passed in through the `yield` statement. However, instead of
    calling the returned function to pass in the values, this coroutine uses promises
    to pass in values. The effect is the same—asynchronous values being passed into
    synchronous code, as this diagram shows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '`Co` 库和我们的早期协程实现之间的另一个相似之处是，值通过 `yield` 语句传递。然而，与调用返回的函数来传递值不同，这个协程使用承诺来传递值。效果是相同的——异步值被传递到同步代码中，如图所示：'
- en: '![Generating promises](img/B05133_09_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![生成承诺](img/B05133_09_01.jpg)'
- en: The asynchronous value actually comes from a promise. The resolved value makes
    its way into the coroutine. We'll dig deeper into the mechanics of how this works
    shortly. Even if we don't yield promises, say we yielded a string for instance,
    the `Co` library will wrap this into a promise for us. But, doing this defeats
    the purpose of using asynchronous values in synchronous code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，异步值来自承诺。已解析的值进入协程。我们将在稍后深入了解其工作原理的机制。即使我们没有产生承诺，比如说我们产生了一个字符串，`Co` 库也会为我们将其包装成一个承诺。但是，这样做就违背了在同步代码中使用异步值的目的。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It cannot be understated how valuable it is for us, as programmers, when we
    find a tool such as `Co`, that encapsulates messy synchronization semantics. Our
    code inside the coroutine is synchronous and maintainable.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们发现像 `Co` 这样的工具，它封装了混乱的同步语义时，这对我们程序员来说是多么有价值。我们在协程内部的代码是同步的且可维护的。
- en: Awaiting values
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 等待值
- en: 'Coroutines created by the `co()` function work a lot like ES7 asynchronous
    functions. The `async` keyword marks a function as asynchronous—meaning that it
    uses asynchronous values within. The `await` keyword, used in conjunction with
    a promise, pauses the execution of the function till the value resolves. If this
    feels a lot like what a generator does, it''s because it''s exactly what a generator
    does. Here''s what the ES7 syntax looks like:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由 `co()` 函数创建的协程工作方式与 ES7 异步函数非常相似。`async` 关键字将一个函数标记为异步——意味着它在其内部使用异步值。`await`
    关键字与承诺一起使用，暂停函数的执行，直到值解析。如果这感觉与生成器所做的工作非常相似，那是因为它确实就是生成器所做的工作。以下是 ES7 语法的样子：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this example, the promises are resolved immediately, so there''s no real
    need to pause the execution. However, it waits even if the promise resolves a
    network request that takes several seconds. We''ll go into more depth on resolving
    promises in the next section. Given that this is ES7 syntax, it''d be nice if
    we could use the same approach today. Here''s how we would implement the same
    thing with `Co`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，承诺立即解析，所以实际上没有必要暂停执行。然而，即使承诺解析一个需要几秒钟的网络请求，它也会等待。我们将在下一节更深入地探讨解析承诺。鉴于这是
    ES7 语法，如果我们可以今天使用相同的方法那就太好了。以下是使用 `Co` 实现相同方法的示例：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It should be no surprise that the `Co` library is moving in the direction of
    ES7; nice move `Co` authors.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，`Co` 库正朝着 ES7 的方向发展；`Co` 的作者们做得很好。
- en: Resolving values
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析值
- en: 'There are at least two places in a given `Co` coroutine where a promise is
    resolved. First, there''s one or more promises yielded from within the generator
    function that we''ll pass to `co()`. If there weren''t any promises yielded within
    this function, there wouldn''t be much point in using `Co`. The return value when
    calling `co()` is another promise, which is kind of cool because it means that
    coroutines can have other coroutines as dependencies. We''ll explore this idea
    in more depth momentarily. For now, let''s look at resolving the promises, and
    how it''s done. Here''s an illustration of the promise resolution order of a coroutine:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的 `Co` 协程中至少有两个地方会解析承诺。首先，有一个或多个从生成器函数中产生的承诺，我们将将其传递给 `co()`。如果没有在这个函数中产生任何承诺，使用
    `Co` 就没有太多意义。调用 `co()` 时的返回值是另一个承诺，这相当酷，因为它意味着协程可以作为其他协程的依赖项。我们稍后将更深入地探讨这个想法。现在，让我们看看如何解析承诺以及如何实现。以下是协程承诺解析顺序的说明：
- en: '![Resolving values](img/B05133_09_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![解析值](img/B05133_09_02.jpg)'
- en: 'The promises are resolved in the same order that they''re named. For instance,
    the first promise causes the execution of the code within the coroutine to pause
    execution until it''s value is resolved. Then, the execution is paused again while
    waiting for the second promise. The final promise that''s returned from `co()`
    is resolved with the return value of the generator function. Let''s look at some
    code now:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 承诺的解析顺序与它们命名的顺序相同。例如，第一个承诺会导致协程中的代码暂停执行，直到其值解析。然后，在等待第二个承诺时再次暂停执行。从 `co()` 返回的最后一个承诺使用生成器函数的返回值解析。现在让我们看看一些代码：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see, the return value from the generator ends up as the resolved promise
    value. Recall that returning from a generator will return the same object as yielding
    does with the `value` and `done` properties. `Co` knows to resolve the promise
    with the `value` property.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，生成器的返回值最终成为解析的承诺值。回想一下，从生成器返回将返回与使用 `value` 和 `done` 属性产生相同的对象。`Co` 知道使用
    `value` 属性解析承诺。
- en: Asynchronous dependencies
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步依赖
- en: 'Coroutines made with `Co` really shine when an action depends on an earlier
    asynchronous value later on in the coroutine. What would otherwise be a tangled
    mess of callbacks and state is instead just placing the assignments in the correct
    order. The dependent action is never called until the value is resolved. Here''s
    an illustration of the idea:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当协程中的某个操作依赖于稍后异步值时，使用 `Co` 创建的协程表现得非常好。否则，原本会是回调和状态混乱的一团糟，而现在只需将赋值放置在正确的顺序即可。依赖的操作只有在值解析后才会被调用。以下是这个想法的说明：
- en: '![Asynchronous dependencies](img/B05133_09_03.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![异步依赖](img/B05133_09_03.jpg)'
- en: 'Now let''s write some code that has two asynchronous actions, where the second
    action depends on the result of the first. This can be tricky, even with the use
    of promises:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写一些代码，其中包含两个异步操作，第二个操作依赖于第一个操作的结果。即使使用承诺，这也可能很棘手：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We used a nested coroutine in this example, but it could have been any type
    of function that required a parameter and returned a promise. This example, if
    nothing else, serves to highlight the versatility of promises in a concurrent
    environment.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了嵌套协程，但它可以是任何需要参数并返回Promise的函数类型。这个例子如果不是其他的话，至少可以突出Promise在并发环境中的多功能性。
- en: Wrapping coroutines
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装协程
- en: 'The last `Co` example that we''ll look at uses the `wrap()` utility to make
    a plain coroutine function into a reusable function that we can call over and
    over. As the name suggests, the coroutine is simply wrapped in a function. This
    is especially useful when we pass arguments to coroutines. Let''s take a look
    at a modified version of the code example that we built:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的最后一个`Co`示例使用`wrap()`实用工具将一个普通的协程函数包装成一个可重复调用的函数。正如其名所示，协程只是被包装在一个函数中。当我们向协程传递参数时，这特别有用。让我们看看我们构建的代码示例的修改版：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, instead of a nested coroutine, we used `co.wrap()` to create a reusable
    coroutine function. That is, it'll create a new coroutine every time it's called,
    passing it all the arguments that the function gets. There really isn't much more
    to it than this, but the gains are noticeable and worthwhile. Instead of a nested
    coroutine function, we have something that can potentially be shared across components.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不是使用嵌套协程，而是使用`co.wrap()`创建一个可重复使用的协程函数。也就是说，每次调用时都会创建一个新的协程，传递给它函数获取的所有参数。实际上，这并没有什么更多，但收益是明显的，值得的。我们不再有一个嵌套的协程函数，而是一个可能被组件间共享的东西。
- en: Child Processes
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子进程
- en: We know that NodeJS uses an evented IO loop as its main concurrency mechanism.
    This is based on the assumption that our application does a lot of IO and very
    little CPU-intensive work. This is probably true for the majority of handlers
    in our code. However, there's always a particular edge case that requires more
    CPU time than usual.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道NodeJS使用事件驱动的IO循环作为其主要并发机制。这是基于我们的应用程序进行大量IO操作和很少的CPU密集型工作的假设。这可能适用于我们代码中大多数处理器的多数情况。然而，总会有一些特殊情况需要比通常更多的CPU时间。
- en: In this section, we'll discuss how handlers can block the IO loop, and why all
    it takes is one bad handler to ruin the experience for everyone else. Then, we'll
    look at ways to get around this limitation by forking new Node child processes.
    We'll also look at how to spawn other non-Node processes in order to get the data
    that we need.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论处理器如何阻塞IO循环，以及为什么只需要一个不良处理器就足以破坏其他所有人的体验。然后，我们将探讨通过创建新的Node子进程来绕过这种限制的方法。我们还将探讨如何启动其他非Node进程以获取我们所需的数据。
- en: Blocking the event loop
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阻塞事件循环
- en: In [Chapter 8](ch08.html "Chapter 8. Evented IO with NodeJS"), *Evented IO with
    NodeJS*, we saw an example that demonstrated how one handler can block the entire
    IO event loop while performing expensive CPU operations. We're going to reiterate
    this point here to highlight the full scope of the problem. It's not just one
    handler that we're blocking, but all handlers. This could be hundreds, or it could
    be thousands, depending on the application, and how it's used.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html "第8章。使用NodeJS的事件驱动IO")，“使用NodeJS的事件驱动IO”，我们看到了一个示例，演示了一个处理器如何在执行昂贵的CPU操作时阻塞整个IO事件循环。我们在这里重申这一点，以突出问题的全貌。我们阻塞的不仅仅是单个处理器，而是所有处理器。这可能是数百个，也可能是数千个，具体取决于应用程序及其使用方式。
- en: 'Since we''re not processing requests in parallel at the hardware level, which
    is the case with the multithreaded approach—it only takes one expensive handler
    to block all handlers. If there''s one request that''s able to cause this expensive
    handler to run, then we''re likely to receive several of these expensive requests,
    bringing our application to a standstill. Let''s look at a handler that blocks
    every other handler that comes in after it:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在硬件级别不是并行处理请求，这与多线程方法的情况不同——只需要一个昂贵的处理器就可以阻塞所有处理器。如果有一个请求能够触发这个昂贵的处理器运行，那么我们很可能会接收到几个这样的昂贵请求，使我们的应用程序陷入停滞。让我们看看一个阻塞其后所有其他处理器的处理器：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first call to `process.nextTick()`simulates actual client requests by scheduling
    functions to run after one second. All these lead to a single promise being resolved;
    and this logs the fact that all the requests have been handled. The next call
    to `process.nextTick()` is expensive and completely blocks these 500 requests.
    This definitely isn't good for our application. The only way around scenarios
    where we run CPU-intensive code inside of NodeJS is to break out of the single-process
    approach. This topic is covered next.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次调用 `process.nextTick()` 通过安排函数在一秒后运行来模拟实际客户端请求。所有这些都导致一个单一的承诺得到解决；并记录了所有请求都已处理的事实。下一次调用
    `process.nextTick()` 是昂贵的，并且完全阻塞了这 500 个请求。这绝对不利于我们的应用程序。在 NodeJS 内运行 CPU 密集型代码的唯一方法就是跳出单进程方法。这个话题将在下一部分进行讨论。
- en: Forking processes
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进程创建
- en: 'We''ve reached the point in our application where there''s simply no way around
    it. We have some relatively expensive requests to process. We need to utilize
    parallelism at the hardware layer. In Node, this means only one thing—forking
    a child process to handle the CPU-intensive work outside of the main process so
    that normal requests may continue on uninterrupted. Here''s an illustration of
    what this tactic looks like:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序已经到了一个点，没有其他办法可以绕过。我们有一些相对昂贵的请求需要处理。我们需要在硬件层利用并行性。在 Node 中，这意味着只有一件事——在主进程之外创建子进程来处理
    CPU 密集型工作，以便正常请求可以不间断地进行。以下是这种策略的示意图：
- en: '![Forking processes](img/B05133_09_04.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![进程创建](img/B05133_09_04.jpg)'
- en: 'Now, let''s write some code that uses the `child_process.fork()` function to
    spawn a new Node process, when we need to process a request that''s CPU-hungry.
    First, the main module:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一些使用 `child_process.fork()` 函数来生成新的 Node 进程的代码，当我们需要处理一个 CPU 密集型请求时。首先，主模块：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The only overhead we face now is that of actually spawning the new process,
    which pales in comparison to the actual work that we need to perform. We can clearly
    see that the main IO loop isn''t blocked because the main process isn''t hogging
    the CPU. The child process, on the other hand, hammers the CPU, but this is okay
    because it''s probably happening on a different core. Here''s what our child process
    code looks like:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在面临的最大开销实际上是生成新进程的开销，与我们需要执行的实际工作相比微不足道。我们可以清楚地看到，主 I/O 循环没有被阻塞，因为主进程没有占用
    CPU。另一方面，子进程正在猛烈地敲击 CPU，但这没关系，因为它可能发生在不同的核心上。以下是我们的子进程代码的样子：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Spawning external processes
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成外部进程
- en: 'Sometimes, our Node applications need to talk to other programs that aren''t
    Node processes. These could be other applications we write, but using a different
    platform or basic system commands. We can spawn these types of processes and talk
    to them, but they don''t work the same as forking another node process. Here''s
    a visualization of the difference:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们的 Node 应用程序需要与其他程序通信，而这些程序不是 Node 进程。这些可能是我们用不同平台或基本系统命令编写的其他应用程序。我们可以生成这些类型的进程并与它们通信，但它们的工作方式与创建另一个节点进程不同。以下是这种差异的示意图：
- en: '![Spawning external processes](img/B05133_09_05.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![生成外部进程](img/B05133_09_05.jpg)'
- en: We could use `spawn()` to create a child Node process if we're so inclined,
    but this puts us at a disadvantage in some cases. For example, we don't get the
    message-passing infrastructure that's setup automatically for us by `fork()`.
    However, the best communication path depends on what we're trying to achieve,
    and most of the time, we don't actually need message-passing.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，可以使用 `spawn()` 创建一个子 Node 进程，但这在某些情况下会让我们处于不利地位。例如，我们得不到 `fork()` 自动为我们设置的消息传递基础设施。然而，最佳通信路径取决于我们想要实现的目标，而且大多数时候，我们实际上并不需要消息传递。
- en: 'Let''s look at some code that spawns a process and reads the output of that
    process:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些生成进程并读取该进程输出的代码：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `ls` command that we spawn doesn't exist on Windows systems. I have no other
    consolatory words of wisdom here—it's just a fact.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的 `ls` 命令在 Windows 系统上不存在。在这里，我没有其他安慰性的智慧之词——这只是一个事实。
- en: Inter-process communication
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进程间通信
- en: 'In the example that we just looked at, the child process was spawned, and our
    main process collected the output, killing the process; but, what about when we
    write servers and other types of long-lived programs? Under these circumstances,
    we probably don''t want to constantly spawn and kill child processes. Instead,
    it''s probably better to keep the process alive alongside the main program and
    keep feeding it messages, as is illustrated here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚才看到的例子中，子进程被生成，我们的主进程收集了输出，杀死了进程；但是，当我们编写服务器和其他类型的长期程序时，我们会怎么做？在这种情况下，我们可能不想不断地生成和杀死子进程。相反，可能更好的是让进程与主程序一起保持活跃，并继续向其发送消息，就像这里所展示的：
- en: '![Inter-process communication](img/B05133_09_06.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![进程间通信](img/B05133_09_06.jpg)'
- en: 'Even if the worker is synchronously processing requests, it still serves as
    an advantage to our main application because nothing blocks it from serving requests.
    For instance, requests that don''t require any heavy-lifting on behalf of the
    CPU can continue to deliver fast responses. Let''s turn our attention to a code
    example now:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 即使工作者正在同步处理请求，它仍然对我们的主应用程序有好处，因为它不会阻止它服务请求。例如，不需要CPU进行繁重操作的任务可以继续提供快速响应。现在让我们看看一个代码示例：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s take a look at the `worker` module that we fork from the main module:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看从主模块分叉的`worker`模块：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Each number in the arrays that we create is passed to the worker process where
    the CPU-heavy work is performed. The result is passed back to the main process,
    and is used to resolve a promise. This technique is very similar to the promise
    approach that we took with web workers in [Chapter 7](ch07.html "Chapter 7. Abstracting
    Concurrency"), *Abstracting Concurrency*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的数组中的每个数字都会传递给执行繁重工作的工作者进程。结果会传回主进程，并用于解决一个承诺。这种技术与我们用[第7章](ch07.html "第7章。抽象并发")中的网络工作者采取的承诺方法非常相似，*抽象并发*。
- en: There are two results we're trying to compute here—one for the `first` array,
    and one for the `second`. The first one has more array items than the second one,
    and the numbers are larger. This means that this will take longer to compute,
    and, in fact, it does. But, if we run this code, we don't see the output from
    the second array until the first has completed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里试图计算两个结果——一个用于`first`数组，另一个用于`second`。第一个数组比第二个数组有更多的数组项，数字也更大。这意味着这将需要更长的时间来计算，事实上也是如此。但是，如果我们运行这段代码，我们不会在第一个完成之前看到第二个数组的输出。
- en: This is because despite requiring less CPU time, the second job is still blocked
    because the order of the messages sent to the worker is preserved. In other words,
    all 100 messages from the first array are processed before even starting on the
    second array. At first glance, this may seem like a bad thing because it doesn't
    actually solve anything for us. Well, this simply not true.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为尽管第二个任务所需的CPU时间更少，但它仍然因为发送给工作者的消息顺序被保留而被阻塞。换句话说，在开始处理第二个数组之前，必须先处理来自第一个数组的所有100条消息。乍一看，这似乎是个坏消息，因为它实际上并没有为我们解决问题。好吧，这并不完全正确。
- en: The only thing that's blocked are the queued messages that arrive at the worker
    process. Because the worker is busy with the CPU, it can't process messages immediately
    as they arrive. However, the purpose of this worker is to remove the heavy processing
    from web request handlers that require it. Not every request handler has this
    type of heavy load, and guess what? They can continue to run normally because
    there's nothing in the process that hogs the CPU.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 被阻塞的只有到达工作者进程的队列消息。因为工作者正忙于CPU，所以它不能像消息到达时那样立即处理它们。然而，这个工作者的目的是从需要它的网络请求处理器中移除繁重的处理。并不是每个请求处理器都有这种类型的繁重负载，你知道吗？它们可以继续正常运行，因为没有在进程中占用CPU的资源。
- en: However, as our applications continue to grow larger and more complex due to
    added features and the ways in which they interact with other features, we'll
    need a better approach to handling expensive request handlers because we'll have
    more of them. This is what we're going to cover in the next section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着我们的应用程序因为添加的功能以及它们与其他功能交互的方式而变得更大、更复杂，我们将需要一个更好的方法来处理昂贵的请求处理器，因为我们将有更多的处理器。这就是我们将在下一节中要讨论的内容。
- en: Process Clusters
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进程集群
- en: In the preceding section, we introduced child process creation in NodeJS. This
    is a necessary measure for web applications when request handlers start consuming
    more and more CPU, because of the way that this can block every other handler
    in the system. In this section, we'll build on this idea, but instead of forking
    a single general-purpose worker process, we'll maintain a pool of general-purpose
    processes, which is capable of handling any request.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了 NodeJS 中的子进程创建。当请求处理器开始消耗越来越多的 CPU 时，这是 Web 应用程序所必需的措施，因为这种方式可能会阻塞系统中的其他所有处理器。在本节中，我们将在此基础上进行扩展，但我们将维护一个通用进程池，它能够处理任何请求。
- en: We'll start by reiterating the challenges posed by manually managing these processes
    that help us with concurrency scenarios in Node. Then, we'll look at the built-in
    process clustering capabilities of Node.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先重申手动管理这些帮助我们处理 Node 中并发场景的流程所面临的挑战。然后，我们将探讨 Node 的内置进程集群功能。
- en: Challenges with process management
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进程管理的挑战
- en: The obvious problem with manually orchestrating processes within our application
    is that the concurrent code is right there, out in the open, intermingling with
    the rest of our application code. We actually experienced the exact same problem
    earlier in this book when implementing web workers. Without encapsulating the
    synchronization and the general management of the workers, our code consists mostly
    of concurrency boilerplate. Once this happens, it's tough to separate the concurrency
    mechanisms from the code that's essential to the features that make our product
    unique.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用程序中手动编排进程的明显问题是，并发代码就在那里，公之于众，与我们的其他应用程序代码交织在一起。实际上，我们在本书早期实现 Web Workers
    时就遇到了完全相同的问题。如果没有封装同步和一般的工作者管理，我们的代码主要由并发样板代码组成。一旦发生这种情况，就很难将并发机制与使我们的产品独特的功能代码区分开来。
- en: One solution with web workers is to create a pool of workers and hide them behind
    a unified API. This way, our feature code that needs to do things in parallel
    can do so without littering our editors with concurrency synchronization semantics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Web Workers 的一个解决方案是创建一个工作进程池并在其后隐藏一个统一的 API。这样，需要并行执行操作的功能代码可以这样做，而不会在我们的编辑器中散布并发同步语义。
- en: It turns out that NodeJS solves the problem of leveraging the hardware parallelism
    available on most systems, which is similar to what we did with web workers. Next,
    we'll jump into how this works.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，NodeJS 解决了利用大多数系统上可用的硬件并行性的问题，这与我们使用 Web Workers 所做的是类似的。接下来，我们将深入了解这是如何工作的。
- en: Abstracting process pools
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽象进程池
- en: We're able to use the `child_process` module to manually fork our Node process
    to enable true parallelism. This is important when doing CPU-intensive work that
    could block the main process, and hence, the main IO event loop that services
    incoming requests. We could increase the level of parallelism beyond just a single
    worker process, but that would require a lot of manual synchronization logic on
    our part.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `child_process` 模块手动分叉 Node 进程以实现真正的并行性。这在进行可能阻塞主进程的 CPU 密集型工作时非常重要，从而阻塞主
    IO 事件循环，该循环处理传入的请求。我们可以将并行级别提升到仅单个工作进程之上，但这将需要我们进行大量的手动同步逻辑。
- en: The `cluster` module requires a little bit of setup code, but the actual communication
    orchestration between worker processes and the main process is entirely transparent
    to our code. In other words, it looks like we're just running a single Node process
    to handle our incoming web requests, but in reality, there are several cloned
    processes that handle them. It's up to the `cluster` module to distribute these
    requests to the worker nodes, and by default, this uses the round-robin approach,
    which is good enough for most cases.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`cluster` 模块需要一点设置代码，但工作进程和主进程之间的实际通信编排对我们的代码来说是完全透明的。换句话说，它看起来我们只是在运行一个单独的
    Node 进程来处理我们的传入 Web 请求，但事实上，有几个克隆进程来处理它们。由 `cluster` 模块负责将这些请求分配给工作节点，默认情况下，它使用轮询方法，这对于大多数情况来说已经足够好了。'
- en: On Windows, the default isn't round-robin. We can manually change the approach
    we want to use, but the round-robin approach keeps things simple and balanced.
    The only challenge is when we have request handlers that are substantially more
    expensive to run than the majority. Then, we can end up distributing requests
    to an overloaded worker process. This is just something to be aware of when troubleshooting
    this module.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 上，默认不是轮询方式。我们可以手动更改我们想要使用的方法，但轮询方法使事情变得简单且平衡。唯一的挑战是我们有请求处理器比大多数处理器运行成本高得多时。那时，我们可能会将请求分配给过载的工作进程。这只是在调试此模块时需要注意的事情。
- en: 'Here''s a visualization showing worker Node processes relative to the main
    Node process:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张显示工作节点进程相对于主节点进程的视觉图：
- en: '![Abstracting process pools](img/B05133_09_07.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![抽象进程池](img/B05133_09_07.jpg)'
- en: 'The main process has two responsibilities in a clustering scenario. First,
    it needs to establish communication channels with worker processes. Second, it
    needs to accept incoming connections and distribute them to the worker processes.
    This is actually trickier to draw and so isn''t represented in the diagram. Let''s
    look at some code before I try to explain this any further:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群场景中，主进程有两个职责。首先，它需要与工作进程建立通信通道。其次，它需要接受传入的连接并将它们分配给工作进程。这实际上很难绘制，所以没有在图中表示。在我进一步解释之前，让我们看看一些代码：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: What's really nice about this approach to parallelizing our request handlers
    is that the concurrent code is unobtrusive. There' are about 10 lines of it in
    total. At a glance, we can easily see what this code does. If we want to see this
    application in action, we can open several browser windows and point them to the
    server at the same time. Since the request handler is expensive in terms of CPU
    cycles, we should be able to see that each page responds with the value that was
    computed as well as the worker ID that computed it. If we hadn't forked these
    worker processes, then we'd probably still be waiting for each of our browser
    tabs to load.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种并行化我们的请求处理器的做法真正令人愉快的是，并发代码并不显眼。总共有大约 10 行。一眼望去，我们就可以轻松地看到这段代码做了什么。如果我们想看到这个应用程序的实际运行情况，我们可以打开几个浏览器窗口并将它们同时指向服务器。由于请求处理器在
    CPU 周期上成本较高，我们应该能够看到每个页面都响应了计算出的值以及计算它的工作进程 ID。如果我们没有分叉这些工作进程，那么我们可能还在等待每个浏览器标签加载。
- en: The only part that's a little tricky is the part where we actually create the
    HTTP server. Because this same code is run by each of the workers, the same host
    and port are used on the same computer—how can this be? Well, this is not actually
    what's happening. The `net` module, the low-level networking library that the
    `http` module uses, is actually cluster-aware. This means that when we ask the
    `net` module to listen to a socket for incoming requests, it first checks if it's
    a worker node. If it is, then it actually shares the same socket handle used by
    the main process. This is pretty neat. There's a lot of ugly logistics required
    to distribute requests to worker processes and actually hand off the request,
    all of which is handled for us by the `cluster` module.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一有点棘手的部分是我们实际创建 HTTP 服务器的那部分。因为每个工作进程都会运行相同的代码，所以在同一台计算机上使用相同的宿主和端口——这怎么可能呢？好吧，这实际上并不是正在发生的事情。`net`
    模块，`http` 模块使用的低级网络库，实际上是集群感知的。这意味着当我们要求 `net` 模块监听一个套接字以接收传入的请求时，它会首先检查它是否是工作节点。如果是，那么它实际上会共享主进程使用的相同的套接字句柄。这很酷。有很多复杂的后勤工作需要将请求分配给工作进程并实际传递请求，所有这些都被
    `cluster` 模块为我们处理了。
- en: Server clusters
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务器集群
- en: It's one thing to scale up a single machine that's running our NodeJS application
    by enabling parallelism through process management. This is a great way to get
    the most of our physical hardware or our virtual hardware—they both cost money.
    However, there's an inherent limitation to scaling up just one machine—it can
    only go so far. At some threshold in some dimension of our scaling problems, we'll
    hit a wall. Before this happens, we need to think about scaling our Node application
    to several machines.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过启用通过进程管理实现并行性来扩展运行我们的 NodeJS 应用程序的单台机器是一回事。这是一个充分利用我们的物理硬件或虚拟硬件的好方法——它们都花钱。然而，仅扩展一台机器存在固有的局限性——它只能走这么远。在我们扩展问题的某个维度达到某个阈值之前，我们会遇到障碍。在这之前，我们需要考虑将
    Node 应用程序扩展到多台机器。
- en: In this section, we'll introduce the idea of proxying our web requests to other
    machines instead of handling them all on the machine where they arrive. Then,
    we'll look at implementing microservices, and how they can help compose a sound
    application architecture. Finally, we'll implement some load balancing code that's
    tailored to our application; and how it handles requests.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍将我们的Web请求代理到其他机器而不是在它们到达的机器上处理它们的概念。然后，我们将探讨实现微服务以及它们如何帮助构建合理的应用程序架构。最后，我们将实现一些针对我们应用程序定制的负载均衡代码，以及它是如何处理请求的。
- en: Proxying requests
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理请求
- en: A request proxy in NodeJS is exactly what it sounds like. The request arrives
    at a server where it's handled by a Node process. However, the request isn't fulfilled
    here—it's proxied to another machine. So the question is, why bother with the
    proxy at all? Why not go straight to the target machine that actually responds
    to our requests?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: NodeJS中的请求代理正是其名称所暗示的。请求到达服务器，在那里它被Node进程处理。然而，请求并不是在这里得到满足——它是代理到另一台机器。所以问题是，为什么要使用代理呢？为什么不直接去响应我们请求的目标机器呢？
- en: The problem with this idea is that Node applications typically respond to HTTP
    requests coming from a browser. This means that we generally need a single entry
    point into the back-end. On the other hand, we don't necessarily want this single
    entry point to be a single Node server. This gets kind of limiting when our application
    grows larger. Instead, we want the ability to spread our application or scale
    it horizontally as they say. Proxy servers remove geographic restrictions; different
    parts of our application can be deployed in different parts of the world, different
    parts of the same data center, or even as different virtual machines. The point
    is that we have the flexibility to change where our application components reside,
    and how they're configured without impacting other parts of the application.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法的问题在于，Node应用程序通常响应来自浏览器的HTTP请求。这意味着我们通常需要一个单一的入口点进入后端。另一方面，我们并不一定希望这个单一的入口点是一个单独的Node服务器。当我们的应用程序变得更大时，这会变得有点限制性。相反，我们希望有扩展我们的应用程序或水平扩展它们的能力，就像他们说的那样。代理服务器消除了地理限制；我们的应用程序的不同部分可以部署在世界上的不同部分，同一个数据中心的不同部分，甚至作为不同的虚拟机。关键是，我们有权改变应用程序组件的存放位置以及它们的配置，而不会影响应用程序的其他部分。
- en: 'Another cool aspect of distributing web requests via proxy is that we can actually
    program our proxy handlers to modify requests and responses. So while the individual
    services that our proxy depends on can implement one specific aspect of our application,
    the proxy can implement the generic parts that apply to every request. Here is
    a visualization of a proxy server and the API endpoints that actually fulfill
    each request:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过代理分发Web请求的另一个有趣方面是，我们实际上可以编写我们的代理处理程序来修改请求和响应。因此，虽然我们的代理所依赖的各个服务可以实施我们应用程序的一个特定方面，但代理可以实施适用于每个请求的通用部分。以下是代理服务器和实际满足每个请求的API端点的可视化：
- en: '![Proxying requests](img/B05133_09_08.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![代理请求](img/B05133_09_08.jpg)'
- en: Facilitating micro-services
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 促进微服务
- en: Depending on the type of application that we're building, our API can be one
    monolithic service, or it can be composed of several microservices. On the one
    hand, monolithic APIs tend to be easier to maintain for smaller applications that
    don't have a large breadth of features and data. On the other hand, APIs for larger
    applications tend to grow outrageously complex to the point that it's impossible
    to maintain because there are so many areas that are all intertwined with one
    another. If we split them out into microservices, it's much easier to deploy them
    to specific environments suited to their needs and have a dedicated team focus
    on one service that's working well.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们构建的应用程序类型，我们的API可以是一个单体服务，也可以由几个微服务组成。一方面，单体API对于没有大量功能和数据的较小应用程序来说，维护起来通常更容易。另一方面，大型应用程序的API往往会变得极其复杂，以至于难以维护，因为有许多区域都相互交织在一起。如果我们将它们拆分成微服务，那么将它们部署到适合它们需求的具体环境中会容易得多，并且可以有一个专门的团队专注于一个运行良好的服务。
- en: Note
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Microservice architecture is a huge topic that obviously goes well beyond the
    scope of this book. The focus here is on microservice enablement—the mechanism
    more so than the design.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构是一个非常大的主题，显然超出了这本书的范围。这里的重点是微服务启用——机制比设计更重要。
- en: 'We''re going to use the node-http-proxy ([https://github.com/nodejitsu/node-http-proxy](https://github.com/nodejitsu/node-http-proxy))
    module to implement our proxy servers. This isn''t a core Node module, so our
    applications need to include it as an `npm` dependency. Let''s look at a basic
    example that proxies requests to the appropriate service:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用node-http-proxy ([https://github.com/nodejitsu/node-http-proxy](https://github.com/nodejitsu/node-http-proxy))模块来实现我们的代理服务器。这不是Node的核心模块，因此我们的应用程序需要将其作为`npm`依赖项包含在内。让我们看看一个基本的示例，该示例将请求代理到适当的服务：
- en: Note
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This example starts three web servers, each running on different ports.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例启动了三个网络服务器，每个服务器在不同的端口上运行。
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The two services hello and world aren't actually listed here because all they
    do is return a single line of plain text for any request. They' listen on ports
    `8082` and `8083` respectively. The `http-proxy` module makes it easy for us to
    simply forward the request to the appropriate service using the minimal amount
    of logic.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个服务hello和world实际上并没有列在这里，因为它们对任何请求都只返回一行纯文本。它们分别监听端口`8082`和`8083`。`http-proxy`模块使我们能够使用最少的逻辑简单地将请求转发到适当的服务。
- en: Informed load balancing
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息负载均衡
- en: Earlier in this chapter, we looked at process clustering. This is where we use
    the `cluster` module to create a pool of processes, each capable of handling requests
    from clients. The main process acts as a proxy in this scenario, and by default,
    distributes requests to the worker processes in a round-robin fashion. We can
    do something similar using the `http-proxy` module, but using a less naive approach
    than round-robin one.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，我们探讨了进程聚类。这是我们在其中使用`cluster`模块创建一个进程池的地方，每个进程都能够处理来自客户端的请求。在这种情况下，主进程充当代理，默认情况下以轮询的方式将请求分配给工作进程。我们可以使用`http-proxy`模块做类似的事情，但采用比轮询更不简单的方法。
- en: 'For example, let''s say we have two instances of the same micro service running.
    Well, one of these services could become busier than the other, which knocks the
    service off balance because the busy node will continue to receive requests even
    though it can''t get to them right away. It makes sense to hold onto the requests
    until the service can handle them. First, we''ll implement a service that randomly
    takes a while to complete:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们运行了相同微服务的两个实例。其中一个服务可能会比另一个更繁忙，这会导致服务失去平衡，因为繁忙的节点将继续接收请求，即使它不能立即处理它们。等到服务可以处理请求时再保留请求是有意义的。首先，我们将实现一个随机花费一段时间才能完成的服务：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we can start two instances of these processes, listening on different ports.
    In practice, these will be running on two different machines, but we''re just
    testing the idea at this point. Now we''ll implement the proxy server that needs
    to figure out which service worker a given request goes to:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以启动这些进程的两个实例，监听不同的端口。在实践中，这些进程将在不同的机器上运行，但在此阶段我们只是测试这个想法。现在我们将实现需要确定给定请求将发送到哪个服务工作者的代理服务器：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The key thing to note about the way this proxy works is that requests are only
    proxied to services that aren't already busy handling a request. This is the informed
    part—the proxy knows when the server is available because it responds with the
    last request that it was busy with. When we know which servers are busy, we know
    not to overload them with yet more work.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这种代理工作方式的关键点是，只有当服务不忙于处理请求时，才会将请求代理到服务。这是信息部分——代理知道服务器何时可用，因为它会响应它正忙于处理的最后一个请求。当我们知道哪些服务器正在忙碌时，我们就知道不要让它们过载更多的工作。
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked beyond the event loop as a concurrency mechanism
    in NodeJS. We started out by implementing coroutines using the `Co` library. From
    there, we learned about launching new processes, including the difference between
    forking another Node process and spawning other non-Node processes. Then, we looked
    at another approach to managing concurrency using the `cluster` module, which
    makes handling web requests in parallel processes as transparent as possible.
    Finally, we wrapped up the chapter with a look at using the `node-http-proxy`
    module to parallelize our web requests at the machine level.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了NodeJS中事件循环作为并发机制之外的内容。我们首先使用`Co`库实现协程。从那里，我们学习了启动新进程，包括在Node进程之间fork和在其他非Node进程中spawn之间的区别。然后，我们探讨了使用`cluster`模块管理并发的另一种方法，该方法使并行处理Web请求尽可能透明。最后，我们通过查看使用`node-http-proxy`模块在机器级别并行化我们的Web请求来结束本章。
- en: That does it for JavaScript concurrency topics. We've covered a lot of ground,
    both in the browser and in Node. But, how do these ideas and components all come
    together to form a concurrent application? In the final chapter of this book,
    we'll walk through the implementation of a concurrent app.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了 JavaScript 并发主题。我们在浏览器和 Node 中覆盖了很多内容。但是，这些想法和组件是如何组合在一起形成一个并发应用的？在这本书的最后一章，我们将探讨一个并发应用的实现过程。
