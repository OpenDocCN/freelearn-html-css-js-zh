<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xml:lang="en"
lang="en"
xmlns="http://www.w3.org/1999/xhtml"
xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Learning JavaScript Data Structures and Algorithms - Fourth Edition</title>


</head>
<body>
<div><div><h1 data-number="3">2 Big O notation</h1>
<p><strong>Before you begin: Join our book community on Discord</strong></p>
<p>Give your feedback straight to the author himself and chat to other early readers on our Discord server (find the "learning-javascript-dsa-4e" channel under EARLY ACCESS SUBSCRIPTION).</p>
<p>
<img style="width:10rem;" src="img/file0.png" width="200" height="200"/>
</p>
<a href="https://packt.link/EarlyAccess/">https://packt.link/EarlyAccess/</a>
<p>In this chapter, we will unlock the power of <strong>Big O notation</strong>, a fundamental tool for analyzing the efficiency of algorithms in terms of both <strong>time complexity</strong> (how runtime scales with input size) and <strong>space complexity</strong> (how memory usage scales). We will explore common time complexities like <em>O(1)</em>, <em>O(log n)</em>, <em>O(n)</em>, and others, along with their real-world implications for choosing the right algorithms and optimizing code. Understanding Big O notation is not only essential for writing scalable and performant software but also for acing technical interviews, as it demonstrates your ability to think critically about algorithmic efficiency. In this chapter we will cover:</p>
<ul>
<li>Big O time complexities</li>
<li>Space complexity</li>
<li>Calculating the complexity of an algorithm</li>
<li>Big O notation and tech interviews</li>
<li>Exercises</li>
</ul>

<h2 data-number="3.1">Understanding Big O notation</h2>
<p>Big O notation is used to describe and classify the performance or complexity of an algorithm according to how much time it will take for the algorithm to run as the input size grows.</p>
<p>And how do we measure the efficiency of an algorithm? We usually use resources such as CPU (time) usage, memory usage, disk usage, and network usage. When talking about Big O notation, we usually consider CPU (time) usage.</p>
<p>In simpler terms, this notation is a way to describe how the running time of an algorithm grows as the size of the input gets bigger. While the actual time an algorithm takes to run can vary depending on factors like processor speed and available resources, Big O notation allows us to focus on the fundamental steps an algorithm must take. Think of it as measuring the number of operations an algorithm performs relative to the input size.</p>
<p>Imagine you have a stack of papers on your desk. If you need to find a specific document, you will have to search through each paper one by one until you locate it. With a small stack of 10 papers, this would not take long. But if you had 20 papers, the search would likely take twice as long, and with 100 papers, it could take ten times as long!</p>
<p>The tasks that a developer must perform daily include choosing what data structure and algorithms to use to resolve a specific problem. It can be an existing algorithm, or you may have to write your own logic to resolve a business user story. It is important to note that any algorithm can work fine and seem okay for a low volume of data, however, then the volume of the input data increases, an inefficient algorithm will grind to halt and impact the application. Knowing how to measure performance is key to achieving these tasks successfully.</p>
<p>Big O notation is important because it helps us compare different algorithms and choose the most efficient one for a particular task. For instance, if you are searching for a specific product in a large online store, you would not want to use an algorithm that requires looking at every single product. Instead, you would use a more efficient algorithm that only needs to look at a small subset of products.</p>


<h2 data-number="3.2">Big O time complexities</h2>
<p>Big O notation uses capital <em>O</em> to denote upper bound. It signifies that the actual running time could be less than but not greater than what the function expresses. It does not tell us the exact running time of an algorithm. Instead, it tells us how bad things could get as the input size grows large.</p>
<p>Imagine you have a messy room and need to find a specific sock. In the worst case, you have to check each item of clothing one by one (this is like a linear time algorithm). Big O tells you that even if your room gets super messy, you will not need to look at more items than are actually there. You might get lucky and find the sock quickly! The actual time might be much less than the Big O prediction.</p>
<p>When analyzing algorithms, the following classifications of time and space complexities are most encountered:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Notation</strong></td>
<td><strong>Name</strong></td>
<td><strong>Explanation</strong></td>
</tr>
<tr class="even">
<td>O(1)</td>
<td>Constant</td>
<td>The algorithm's runtime or space usage remains the same regardless of the input size (n).</td>
</tr>
<tr class="odd">
<td>O(log(n))</td>
<td>Logarithmic</td>
<td>The algorithm's runtime or space usage grows logarithmically with the input size (n). This means that as the input size doubles, the number of operations or memory usage increases by a constant amount.</td>
</tr>
<tr class="even">
<td>O(n)</td>
<td>Linear</td>
<td>The algorithm's runtime or space usage grows linearly with the input size (n). This means that as the input size doubles, the number of operations or memory usage also doubles.</td>
</tr>
<tr class="odd">
<td>O(n <sup>2</sup> )</td>
<td>Quadratic</td>
<td>The algorithm's runtime or space usage grows quadratically with the input size (n). This means that as the input size doubles, the number of operations or memory usage quadruples.</td>
</tr>
<tr class="even">
<td>O(n <sup>c</sup> )</td>
<td>Polynomial</td>
<td>The algorithm's runtime or space usage grows as a polynomial function of the input size (n). This means that as the input size doubles, the number of operations or memory usage increases by a factor (c) that is a polynomial function of the input size.</td>
</tr>
<tr class="odd">
<td>O(c <sup>n</sup> )</td>
<td>Exponential</td>
<td>The algorithm's runtime or space usage grows exponentially with the input size (n). This means that as the input size increases, the number of operations or memory usage grows at an increasingly rapid rate.</td>
</tr>
</tbody>
</table>
<p>Table 2.1: Big O notation classifications of time and space complexities</p>
<p>Let's review each one to understand time complexities in detail.</p>

<h3 data-number="3.2.1">O(1): constant time</h3>
<p><em>O(1)</em> signifies that an algorithm's runtime (or sometimes space complexity) remains constant, regardless of the size of the input data. Whether we are dealing with a small input or a massive one, the time it takes to execute the algorithm does not change significantly.</p>
<p>For example, suppose we would like to calculate the number of seconds of a given number of days. We could create the following function to resolve this request:</p>
<div><pre><code>function secondsInDays(numberOfDays) {
  if (numberOfDays &lt;= 0 || !Number.isInteger(numberOfDays)) {
    throw new Error('Invalid number of days');
  }
  return 60 * 60 * 24 * numberOfDays;
}</code></pre>
</div>
<p>Each minute has 60 seconds, each hour has 60 minutes, and each day has 24 hours.</p>
<p>And we can use <code>console.log</code> to see the output of the results passing different numbers of days:</p>
<div><pre><code>console.log(secondsInDays(1)); // 86400
console.log(secondsInDays(10)); // 864000
console.log(secondsInDays(100)); // 8640000</code></pre>
</div>
<p>If we call this function passing <code>1</code> as argument (<code>secondsinDays(1)</code>), it will take a few milliseconds for this code to output the results. If we execute the function again passing <code>10</code> as argument (<code>secondsinDays(10)</code>), it will also take a few milliseconds for the code to output the results.</p>
<p>This <code>secondsInDays</code> function has a time complexity of <em>O(1)</em> â€“ constant time. The number of operations it performs (multiplication) is fixed and doesn't change with the input <code>numberOfDays</code>. It will take the same amount of time to calculate the result, whether you input 1 day or 1000 days.</p>
<p><em>O(1)</em> algorithms typically do not involve loops that iterate over the data or recursive calls that multiply operations. They often involve direct access to data, like looking up a value in an array by its index or performing a simple calculation. And while <em>O(1)</em> algorithms are incredibly efficient, they are not always applicable to every problem. Some tasks inherently require processing each item in the input, leading to different time complexities.'</p>


<h3 data-number="3.2.2">O(log(n)): logarithmic time</h3>
<p>An <em>O(log n)</em> algorithm's runtime (or sometimes space complexity) grows logarithmically with the input size (<em>n</em>). This means that each step of the algorithm significantly reduces the problem size, often by dividing it in half or a similar fraction. The larger the input size, the smaller the impact each additional element has on the overall runtime. In other words, as the input size doubles, the runtime increases by a constant amount (for example, only one more step).</p>
<p>Imagine you are playing a "<em>guess the number</em>" game. You start with a range of 1 to 64, and with each guess, you cut the possible numbers in half. Let's say your first guess is 30. If it is too high, you now know the number is somewhere between 1 and 29. You have effectively halved the search space! Next, you guess 10 (too low), narrowing the range further to 11 through 29. Your third guess, 20, happens to be correct!</p>
<p>Even if you had started with a much larger range of numbers (like 1 to 1000 or even 1 to 1 million), this halving strategy would still allow you to find the number in a surprisingly small number of guesses â€“ around 7 for 1 to 64, 10 for 1 to 1000, and 20 for 1 to 1 million. This demonstrates the power of logarithmic growth.'</p>
<p>We can say this approach has a time complexity ofâ€¯<em>O(log(n))</em>. With each step, the algorithm eliminates a significant portion of the input, making the remaining work much smaller.</p>
<p>A function that has a time complexity of <em>O(log(n))</em> typically halves the problem size with each step. This complexity is often related to divide and conquer algorithms, which we will cover in <em>Chapter 18, Algorithm Designs and Techniques</em>.</p>
<p>Logarithmic algorithms are incredibly efficient, especially for large datasets. They are often used in scenarios where you need to quickly search or manipulate sorted data, which we will also cover later in this book.</p>


<h3 data-number="3.2.3">O(n): linear time</h3>
<p><em>O(n)</em> signifies that an algorithm's runtime (or sometimes space complexity) grows linearly and proportionally with the input size (<em>n</em>). If we double the size of the input data, the algorithm will take approximately twice as long to run. If we triple the input, it will take about three times as long, and so on.</p>
<p>Imagine you have an array of monthly expenses and want to calculate the total amount spent. Here is how we could do it:</p>
<div><pre><code>function calculateTotalExpenses(monthlyExpenses) {
  let total = 0;
  for (let i = 0; i &lt; monthlyExpenses.length; i++) {
    total += monthlyExpenses[i];
  }
  return total;
}</code></pre>
</div>
<p>The for loop iterates through each element (<code>monthlyExpense</code>) in the array adding it to the <code>total</code> variable, which is then returned with the amount of the total expenses.</p>
<p>We can use the following code to check the output of this function, passing different parameters:</p>
<div><pre><code>console.log(calculateTotalExpenses([100, 200, 300])); // 600
console.log(calculateTotalExpenses([200, 300, 400, 50])); // 950
console.log(calculateTotalExpenses([30, 40, 50, 100, 50])); //270</code></pre>
</div>
<p>The number of iterations (and additions to the <code>total</code>) directly depends on the size of the array (<code>monthlyExpenses.length</code>). If the array has 12 months of expenses, the loop runs 12 times. If it has 24 months, the loop runs 24 times. The runtime increases proportionally to the number of elements in the array.</p>
<p>This is because the function contains a loop that runs <em>n</em> times. Therefore, the time it takes to run this function grows in proportion to the size of the input <em>n</em>. If <em>n</em> doubles, the time to run the function approximately doubles as well. For this reason, we can say the preceding function has a complexity ofâ€¯<em>O(n)</em>, where in this context, <em>n</em> is the input size.</p>
<p>While <em>O(n)</em> algorithms are not as fast as constant time (<em>O(1)</em>) algorithms, they are still considered efficient for many tasks. There are many situations where you need to process every element of the input, making linear time a reasonable expectation.</p>


<h3 data-number="3.2.4">O(nË†2): quadratic time</h3>
<p><em>O(nÂ²)</em> signifies that an algorithm's runtime (or sometimes space complexity) grows quadratically with the input size (<em>n</em>). This means that as the input size doubles, the runtime roughly quadruples. If you triple the input, the runtime increases by a factor of nine, and so on. <em>O(nÂ²)</em> algorithms often involve nested loops, where the inner loop iterates <em>n</em> times for each iteration of the outer loop. This results in approximately <em>n * n</em> (or <em>nÂ²</em>) operations.</p>
<p>Let's go back to the calculation of expenses example. Suppose you have the following data in a spreadsheet, with each expense by month:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Month/Expense</strong></td>
<td><strong>January</strong></td>
<td><strong>February</strong></td>
<td><strong>March</strong></td>
<td><strong>April</strong></td>
<td><strong>May</strong></td>
<td><strong>June</strong></td>
</tr>
<tr class="even">
<td>Water Utility</td>
<td>100</td>
<td>105</td>
<td>100</td>
<td>115</td>
<td>120</td>
<td>135</td>
</tr>
<tr class="odd">
<td>Power Utility</td>
<td>180</td>
<td>185</td>
<td>185</td>
<td>185</td>
<td>200</td>
<td>210</td>
</tr>
<tr class="even">
<td>Trash Fees</td>
<td>30</td>
<td>30</td>
<td>30</td>
<td>30</td>
<td>30</td>
<td>30</td>
</tr>
<tr class="odd">
<td>Rent/Mortgage</td>
<td>2000</td>
<td>2000</td>
<td>2000</td>
<td>2000</td>
<td>2000</td>
<td>2000</td>
</tr>
<tr class="even">
<td>Groceries</td>
<td>600</td>
<td>620</td>
<td>610</td>
<td>600</td>
<td>620</td>
<td>600</td>
</tr>
<tr class="odd">
<td>Hobbies</td>
<td>150</td>
<td>100</td>
<td>130</td>
<td>200</td>
<td>150</td>
<td>100</td>
</tr>
</tbody>
</table>
<p>Table 2.2: Example of monthly expenses</p>
<p>What if we want to write a function that calculates the total expenses for several months? The code for this function is as follows:</p>
<div><pre><code>function calculateExpensesMatrix(monthlyExpenses) {
  let total = 0;
  for (let i = 0; i &lt; monthlyExpenses.length; i++) {
    for (let j = 0; j &lt; monthlyExpenses[i].length; j++) {
      total += monthlyExpenses[i][j];
    }
  }
  return total;
}</code></pre>
</div>
<p>The function has two nested loops:</p>
<ol>
<li>The outer loop (<code>i</code>) iterates over the rows of the matrix (categories or types of expenses within each month).</li>
<li>The inner loop (<code>j</code>) iterates over the columns of the matrix (each month) for each row.</li>
</ol>
<p>Inside the nested loop we simply add the expense to the <code>total</code>, which is then returned at the end of the function.</p>
<p>Let's test this function with the data we previous represented:</p>
<div><pre><code>const monthlyExpenses = [
  [100, 105, 100, 115, 120, 135],
  [180, 185, 185, 185, 200, 210],
  [30, 30, 30, 30, 30, 30],
  [2000, 2000, 2000, 2000, 2000, 2000],
  [600, 620, 610, 600, 620, 600],
  [150, 100, 130, 200, 150, 100]
];
console.log('Total expenses: ', calculateExpensesMatrix(monthlyExpenses)); // 18480</code></pre>
</div>
<p>We can say the preceding function has a complexity of <em>O(nË†2)</em>. This is because the function contains two nested loops. The outer loop will run 6 times (<em>n</em>) and the inner loop will also run 6 times as we have 6 months (<em>m</em>). We can say the total number of operations is <em>n * m</em>. If <em>n</em> and <em>m</em> are similar numbers, we can say <em>n * n</em>, hence <em>nË†2</em>.</p>
<p>In Big O notation, we simplify this to the highest order of magnitude, which is <em>nË†2</em>. This means the time complexity of the function grows quadratically (input size squared) with the input size. So, If you have a 12x12 matrix (12 categories of expenses with 12 months each), the inner loop runs 12 times for each of the 12 months, resulting in 144 operations. If we expand the list of expenses and also the number of months, with a matrix 24x24, the number of operations becomes 576 (24 * 24). This is characteristic of an algorithm with <em>O(nË†2)</em> time complexity.</p>


<h3 data-number="3.2.5">O(2^n): exponential time complexity</h3>
<p><em>O(2^n)</em> signifies that an algorithm's runtime (or sometimes space complexity) doubles with each additional unit of input size (<em>n</em>). If you add just one more element to the input, the algorithm takes approximately twice as long. If you add two more elements, it takes about four times as long, and so on. The runtime increases exponentially. An algorithm with exponential time complexity does not have satisfactory performance.</p>
<p>A classic example of an algorithm that is <em>O(2Ë†n)</em> is when we have brute force that will try all possible combinations of a set of values.</p>
<p>Imagine we want to know how many unique combinations we can have with ice cream toppings or no toppings at all. The available toppings are chocolate sauce, maraschino cherries and rainbow sprinkles.</p>
<p>What are the possible combinations?</p>
<p>Since each topping can be either present or absent, and we have three different toppings, the total number of possible combinations is: 2 * 2 * 2 = 2^3 = 8.</p>
<p>Here is a list of the following combinations:</p>
<ul>
<li>No toppings</li>
<li>Chocolate sauce only</li>
<li>Maraschino cherries only</li>
<li>Rainbow sprinkles only</li>
<li>Chocolate sauce + maraschino cherries</li>
<li>Chocolate sauce + rainbow sprinkles</li>
<li>Maraschino cherries + rainbow sprinkles</li>
<li>Chocolate sauce + maraschino cherries + rainbow sprinkles</li>
</ul>
<p>If we had 10 toppings to choose from, we would have 2 ^ 10 possible combinations, totaling 1024 different combinations.</p>
<p>Another example of exponential complexity algorithm is the brute force attack to break passwords or PINs. If we have a 4-digit (0-9) code PIN, we have a total of 10Ë†4 combinations, totaling 10000 combinations. If we have passwords using letters only, we will have a total of 26Ë†n combinations, where n is the number of letters in the password. If we allow uppercase and lowercase characters in the password, we have a total of 62Ë†n combinations. This is one of the reasons it is important to always create long passwords with letters (both uppercase and lowercase), numbers and especial characters, as the number of possible combinations grow exponentially, making it more difficult to break the password by using brute force.</p>
<p>Exponential algorithms are generally considered impractical for large inputs due to their incredibly rapid growth in runtime. They can quickly become infeasible even for moderately sized datasets. It is crucial to find more efficient algorithms whenever possible.</p>


<h3 data-number="3.2.6">O(n!): factorial time</h3>
<p><em>O(n!)</em> signifies an algorithm's runtime (or sometimes space complexity) grows incredibly rapidly with the input size (<em>n</em>). This growth is even faster than exponential time complexity. An algorithm with factorial time complexity has one of the worst performances.</p>
<p>The factorial of a number <em>n</em> (denoted as <em>n!</em>) is calculated as <em>n * (n-1) * (n-2) , â€¦, * 1</em>. For example, 4! is 4 * 3 * 2 * 1 = 24 .1 As we can see, factorials get very large very quickly</p>
<p>A classic example of an algorithm that is <em>O(n!)</em> is when we try to find all possible permutations of a set, for example, the letters ABCD as follows:</p>
<table>
<tbody>
<tr class="odd">
<td>ABCD</td>
<td>BACD</td>
<td>CABD</td>
<td>DABC</td>
</tr>
<tr class="even">
<td>ABDC</td>
<td>BADC</td>
<td>CADB</td>
<td>DACB</td>
</tr>
<tr class="odd">
<td>ACBD</td>
<td>BCAD</td>
<td>CBAD</td>
<td>DBAC</td>
</tr>
<tr class="even">
<td>ACDB</td>
<td>BCDA</td>
<td>CBDA</td>
<td>DBCA</td>
</tr>
<tr class="odd">
<td>ADBC</td>
<td>BDAC</td>
<td>CDAB</td>
<td>DCAB</td>
</tr>
<tr class="even">
<td>ADCB</td>
<td>BDCA</td>
<td>CDBA</td>
<td>DCBA</td>
</tr>
</tbody>
</table>
<p>Table 2.3: All permutations of letters ABCD</p>
<p>Algorithms with factorial time complexity are generally considered highly inefficient and should be avoided whenever possible. For many problems that initially seem to require <em>O(n!)</em> solutions, there are often cleverer algorithms with much better time complexities (for example: dynamic programming technique).</p>
<blockquote>
<p>We will cover algorithms with exponential and factorial times in <em>Chapter 18, Algorithm Designs and Techniques</em>.</p>
</blockquote>


<h3 data-number="3.2.7">Comparing complexities</h3>
<p>We can create a table with some values to exemplify the cost of the algorithm based on its time complexity and input size, as follows:</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Input Size (n)</strong></td>
<td><strong>O(1)</strong></td>
<td><strong>O(log (n))</strong></td>
<td><strong>O(n)</strong></td>
<td><strong>O(n log(n))</strong></td>
<td><strong>O(nË†2)</strong></td>
<td><strong>O(2Ë†n)</strong></td>
<td><strong>O(n!)</strong></td>
</tr>
<tr class="even">
<td>10</td>
<td>1</td>
<td>1</td>
<td>10</td>
<td>10</td>
<td>100</td>
<td>1024</td>
<td>3628800</td>
</tr>
<tr class="odd">
<td>20</td>
<td>1</td>
<td>1.30</td>
<td>20</td>
<td>26.02</td>
<td>400</td>
<td>1048576</td>
<td>2.4329E+18</td>
</tr>
<tr class="even">
<td>50</td>
<td>1</td>
<td>1.69</td>
<td>50</td>
<td>84.94</td>
<td>2500</td>
<td>1.1259E+15</td>
<td>3.04141E+64</td>
</tr>
<tr class="odd">
<td>100</td>
<td>1</td>
<td>2</td>
<td>100</td>
<td>200</td>
<td>10000</td>
<td>1.26765E+30</td>
<td>9.33262E+157</td>
</tr>
<tr class="even">
<td>500</td>
<td>1</td>
<td>2.69</td>
<td>500</td>
<td>1349.48</td>
<td>250000</td>
<td>3.27339E+150</td>
<td>Very big number</td>
</tr>
<tr class="odd">
<td>1000</td>
<td>1</td>
<td>3</td>
<td>1000</td>
<td>3000</td>
<td>1000000</td>
<td>1.07151E+301</td>
<td>Very big number</td>
</tr>
<tr class="even">
<td>10000</td>
<td>1</td>
<td>4</td>
<td>10000</td>
<td>40000</td>
<td>100000000</td>
<td>Very big number</td>
<td>Very big number</td>
</tr>
</tbody>
</table>
<p>Table 2.4: Comparing Big O time complexity based on input size</p>
<p>We can draw a chart based on the information presented in the preceding table to display the cost of different Big O notation complexities as follows:</p>
<figure>
<img src="img/file7.png" alt="Figure 2.1 â€“ Big O Notation complexity chart" width="1316" height="1048"/><figcaption aria-hidden="true">Figure 2.1 â€“ Big O Notation complexity chart</figcaption>
</figure>
<blockquote>
<p>The preceding chart was also plotted using JavaScript. You can find its source code in the <code>src/02-bigOnotation</code>â€¯directory of the source code bundle.</p>
</blockquote>
<p>When we plot the runtime of algorithms with different time complexities against the input size on a graph, distinct patterns emerge:</p>
<ul>
<li><strong><em>O(1) - Constant Time</em></strong>: a horizontal line. The runtime remains the same regardless of the input size.</li>
<li><strong><em>O(log n) - Logarithmic Time</em></strong>: a gently rising curve that gradually flattens as the input size increases. Think of it as a slope that gets less and less steep. Each additional input element has a diminishing impact on the overall runtime.</li>
<li><strong><em>O(n) - Linear Time</em></strong>: a straight line with a positive slope. The runtime increases proportionally with the input size. Double the input, and the runtime roughly doubles.</li>
<li><strong><em>O(nÂ²) - Quadratic Time</em></strong>: a curve that starts shallow but becomes increasingly steep. The runtime grows much faster than the input size. Double the input, and the runtime roughly quadruples.</li>
<li><strong><em>O(2^n) - Exponential Time</em></strong>: a curve that initially seems flat but then explodes upwards as the input size increases even slightly. The runtime grows incredibly rapidly.</li>
<li><strong><em>O(n!) - Factorial Time</em></strong>: a curve that rises almost vertically. The runtime becomes astronomically large even for relatively small inputs, quickly becoming impractical to compute.</li>
</ul>
<p>These visualizations are invaluable tools for understanding the long-term behavior of algorithms as the input size grows. They help us make informed choices about which algorithms are best suited for different scenarios, especially when dealing with large datasets.</p>



<h2 data-number="3.3">Space complexity</h2>
<p>Space complexity refers to the amount of memory (or space) an algorithm uses to solve a problem. It is a measure of how much additional storage the algorithm requires beyond the space occupied by the input data itself.</p>
<p>It is important to understand space complexity as real-world computers have finite memory. If the algorithm's space complexity is too high, it might run out of memory on large datasets. And even if we have plenty of memory, an algorithm with a high space complexity can still be slower due to factors like increased memory access times and cache issues. Also, it is all about tradeoffs. Sometimes, we might choose an algorithm with a slightly higher space complexity if it offers a significant improvement in time complexity. This of course, needs to be reviewed case by case.</p>
<p>Big O notation works for space complexity just like it does for time complexity. It expresses the upper bound of how the algorithm's memory usage grows as the input size increases. Let's review the common Big O space complexities:</p>
<ul>
<li><strong><em>O(1) - Constant Space</em>:</strong> the algorithm uses a fixed amount of memory, regardless of the input size. This is ideal, as the memory usage will not become a bottleneck.
<ul>
<li>For example: swapping two variables.</li>
</ul></li>
<li><strong><em>O(n) - Linear Space</em>:</strong> the algorithm's memory usage grows linearly with the input size. If we double the input, the memory usage roughly doubles.
<ul>
<li>For example: storing a copy of an input array.</li>
</ul></li>
<li><strong><em>O(log n) - Logarithmic Space</em>:</strong> the algorithm's memory usage grows logarithmically. This is relatively efficient, especially for large datasets.
<ul>
<li>For example: certain recursive algorithms where the depth of recursion is logarithmic.</li>
</ul></li>
<li><strong><em>O(nË†2) - Quadratic Space</em>:</strong> the algorithm's memory usage grows quadratically. This can become a problem for large inputs.
<ul>
<li>For example: storing a multiplication table in a 2D array.</li>
</ul></li>
<li><strong><em>O(2^n) - Exponential Space</em>:</strong> like the exponential time complexity, this indicates extremely rapid growth in memory usage. It is generally not practical and should be avoided.</li>
</ul>


<h2 data-number="3.4">Calculating the complexity of an algorithm</h2>
<p>It is also important to understand how to read algorithmic code and identify its complexity in terms of Big O notation. By analyzing the complexity of an algorithm, we can identify potential bottlenecks and focus on improving that specific area.</p>
<p>To determine the cost of a code in terms of <strong><em>time complexity</em></strong>, we need to review it step by step, and focus on the following points:</p>
<ul>
<li>Basic operations such as assignments, bits and math operations, which will usually have constant time (<em>O(1)</em>).</li>
<li>Logarithmic algorithms (<em>O(log (n))</em>) typically follow a divide-and-conquer strategy. They break the problem into smaller subproblems and solve them recursively.</li>
<li>Loops: the number of times a loop runs directly impacts time complexity. Nested loops multiply their effects. So, if we have one loop iterating through the input of size <em>n</em>, it will be linear time (<em>O(n)</em>), two nested loops (<em>O(nË†2)</em>), and so on.</li>
<li>Recursions: recursive functions call themselves, potentially leading to exponential time complexity if not carefully designed. We will cover recursion in <em>Chapter 9, Recursion</em>.</li>
<li>Function calls: consider the time complexity of any functions that are called within your code.</li>
</ul>
<p>And to determine the cost of a code in terms of <strong>space complexity</strong>, we need to review it step by step, and focus on the following points:</p>
<ul>
<li>Variables: how much memory do variables used in the algorithm consume? Does the number of variables grow with the input size?</li>
<li>Data structures: what data structures are being used (arrays, lists, trees, etc.)? How does their size scale with the input?</li>
<li>Function calls: if the algorithm uses recursion, how many recursive calls are made? Each call adds to the space complexity of the call stack.</li>
<li>Allocations: are we dynamically allocating memory within the algorithm? How much memory is allocated, and how does it relate to the input size?</li>
</ul>
<p>Let's see an example of a function that logs the multiplication table of a given number:</p>
<div><pre><code>function multiplicationTable(num, x) {
  let s = '';
  let numberOfAsterisks = num * x;
  for (let i = 1; i &lt;= numberOfAsterisks; i++) {
    s += '*';
  }
  console.log(s);
  for (let i = 1; i &lt;= num; i++) {
    console.log(`Multiplication table for ${i} with x = ${x}`);
    for (let j = 1; j &lt;= x; j++) {
      console.log(`${i} * ${j} = `, i * j);
    }
  }
}</code></pre>
</div>
<p>Let's break down the time and space complexity of the <code>multiplicationTable</code> function using Big O notation. First, let's focus on time complexity:</p>
<ul>
<li><strong><em>O(1) operations</em></strong>:
<ul>
<li>Assigning variables (<code>let s = ''</code> and <code>let numberOfAsterisks = num * x</code>)</li>
<li>Printing fixed strings (<code>console.log('Calculating the time complexity of a function')</code>)</li>
</ul></li>
<li><strong><em>O(n) operations</em></strong>:
<ul>
<li>Building the asterisk string: the loop iterates <em>num * x</em> times, and each iteration involves string concatenation, which can be a linear operation depending on the JavaScript implementation.</li>
<li>Printing the asterisk string: outputting a string of length <em>num * x</em> takes time proportional to its length.</li>
</ul></li>
<li><strong><em>O(nË†2) operations</em></strong>:
<ul>
<li>Nested loops: the outer loop runs num times, and for each iteration, the inner loop runs <em>x</em> times. This leads to roughly <em>num * x</em> (or <em>nË†2</em>) iterations of the innermost <code>console.log</code> statement, where the actual multiplication takes place.</li>
</ul></li>
</ul>
<p>While there are <em>O(1)</em> and <em>O(n)</em> operations in the function, the dominant factor in the time complexity is the nested loop structure, which leads to quadratic time complexity <em>O(n^2)</em>. In Big O notation, we simplify this to the highest order of magnitude, which is <em>n^2</em>. Therefore, the overall time complexity of the function is <em>O(n^2)</em>.</p>
<p>Now let's review the space complexity:</p>
<ul>
<li><strong><em>O(1) space</em></strong>:
<ul>
<li>Simple variables (<code>s</code>, <code>numberOfAsterisks</code>, loop counters <code>i</code> and <code>j</code>) use a fixed amount of memory, regardless of the input values <code>num</code> and <code>x</code>.</li>
</ul></li>
<li><strong><em>O(n) space</em></strong> (<em>potential</em>):
<ul>
<li>The string <code>s</code> could potentially grow to a size of <em>num * x</em>, meaning its space usage is linear in the input size. However, in most implementations, string concatenation is optimized, so this might not be a major concern unless the input values are very large.</li>
</ul></li>
</ul>
<p>So, overall, the space complexity could be considered <em>O(n)</em> due to the potential growth of the asterisk string. However, for practical purposes, the space usage is usually not a significant issue, and we often focus on the <em>O(nÂ²)</em> time complexity as the primary concern for this function.</p>


<h2 data-number="3.5">Big O notation and tech interviews</h2>
<p>During technical interviews for software developer positions, it is common for companies to do a coding test using some services online such as <strong>LeetCode</strong>, <strong>Hackerrank</strong>, and other similar services.</p>
<p>Choosing the correct data structure or algorithm to solve a problem can tell the company some information about how you solve problems that might pop up for you to resolve.</p>
<p>Interviewers might ask you to analyze code and predict how its runtime or memory usage might change under different input sizes. Once you write code to resolve a problem, interviewers might also ask you to pinpoint potential performance problems in your code and if you can identify areas of optimization. Also, different algorithms and data structures have different time complexities, and knowing Big O allows you to make informed decisions about which solution is best suited for a particular problem, considering all the tradeoffs.</p>
<p>During interviews, you can also showcase your velocity in resolving problems and how to optimize them. For example, in case there is any problem involving array search, you can start with a simple algorithm, to demonstrate you can resolve a problem quickly, depending on the criticality, and once the problem is fixed, demonstrate it can be optimized to use a more performative search, if you have more time to resolve the problem.</p>
<p>In each chapter of this book, we will cover some problems pertaining to the chapter topic, and what we can do to further optimize them.</p>


<h2 data-number="3.6">Exercises</h2>
<p>Now that you've explored the fundamentals of time and space complexity with Big O notation, it's time to test your understanding! Analyze the following JavaScript functions and determine their time and space complexities. Experiment with different inputs to see how the functions behave.</p>
<p><strong><em>1</em></strong>: determines if the array's size is odd or even:</p>
<div><pre><code>const oddOrEven = (array) =&gt; array.length % 2 === 0 ? 'even' : 'odd';</code></pre>
</div>
<p><strong><em>2</em></strong>: calculates and returns the average of an array of numbers:</p>
<div><pre><code>function calculateAverage(array) {
  let sum = 0;
  for (let i = 0; i &lt; array.length; i++) {
    sum += array[i];
  }
  return sum / array.length;
}</code></pre>
</div>
<p><strong><em>3</em></strong>: checks if two arrays have any common values:</p>
<div><pre><code>function hasCommonElements(array1, array2) {
  for (let i = 0; i &lt; array1.length; i++) {
    for (let j = 0; j &lt; array2.length; j++) {
      if (array1[i] === array2[j]) {
        return true;
      }
    }
  }
  return false;
}</code></pre>
</div>
<p><strong><em>4</em></strong>: filters odd numbers from an input array:</p>
<div><pre><code>function getOddNumbers(array) {
  const result = [];
  for (let i = 0; i &lt; array.length; i++) {
    if (array[i] % 2 !== 0) {
      result.push(array[i]);
    }
  }
  return result;
}</code></pre>
</div>
<p>You will find the answers in the source code for this chapter (file <code>src/02-bigOnotation/03-exercises.js</code>). Compare your analysis with the provided solutions to solidify your understanding of Big O notation in real-world JavaScript code!</p>


<h2 data-number="3.7">Summary</h2>
<p>In this chapter, we delved into the fundamental concept of Big O notation, a powerful tool for analyzing and expressing the efficiency of algorithms. We explored how to calculate both time complexity (the relationship between input size and runtime) and space complexity (the relationship between input size and memory usage). We also discussed how Big O analysis is a crucial skill for software developers, aiding in algorithm selection, performance optimization, and technical interviews.</p>
<p>In the next chapter, we will dive into our first data structure: the versatile <strong>Array</strong>. We will explore its common operations, analyze their time complexities, and tackle some practical coding challenges.</p>


</div>
</div>
</body>
</html>