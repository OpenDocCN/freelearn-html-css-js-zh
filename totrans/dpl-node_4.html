<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Managing Memory and Space"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Managing Memory and Space</h1></div></div></div><p>Today's developer has easy access to surprisingly inexpensive storage solutions. The movement away from monolithic systems toward composed and distributed ones has certain advantages, yet inevitably introduces a few new problems. The availability of cheap storage should not be an excuse to push everything you can into memory or onto a disk without any limit, for instance. Also, where does the state reside in such a system? Does a cluster of servers share a common database connection? How is data synchronized in such a setup? If you are using a <span class="emphasis"><em>shared-nothing noSQL</em></span> architecture, how are state changes communicated across all actors?</p><p>There are many considerations. Always seeking to use a minimum of resources is a good guiding principle. In this chapter, we will look at ways to reduce the cost of data storage in your Node programs, including tips on writing efficient, optimized code. Certain strategies for efficiently sharing data across distributed servers will be discussed, including caching strategies, microservices, interprocess messaging, and other techniques to keep your systems fast, light, and scalable. Examples demonstrating how to use tokens to manage user session data efficiently at scale and storing extensive user activity data compactly using Redis will help you put these ideas into practice.</p><div class="section" title="Dealing with large crowds"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec20"/>Dealing with large crowds</h1></div></div></div><p>Because Node is designed to make the writing of networked applications easier, those using Node are often building applications composed of many isolated services that are connected via message queues, sockets, REST APIs, and so on. I will describe these as distributed applications composed of isolated services coupled and coordinated through a network into systems that appear integrated to clients. In this section and the sections that follow, we will <a id="id421" class="indexterm"/>consider how isolated services can be designed to be<a id="id422" class="indexterm"/> memory efficient with a small footprint.</p><p>For the purposes of this section and what follows, the word <span class="strong"><strong>microservice</strong></span> will be used when referring to application architectures composed of many small cooperating services. Generally, we'll explore ideas around how well-designed modularization can often help keep a system from becoming inscrutable by helping maintain expressive, scalable, testable systems that maintain production readiness.</p><p>Then, we'll put the microservice theory into practice by using Richard Rogers' microservice toolkit for Node, <span class="strong"><strong>Seneca</strong></span> (<a class="ulink" href="https://github.com/rjrodger/seneca">https://github.com/rjrodger/seneca</a>). Finally, we'll take a look at<a id="id423" class="indexterm"/> how to use Redis pub/sub as a cross-process communication<a id="id424" class="indexterm"/> system, thus demonstrating another way to compose your own microservice clusters.</p><div class="section" title="Microservices"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec42"/>Microservices</h2></div></div></div><p>Any nontrivial, network-based application is composed of several independent subsystems that <a id="id425" class="indexterm"/>must cooperate to fulfill the business or other requirements <a id="id426" class="indexterm"/>of the larger system. For example, many web applications present browser-based interfaces composed of one or several libraries and/or UI frameworks translating user actions against JavaScript controllers into formalized network requests issued across several web protocols. These ultimately communicate with any number of servers running programs that implement various sorts of business logic—all sharing one or several databases, perhaps across several data centers. These initiate and coordinate even longer chains of requests.</p><p>Because there is no absolute <span class="emphasis"><em>right way</em></span> to build software, every design is biased toward one or a few key principles, in particular, principles guiding how a system should scale, which normally affects how it is deployed. A few of the key principles informing the Node community—modular systems composed of small programs that do one thing well and are event-driven, I/O focused, and network focused—align closely with those underpinning microservices.</p><p>Microservice architecture designs<a id="id427" class="indexterm"/> typically respect the following principles:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A system should be broken down into many small services that each do one thing and no more. This helps with clarity.</li><li class="listitem" style="list-style-type: disc">The code-powering services should be short and simple. A common guideline in the Node community is to limit programs to somewhere near 100 lines of code. This helps with maintainability.</li><li class="listitem" style="list-style-type: disc">No service should depend on the existence of another service or even know of the existence of other services. Services are decoupled. This helps with scalability, clarity, and maintainability.</li><li class="listitem" style="list-style-type: disc">Data models should be decentralized, with a common (but not required) microservice pattern—that each service maintains its own database or a similar model. Services are stateless (this reinforces the previous point).</li><li class="listitem" style="list-style-type: disc">Independent services are easy to replicate (or cull). Scaling (in both directions) is a natural feature of microservice architectures as new <span class="emphasis"><em>nodes</em></span> can be added or removed as necessary. This also enables easy experimentation, where prototype services can be tested, new features can be tested or deployed temporarily, and so on.</li><li class="listitem" style="list-style-type: disc">Independent, stateless services can be replaced or upgraded (or downgraded) independently regardless of the state of any system they form a part of. This opens the possibility of more focused, discrete deployments and refactors.</li><li class="listitem" style="list-style-type: disc">Failure is <a id="id428" class="indexterm"/>unavoidable, so systems should be<a id="id429" class="indexterm"/> designed to fail gracefully. Localize points of failure (the first and second points of this list), isolate failure (the third and fourth points of this list), and implement recovery mechanisms (easier when error boundaries are clearly defined, small, and noncritical). Promote robustness by reducing the scope of unreliability.</li><li class="listitem" style="list-style-type: disc">Testing is essential to any nontrivial system. Unambiguous and simple stateless services are easy to test. A key aspect of testing is simulation—the <span class="emphasis"><em>stubbing</em></span> or <span class="emphasis"><em>mocking</em></span> of services in order to test service interoperability. Clearly delineated services are also easy to simulate and can, therefore, be intelligently composed into testable systems.</li></ul></div><p>The idea is simple: smaller services are easy to reason about individually, encouraging correctness of specifications (little or no gray area) and clarity of APIs (constrained sets of output follow constrained sets of input). Being stateless and decoupled, services promote system composability, help with scaling and maintainability, and are easier to deploy. Also, very precise, discrete monitoring of these sorts of systems is possible.</p></div><div class="section" title="Redis pub/sub"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec43"/>Redis pub/sub</h2></div></div></div><p>In the previous chapter, we discussed the use of message queues, an excellent technique for rapid<a id="id430" class="indexterm"/> cross-process communication. Redis <a id="id431" class="indexterm"/>offers an interface allowing connected clients to subscribe to a particular channel and broadcast messages to that channel. This is generally described as a publish/subscribe paradigm. When you do not need more complex message exchanges and brokers but a simple and fast notification network, pub/sub works well.</p><p>Let's set up a basic pub/sub example and then move on to an example of using pub/sub to create a microservice architecture where many components doing a particular job are passed requests for their services and pass back results—all coordinated via Redis.</p><p>First, let's<a id="id432" class="indexterm"/> look at the most basic example <a id="id433" class="indexterm"/>of pub/sub—a script that demonstrates how to subscribe to a channel and how to publish to that channel:</p><div class="informalexample"><pre class="programlisting">var redis = require("redis");

var publisher = redis.createClient();
var subscriber = redis.createClient();

subscriber.subscribe('channel5');

subscriber.on('message', function(channel, message) {
  console.log('channel: ', channel)
  console.log('message: ', message)
})

subscriber.on('subscribe', function() {
  publisher.publish('channel5', 'This is a message')
})</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>We are using <a id="id434" class="indexterm"/>Matt Ranney's <span class="strong"><strong>Redis</strong></span> npm module. Find out more at <a class="ulink" href="https://github.com/mranney/node_redis">https://github.com/mranney/node_redis</a>.</p></div></div><p>To create both a publisher and a subscriber, we create two Redis clients. Note that, once a <code class="literal">subscribe</code> or <code class="literal">psubscribe</code> (more on <code class="literal">psubscribe</code> later) command is issued to a client, that client will enter <span class="emphasis"><em>subscriber mode</em></span>, no longer accepting standard Redis commands. Typically, you will create two clients: one listening for messages on subscribed channels and the other a standard Redis client used for all other commands.</p><p>Also note that we must wait for the <code class="literal">subscribe</code> event to be fired on the <code class="literal">subscriber</code> client prior to publishing any messages. Redis does not hold a queue of published messages, which involves waiting for subscribers. A message for which there are no subscribers is simply dropped. The following is based on the Redis documentation:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"…published messages are characterized into channels, without knowledge of what (if any) subscribers there may be. Subscribers express interest in one or more channels, and only receive messages that are of interest, without knowledge of what (if any) publishers there are. This decoupling of publishers and subscribers can allow for greater scalability and a more dynamic network topology."</em></span></p></blockquote></div><p>So, we must wait for a subscriber prior to publishing. Once that subscription is made, we can publish to the <code class="literal">channel5</code> channel, and the <code class="literal">subscriber</code> handle listening <code class="literal">on</code> that channel receives our message:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>channel: channel5</strong></span>
<span class="strong"><strong>message: This is a message</strong></span>
</pre></div><p>Let's take<a id="id435" class="indexterm"/> this a little further by creating two <a id="id436" class="indexterm"/>distinct Node processes, each performing a simple (micro) service. We'll build a calculator service with two operations—add and subtract. A separate, dedicated process will perform each operation, and the two-way communication between the calculator service and its helper services will be managed by Redis pub/sub.</p><p>First, we design two Node programs, one that adds and one that subtracts. We'll only show the adder here:</p><div class="informalexample"><pre class="programlisting">var redis = require("redis");
var publisher = redis.createClient();
var subscriber = redis.createClient();

subscriber.subscribe('service:add');
subscriber.on('message', function(channel, operands) {
  var result = JSON.parse(operands).reduce(function(a, b) {
    return a + b;
  })
  publisher.publish('added', result);
})
subscriber.on('subscribe', function() {
  process.send('ok')
})</pre></div><p>The subtraction program is nearly identical, differing only in the channel it listens on and the calculation it performs. These two services exist in the <code class="literal">add.js</code> and <code class="literal">subtract.js</code> files.</p><p>We can see what this service does. When it receives a message on the <code class="literal">service:add</code> channel, it will fetch the two operands passed to it, add them, and publish the result to the <code class="literal">added</code> channel. As we'll soon see, the calculator service will listen for results on the <code class="literal">added</code> channel. Also, you will notice a call to <code class="literal">process.send</code>—this is used to notify the calculator service that the add service is ready. This will make more sense shortly.</p><p>Now, let's build the <code class="literal">calculator.js</code> service itself:</p><div class="informalexample"><pre class="programlisting">var redis = require("redis");
var publisher = redis.createClient();
var subscriber = redis.createClient();

var child_process = require('child_process');
var add = child_process.fork('add.js');
var subtract = child_process.fork('subtract.js');

add.on('message', function() {
  publisher.publish('service:add', JSON.stringify([7,3]))
})
subtract.on('message', function() {
  publisher.publish('service:subtract', JSON.stringify([7,3]))
})
subscriber.subscribe('result:added')
subscriber.subscribe('result:subtracted')
subscriber.on('message', function(operation, result) {
  console.log(operation + ' = ', result);
});</pre></div><p>The main <a id="id437" class="indexterm"/>calculator service forks two new <a id="id438" class="indexterm"/>processes running the <code class="literal">add.js</code> and <code class="literal">subtract.js</code> microservices. Typically, in a real system, the creation of these other services would be done independently, perhaps even on completely separate machines. This simplification is useful for our example, but it does demonstrate a simple way to create vertical scaling across cores. Clearly, each child process in Node on which <code class="literal">fork</code> has been used comes with a communication channel built in, allowing child processes to communicate with their parents as seen in the calculator service's use of <code class="literal">add.on(…)</code> and <code class="literal">substract.on(...)</code> and in our calculation services with <code class="literal">process.send(…)</code>.</p><p>Once the calculator service receives notice that its dependent services are ready, it publishes a request for work to be done on the <code class="literal">service:add</code> and <code class="literal">service:subtract</code> channels by passing operands. As we saw earlier, each service listens on its own channel and performs the work requested, publishing a result that this calculator service can then receive and use. When <code class="literal">calculator.js</code> is executed, the following will be displayed in your terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>result:subtracted = 4</strong></span>
<span class="strong"><strong>result:added = 10</strong></span>
</pre></div><p>Earlier, we mentioned the <code class="literal">psubscribe</code> method. The <code class="literal">p</code> prefix signifies <span class="emphasis"><em>pattern</em></span> and is useful when you want to subscribe to channels using a typical glob pattern. For example, rather than the calculator service subscribing to two channels with the common <code class="literal">result:</code> prefix, we can simplify it as follows:</p><div class="informalexample"><pre class="programlisting">subscriber.psubscribe('result:*')
subscriber.on('pmessage', function(operation, result) {
  console.log(operation + ' = ', result);
})</pre></div><p>Now, any additional service can publish results with the <code class="literal">result:</code> prefix and can be picked up by <a id="id439" class="indexterm"/>our calculator. Note that the <code class="literal">p</code> prefix<a id="id440" class="indexterm"/> must also be reflected in the <code class="literal">pmessage</code> event listener.</p></div><div class="section" title="Microservices with Seneca"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec44"/>Microservices with Seneca</h2></div></div></div><p>Seneca is a Node-based microservice construction kit that helps you organize your code into<a id="id441" class="indexterm"/> distinct <span class="strong"><strong>actions</strong></span> triggered by <span class="strong"><strong>patterns</strong></span>. Seneca <a id="id442" class="indexterm"/>applications are composed of services that <a id="id443" class="indexterm"/>can accept JSON messages and, optionally, return some JSON. Services register an interest in messages with certain characteristics. For example, a service might run whenever a JSON message displaying <code class="literal">{ cmd: "doSomething" }</code> is broadcast.</p><p>To start, let's create a service that responds to two patterns, one pattern returning <code class="literal">"Hello!"</code> and the other returning <code class="literal">"Goodbye!"</code>. Create a <code class="literal">hellogoodbye.js</code> file containing the following code:</p><div class="informalexample"><pre class="programlisting">var seneca = require('seneca')();
var client = seneca.client(8080);

require('seneca')()
.add({
  operation:'sayHello'
},
function(args, done) {
  done(null, {message: "Hello!"})
})
.add({
  operation:'sayGoodbye'
},
function(args, done) {
  done(null, {message: "Goodbye!"})
})
.listen(8080);

client.act({ operation: "sayHello" }, function(err, result) {
  console.log(result.message);
})

client.act({ operation: "sayGoodbye" }, function(err, result) {
  console.log(result.message);
})</pre></div><p>The call to <code class="literal">seneca()</code> starts up a service that will listen on port <code class="literal">8080</code> on <code class="literal">localhost</code> for patterns rendered in the JSON format—one of either <code class="literal">{ operation: "sayHello" }</code> or <code class="literal">{ operation: "sayGoodbye" }</code>. We also create a <code class="literal">client</code> object connected to the Seneca service on <code class="literal">8080</code> and have that client act against those patterns. When this program is executed, you will see <code class="literal">Hello!</code> and <code class="literal">Goodbye!</code> displayed in your terminal.</p><p>Because <a id="id444" class="indexterm"/>the Seneca service is listening on <a id="id445" class="indexterm"/>HTTP by default, you can achieve the same result by<a id="id446" class="indexterm"/> making a direct call over HTTP, operating against the <code class="literal">/act</code> route:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -d '{"operation":"sayHello"}' http://localhost:8080/act</strong></span>
<span class="strong"><strong>// {"message":"Hello!"}</strong></span>
</pre></div><p>Now, let's replicate the calculator application developed earlier, this time using Seneca. We're going to create two services, each listening on a distinct port, with one performing addition and the other performing subtraction. As in the previous calculator example, each will be started as an individual process and called remotely.</p><p>Create an <code class="literal">add.js</code> file as follows:</p><div class="informalexample"><pre class="programlisting">require('seneca')()
.add({
  operation:'add'
},
function(args, done) {
  var result = args.operands[0] + args.operands[1];
  done(null, {
    result : result
  })
})
.listen({
  host:'127.0.0.1',
  port:8081
})</pre></div><p>Next, create a <code class="literal">subtract.js</code> file identical to <code class="literal">add.js</code>, changing only its operation parameter and, of course, its algorithm:</p><div class="informalexample"><pre class="programlisting">...
.add({
  operation:'subtract'
},
...
  var result = args.operands[0] - args.operands[1];
...</pre></div><p>Open two terminals, and start both services:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>node add.js</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>node subtract.js</strong></span>
</pre></div><p>To demonstrate the usage of these services, create a <code class="literal">calculator.js</code> file that binds a client to <a id="id447" class="indexterm"/>each service on its unique <a id="id448" class="indexterm"/>port and acts against them. Note that you must create <a id="id449" class="indexterm"/>distinct Seneca clients:</p><div class="informalexample"><pre class="programlisting">var add = require('seneca')().client({
  host:'127.0.0.1',
  port:8081
})
var subtract = require('seneca')().client({
  host:'127.0.0.1',
  port:8082
})
add.act({
  operation:'add',
  operands: [7,3]
},
function(err, op) {
  console.log(op.result)
})
subtract.act({
  operation:'subtract',
  operands: [7,3]
},
function(err, op) {
  console.log(op.result)
})</pre></div><p>Executing this program will result in the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>10 // adding</strong></span>
<span class="strong"><strong>4 // subtracting</strong></span>
</pre></div><p>Just as with the previous example, we can make a direct HTTP call:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -d '{"operation":"add","operands":[7,3]}' http://127.0.0.1:8081/act</strong></span>
<span class="strong"><strong>// {"result":10}</strong></span>
</pre></div><p>By building out your calculator in this way, each operation can be isolated into its own service, and you can add or remove functionality as needed without affecting the overall program. Should a service develop bugs, you can fix and replace it without stopping the general calculator application. If one operation requires more powerful hardware or more memory, you can shift it to its own server without stopping the calculator application or altering your application logic—you only need to change the IP address of the targeted service. In the <a id="id450" class="indexterm"/>same way, it is easy to see how, by stringing together the <a id="id451" class="indexterm"/>database, authentication, transaction, mapping, and other services, they can be more easily modeled, deployed, scaled, monitored, and<a id="id452" class="indexterm"/> maintained than if they were all coupled to a centralized service manager.</p></div></div></div>
<div class="section" title="Reducing memory usage"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec21"/>Reducing memory usage</h1></div></div></div><p>JavaScript <a id="id453" class="indexterm"/>was born and raised in the browser environment. For most of its history, this also implied that JavaScript programs were running on desktop systems with an enormous pool of available memory. For this reason, many JavaScript programmers have not traditionally thought much about managing memory in their applications.</p><p>In the world <a id="id454" class="indexterm"/>of Node, memory is not so cheap. According to Joyent (<a class="ulink" href="https://github.com/joyent/node/wiki/FAQ#what-is-the-memory-limit-on-a-node-process">https://github.com/joyent/node/wiki/FAQ#what-is-the-memory-limit-on-a-node-process</a>):</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Currently, by default, v8 has a memory limit of 512 MB on 32-bit systems and 1 GB on 64-bit systems. The limit can be raised by setting --max_old_space_size to a maximum of ~1024 (~1 GB) (32-bit) and ~1741 (~1.7 GiB) (64-bit), but it is recommended that you split your single process into several workers if you are hitting memory limits."</em></span></p></blockquote></div><p>Let's go over <a id="id455" class="indexterm"/>possible strategies to reduce the amount of memory your Node programs consume. We'll end with a discussion of how to make use of two memory-efficient data structures supported by Redis when developing your projects.</p><div class="section" title="Use streams, not buffers"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec45"/>Use streams, not buffers</h2></div></div></div><p>The design and implementation of Node.js native modules follow a simple directive: keep everything asynchronous. This design ethic, by convention, informs the design of modules<a id="id456" class="indexterm"/> contributed by the Node community.</p><p>When a process <a id="id457" class="indexterm"/>operates synchronously, it holds, or locks, the total amount <a id="id458" class="indexterm"/>of memory it needs to fully complete, at which point the memory it has held is flushed, usually returning this result to the calling method or process. For example, the following operation will load the entirety of a file into the memory prior to returning it:</p><div class="informalexample"><pre class="programlisting">var http = require('http')
var fs = require('fs')
http.createServer(function(req, res) {
  fs.readFile('./somefile.js', function(err, data) {
    res.writeHead(200);
    res.end(data)
  })
}).listen(8000)</pre></div><p>When a request is made to <code class="literal">localhost:8000</code>, the <code class="literal">somefile.js</code> file is read off the filesystem <span class="emphasis"><em>in its entirety</em></span> and returned to the client. That is the desired effect—but there is a slight problem. Because the entire file is being pushed into a buffer prior to being returned, an amount of memory equal to the byte size of the file must be allocated on each request. While the operation is itself asynchronous (allowing other operations to proceed), just a few requests for a very large file (of several MB, for example) can overflow the memory and take down the Node process.</p><p>Node excels at creating scalable web services. One of the reasons for this is the focus on providing robust <code class="literal">Stream</code> interfaces.</p><p>A better strategy is to stream the file directly to the HTTP response object (which is a writable stream):</p><div class="informalexample"><pre class="programlisting">http.createServer(function(req, res) {
  fs.createReadStream('./static_buffered.js').pipe(res);
}).listen(8000)</pre></div><p>In addition to requiring less code, data is sent (piped) directly to the out stream, using very little memory.</p><p>On the other hand, we can use Stream to enable a very nice and composable pipeline of transformations. There are several ways to achieve this goal (such as with <code class="literal">Transform Stream</code>), but we'll just create our own transformer.</p><p>This script will take an input from <code class="literal">process.stdin</code> and convert what is received to uppercase, piping the result back to <code class="literal">process.stdout</code>:</p><div class="informalexample"><pre class="programlisting">var Stream = require('stream')
var through = new Stream;
through.readable = true;
through.writable = true;
through.write = function(buf) {
  through.emit('data', buf.toString().toUpperCase())
}
through.end = function(buf) {
  arguments.length &amp;&amp; through.write(buf)
  through.emit('end')
}
process.stdin.pipe(through).pipe(process.stdout);</pre></div><p>As much as<a id="id459" class="indexterm"/> possible, convert your program logic into <a id="id460" class="indexterm"/>discrete stream transformations, and compose useful pipelines that do good things with data without touching the memory.</p></div><div class="section" title="Understanding prototypes"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec46"/>Understanding prototypes</h2></div></div></div><p>JavaScript<a id="id461" class="indexterm"/> is<a id="id462" class="indexterm"/> an <span class="strong"><strong>Object-oriented</strong></span> (<span class="strong"><strong>OO</strong></span>) prototype-based language. It is important for you to understand what this<a id="id463" class="indexterm"/> means and how this sort of design is more memory efficient than many traditional OO language designs when used correctly. Because the storage of state data within Node processes is a common practice (such as connection data lookup tables within a socket server), we should leverage the prototypal nature of the language to minimize memory usage. What follows is a brief but pointed comparison of the classical inheritance-based object model and the object system that JavaScript provides in terms of memory usage and efficiency.</p><p>In class-based systems, a <span class="strong"><strong>class</strong></span> contains instructions on how to create instances of itself. In other words, a class describes a set containing objects built according to a class specification, which includes things such as default values for attributes of constructed objects. To create an instance of a class, there must be a class definition that describes how to build that instance. Classes can also inherit properties from each other, creating new instance blueprints that share characteristics with other blueprints—an inheritance model describing the provenance of objects.</p><p>The primary purpose of any OO system is to facilitate the sharing of common knowledge between related objects. For example, this is how you would create two Point instances using an inheritance model:</p><div class="mediaobject"><img src="graphics/1403OS_04_01.jpg" alt="Understanding prototypes"/></div><p>Note that both instances now maintain an identical attribute structure. Additionally, the property x of both point instances has been copied from the base point class. Importantly, notice that the value of x has been copied to each instance even though this attribute value is identical in both instances.</p><p>Objects in a prototypal language do not require a class to define their composition. For example, an object in JavaScript can be created literally:</p><div class="informalexample"><pre class="programlisting">var myPoint = {
  x : 100,
  y : 50
}</pre></div><p>Not requiring the storage of a class definition prior to creating an object instance is already<a id="id464" class="indexterm"/> more memory efficient. Now, consider this use <a id="id465" class="indexterm"/>of prototypes to replicate the inheritance-based example discussed previously. In the following code, we see how a single object, <code class="literal">myPoint</code>, is passed as the first object to <code class="literal">Object.create</code>, which returns a new object with <code class="literal">myPoint</code> as its prototype:</p><div class="informalexample"><pre class="programlisting"> var myPoint = {
  x: 100,
  y: 50
}
var pointA = Object.create(myPoint, {
  y: 100
})
var pointA = Object.create(myPoint, {
  y: 200
})</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>
<code class="literal">Object.create</code> is the <a id="id466" class="indexterm"/>preferred method in modern JavaScript (ES5+) to create objects. Older browsers will not support this syntax. For <a id="id467" class="indexterm"/>more information on compatibility, visit <a class="ulink" href="http://kangax.github.io/compat-table/es5/#Object.create">http://kangax.github.io/compat-table/es5/#Object.create</a>.</p></div></div><p>This creates the following object construct:</p><div class="mediaobject"><img src="graphics/1403OS_04_02.jpg" alt="Understanding prototypes"/></div><p>Note<a id="id468" class="indexterm"/> that each point instance <span class="emphasis"><em>does not store copies</em></span> of <a id="id469" class="indexterm"/>attributes, the value of which is not explicitly declared. Prototypal systems employ message delegation, not inheritance. When a point instance receives the message <span class="emphasis"><em>give me x</em></span>, and it cannot satisfy that request, it delegates the responsibility for satisfying that message to its prototype (which, in this case, does have a value for x). It should be obvious that, in real-world scenarios with large and complex objects, the ability to share default values across many instances without redundantly copying identical bytes will lead to a smaller memory footprint. Additionally, these instances can themselves function as prototypes for other objects, continuing a delegation chain indefinitely and enabling elegant object graphs using only as much memory as necessary to distinguish unique object properties.</p><p>Memory efficiency also speeds up instantiation. As should be clear from the preceding code, delegating responsibility for messages to a prototype implies that your extended receiver requires a smaller instance footprint—fewer slots need to be allocated per object. The following are two construction function definitions:</p><div class="informalexample"><pre class="programlisting">var rec1 = function() {}
rec1.prototype.message = function() { ... }
var rec2 = function() {
  this.message = function() { ... }
}</pre></div><p>Even with these simple definitions, many instances built from the first constructor will consume much less memory than an equal number of instances constructed from the second—<code class="literal">new Rec1()</code> will complete well before <code class="literal">new Rec2()</code> due to the redundant copying seen in the second prototype-less constructor.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note26"/>Note</h3><p>You can see a<a id="id470" class="indexterm"/> performance comparison of the two instantiation methods at <a class="ulink" href="http://jsperf.com/prototype-speeds">http://jsperf.com/prototype-speeds</a>.</p></div></div><p>Use<a id="id471" class="indexterm"/> prototypes intelligently to reduce memory usage<a id="id472" class="indexterm"/> in your objects and to lower instantiation times. Determine the static or infrequently changed attributes and methods of your objects and put those into prototypes. This will allow you to create thousands of objects quickly, while reducing redundancy.</p></div><div class="section" title="Memory-efficient data structures with Redis"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec47"/>Memory-efficient data structures with Redis</h2></div></div></div><p>While you<a id="id473" class="indexterm"/> should use the memory allotted to you in <a id="id474" class="indexterm"/>each Node process, more memory will likely<a id="id475" class="indexterm"/> be needed. In this section, we will look at Redis, an in-memory, high-speed database, and how it can be used to efficiently extend the amount of memory available to your programs.</p><p>At its most basic, Redis is a fast key-value store. We'll see later how it can be used as a cache for commonly used pieces of data. However, it also provides powerful data structures and an API allowing complex operations on those structures, thus helping with the modeling of sets of data and the relationships between sets of data. Here, we will discuss how <a id="id476" class="indexterm"/>to use Redis support for <span class="strong"><strong>Bit Operations</strong></span> (<span class="strong"><strong>bitops</strong></span>) and <span class="strong"><strong>HyperLogLog</strong></span>—two space-efficient and, importantly, space-predictable memory <a id="id477" class="indexterm"/>structures to store and analyze the activity of data.</p><div class="section" title="Using bitwise operations to analyze user actions over time"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec26"/>Using bitwise operations to analyze user actions over time</h3></div></div></div><p>One <a id="id478" class="indexterm"/>of the more interesting features Redis provides <a id="id479" class="indexterm"/>is the ability to store binary numbers as values for keys. Multiple keys containing binary values can be compared by using<a id="id480" class="indexterm"/> the <span class="strong"><strong>bitwise operators</strong></span> AND, OR, and XOR. By applying bitmasks mapping a range of bits to other binary values, you can make very rapid and memory-efficient analytical comparisons. In this section, we will learn some typical examples of how to use this technique.</p><p>Any key in a Redis database can store <code class="literal">(2^32 - 1)</code> bits or just under 512 MiB. This means that there are approximately 4.29 billion columns, or offsets, that can be set per key. This is a large number of data points referenced by a single key. We can set bits along these ranges to describe the characteristics of an item we would like to track, such as the number of users who have viewed a given article. Furthermore, we can use bit operations to gather other dimensions of information, such as what percentage of viewers of an article are female. Let's look at a few examples.</p><div class="section" title="Setting, getting, and counting bits"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec01"/>Setting, getting, and counting bits</h4></div></div></div><p>Let's assume<a id="id481" class="indexterm"/> that we are serving many different articles and <a id="id482" class="indexterm"/>each article is assigned a unique identifier. Also <a id="id483" class="indexterm"/>assume that we have 100,000 active members on our website, and that each user also has a unique identifier—a number between 1 and 100,000. Using bit operations, we can easily track article view activity on a given day by creating a key in Redis, which can be done by combining the article's unique key and a date string and setting bits at that key corresponding to the user ID associated with an article view. For example:</p><div class="informalexample"><pre class="programlisting">article:324:01-03-2014 : 00010100111010001001111...</pre></div><p>This key represents article 324 on a specific date, efficiently storing the unique user IDs of viewers on that day by <span class="emphasis"><em>flipping a bit</em></span> at an offset corresponding to the user's assigned ID. Whenever a user views an article, fetch that user's ID, use that number as an offset value, and use the <code class="literal">setbit</code> command to set a bit at that offset:</p><div class="informalexample"><pre class="programlisting">redis.setbit('article:324:01-03-2014', userId, 1)</pre></div><p>In what follows, we're going to demonstrate how to use Redis bitops to efficiently store and analyze data. First, let's create data for three articles:</p><div class="informalexample"><pre class="programlisting">var redis = require('redis');
var client = redis.createClient();
var multi = client.multi();
//  Create three articles with randomized hits representing user views
var id = 100000;
while(id--) {
  multi.setbit('article1:today', id, Math.round(Math.random(1)));
  multi.setbit('article2:today', id, Math.round(Math.random(1)));
  multi.setbit('article3:today', id, Math.round(Math.random(1)));
}
multi.exec(function(err) {
  // done
})</pre></div><p>Here, we simply created three Redis keys, <code class="literal">'article (1-3):today'</code>, and randomly set 100,000 bits on each key—either 0 or 1. Using the technique of storing user activity based on user ID offsets, we now have sample data for a hypothetical day of traffic against three articles.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note27"/>Note</h3><p>We're using Matt Ranney's <code class="literal">node_redis</code> module (<a class="ulink" href="https://github.com/mranney">https://github.com/mranney</a>), which supports the Redis <span class="strong"><strong>multi</strong></span> construct, allowing <a id="id484" class="indexterm"/>the execution of several instructions in <a id="id485" class="indexterm"/>one pipeline rather than suffering the cost of calling each individually. Always use <code class="literal">multi</code> when performing several operations in order to speed up your operations. Note also how the ordering guarantees provided by Redis ensure ordered execution and how its atomicity guarantees that either all or none of the instructions in a transaction<a id="id486" class="indexterm"/> will succeed. See <a class="ulink" href="http://redis.io/topics/transactions">http://redis.io/topics/transactions</a>.</p></div></div><p>To <a id="id487" class="indexterm"/>count <a id="id488" class="indexterm"/>the number <a id="id489" class="indexterm"/>of users who have viewed an article, we can use <code class="literal">bitcount</code>:</p><div class="informalexample"><pre class="programlisting">client.bitcount('article1:today', function(err, count) {
  console.log(count)
})</pre></div><p>This is straightforward: the number of users who saw the article equals the number of bits set on the key. Now, let's count the total number of article views:</p><div class="informalexample"><pre class="programlisting">client.multi([
  ["bitcount", "article1:today"],
  ["bitcount", "article2:today"],
  ["bitcount", "article3:today"]
]).exec(function(err, totals) {
  var total = totals.reduce(function(prev, cur) {
    return prev + cur;
  }, 0);
  console.log("Total views: ", total);
})</pre></div><p>Once <code class="literal">multi</code> returns an array of results corresponding to the results returned by Redis for each operation (a count of bits), we <code class="literal">reduce</code> the count to a sum representing the total number of views of all our articles.</p><p>If we are interested, instead, in how many articles user 123 has viewed today, we can use <code class="literal">getbit</code>, which simply returns the value (either 0 or 1) at a given offset. The result will be in the range 0–3:</p><div class="informalexample"><pre class="programlisting">client.multi([
  ["getbit", "article1:today", 123],
  ["getbit", "article2:today", 123],
  ["getbit", "article3:today", 123]
]).exec(function(err, hits) {
  var total = hits.reduce(function(prev, cur) {
    return prev + cur;
  }, 0);
  console.log(total); // 0, 1, 2 or 3
})</pre></div><p>These <a id="id490" class="indexterm"/>are <a id="id491" class="indexterm"/>very <a id="id492" class="indexterm"/>useful and direct ways to glean information from bit representations. Let's go a little further and learn about filtering bits using bitmasks and the AND, OR, and XOR operators.</p></div><div class="section" title="Bitmasks and filtering results"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec02"/>Bitmasks and filtering results</h4></div></div></div><p>Previously, we<a id="id493" class="indexterm"/> learned how to count the number of articles user<a id="id494" class="indexterm"/> 123 has seen. What if we want to check whether user 123 has read both articles? Using the bitop AND, this is easy to accomplish:</p><div class="informalexample"><pre class="programlisting">client.multi([
  ['setbit', 'user123', 123, 1],
  ['bitop', 'AND','123:sawboth','user123','article1:today','article3:today'], 
  ['getbit', '123:sawboth', 123]
]).exec(function(err, result) {
  var sawboth = result[2];
  console.log('123 saw both articles: ', !!sawboth);
});</pre></div><p>First, we create a mask that isolates a specific user stored at the key <code class="literal">'user123'</code>, containing a single positive bit at offset 123 (again, representing the user's ID). The results of an AND operation on two or more bit representations is not returned as a value by Redis but rather written to a specified key, which is given in the preceding example as <code class="literal">'123:sawboth'</code>. This key contains the bit representation that answers the question whether <span class="emphasis"><em>both</em></span> the article keys contain bit representations that also have a positive bit at the same offset as the 'user123' key.</p><p>What if we <a id="id495" class="indexterm"/>wanted to find the total number of users who have <a id="id496" class="indexterm"/>seen at least one article? The bitop OR works well in this case:</p><div class="informalexample"><pre class="programlisting">client.multi([
  ['bitop', 'OR','atleastonearticle','article1:today','article2:today','article3:today'],
  ['bitcount', 'atleastonearticle']
]).exec(function(err, results) {
  console.log("At least one: ", results[1]);
});</pre></div><p>Here, the <code class="literal">'atleastonearticle'</code> key flags bits at all offsets that were set in any one of the three articles.</p><p>We can use these techniques to create a simple recommendation engine. For example, if we are able to determine via other means that two articles are similar (based on tags, keywords, and so on), we can find all users that have read one and recommended the other. To do this, we will use XOR in order to find all users that have read the first article or the second article, but not both. We then break that set into two lists: those who have read the first article and those who have read the second article. We can then use these lists to offer recommendations:</p><div class="informalexample"><pre class="programlisting">client.multi([
  ['bitop','XOR','recommendother','article1:today','article2:today'], 
['bitop','AND','recommend:article1','recommendother','article2:today'], 
  ['bitop','AND','recommend:article2','recommendother','article1:today'], 
  ['bitcount', 'recommendother'], 
  ['bitcount', 'recommend:article1'], 
  ['bitcount', 'recommend:article2'], 
  ['del', 'recommendother', 'recommend:article1', 'recommend:article2'] 
]).exec(function(err, results) { 
  //  Note result offset due to first 3 setup ops 
  console.log("Didn't see both articles: ", results[3]); 
  console.log("Saw article2; recommend article1: ", results[4]); 
  console.log("Saw article1; recommend article2: ", results[5]); 
})</pre></div><p>While it is not necessary, we also fetch a count of each list and delete the result keys when we are done.</p><p>The total number of bytes occupied by a binary value in Redis is calculated by dividing the largest offset by 8. This means that storing access data for even 1,000,000 users on one article <a id="id497" class="indexterm"/>requires 125 KB—not a lot. If you have 1,000 articles in <a id="id498" class="indexterm"/>your database, you can store full-access data for 1,000,000 users in 125 MB—again, not a very large amount of memory or storage to spend in return for such a rich set of analytics data. Also, the amount of storage needed can be precisely calculated ahead of time.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note28"/>Note</h3><p>View the code bundle for an example of building a <span class="emphasis"><em>like this page</em></span> service, where we use a bookmarklet to trigger <span class="emphasis"><em>likes</em></span> on any URL using bit operations to store the time at which each <span class="emphasis"><em>like</em></span> occurs (offsetting by the current second on a given day).</p></div></div><p>Other useful ways to deploy bitwise ideas are easy to find. Consider that if we allocate 86,400 bits to a key (the number of seconds in a day) and set a bit corresponding to the current second in the day, whenever a particular action is performed (such as a login), we have spent <span class="emphasis"><em>86400 / 8 / 1000 = 10.8 KB</em></span> to store login data that can easily be filtered using bitmasks to deliver analytics data.</p><p>As an exercise, use bitmasks to demonstrate gender breakdown in article readership. Assume that we have stored two keys in Redis, one reflecting the user IDs identified as female and the other as male:</p><div class="informalexample"><pre class="programlisting">users:female  : 00100001011000000011110010101...
users:male  : 11011110100111111100001101010...</pre></div><p>Using bit operations, we filter articles by gender.</p></div></div><div class="section" title="Using HyperLogLog to count unique anonymous visitors"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec27"/>Using HyperLogLog to count unique anonymous visitors</h3></div></div></div><p>One of the <a id="id499" class="indexterm"/>most common things done with databases is storing and counting unique things. How many events of a certain type have occurred? How <a id="id500" class="indexterm"/>many tags have been created?</p><p>Consider the nearly ubiquitous task of every marketer: counting the number of unique visitors to a web page. Traditionally, counting is done in databases by writing a row of data or in logs by writing a line of text whenever a visitor lands on a page. Each unique visit increments the set length by one. These are simple and straightforward techniques.</p><p>However, there is a problem: what if the same person arrives at the same page more than once? Whenever the user <span class="emphasis"><em>John</em></span> lands on a page, some work must be done to determine whether this is a first-time occurrence (record it), or a repeat occurrence (don't record it). And there is another problem: the entire sequence of bytes representing a unique identifier—typically a very long hash—must be stored. Each unique item adds to the total memory expended in keeping track of item counts of the cardinality of a set. As we can't know in advance how many unique hits will occur, we cannot know how much memory will be needed to store this potential activity; we are, therefore, exposed to the risk of our system being overwhelmed when one page or another becomes very popular, goes viral, and so on, overnight.</p><p>HyperLogLog<a id="id501" class="indexterm"/> is a probabilistic data structure that allows a nearly infinite number of unique items to be counted within a fixed memory allocation. As Salvatore Sanfilippo puts it at <a class="ulink" href="http://antirez.com/news/75">http://antirez.com/news/75</a>:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"HyperLogLog is remarkable as it provides a very good approximation of the cardinality of a set even using a very small amount of memory. In the Redis implementation it only uses 12kbytes per key to count with a standard error of 0.81%, and there is no limit to the number of items you can count, unless you approach 2^64 items (which seems quite unlikely)."</em></span></p></blockquote></div><p>In your code bundle, you will find the <code class="literal">/hyperloglog</code> folder containing a simple counting application. Start this application by running <code class="literal">server.js</code>, and then, in your browser, visit <code class="literal">localhost:8080</code>. When you get there, click on the <span class="strong"><strong>Send a specific value</strong></span> button. You should see the following output:</p><div class="mediaobject"><img src="graphics/1403OS_04_03.jpg" alt="Using HyperLogLog to count unique anonymous visitors"/></div><p>You have<a id="id502" class="indexterm"/> inserted the value <code class="literal">123</code> into a HyperLogLog key, and the number returned (<code class="literal">1</code>) is the cardinality of that key's set. Click on the <a id="id503" class="indexterm"/>same button a few times— given that this structure maintains a count of unique values, the number should not change. Now, try adding random values. You will see the numbers returned go up. Regardless of how many entries you make in the log key, the same amount of memory will be used. This sort of predictability is great when scaling out your application.</p><p>You can find the <code class="literal">index.html</code> page describing this client interface in the code bundle. All that the client needs to do is send an XHR request to <code class="literal">localhost:8080/log/&lt;some value&gt;</code>. Feel free to browse the code. More to the point, let's look at how the relevant route handler is defined on the server to insert values into HyperLogLog and retrieve log cardinality:</p><div class="informalexample"><pre class="programlisting">var http   = require('http');
var redis  = require('redis');
var client = redis.createClient();
var hyperLLKey = 'hyper:uniques';

...

http.createServer(function(request, response) {

  var route  = request.url;
  var val    = route.match(/^\/log\/(.*)/);

...

  if(val) {
    val = val[1];
    return client.pfadd(hyperLLKey, val, function() {
      client.pfcount(hyperLLKey, function(err, card) {
        respond(response, 200, JSON.stringify({
          count: err ? 0 : card
        }))
      })
    });
  }
}).listen(8080)</pre></div><p>After validating <a id="id504" class="indexterm"/>that we have received a new value on the <code class="literal">/log</code> route, we add that value to <code class="literal">hyperLLKey</code> using the <code class="literal">PFADD</code> command (in Redis, if <a id="id505" class="indexterm"/>a key does not exist when performing an insert operation, it is automatically created). Once inserted successfully, the key is queried for its <code class="literal">PFCOUNT</code>, and the updated set's cardinality is returned to the client.</p><p>In addition, the <code class="literal">PFMERGE</code> command lets you merge (create the union of) several HyperLogLog sets and fetch the cardinality of the resulting set. The following code will result in a cardinality value of <code class="literal">10</code>:</p><div class="informalexample"><pre class="programlisting">var redis  = require('redis');
var client= redis.createClient();
var multi  = client.multi();

client.multi([
  ['pfadd', 'merge1', 1, 2, 3, 4, 5, 6, 10],
  ['pfadd', 'merge2', 1, 2, 3, 4, 5, 6, 7, 8, 9],
  ['pfmerge', 'merged', 'merge1', 'merge2'],
  ['pfcount', 'merged'],
  ['del', 'merge1', 'merge2', 'merged']
]).exec(function(err, result) {
  console.log('Union set cardinality', result[3]);
});</pre></div><p>The ability to <a id="id506" class="indexterm"/>approximate the cardinality of merged sets brings to mind the sort of efficient analytics possibilities <a id="id507" class="indexterm"/>we saw when exploring bitwise operations. Consider HyperLogLog when counts of many unique values are useful analytically and an imprecise but very closely approximated count is sufficient (such as tracking the number of users who logged in today, the total number of pages viewed, and so on).</p></div></div></div>
<div class="section" title="Taming V8 and optimizing performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec22"/>Taming V8 and optimizing performance</h1></div></div></div><p>V8 manages<a id="id508" class="indexterm"/> Node's main process thread. When executing JavaScript, V8 does so in its<a id="id509" class="indexterm"/> own process, and its internal behavior is <span class="emphasis"><em>not</em></span> controlled by Node. However, we can write JavaScript in a way that helps V8 achieve optimal compilation results. In this section, we'll focus on how to write efficient JavaScript and take a look at special configuration flags we can pass to V8 that help with keeping our Node process fast and light.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip08"/>Tip</h3><p>The version of V8 used by your Node installation can be viewed by typing the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>node –e "console.log(process.versions.v8)"</strong></span>
</pre></div></div></div><div class="section" title="Optimizing JavaScript"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec48"/>Optimizing JavaScript</h2></div></div></div><p>The convenience of a dynamic language is in avoiding the strictness that compiled languages<a id="id510" class="indexterm"/> impose. For example, you need not explicitly define <a id="id511" class="indexterm"/>object property types and can actually change those property types at will. This dynamism makes traditional compilation impossible but opens up interesting new opportunities for exploratory languages, such as JavaScript. Nevertheless, dynamism introduces a significant penalty in terms of execution speeds when compared to statically compiled languages. The limited speed of JavaScript has regularly been identified as one of its major weaknesses.</p><p>V8 attempts to achieve the sorts of speeds with JavaScript that one observes for compiled languages. V8 attempts to compile JavaScript into native machine code rather than interpreting bytecode or using other just-in-time techniques. Because the precise runtime topology of a JavaScript program cannot be known ahead of time (the language is dynamic), compilation consists of a two-stage, speculative approach:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Initially, a first-pass compiler converts your code into a runnable state as quickly as possible. During this step, type analysis and other detailed analysis of the code is deferred, achieving fast compilation—your JavaScript can begin executing as close to instantly as possible. Further optimizations are accomplished during the second step.</li><li class="listitem">Once the program is up and running, an optimizing compiler then begins its job of watching how your program runs and attempting to determine its current and future runtime characteristics, optimizing and re-optimizing as necessary. For example, if a certain function is called many thousands of times with similar arguments of a consistent type, V8 recompiles that function with optimized code. While the first compile step was conservative with an as-yet unknown and untyped functional signature, this <span class="emphasis"><em>hot</em></span> function's predictable texture impels V8 to assume a certain optimal profile and recompile based on that assumption.</li></ol></div><p>Assumptions help us make decisions more quickly but can lead to mistakes. What if the hot function that V8's compiler just optimized against a certain type signature is now called <a id="id512" class="indexterm"/>with arguments violating that optimized profile? V8 has <a id="id513" class="indexterm"/>no choice in that case: it must de-optimize the function—V8 must admit its mistake and roll back the work it has done. It will re-optimize in the future if a new pattern is seen. However, if V8 must again de-optimize at a later time and if this binary switching of optimizing/de-optimizing continues, V8 simply <span class="emphasis"><em>gives up</em></span> and leaves your code in a de-optimized state.</p><p>Two areas of focus for the V8 team are achieving fast property access and dynamically creating efficient machine code. Let's look at ways to approach the design and declaration of arrays, objects, and functions so that you are helping, rather than hindering, the compiler.</p><div class="section" title="Numbers and tracing optimization/de-optimization"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec28"/>Numbers and tracing optimization/de-optimization</h3></div></div></div><p>The ECMA-262<a id="id514" class="indexterm"/> specification<a id="id515" class="indexterm"/> defines the <code class="literal">Number</code> value as a <span class="emphasis"><em>primitive value corresponding to a double-precision, 64-bit binary format IEEE 754 value</em></span>. The point is that there is no integer type in JavaScript; there is a <code class="literal">Number</code> type defined as a double-precision floating-point number.</p><p>V8 uses 32-bit numbers for <span class="emphasis"><em>all</em></span> values internally for performance reasons that are too technical to discuss here. It can be said that, should greater width be needed, one bit is used to point to another 32-bit number. Regardless, it is clear that there are two types of values tagged as numbers by V8 and switching between these types will cost you something. Try to restrict your needs to 31-bit signed integers where possible.</p><p>Because of the type ambiguity of JavaScript, switching the types of numbers assigned to a slot is allowed. The following code does not throw an error:</p><div class="informalexample"><pre class="programlisting">var a = 7;
a = 7.77;</pre></div><p>However, a speculative compiler such as V8 will be unable to optimize this variable assignment given that its <span class="emphasis"><em>guess</em></span> that <code class="literal">a</code> will always be an integer turned out to be wrong, forcing de-optimization.</p><p>We can demonstrate this using powerful V8 options available to you when executing code: executing V8 native commands in your Node program and tracing how V8 optimizes/de-optimizes your code.</p><p>Consider <a id="id516" class="indexterm"/>the following Node program:</p><div class="informalexample"><pre class="programlisting">var someFunc = function foo(){}
console.log(%FunctionGetName(someFunc));</pre></div><p>If you try to<a id="id517" class="indexterm"/> run this normally, you receive an <code class="literal">Unexpected Token</code> error—the modulo (<code class="literal">%</code>) symbol cannot be used within an identifier name in JavaScript. What is this strange method with a <code class="literal">%</code> prefix? It is a V8 native command, and we can turn to the execution of these types of functions using the <code class="literal">--allow-natives-syntax</code> flag as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>node --allow-natives-syntax program.js</strong></span>
<span class="strong"><strong>// foo</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note29"/>Note</h3><p>You can learn<a id="id518" class="indexterm"/> about the available native functions<a id="id519" class="indexterm"/> by browsing the V8 source at <a class="ulink" href="https://code.google.com/p/v8/source/browse/trunk/src/runtime.cc?r=22500">https://code.google.com/p/v8/source/browse/trunk/src/runtime.cc?r=22500</a>, and searching for <span class="strong"><strong>runtime_function</strong></span>.</p></div></div><p>Now, consider the following code, which uses native functions to assert information about the optimization status of the <code class="literal">square</code> function using the <code class="literal">%OptimizeFunctionOnNextCall</code> native method:</p><div class="informalexample"><pre class="programlisting">var operand = 3;
function square() {
  return operand * operand;
}
//  Make first pass to gather type information
square();
//  Ask that the next call of #square trigger an optimization attempt;
//  Call
%OptimizeFunctionOnNextCall(square);
square();</pre></div><p>Create a file using the preceding code and execute it using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>node --allow-natives-syntax --trace_opt --trace_deopt myfile.js</strong></span>
</pre></div><p>You will see something like the following output returned:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[deoptimize context: c39daf14679]</strong></span>
<span class="strong"><strong>[optimizing: square / c39dafca921 - took 1.900, 0.851, 0.000 ms]</strong></span>
</pre></div><p>We can see that V8 has no problem optimizing the <code class="literal">square</code> function as the operand is declared once and never changed. Now, append the following lines to your file and run it again:</p><div class="informalexample"><pre class="programlisting">%OptimizeFunctionOnNextCall(square);
operand = 3.01;
square();</pre></div><p>On this <a id="id520" class="indexterm"/>execution, following the optimization report given earlier, you<a id="id521" class="indexterm"/> should now receive something like the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>**** DEOPT: square at bailout #2, address 0x0, frame size 8</strong></span>
<span class="strong"><strong>[deoptimizing: begin 0x2493d0fca8d9 square @2]</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[deoptimizing: end 0x2493d0fca8d9 square =&gt; node=3, pc=0x29edb8164b46, state=NO_REGISTERS, alignment=no padding, took 0.033 ms]</strong></span>
<span class="strong"><strong>[removing optimized code for: square]</strong></span>
</pre></div><p>This very expressive optimization report tells the story very clearly—the once-optimized <code class="literal">square</code> function was de-optimized following the change we made in one number's type. You are encouraged to spend time writing code and to test it using these methods now and as you move through this section.</p></div><div class="section" title="Objects and arrays"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec29"/>Objects and arrays</h3></div></div></div><p>As we learned <a id="id522" class="indexterm"/>when investigating numbers, V8 works best when your<a id="id523" class="indexterm"/> code is predictable. The same holds true with arrays and objects. Nearly all of the following <span class="emphasis"><em>bad practices</em></span> are bad for the simple reason that they create unpredictability.</p><p>Remember that, in JavaScript, an object and an array are very similar <span class="emphasis"><em>under the hood</em></span>. We won't be discussing those differences but only the important similarities, specifically in terms of how both these data constructs benefit from similar optimization techniques.</p><p>Avoid mixing types in arrays. It is always better to have a consistent data type, such as <span class="emphasis"><em>all integers</em></span> or <span class="emphasis"><em>all strings</em></span>. Also, avoid changing types in arrays or in property assignments after initialization, if possible. V8 creates <span class="emphasis"><em>blueprints</em></span> of objects by creating hidden classes to track types, and, when those types change, the optimization blueprints will be destroyed and<a id="id524" class="indexterm"/> rebuilt—if you're lucky. See the following link for more information:</p><p>
<a class="ulink" href="https://developers.google.com/v8/design">https://developers.google.com/v8/design</a>
</p><p>Don't create arrays with gaps, an example of which is shown as follows:</p><div class="informalexample"><pre class="programlisting">var a = [];
a[2] = 'foo';
a[23] = 'bar';</pre></div><p>Sparse arrays are bad for this reason: V8 can either use a very efficient <span class="emphasis"><em>linear storage</em></span> strategy to store (and access) your array data, or it can use a hash table (which is much slower). If your array is sparse, V8 must choose the less efficient of the two. For the same reason, always<a id="id525" class="indexterm"/> start your arrays at the zero index. Also, don't ever use <code class="literal">delete</code> to <a id="id526" class="indexterm"/>remove elements from an array. You are simply inserting an <code class="literal">undefined</code> value at that position, which is just another way of creating a sparse array. Similarly, be careful about populating an array with empty values—ensure that the external data you are pushing into an array is not incomplete.</p><p>Try not to pre-allocate large arrays—grow as you go. Similarly, do not pre-allocate an array and then exceed that size. You always want to avoid spooking V8 into turning your array into a hash table.</p><p>V8 creates a new hidden class whenever a new property is added to an object constructor. Try to avoid adding properties after an object is instantiated. Initialize all members in constructor functions in the same order. <span class="emphasis"><em>Same properties + same order = same object</em></span>.</p><p>Remember that JavaScript is a dynamic language that allows object (and object prototype) modifications <span class="emphasis"><em>after</em></span> instantiation. Since the shape and volume of an object can, therefore, be altered <span class="emphasis"><em>after the fact</em></span>, how does V8 allocate memory for objects? It makes certain reasonable assumptions. After a set number of objects is instantiated from a given constructor (I believe 8 is the trigger number), the largest of these is assumed to be of the maximum size, and all further instances are allocated that amount of memory (and the initial objects are similarly resized). A total of 32 <span class="emphasis"><em>fast property slots</em></span> is then allocated to each instance based on this assumed maximum size. Any <span class="emphasis"><em>extra properties</em></span> are slotted into a (slower) overflow property array that can be resized to accommodate any further new properties.</p><p>With objects, just as with arrays, try as much as possible to define the shape of your data structures in a <span class="emphasis"><em>futureproof</em></span> manner, with a set number of properties, types, and so on.</p></div><div class="section" title="Functions"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec30"/>Functions</h3></div></div></div><p>Functions<a id="id527" class="indexterm"/> are typically called often and should be one of your prime optimization focuses. Functions containing try-catch constructs are <span class="emphasis"><em>not optimizable</em></span>, nor are functions containing other <span class="emphasis"><em>unpredictable</em></span> constructs, such as <code class="literal">with</code> and <code class="literal">eval</code>. If, for some reason, your function is not optimizable, keep its use to a minimum.</p><p>A very common optimization error involves the use of polymorphic functions. Functions that accept variable function arguments will be de-optimized. Avoid polymorphic functions.</p></div></div></div>
<div class="section" title="Caching strategies"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Caching strategies</h1></div></div></div><p>
<span class="strong"><strong>Caching</strong></span>, generally, is the strategy of creating easily accessible intermediate versions of assets. When <a id="id528" class="indexterm"/>retrieving an asset is expensive—in terms of time, processor cycles, memory, and so on—you should consider caching that asset. For example, if a list <a id="id529" class="indexterm"/>of Canadian provinces must be fetched from your database each time a person from that country visits, it is a good idea to store that list in a static format, obviating the expensive operation of running a database query on each visit. A good caching strategy is essential to any web-based application that serves large numbers of rendered data views, be they HTML pages or JSON structures. Cached content can be served cheaply and quickly.</p><p>Whenever you deploy content that doesn't change often, you most likely want to cache your files. Two general types of <span class="emphasis"><em>static</em></span> assets are commonly seen. Assets such as a company logo, existing as-is in a content folder, will change very rarely. Other assets do change more often but much less frequently than on every request of the asset. This second class encompasses such things as CSS style sheets, lists of user contacts, latest headlines, and so on. Creating a reliable and efficient caching system is a nontrivial problem:</p><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>"There are only two hard things in Computer Science: cache invalidation and naming things."</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<span class="attribution"><span class="emphasis"><em>Phil Karlton</em></span></span></td></tr></table></div><p>In this section, we'll look at two strategies to cache your application content. First, we'll look at using Redis as an in-memory key-value cache for regularly used JSON data, learning about the Redis key expiry and key scanning. Finally, we'll investigate how to manage your content using the CloudFlare <span class="strong"><strong>content delivery network</strong></span> (<span class="strong"><strong>CDN</strong></span>), in the process learning<a id="id530" class="indexterm"/> something about using Node to <a id="id531" class="indexterm"/>watch for file changes and then invalidating a CDN cache when change <a id="id532" class="indexterm"/>events are detected.</p><div class="section" title="Using Redis as a cache"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec49"/>Using Redis as a cache</h2></div></div></div><p>In the<a id="id533" class="indexterm"/> example session-store implemented earlier, cookie values were <a id="id534" class="indexterm"/>stored in Redis and matched against incoming values to provide simple session management. This sort of regular checking of small, in-memory values is a common pattern in multiuser environments, for which technologies such<a id="id535" class="indexterm"/> as <span class="strong"><strong>memcached</strong></span> were developed.</p><p>Redis is fully capable of functioning as a similar in-memory caching system. Let's implement a simple caching layer using Redis that intelligently manages key association and expiry.</p><p>Because many types of information will be cached, it is a good idea to namespace your cache keys. We'll structure our cache library such that individual namespace-aware cache APIs can be instantiated:</p><div class="informalexample"><pre class="programlisting">var redis  = require('redis');
var util   = require('util');
var Promise = require('bluebird');
var Cache = function(config) {
  config = config || {};
  this.prefix = config.prefix ? config.prefix + ':' : 'cache:';

  var port = config.port || 6379;
  var host = config.host || 'localhost';

  this.client = redis.createClient(port, host, config.options || {});

  config.auth &amp;&amp; this.client.auth(config.auth);
};</pre></div><p>Typically, our caching layer will be decoupled from any given server, so here we design a constructor that expects Redis's connection and authentication information. Note the prefix argument. To instantiate a cache instance, use the following code:</p><div class="informalexample"><pre class="programlisting">var cache = new Cache({ prefix: 'articles:cache' });</pre></div><p>Also note<a id="id536" class="indexterm"/> that we're going to implement the cache API using <span class="strong"><strong>Promises</strong></span> via<a id="id537" class="indexterm"/> the <code class="literal">bluebird</code> library (<a class="ulink" href="https://github.com/petkaantonov/bluebird">https://github.com/petkaantonov/bluebird</a>).</p><p>Getting a cached value is straightforward:</p><div class="informalexample"><pre class="programlisting">Cache.prototype.get = function(key) {
  key = this.prefix + key;
  var client = this.client;
  return new Promise(function(resolve, reject) {
    client.hgetall(key, function(err, result) {
      err ? reject() : resolve(result);
    });
  });
};</pre></div><p>All cache keys will be implemented as Redis hashes, so a <code class="literal">GET</code> operation will involve calling <code class="literal">hmget</code> on a key. The Promises-powered API now enables the following easy-to-follow syntax:</p><div class="informalexample"><pre class="programlisting">cache.get('sandro').then(function(val) {
  console.log('cached: ' + val);
}).catch() {
  console.log('unable to fetch value from cache');
})</pre></div><p>Setting a <a id="id538" class="indexterm"/>value <a id="id539" class="indexterm"/>is simply a matter of passing an object:</p><div class="informalexample"><pre class="programlisting">Cache.prototype.set = function(key, val, ttl) {
  var _this = this;
  var pkey = this.prefix + key;
  var client = this.client;
  var setArr = [];

  for(var k in val) {
    setArr[k] = val[k];
  }
  return new Promise(function(resolve, reject) {
    client.hmset(pkey, setArr, function(err) {
      err ? reject() : resolve();
      ttl &amp;&amp; _this.expire(key, ttl);
    });
  });
};</pre></div><p>When <code class="literal">val</code> is received, we reflect its key-value map in the Redis hash stored at <code class="literal">key</code>. The optional third argument, <code class="literal">ttl</code>, allows a flag to be set in Redis to expire this key after a number of seconds, flagging it for deletion. The key bit of code in <code class="literal">this.expire</code> is the following:</p><div class="informalexample"><pre class="programlisting">client.expire(key, ttl, function(err, ok) { // ...flagged for removal }</pre></div><p>For more information <a id="id540" class="indexterm"/>on Redis <code class="literal">expire</code>, visit <a class="ulink" href="http://redis.io/commands/expire">http://redis.io/commands/expire</a>.</p><p>The <code class="literal">remove</code> method is simply a <code class="literal">del</code> operation on the Redis keyspace, so there is no need to explain it here. More interesting is the implementation of the <code class="literal">clear</code> method to remove all keys with a given prefix from Redis:</p><div class="informalexample"><pre class="programlisting">Cache.prototype.clear = function() {
 var prefixMatch = this.prefix + '*';
 var client   = this.client;
 return new Promise(function(resolve, reject) {
  var multi = client.multi();
  (function scanner(cursor) {
   client.scan([+cursor, 'match', prefixMatch], function(err, scn) {
    if(err) {
     return reject();
    }
    // Add new delete candidates
    multi.del(scn[1]);
    // More? Continue scan.
    if(+scn[0] !== 0) {
     return scanner(scn[0]);
    }
    // Delete candidates, then resolve.
    multi.exec(resolve);
   })
  })(0);
 });
};</pre></div><p>Note the <code class="literal">scan</code> method we are using to target and delete keys matching our cache prefix. Redis is <a id="id541" class="indexterm"/>designed for efficiency, and, as much as possible, its designers aim to<a id="id542" class="indexterm"/> avoid adding slow features. Unlike other databases, Redis has no advanced <span class="emphasis"><em>find</em></span> method of searching its keyspace, with developers limited to <span class="emphasis"><em>keys</em></span> and basic glob pattern matching. Because it's common to have many millions of keys in a Redis keyspace, operations using <span class="emphasis"><em>keys</em></span>, unavoidably or through sloppiness, can end up being punitively expensive because a long operation blocks other operations—transactions are atomic, and Redis is single-threaded.</p><p>The <code class="literal">scan</code> method allows you to fetch limited ranges of the keyspace in an iterative manner, enabling (nonblocking) asynchronous keyspace scanning. The scan object itself is stateless, passing only a cursor indicating whether there are further records to be fetched. Using this technique, we are able to clean out all keys prefixed with our target cache key (pattern: <code class="literal">this.prefix + '*'</code>). On each scan iteration, we queue up any returned keys for deletion using the <code class="literal">multi.del</code> function, continuing until the scanner returns a zero value (indicating that all sought keys have been returned), at which point we delete all those keys in one command.</p><p>Tie these methods together:</p><div class="informalexample"><pre class="programlisting">cache.set('deploying', { foo: 'bar' })
.then(function() {
 return cache.get('deploying');
})
.then(function(val) {
 console.log(val); // foo:bar
 return cache.clear();
})
.then(cache.close.bind(cache));</pre></div><p>This is a simple caching strategy to get you started. While managing key expiration yourself is a perfectly valid technique, as you move into larger production implementations, consider configuring Redis's eviction policies directly. For example, you will want to set the <code class="literal">maxmemory</code> value in <code class="literal">redis.conf</code> to some maximum upper bound for the cache memory and configure Redis to use one of the six documented eviction <a id="id543" class="indexterm"/>policies <a id="id544" class="indexterm"/>when memory<a id="id545" class="indexterm"/> limits are reached, such as <span class="strong"><strong>Least Recently Used</strong></span> (<span class="strong"><strong>LRU</strong></span>). For <a id="id546" class="indexterm"/>more information, visit: <a class="ulink" href="http://redis.io/topics/lru-cache">http://redis.io/topics/lru-cache</a>.</p></div><div class="section" title="Deploying CloudFlare as a CDN"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec50"/>Deploying CloudFlare as a CDN</h2></div></div></div><p>A CDN is<a id="id547" class="indexterm"/> typically a globe-spanning network <a id="id548" class="indexterm"/>of servers leased out to companies unable to fund and build their own network. A CDN is set up to ensure that your application or other content remains available to anyone who wishes to access it, wherever they choose to access it in the world, and that your content is delivered quickly. Akamai is perhaps the most famous CDN, and CloudFlare is a recent arrival with a particular focus on security and "attack proofing" networks.</p><p>Usefully for our purposes, CloudFlare provides a free tier of service that satisfies the needs of most deployed applications. In the example that follows, you'll learn how to enable caching with CloudFlare. We'll then use the <code class="literal">cloudflare</code> module to purge your domain files when they change, in the process learning how to use Node's <code class="literal">fs.watch</code> method to watch for file changes.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note30"/>Note</h3><p>CloudFlare has <a id="id549" class="indexterm"/>also embarked on an ambitious effort to host <span class="emphasis"><em>all the JS</em></span> on its CDN at <a class="ulink" href="https://cdnjs.com/">https://cdnjs.com/</a>. Unlike other popular hosting services that <a id="id550" class="indexterm"/>only host the most popular JavaScript libraries, CloudFlare hosts all projects represented in the open GitHub project at <a class="ulink" href="https://github.com/cdnjs/cdnjs">https://github.com/cdnjs/cdnjs</a>. Consider deploying your JavaScript files <a id="id551" class="indexterm"/>via this service.</p></div></div><p>To start, visit <a class="ulink" href="https://www.cloudflare.com/sign-up">https://www.cloudflare.com/sign-up</a> and set up a free account. You will need a<a id="id552" class="indexterm"/> domain to host files on—follow the instructions to configure your name servers and other DNS information. Once signed up, you will receive an authentication token and will use this to add CDN support to your application. CloudFlare does not cache HTML files by default. To enable HTML caching, visit your dashboard, locate your domain, open the options menu, and select <span class="strong"><strong>Page rules</strong></span>. If your domain is <code class="literal">foo.com</code>, the following page rule will enable full caching: <code class="literal">*foo.com/*</code>. Finally, locate the <span class="strong"><strong>Custom Caching</strong></span> dropdown on the page rules admin page and select <span class="strong"><strong>Cache everything</strong></span>.</p><p>Now, let's establish a connection with CloudFlare:</p><div class="informalexample"><pre class="programlisting">var http = require('http');
var fs = require('fs');
var cloudflare = require('cloudflare');
var config = {
  "token": "your token",
  "email": "your account email",
  "domain": "yourdomain.com",
  "subdomain": "www",
  "protocol": "http"
};
var cloudflareClient = cloudflare.createClient({
  email: config.email,
  token: config.token
});</pre></div><p>In our <a id="id553" class="indexterm"/>example, we will serve (and modify) a<a id="id554" class="indexterm"/> single <code class="literal">index.html</code> file. For this example, we will create a simple server:</p><div class="informalexample"><pre class="programlisting">var indexFile = './index.html';
http.createServer(function(request, response) {
  var route = request.url;
  if(route === "/index.html") {
    response.writeHead(200, {
      "content-type": "text/html",
      "cache-control": "max-age=31536000"
    });
    return fs.createReadStream(indexFile).pipe(response);
  }
}).listen(8080);</pre></div><p>Note how <code class="literal">max-age</code> is set on the <code class="literal">cache-control</code> header. This will indicate to CloudFlare that we want this file cached.</p><p>With the server set up, we will now add the following <code class="literal">purge</code> method:</p><div class="informalexample"><pre class="programlisting">function purge(filePath, cb) {
  var head = config.protocol + '://';
  var tail = config.domain + '/' + filePath;
  //  foo.com &amp;&amp; www.foo.com each get a purge call
  var purgeFiles = [
    head + tail,
    head + config.subdomain + '.' + tail
  ];
  var purgeTrack = 2;
  purgeFiles.forEach(function(pf) {
    cloudflareClient.zoneFilePurge(config.domain, pf, function(err) {
      (--purgeTrack === 0) &amp;&amp; cb();
    });
  });
};</pre></div><p>When this method is passed a file path, it asks CloudFlare to purge its cache of this file. Note how we must use two purge actions to accommodate subdomains.</p><p>With purging set up, all that is left to do is watch the filesystem for changes. This can be accomplished via the <code class="literal">fs.watch</code> command:</p><div class="informalexample"><pre class="programlisting">fs.watch('./index.html', function(event, filename) {
  if(event === "change") {
    purge(filename, function(err) {
      console.log("file purged");
   });
  }
});</pre></div><p>Now, whenever the <code class="literal">index.html</code> file is changed, our CDN will flush its cached version. Create that file, start<a id="id555" class="indexterm"/> up the server, and point your browser <a id="id556" class="indexterm"/>to <code class="literal">localhost:8080</code>, bringing up your index file. In your browser's developer console, inspect the response headers—you should see a <code class="literal">CF-Cache-Status: MISS</code> record. This means that <span class="strong"><strong>CloudFlare</strong></span> (<span class="strong"><strong>CF</strong></span>) has <a id="id557" class="indexterm"/>fetched and served the original file from your server—on the first call, there is no cached version yet, so the cache was <span class="emphasis"><em>missed</em></span>. Reload the page. The same response header should now read <code class="literal">CF-Cache-Status: HIT</code>. Your file is cached!</p><p>Go ahead <a id="id558" class="indexterm"/>and change the index file in some way. When <a id="id559" class="indexterm"/>you reload your browser, the changed version will be displayed—its cached version has been purged, the file has been fetched once again from your server, and you will see the <code class="literal">MISS</code> header value again.</p><p>You will want to <a id="id560" class="indexterm"/>expand this functionality to include a larger group of files<a id="id561" class="indexterm"/> and folders. To learn more about <code class="literal">fs.watch</code>, go to <a class="ulink" href="http://nodejs.org/api/fs.html#fs_fs_watch_filename_options_listener">http://nodejs.org/api/fs.html#fs_fs_watch_filename_options_listener</a>.</p></div></div>
<div class="section" title="Managing sessions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec24"/>Managing sessions</h1></div></div></div><p>The HTTP protocol is stateless. Any given request has no information about previous requests. For a server, this means that determining whether two requests originated from the same<a id="id562" class="indexterm"/> browser is not possible without further work. That's fine for general information, but targeted interactions require a user to be verified via some sort of unique identifier. A uniquely identified client can then be served targeted content—from lists of friends to advertisements.</p><p>This semipermanent communication between a client (often a browser) and a server persists for a period of time—at least until the client disconnects. That period of time is understood as a <span class="emphasis"><em>session</em></span>. An application that manages sessions must be able to create a unique user session identifier, track the activity of an identified user during that session, and disconnect that user when requested or for some other reason, such as on reaching a session limit.</p><p>In this section, we'll implement a <span class="strong"><strong>JSON Web Token</strong></span> (<span class="strong"><strong>JWT</strong></span>) system for session management. JWT's have an advantage over traditional cookie-based sessions in that they do not<a id="id563" class="indexterm"/> require the server to maintain a session store as JWTs are self-contained. This greatly helps with deployments and scaling. They are also mobile friendly and can be shared between clients. While a new standard, JWTs should be considered as a simple and scalable session storage solution for your applications.</p><div class="section" title="JSON Web Token authentication and sessions"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec51"/>JSON Web Token authentication and sessions</h2></div></div></div><p>A basic <a id="id564" class="indexterm"/>authentication system might require a client to <a id="id565" class="indexterm"/>send a username and password on each<a id="id566" class="indexterm"/> request. To initiate a token-based authenticated session, a client sends credentials just once, receives a token in exchange, and then sends only that token on subsequent requests, gaining any access that token provides. Incessantly passing around sensitive credentials is no longer required, as the following diagram demonstrates:</p><div class="mediaobject"><img src="graphics/1403OS_04_04.jpg" alt="JSON Web Token authentication and sessions"/></div><p>One particular advantage of JWTs is that servers are no longer responsible for maintaining <a id="id567" class="indexterm"/>access to a common database<a id="id568" class="indexterm"/> of credentials as only the <span class="emphasis"><em>issuing authority</em></span> needs to validate an initial signin. There is no need to maintain a session store when you are using JWTs. The issued token (think of it as an access card) can, therefore, be used within any domain (or server) that recognizes and accepts it. In terms of performance, the cost of a request is now the cost of decrypting a hash versus the cost of making a database call to validate credentials. We also avoid the problems we can face using cookies on mobile devices, such as cross-domain issues (cookies are domain-bound), certain types of request forgery attacks, and so on.</p><p>Let's look at the <a id="id569" class="indexterm"/>structure of a JWT and build a simple example demonstrating how to issue, validate, and otherwise use JWTs to manage sessions.</p><p>A JWT token has the following format:</p><div class="informalexample"><pre class="programlisting">&lt;base64-encoded header&gt;.&lt;base64-encoded claims&gt;.&lt;base64-encoded signature&gt;</pre></div><p>Each segment is described in the JSON format.</p><p>A <a id="id570" class="indexterm"/>
<span class="strong"><strong>header</strong></span> simply describes the token—its type and encryption algorithm. Take the following code as an example:</p><div class="informalexample"><pre class="programlisting">{
  "typ":"JWT",
  "alg":"HS256"
}</pre></div><p>Here, we declare that this is a JWT token, which is encrypted using <code class="literal">HMAC SHA-256</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note31"/>Note</h3><p>See <a class="ulink" href="http://nodejs.org/api/crypto.html">http://nodejs.org/api/crypto.html</a> for more information about encryption<a id="id571" class="indexterm"/> and how to perform encryption with Node. The JWT specification itself can be found at <a class="ulink" href="http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html">http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html</a>. Note that the JWT specification is in a draft state<a id="id572" class="indexterm"/> at the time of writing this, so changes may be made in the future.</p></div></div><p>The <span class="strong"><strong>claims</strong></span> segment outlines security and other constraints that should be checked by any service<a id="id573" class="indexterm"/> receiving the JWT. Check the specification <a id="id574" class="indexterm"/>for a full accounting. Typically, a JWT claims <a id="id575" class="indexterm"/>manifest will want to indicate when the JWT was issued, who issued it, when it expires, who the subject of the JWT is, and who should accept the JWT:</p><div class="informalexample"><pre class="programlisting">{
  "iss" : "http://blogengine.com",
  "aud" : ["http://blogsearch.com", "http://blogstorage"],
  "sub" : "blogengine:uniqueuserid",
  "iat" : "1415918312",
  "exp" : "1416523112",
  "sessionData" : "&lt;some data encrypted with secret&gt;"
}</pre></div><p>The <code class="literal">iat</code> (issued-at) and <code class="literal">exp</code> (expires) claims are both set to numeric values indicating the number of seconds since the Unix epoch. The <code class="literal">iss</code> (issuer) should be a URL describing the issuer of the JWT. Any service that receives a JWT must inspect the <code class="literal">aud</code> (audience), and that service must reject the JWT if it does not appear in the audience list. The <code class="literal">sub</code> (subject) of the JWT identifies the subject of the JWT, such as the user of an application—a unique value that is never reassigned, such as the name of the issuing service and a unique user ID.</p><p>Finally, useful data is attached using a key-value pairing of your choice. Here, let's call the token data <code class="literal">sessionData</code>. Note that we need to encrypt this data—the signature segment of a JWT prevents tampering with session data, but JWTs are not themselves encrypted (you can always encrypt the entire token itself though).</p><p>The last step is to create a signature, which, as mentioned, prevents tampering—a JWT validator specifically checks for mismatches between the signature and the packet received.</p><p>What follows is a scaffold server and client example demonstrating how to implement a JWT-driven authentication system. Rather than implementing the various signing and validation steps <span class="emphasis"><em>by hand</em></span>, we'll use the <code class="literal">jwt-simple</code> package. Feel free to browse the <code class="literal">/jwt</code> folder in your code bundle, which contains the full code we'll be unpacking next.</p><p>To ask for a token, we will use the following client code:</p><div class="informalexample"><pre class="programlisting">var token;

function send(route, formData, cb) {
  if(!(formData instanceof FormData)) {
    cb = formData;
    formData = new FormData();
  }
  var caller = new XMLHttpRequest();
  caller.onload = function() {
    cb(JSON.parse(this.responseText));
  };
  caller.open("POST", route);
  token &amp;&amp; caller.setRequestHeader('Authorization', 'Bearer ' + token);
  caller.send(formData);
}
// ...When we have received a username and password in some way
formData = new FormData();
formData.append("username", username);
formData.append("password", password);

send("/login", formData, function(response) {
  token = response.token;
  console.log('Set token: ' + token);
});</pre></div><p>We'll<a id="id576" class="indexterm"/> implement the server code next. For now, note<a id="id577" class="indexterm"/> that we have a <code class="literal">send</code> method that expects, at some point, to have a global <code class="literal">token</code> set for it to pass along when making requests. The initial <code class="literal">/login</code> is where we ask for that token.</p><p>Using the Express web framework, we create the following server and <code class="literal">/login</code> route:</p><div class="informalexample"><pre class="programlisting">var express = require('express');
...
var jwt = require('jwt-simple');
var app = express();

app.set('jwtSecret', 'shhhhhhhhh');

app.post('/login', auth, function(req, res) {
  var nowSeconds   = Math.floor(Date.now()/1000);
  var plus7Days   = nowSeconds + (60 * 60 * 24 * 7);
  var token = jwt.encode({
    "iss" : "http://blogengine.com",
    "aud" : ["http://blogsearch.com", "http://blogstorage"],
    "sub" : "blogengine:uniqueuserid",
    "iat" : nowSeconds,
    "exp" : plus7Days
  }, app.get('jwtSecret'));

  res.send({
    token : token
  })
})</pre></div><p>Note that <a id="id578" class="indexterm"/>we store <code class="literal">jwtsecret</code> on the app server. This<a id="id579" class="indexterm"/> is the key that is used when we are signing tokens. When a login attempt is made, the server will return the result of <code class="literal">jwt.encode</code>, which encodes the JWT claims discussed previously. That's it. From now on, <span class="emphasis"><em>any</em></span> client that mentions this token to the correct audience will be allowed to interact with any services those audience members provide for a period expiring 7 days from the date of issue. These services will implement something like the following code:</p><div class="informalexample"><pre class="programlisting">app.post('/someservice', function(req, res) {
  var token = req.get('Authorization').replace('Bearer ', '');
  var decoded = jwt.decode(token, app.get('jwtSecret'));
  var now = Math.floor(Date.now()/1000);
  if(now &gt; decoded.exp) {
    return res.end(JSON.stringify({
      error : "Token expired"
    }));
  }
  res.send(&lt;some sort of result&gt;);
})</pre></div><p>Here, we are simply fetching the <code class="literal">Authorization</code> header (stripping out <code class="literal">Bearer</code>) and decoding via <code class="literal">jwt.decode</code>. A service must at least check for token expiry, which we do here by<a id="id580" class="indexterm"/> comparing the current number of seconds from the <code class="literal">epoch</code> to the token's <span class="strong"><strong>expiry </strong></span>time.</p><p>Using this simple framework, you can create an easily scalable authentication/session system <a id="id581" class="indexterm"/>using a secure standard. No longer <a id="id582" class="indexterm"/>required to maintain a connection to a common credentials database, individual services (deployed perhaps as microservices) can use JWTs to validate requests, incurring little CPU latency or memory cost.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec25"/>Summary</h1></div></div></div><p>We covered a lot of ground in this chapter. Best practices for writing efficient JavaScript that the V8 interpreter can handle properly were outlined, including an exploration of garbage collection, the advantages of Node streams, and how JavaScript prototypes should be deployed in order to save memory. Continuing with the theme of reducing storage costs, we explored various ways in which Redis can help with storing large amounts of data in a space-efficient way.</p><p>Additionally, we looked at strategies to build composable, distributed systems. In the discussion on microservices, we touched on approaches to network many individual services and build the networks they can use to communicate with each other—from pub/sub to Seneca's pattern and action models. Joined with the examples of caching techniques, a reasonably complete picture of the issues you might want to consider when planning out resource management in your application was established.</p><p>After you build up a reasonably sophisticated architecture, it becomes more and more necessary to build probes and other monitoring tools to stay on top of what is going on. In the next chapter, we'll build tools to help you trace the changing topology of running applications.</p></div></body></html>