- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Asynchronous Microservices
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步微服务
- en: Microservices are designed to be independent and self-contained. Clearly defined
    communication protocols and APIs ensure these services interact without relying
    on each other’s internal workings. Defining proper communication between microservices
    is important for a well-functioning microservices architecture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务被设计成独立和自包含的。明确定义的通信协议和API确保这些服务在没有依赖彼此内部工作的情况下进行交互。定义微服务之间的适当通信对于良好的微服务架构至关重要。
- en: 'In this chapter, we plan to discuss and learn about another important communication
    mechanism: asynchronous communication between microservices.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们计划讨论和学习另一个重要的通信机制：微服务之间的异步通信。
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Understanding the requirements
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解需求
- en: Exploring asynchronous communication
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索异步通信
- en: Implementing an asynchronous transaction microservice
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现异步事务微服务
- en: Adapting an account service to new requirements
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应账户服务的新需求
- en: Testing our microservices together
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试我们的微服务
- en: Let’s get into it!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨！
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow along with the chapter, you’ll need an IDE (we prefer Visual Studio
    Code), Postman, Docker, and a browser of your choice.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本章内容，您需要一个IDE（我们更喜欢Visual Studio Code）、Postman、Docker以及您选择的浏览器。
- en: It is preferable to download the repository from [https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript)
    and open the `Ch07` folder to easily follow the code snippets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 建议您从[https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript)下载仓库，并打开`Ch07`文件夹，以便轻松地跟随代码片段。
- en: Understanding the requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解需求
- en: 'Up until now, we have developed two simple microservices and for the current
    chapter we plan to extend our transaction microservice to meet the following requirements:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经开发了两个简单的微服务，对于当前章节，我们计划扩展我们的事务微服务以满足以下需求：
- en: 'Every transaction should support the following statuses: `CREATED`, `FAILED`,
    `APPROVED`, `DECLINED`, and `FRAUD`.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个事务都应该支持以下状态：`CREATED`、`FAILED`、`APPROVED`、`DECLINED`和`FRAUD`。
- en: The transaction service should now have a new method that changes the status
    of the given transaction to `FRAUD`. It will update the status of the transaction
    to `FRAUD` and produce a message about the transaction.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事务服务现在应该有一个新的方法，将给定事务的状态更改为`FRAUD`。它将更新事务的状态为`FRAUD`并产生关于事务的消息。
- en: The account service will consume this message and after *three* fraudulent attempts,
    the account service should read and suspend/block the given account.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 账户服务将消费此消息，并在三次欺诈尝试后，账户服务应读取并暂停/阻止指定的账户。
- en: We plan to use asynchronous communication between microservices and any other
    microservice may use this message for internal purposes. You can check [*Chapter
    2*](B09148_02.xhtml#_idTextAnchor027) for more information about asynchronous
    communication between microservices.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计划在微服务之间使用异步通信，任何其他微服务都可能使用此消息进行内部目的。您可以查看[*第2章*](B09148_02.xhtml#_idTextAnchor027)以获取有关微服务之间异步通信的更多信息。
- en: Exploring asynchronous communication
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索异步通信
- en: 'You can implement asynchronous communication between microservices using various
    patterns and technologies, each suitable for different use cases and requirements.
    Here are some of the common ones:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用各种模式和技术在微服务之间实现异步通信，每种都适合不同的用例和需求。以下是一些常见的例子：
- en: '**Message brokers**: Message brokers facilitate asynchronous communication
    by allowing microservices to publish and subscribe to messages. Popular message
    brokers include **RabbitMQ**, which supports multiple messaging protocols and
    patterns such as pub/sub and routing, and **Apache Kafka**, designed for high-throughput
    and fault-tolerant event streaming – one of the best choices for real-time data
    processing. An example of a message broker would be a producer service sending
    a message to a queue or topic and the consumer service subscribing to the queue
    or topic and processing messages.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息代理**：消息代理通过允许微服务发布和订阅消息来促进异步通信。流行的消息代理包括**RabbitMQ**，它支持多种消息协议和模式，如发布/订阅和路由，以及为高吞吐量和容错性事件流设计的**Apache
    Kafka**——这是实时数据处理的最佳选择之一。一个消息代理的例子是一个生产服务向队列或主题发送消息，而消费者服务订阅队列或主题并处理消息。'
- en: '**Event streaming platforms**: Event streaming platforms capture and process
    streams of events. These platforms are particularly useful for real-time analytics
    and data pipeline construction. Popular event streaming platforms include **Apache
    Kafka**, which is often used as both a message broker and an event streaming platform,
    and **Amazon Kinesis**, a managed service for real-time data processing at scale.
    Here is an example: a producer service emits events to a Kafka topic and consumer
    services consume events from the topic and react to them.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件流平台**: 事件流平台捕获和处理事件流。这些平台对于实时分析和数据管道构建特别有用。流行的事件流平台包括常被用作消息代理和事件流平台的**Apache
    Kafka**，以及用于大规模实时数据处理的托管服务**Amazon Kinesis**。以下是一个例子：生产服务向Kafka主题发出事件，消费者服务从主题中消费事件并对它们做出反应。'
- en: '**The Publish-Subscribe pattern**: In the pub/sub pattern, messages are published
    to a topic and multiple subscribers can consume these messages asynchronously.
    Popular services that use the pub/sub pattern include **Google Pub/Sub**, a fully
    managed real-time messaging service, and **AWS Simple Notification Service** (**SNS**),
    which allows publishing messages to multiple subscribers. For example, a publisher
    service publishes an event to a topic and the subscriber services receive notifications
    and process the event.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发布-订阅模式**: 在发布/订阅模式中，消息被发布到一个主题，多个订阅者可以异步地消费这些消息。使用发布/订阅模式的流行服务包括完全托管的实时消息服务**Google
    Pub/Sub**，以及允许向多个订阅者发布消息的**AWS Simple Notification Service**（**SNS**）。例如，发布者服务将事件发布到主题，而订阅者服务接收通知并处理事件。'
- en: '**Task queues**: Task queues are used to distribute tasks to worker services
    asynchronously. This is useful for offloading heavy or time-consuming tasks from
    the main service. Some of the more popular task queues are **Celery**, an asynchronous
    task queue/job queue based on distributed message passing, and **Amazon Simple
    Queue Service** (**SQS**), a fully managed message queue service. Here’s how a
    task queue works: a producer service creates a task and places it in the queue
    and the worker service picks up the task from the queue and processes it.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务队列**: 任务队列用于异步地将任务分配给工作服务。这对于从主服务中卸载重或耗时的任务非常有用。一些流行的任务队列包括基于分布式消息传递的异步任务队列/作业队列**Celery**，以及完全托管的**Amazon
    Simple Queue Service**（**SQS**），这是一个完全托管的消息队列服务。以下是任务队列的工作原理：生产服务创建一个任务并将其放入队列，然后工作服务从队列中提取任务并处理它。'
- en: '**Event-driven architecture**: In an event-driven architecture, services communicate
    through events. When something notable happens in one service, it emits an event
    that other services can listen to and act upon. In event-driven architecture an
    event source service publishes an event, and the event listener services react
    to the event and execute their logic.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件驱动架构**: 在事件驱动架构中，服务通过事件进行通信。当一个服务发生显著事件时，它会发出一个其他服务可以监听并对其做出反应的事件。在事件驱动架构中，事件源服务发布一个事件，事件监听服务对事件做出反应并执行它们的逻辑。'
- en: '**WebSockets**: WebSockets allow for full-duplex communication channels over
    a single TCP connection, useful for real-time applications such as chat apps or
    live updates. Here’s an example: the server pushes updates to clients via WebSockets
    and clients receive updates in real time and act upon them.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WebSockets**: WebSockets允许在单个TCP连接上建立全双工通信通道，这对于实时应用程序（如聊天应用或实时更新）非常有用。以下是一个例子：服务器通过WebSockets向客户端推送更新，客户端实时接收更新并对其做出反应。'
- en: '**Server-Sent Events** (**SSE**): SSE is a server push technology enabling
    servers to push real-time updates to the client once an initial client connection
    is established. Let’s take an example: the server sends events to clients over
    an HTTP connection and clients listen to incoming messages and process them.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器发送事件**（**SSE**）：SSE是一种服务器推送技术，允许服务器在建立初始客户端连接后向客户端推送实时更新。以下是一个例子：服务器通过HTTP连接向客户端发送事件，客户端监听传入的消息并处理它们。'
- en: '**gRPC with streaming**: gRPC supports bidirectional streaming, allowing both
    client and server to send a sequence of messages using a single connection. gRPC
    works like this: the client and server can continuously exchange streams of messages
    as part of a single RPC call.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持流式传输的gRPC**: gRPC支持双向流，允许客户端和服务器使用单个连接发送一系列消息。gRPC的工作方式是这样的：客户端和服务器可以作为单个RPC调用的部分，持续交换消息流。'
- en: For this chapter, we will actively use Apache Kafka, an open source, high-performance
    event streaming platform. It is a popular choice for asynchronous communication
    between microservices due to its strengths in enabling a robust and scalable event-driven
    architecture. While we have already talked about how to run services via Docker,
    this chapter will focus on hosting Apache Kafka on Docker.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将积极使用Apache Kafka，这是一个开源、高性能的事件流平台。由于其能够实现强大且可扩展的事件驱动架构，它成为微服务之间异步通信的流行选择。虽然我们已经讨论了如何通过Docker运行服务，但本章将专注于在Docker上托管Apache
    Kafka。
- en: 'Let’s take a quick look at the problems that Apache Kafka solves:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下Apache Kafka解决的问题：
- en: '**Communication complexity**: In a microservice environment, you have *multiple
    sources* (every API acts as a source) and *multiple targets* (every API can have
    multiple sources to write to). The fact that sources and targets are scaled is
    always accompanied by a communication problem. In this case, the problem is that
    we should solve the complexities created by the source and target rather than
    focus on business requirement implementations. Now you have multiple sources and
    targets, which can create the following issues:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信复杂性**：在微服务环境中，你有多个来源（每个API都充当来源）和多个目标（每个API可以有多个来源写入）。来源和目标可扩展的事实总是伴随着通信问题。在这种情况下，问题是我们应该解决由来源和目标产生的复杂性，而不是专注于业务需求实现。现在你有多个来源和目标，这可能会产生以下问题：'
- en: Every target requires a different protocol to communicate.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个目标都需要不同的协议进行通信。
- en: Every target has its data format to work with.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个目标都有自己的数据格式来处理。
- en: Every different target requires maintenance and support.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个不同的目标都需要维护和支持。
- en: In simple terms, say you have a microservice application, and every service
    has its own target. Besides that, every service can have multiple sources, and
    the services can use common sources. Apache Kafka helps you to avoid complex communication
    between microservices.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单来说，假设你有一个微服务应用，每个服务都有自己的目标。除此之外，每个服务还可以有多个来源，并且服务可以使用公共来源。Apache Kafka帮助你避免微服务之间的复杂通信。
- en: '**Communication complexity duplication**: Whenever similar systems are developed;
    we have to rewrite such communication processes again and again. Let’s imagine
    that we are working on several different projects. Although the domain of these
    projects is different, and although they solve different problems at an abstract
    level, the common aspect of these projects is communication complexity. So, it
    means we’re repeating ourselves and trying to resolve the same issue every time.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信复杂性重复**：每当开发类似系统时，我们必须一次又一次地重写这样的通信过程。让我们想象一下，我们正在处理几个不同的项目。尽管这些项目的领域不同，尽管它们在抽象层面上解决不同的问题，但这些项目的共同点是通信复杂性。这意味着我们在重复自己，每次都试图解决相同的问题。'
- en: '**Fault tolerance**: The system should be able to continue functioning and
    provide reliable data processing and message delivery even in the presence of
    various types of failures, such as hardware failures, network issues, or software
    crashes.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：系统应该能够在各种类型的故障（如硬件故障、网络问题或软件崩溃）存在的情况下继续运行并提供可靠的数据处理和消息传递。'
- en: '**High performance**: In most cases, such a communication problem (sources
    - targets) causes the application performance to drop. Regardless of dynamic changes
    in the number of targets and sources in the application, the program should always
    support the high-performance attribute.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高性能**：在大多数情况下，这种通信问题（来源-目标）会导致应用程序性能下降。无论应用程序中目标数和来源数的动态变化如何，程序都应该始终支持高性能属性。'
- en: '**Scalability**: The system should be possible to horizontally scale sources
    and targets. Horizontal scaling, also known as scaling out, is a technique in
    software design for increasing the capacity of a system by adding more machines
    (nodes) to distribute the workload.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可伸缩性**：系统应该能够水平扩展来源和目标。水平扩展，也称为扩展，是软件设计中通过添加更多机器（节点）来增加系统容量的技术。'
- en: '**Real-time communication**: One of the possible target and source communication
    attributes is real-time communication. Depending on the use cases, the system
    should allow real-time data exchange between the source and the target.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时通信**：可能的目标和来源通信属性之一是实时通信。根据用例，系统应允许来源和目标之间进行实时数据交换。'
- en: '**Log and data aggregation**: This is the ability to combine and process logs
    and data in certain aggregates. Log and data aggregation play a crucial role in
    modern software by centralizing and organizing information from various sources,
    making it easier to analyze, troubleshoot, and optimize applications.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志和数据聚合**：这是将日志和数据组合并处理的能力。日志和数据聚合在现代软件中发挥着至关重要的作用，通过集中和组织来自各种来源的信息，使其更容易分析、故障排除和优化应用程序。'
- en: '**Data transformation and processing**: The communication between the target
    and source is not only in the form of data exchange but also information should
    be based on the possibility of transformation.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换和处理**：目标与源之间的通信不仅限于数据交换，信息还应基于转换的可能性。'
- en: Now let’s talk about the infrastructure we need to use to implement our microservices.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们谈谈我们需要用于实现我们的微服务的基础设施。
- en: Implementing an asynchronous transaction microservice
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现异步事务微服务
- en: 'We will use the same transaction microservice we implemented in [*Chapter 6*](B09148_06.xhtml#_idTextAnchor104)
    but with additional changes that will help us add asynchronous behavior to it.
    First, we should prepare our infrastructure. Here is what we will have in it:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们在[*第6章*](B09148_06.xhtml#_idTextAnchor104)中实现的相同事务微服务，但会进行一些额外的更改，以帮助我们为其添加异步行为。首先，我们应该准备我们的基础设施。以下是它将包含的内容：
- en: '**Apache Kafka**: To create loose coupling between microservices.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka**：用于在微服务之间创建松散耦合。'
- en: '**Kafka UI**: This is a web application designed for managing Apache Kafka
    clusters. It provides a **graphical user interface** (**GUI**) instead of the
    traditional **command-line interface** (**CLI**) for Kafka, making it easier to
    interact with Kafka for many users.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka UI**：这是一个用于管理Apache Kafka集群的Web应用程序。它为Kafka提供了一个**图形用户界面**（GUI），而不是传统的**命令行界面**（CLI），这使得许多用户与Kafka交互变得更加容易。'
- en: '**Zookeeper**: This is open source software that acts as a central coordinator
    for large distributed systems. Think of it as a conductor for an orchestra, keeping
    everything in sync.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Zookeeper**：这是一款开源软件，作为大型分布式系统的中央协调器。将其想象为交响乐队的指挥，保持一切同步。'
- en: '**PostgreSQL**: To store data.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PostgreSQL**：用于存储数据。'
- en: '**PgAdmin**: A graphical tool to visually see database elements.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PgAdmin**：一个用于直观查看数据库元素的图形工具。'
- en: We have our `docker-compose.yml` file in our root folder (`Ch07/transactionservice`).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在根目录（`Ch07/transactionservice`）中有一个`docker-compose.yml`文件。
- en: This `docker-compose` file defines a multi-service setup for a PostgreSQL database,
    a PgAdmin instance for managing the database, and a Kafka messaging system with
    Zookeeper for coordination. The services are connected through a custom Docker
    network, `my-app-network`, which enables inter-container communication. For Kafka,
    ensure the correct network settings are configured to avoid connectivity issues,
    especially for multi-network setups where `advertised.listeners` may be needed
    for both internal and external addresses. The PostgreSQL service stores its data
    in a named volume, `postgres_data`, while `PgAdmin` depends on PostgreSQL to be
    up and running. The Kafka and Zookeeper services are set up for message brokering,
    with Kafka UI providing management and monitoring, relying on Zookeeper to maintain
    a distributed system configuration.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此`docker-compose`文件定义了一个多服务设置，用于PostgreSQL数据库、一个用于管理数据库的PgAdmin实例以及一个带有Zookeeper协调器的Kafka消息系统。服务通过一个自定义Docker网络`my-app-network`连接，该网络使容器间通信成为可能。对于Kafka，确保已配置正确的网络设置以避免连接问题，特别是在多网络设置中，可能需要`advertised.listeners`以同时支持内部和外部地址。PostgreSQL服务将其数据存储在名为`postgres_data`的命名卷中，而`PgAdmin`依赖于PostgreSQL处于运行状态。Kafka和Zookeeper服务已设置为消息代理，Kafka
    UI提供管理和监控，依赖于Zookeeper来维护分布式系统配置。
- en: Navigate to the root folder and run the `docker-compose up -d` command to spin
    up the infrastructure.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到根目录并运行`docker-compose up -d`命令以启动基础设施。
- en: Here is how it should look after a successful run (*Figure 7**.1*).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是成功运行后的样子（*图7.1*）。
- en: '![Figure 7.1: Docker infrastructure](img/B09148_07_001.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1：Docker基础设施](img/B09148_07_001.jpg)'
- en: 'Figure 7.1: Docker infrastructure'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：Docker基础设施
- en: After successfully running our docker infrastructure, we are ready to switch
    to our source code to implement our requirements.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功运行我们的Docker基础设施后，我们就可以切换到源代码来实现我们的需求。
- en: 'First, we need to update our transaction service to support additional statuses.
    Open the `schema.prisma` file under the `prisma/migrations` folder and change
    `enum` to the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要更新我们的交易服务以支持额外的状态。打开位于`prisma/migrations`文件夹下的`schema.prisma`文件，并将`enum`更改为以下内容：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As we already know, one of the responsibilities of Prisma is to isolate us
    from database internals and provide a unique, more understandable language over
    these internals. That is why we have the `.prisma` extension and to map it to
    real SQL, we need to run migration. We already know about the migration steps
    and their impact on your development (check [*Chapter 6*](B09148_06.xhtml#_idTextAnchor104)
    for more detailed information), so in this chapter, we just provide the exact
    command without explanation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经知道的，Prisma的一个职责是隔离我们与数据库内部，并在这内部提供一种独特、更易于理解的编程语言。这就是为什么我们有`.prisma`扩展，要将它映射到真实的SQL，我们需要运行迁移。我们已经了解了迁移步骤及其对您开发的影响（有关更详细的信息，请参阅[*第6章*](B09148_06.xhtml#_idTextAnchor104)），因此在本章中，我们只提供确切命令而不作解释：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After running the command, you should end up with an additional folder that
    contains `migration.sql` and the folder name is a combination of the generation
    date and the name you provided from the command (*Figure 7**.2*).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 运行命令后，您应该得到一个额外的文件夹，其中包含`migration.sql`文件，文件夹名称是生成日期和您从命令中提供的名称的组合（*图7**.2*）。
- en: '![Figure 7.2: Newly generated migration context for statuses](img/B09148_07_002.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2：新生成的状态迁移上下文](img/B09148_07_002.jpg)'
- en: 'Figure 7.2: Newly generated migration context for statuses'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：新生成的状态迁移上下文
- en: The main functionality we plan to add to the transaction service is fraud functionality.
    This method should change the status of a transaction to `FRAUD` if it is not
    a failed transaction. After updating the status, it should publish a message to
    the broker (Apache Kafka in this case).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计划添加到交易服务的主要功能是欺诈功能。如果交易不是失败的，则此方法应将交易状态更改为`FRAUD`。更新状态后，它应向代理（在这种情况下为Apache
    Kafka）发布一条消息。
- en: Getting started with Kafka for NestJS
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka在NestJS中的入门
- en: 'As we learned in [*Chapter 6*](B09148_06.xhtml#_idTextAnchor104), NestJS has
    a lot of useful packages to work with different technologies. You don’t need to
    write any of them to integrate them into your project. This applies to Apache
    Kafka also. We don’t need to develop a separate package from scratch for it; just
    run the following command to install the required packages:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第6章*](B09148_06.xhtml#_idTextAnchor104)中学到的，NestJS有很多有用的包可以与不同的技术一起使用。您不需要编写任何这些包就可以将它们集成到您的项目中。这也适用于Apache
    Kafka。我们不需要从头开始开发一个单独的包；只需运行以下命令来安装所需的包：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After successful installation, you will end up with additional changes in your
    `package.json` file. NestJS has a special pattern combination to configure services.
    That is why we first need to create our `kafka` module. As we already learned,
    there is no need to create this file manually. You just need to run the following
    command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功后，您将在`package.json`文件中看到额外的更改。NestJS有一个特殊的模式组合来配置服务。这就是为什么我们首先需要创建我们的`kafka`模块。正如我们已经学到的，没有必要手动创建此文件。您只需运行以下命令：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It should generate a folder called `kafka` that contains the `kafka.module.ts`
    file. This module should have `KafkaService` as its provider element, but we don’t
    have a Kafka service. Running the following command will generate `kafka.service.ts`
    and `kafka.service.spec.ts` files:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该生成一个名为`kafka`的文件夹，其中包含`kafka.module.ts`文件。此模块应将`KafkaService`作为其提供元素，但我们没有Kafka服务。运行以下命令将生成`kafka.service.ts`和`kafka.service.spec.ts`文件：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We don’t need to work on `kafka.service.spec.ts` and it is up to you to remove
    it. These files are automatically generated test files, and we won’t run any tests
    for this chapter. To make things as simple as possible, we remove it. After running
    the last command, you should realize that `kafka.module.ts` was also automatically
    updated. Here is what it looks like:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要在`kafka.service.spec.ts`上工作，是否删除它取决于您。这些文件是自动生成的测试文件，我们不会为本章运行任何测试。为了使事情尽可能简单，我们将其删除。运行最后一个命令后，您应该意识到`kafka.module.ts`也被自动更新了。以下是它的样子：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The code in `kafka.module.ts` is straightforward and easy to understand due
    to its minimal lines.: A bit later we will talk about the `nestjs/config` package
    also. We will implement the main functionality inside `kafka.service.ts` file.
    Open your `kafka.service.ts` file and replace it with the following code lines:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其极简的行数，`kafka.module.ts`中的代码简单易懂。稍后我们还将讨论`nestjs/config`包。我们将在`kafka.service.ts`文件中实现主要功能。打开您的`kafka.service.ts`文件，并将其替换为以下代码行：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s understand what we just did:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来理解我们刚才做了什么：
- en: '`Injectable`: This indicates that the class is injectable into other services.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Injectable`: 这表示该类可注入到其他服务中。'
- en: '`OnModuleInit` and `OnModuleDestroy`: These are lifecycle hooks for initialization
    and cleanup.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OnModuleInit` 和 `OnModuleDestroy`: 这些是初始化和清理的生命周期钩子。'
- en: '`ConfigService`: This provides access to environment variables and configuration.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConfigService`: 这提供了访问环境变量和配置的权限。'
- en: '`Kafka` and `Producer`: These are classes from the `kafkajs` library.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Kafka` 和 `Producer`: 这些是来自`kafkajs`库的类。'
- en: '`@Injectable()`: This makes the `KafkaService` injectable.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@Injectable()`: 这使得`KafkaService`可注入。'
- en: '`implements OnModuleInit` and `OnModuleDestroy`: This implements the lifecycle
    hooks.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`implements OnModuleInit` 和 `OnModuleDestroy`: 这实现了生命周期钩子。'
- en: '`producer`: The Kafka `producer` instance is used for sending messages.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`producer`: Kafka `producer`实例用于发送消息。'
- en: '`topic`: This is the pre-configured Kafka topic for message delivery (fetched
    from environment variables).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topic`: 这是预配置的用于消息传递的Kafka主题（从环境变量中获取）。'
- en: '`configService`: This is the injected instance for accessing configuration.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`configService`: 这是用于访问配置的注入实例。'
- en: 'The constructor of the class fetches Kafka configuration values from environment
    variables:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类的构造函数从环境变量中获取Kafka配置值：
- en: '`KAFKA_CLIENT_ID`: This is the client ID for your application.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_CLIENT_ID`: 这是您应用程序的客户端ID。'
- en: '`KAFKA_BROKERS`: This is a comma-separated list of Kafka broker addresses.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_BROKERS`: 这是一个以逗号分隔的Kafka代理地址列表。'
- en: '`KAFKA_TOPIC`: This is the Kafka topic for sending messages.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KAFKA_TOPIC`: 这是发送消息的Kafka主题。'
- en: '`const kafka = new Kafka({ clientId, brokers });`: This creates a Kafka client
    using the configuration.'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`const kafka = new Kafka({ clientId, brokers });`: 这使用配置创建了一个Kafka客户端。'
- en: '`this.producer = kafka.producer({ retry: { retries: 3 } })`: This creates a
    producer instance with a retry configuration for message reliability (set to retry
    three times by default).'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`this.producer = kafka.producer({ retry: { retries: 3 } })`: 这创建了一个具有消息可靠性重试配置的生产者实例（默认设置为重试三次）。'
- en: '`onModuleInit`: This connects the Kafka producer when the NestJS module is
    initialized, ensuring the producer is ready to send messages.'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onModuleInit`: 这在NestJS模块初始化时连接Kafka生产者，确保生产者准备好发送消息。'
- en: '`onModuleDestroy`: This disconnects the Kafka producer when the NestJS module
    is destroyed, releasing resources.'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onModuleDestroy`: 这在NestJS模块被销毁时断开Kafka生产者连接，释放资源。'
- en: '`send`: This takes a value (any) to be sent and an optional key (string) for
    message identification. It constructs a message object with a key and value (serialized
    as JSON) and sends the message to the pre-configured topic using the producer.'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`send`: 这接受一个要发送的值（任何类型）和一个可选的键（字符串）用于消息标识。它构建一个带有键和值（序列化为JSON）的消息对象，并使用生产者将消息发送到预配置的主题。'
- en: 'Sensitive information should not be stored directly in kafka.service.ts. For
    this chapter, store configuration settings in a `.env` file locally. However,
    avoid committing this file to version control. For production deployments, consider
    using a secure vault service, like AWS Secrets Manager or Azure Key Vault, to
    manage sensitive configurations securely. From the previous code, it is obvious
    that we store our three main Kafka configurations in a `.env` file. Open your
    `.env` file and add the following lines to the end of the file:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感信息不应直接存储在`kafka.service.ts`中。对于本章，请在本地将配置设置存储在`.env`文件中。然而，请避免将此文件提交到版本控制。对于生产部署，考虑使用安全的保险库服务，如AWS
    Secrets Manager或Azure Key Vault，以安全地管理敏感配置。从前面的代码中，很明显我们将在`.env`文件中存储我们的三个主要Kafka配置。打开您的`.env`文件，并在文件末尾添加以下行：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We have already used the `.env` file to configure `postgresql` ([*Chapter 6*](B09148_06.xhtml#_idTextAnchor104)),
    but for this chapter, we need to specify a mechanism that can read .`env` files.
    Another NestJS package, called `config`, will help us to deal with this issue.
    Let’s install it using the following command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用 `.env` 文件来配置 `postgresql` ([*第 6 章*](B09148_06.xhtml#_idTextAnchor104))，但为了本章，我们需要指定一个可以读取
    .`env` 文件的机制。另一个 NestJS 包，名为 `config`，将帮助我们处理这个问题。让我们使用以下命令安装它：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That is all. We have imported the package to `kafka.service.js` to work with
    it. Now it is time to talk about Kafka’s essentials. When we produce or consume
    messages, we need to interact with Apache Kafka, and you need to understand some
    basics of Kafka before using it.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。我们已经将包导入到 `kafka.service.js` 中以使用它。现在是我们讨论 Kafka 知识要点的时候了。当我们生产或消费消息时，我们需要与
    Apache Kafka 交互，在使用 Kafka 之前，你需要了解一些 Kafka 的基础知识。
- en: Cluster and brokers in Kafka
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka 中的集群和代理
- en: In production, a Kafka cluster typically consists of multiple brokers, each
    of which stores and manages partitions for assigned topics. Kafka uses ZooKeeper
    (or KRaft in newer versions) to coordinate broker metadata and ensure consistent
    partition distribution across the cluster. A **broker** is synonymous with a **Kafka
    server**. Each broker is a server. The purpose of a broker is to serve data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中，一个 Kafka 集群通常由多个代理组成，每个代理存储和管理分配给主题的分区。Kafka 使用 ZooKeeper（或较新版本中的 KRaft）来协调代理元数据并确保集群中分区分布的一致性。**代理**与**Kafka
    服务器**同义。每个代理都是一个服务器。代理的目的是服务数据。
- en: It doesn’t matter whether it is physical or not; in the end, a broker should
    function as a server. While it is technically possible to have only one broker
    in a cluster, this is usually done only for testing or self-learning purposes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 无论它是否是物理的，最终，一个代理应该像服务器一样运行。虽然从技术上讲，在集群中只有一个代理是可能的，但这通常只用于测试或自学目的。
- en: 'The purposes of a cluster include the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的目的包括以下内容：
- en: Handling multiple requests in parallel using multiple brokers
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个代理并行处理多个请求
- en: Providing high throughput
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供高吞吐量
- en: Ensuring scalability
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保可伸缩性
- en: Each broker within a Kafka cluster is also a bootstrap server, containing metadata
    about all other brokers, topics, and partitions. When consumers join a consumer
    group, Kafka's group coordinator uses assignment strategies like Range or RoundRobin
    to assign partitions, ensuring even distribution and balancing the load across
    consumers. This means that when you connect to one broker, you are automatically
    connected to the entire cluster. In most cases, a good starting point is to have
    three brokers. Three brokers in Apache Kafka provide a balance between fault tolerance
    and efficiency. With three brokers, Kafka can replicate data across multiple nodes,
    ensuring high availability even if one broker fails. It also enables a replication
    factor of three, which allows the system to tolerate a broker failure without
    losing data, while avoiding the overhead of managing too many brokers. However,
    in high-load systems, you might end up with hundreds of brokers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 集群中的每个代理也是一个引导服务器，包含有关所有其他代理、主题和分区的元数据。当消费者加入消费者组时，Kafka 的组协调器使用范围或轮询等分配策略来分配分区，确保均匀分布并平衡消费者之间的负载。这意味着当你连接到一个代理时，你自动连接到整个集群。在大多数情况下，一个好的起点是拥有三个代理。Apache
    Kafka 中的三个代理在容错性和效率之间提供了平衡。有了三个代理，Kafka 可以在多个节点之间复制数据，即使一个代理失败也能确保高可用性。它还允许三个副本因子，这使得系统在发生代理故障时不会丢失数据，同时避免了管理过多代理的开销。然而，在高负载系统中，你可能会拥有数百个代理。
- en: Depending on the topic’s configuration, a broker, as a storage type, consists
    of multiple **partitions**. When we create a topic, we define the number of partitions
    under that topic. Kafka, as a distributed system, employs the best algorithm to
    distribute these partitions among brokers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 根据主题的配置，作为存储类型的代理由多个 **分区** 组成。当我们创建一个主题时，我们定义该主题下的分区数量。作为分布式系统，Kafka 使用最佳算法将这些分区分配给代理。
- en: Let’s consider a Kafka cluster with three brokers. When creating a topic named
    `tracking_accounts` with three partitions, Kafka will attempt to distribute the
    partitions among the three brokers. In the best scenario, this will result in
    one partition per broker. Of course, this depends on various factors, including
    load balancing. You don’t need to intervene; Kafka, as a **distributed framework**,
    automatically manages all these internal operations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个有三个代理的 Kafka 集群。当创建一个名为 `tracking_accounts` 的主题并具有三个分区时，Kafka 将尝试将分区分配给三个代理。在最佳情况下，这将导致每个代理一个分区。当然，这取决于各种因素，包括负载均衡。你不需要干预；作为
    **分布式框架** 的 Kafka 自动管理所有这些内部操作。
- en: If you have three partitions and four brokers, Kafka will attempt to distribute
    them, assigning one partition to each broker, leaving one broker without a partition.
    But why create more brokers than the partition count? The value becomes apparent
    when you encounter issues with a broker going down. As we know, one of the most
    important attributes of Kafka is its fault tolerance. When a broker fails, Kafka
    automatically recovers using other brokers. The other important question is how
    producers and consumers know which broker to communicate with for reading and
    writing data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有三个分区和四个代理，Kafka 将尝试将它们分配，每个代理分配一个分区，留下一个代理没有分区。但为什么创建比分区数量更多的代理呢？当你遇到代理宕机的问题时，这个值就变得明显了。众所周知，Kafka
    最重要的属性之一是其容错性。当一个代理失败时，Kafka 会自动使用其他代理进行恢复。另一个重要的问题是生产者和消费者如何知道与哪个代理通信来读取和写入数据。
- en: The answer is simple. Since Kafka brokers also act as **bootstrap servers**,
    they possess all the essential information about other servers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 答案很简单。由于 Kafka 代理也充当 **引导服务器**，它们拥有关于其他服务器所有必要的信息。
- en: For example, consider any producer. Before producing data, the producer sends
    a background request to any broker (it doesn’t matter which one – even the nearest
    broker will do) to retrieve metadata information. This metadata contains all the
    relevant details about other brokers, their topics, partitions, and leader partitions
    (which we will cover in future discussions).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑任何生产者。在生产数据之前，生产者向任何代理（无论哪个代理——即使是最近的代理也可以）发送一个后台请求，以检索元数据信息。这些元数据包含有关其他代理、它们的主题、分区和领导者分区的所有相关细节（我们将在未来的讨论中介绍）。
- en: Using this metadata, the producer knows which broker to send data to. We call
    this process **Kafka** **broker discovery**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些元数据，生产者知道应该将数据发送给哪个代理。我们称这个过程为 **Kafka** **代理发现**。
- en: Topic and partition concepts in Apache Kafka
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Kafka 中的主题和分区概念
- en: The responsibility of the Kafka producer is to produce data. On the other hand,
    the Kafka consumer is a client for your message. The Kafka cluster acts as an
    isolator and *storage* for the producer and consumer. Before producing data, Kafka
    brokers need temporary storage to hold the data. These storage boxes are called
    **topics**.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 生产者的责任是生产数据。另一方面，Kafka 消费者是你消息的客户端。Kafka 集群充当生产者和消费者的隔离器和 *存储*。在生产数据之前，Kafka
    代理需要临时存储来保存数据。这些存储容器被称为 **主题**。
- en: A topic is a stream of data that acts as a logical isolator over partitions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 主题是一个数据流，它作为分区上的逻辑隔离器。
- en: The topic is important from the user’s point of view because when reading/writing
    the data, we’re referring mostly to the topic rather than partitions. (Of course,
    when defining partitions in the producing/consuming process, it is mandatory to
    point to the topic name, but in general, it is possible to produce/consume data
    without directly indicating partitions.) The topic concept helps us mere mortals
    to interact with Kafka without worrying about the internal storage mechanism.
    Every topic should have a unique name because the identification process for topics
    is done through their names. You can create as many topics as you want/your business
    requires.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户的角度来看，主题很重要，因为在读取/写入数据时，我们主要指的是主题而不是分区。（当然，在生产和消费过程中定义分区时，必须指定主题名称，但在一般情况下，可以在不直接指定分区的情况下生产和消费数据。）主题概念帮助我们普通人与
    Kafka 交互，而不用担心内部存储机制。每个主题都应该有一个唯一的名称，因为主题的识别过程是通过它们的名称来完成的。你可以创建尽可能多的主题，或者根据你的业务需求创建。
- en: Topics are not a thing that can live in one broker in production systems. Instead,
    using partitions, topics spread out to brokers. This means that, using partitions,
    a topic lives in multiple brokers. It helps Kafka to make a fault-tolerant, scalable,
    and distributed system.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产系统中，主题不是只能存在于一个代理中的东西。相反，通过使用分区，主题分散到各个代理。这意味着，通过使用分区，一个主题存在于多个代理中。这有助于Kafka构建一个容错、可扩展和分布式系统。
- en: Topics are durable, meaning that the data in them is persisted on disk. This
    makes Kafka a good choice for applications that need to reliably store and process
    data streams.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 主题是持久的，这意味着它们中的数据被持久化到磁盘上。这使得Kafka成为需要可靠存储和处理数据流的应用程序的良好选择。
- en: 'But how about partitions? Under the hood, Kafka uses partitions to store data.
    Every topic in production consists of multiple partitions. Kafka uses the topic
    concept for mainly two purposes:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 但分区又是如何工作的呢？在底层，Kafka使用分区来存储数据。生产环境中的每个主题都由多个分区组成。Kafka使用主题概念主要出于两个目的：
- en: To group partitions under one box for storing “one business point” data
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将分区分组在一个盒子下存储“一个业务点”数据
- en: To help users interact with Kafka without worrying about the internal structure
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了帮助用户与Kafka交互，无需担心其内部结构
- en: Kafka uses partitions to achieve parallelism and scalability. This means that
    multiple producers and consumers can work on the same topic at the same time,
    and the data is evenly distributed across the brokers in the cluster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka使用分区来实现并行性和可扩展性。这意味着多个生产者和消费者可以同时处理同一个主题，数据在集群中的代理之间均匀分布。
- en: So, why do we need the concept of partitions if we have topics? Well, using
    partitions, Kafka achieves distributive data storage and the **in-sync replica**
    (**ISR**) concept. Partitions help us to distribute topics and achieve fault-tolerant
    systems.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们需要分区的概念，如果我们已经有了主题呢？嗯，通过使用分区，Kafka实现了分布式数据存储和**同步副本**（ISR）的概念。分区帮助我们分配主题并实现容错系统。
- en: Every partition is identified by its ID. Every topic can have as many partitions
    as you want/your business requires. In production, it is very important to define
    the partition count when creating a topic – otherwise, the system will use the
    default configuration for the partition count. This means that, without defining
    the partition count, the system will automatically create the number of partitions
    per topic every time. The partition count should align with business requirements;
    for example, one topic might need forty partitions, while another may need two-hundred.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分区都通过其ID来识别。每个主题可以拥有你想要的/业务所需的任意数量的分区。在生产环境中，在创建主题时定义分区数量非常重要——否则，系统将使用分区数量的默认配置。这意味着，如果没有定义分区数量，系统将自动为每个主题创建分区数量。分区数量应与业务需求相一致；例如，一个主题可能需要四十个分区，而另一个可能需要两百个。
- en: You can think about partitions as a collection with a stack algorithm. Every
    partition is an array, and their indexes are called **offsets.** A partition has
    a dynamic offset count and there is no fixed size for it. Partitions are dynamically
    extendable, and their sizes can vary within the same topic. Every unit of information
    in a partition is called a message. Consumers can read data in a stacked manner.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将分区视为一个具有堆栈算法的集合。每个分区都是一个数组，它们的索引被称为**偏移量**。分区具有动态的偏移量计数，并且没有固定的大小。分区是动态可扩展的，它们的大小可以在同一个主题内变化。分区中的每个信息单元称为消息。消费者可以以堆叠的方式读取数据。
- en: Kafka partitions are split into Kafka brokers using a round-robin algorithm.
    This means that each broker in the cluster is assigned an equal number of partitions,
    as much as possible.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka分区使用轮询算法分配到Kafka代理。这意味着集群中的每个代理尽可能被分配相同数量的分区。
- en: 'But the process of splitting partitions across Kafka brokers also depends on
    the following factors:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，将分区分配到Kafka代理的过程也取决于以下因素：
- en: '**Number of partitions**: When you create a Kafka topic, you should specify
    the number of partitions it can have. This number determines how many parallel
    consumers or producers can work with the topic. The number of partitions should
    be chosen based on the expected workload and the level of parallelism required.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区数量**：当你创建一个Kafka主题时，你应该指定它可以拥有的分区数量。这个数字决定了可以与主题一起工作的并行消费者或生产者的数量。分区数量应根据预期的负载和所需的并行级别来选择。'
- en: '**Broker assignment**: The assignment is typically done in a balanced manner
    to ensure an even distribution of partitions across brokers, but it can be influenced
    by partition assignment strategies.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理分配**：分配通常以平衡的方式进行，以确保分区在代理之间均匀分布，但它可以受到分区分配策略的影响。'
- en: '**Partition assignment strategies**: Kafka provides different strategies for
    partition assignment, mainly controlled by the consumer group coordinator.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区分配策略**：Kafka 提供了不同的分区分配策略，主要由消费者组协调器控制。'
- en: '**Replication factor**: Kafka ensures fault tolerance through data replication
    across multiple brokers. Each partition has a specified replication factor, which
    determines how many copies of the data are maintained.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副本因子**：Kafka 通过在多个代理之间复制数据来确保容错性。每个分区都有一个指定的副本因子，它决定了维护的数据副本数量。'
- en: In short, we need partitions in Kafka because they are a core unit of parallelism
    and distribution and help Kafka to horizontally scale and distribute data. They
    also enable high throughput and fault tolerance and act as an internal storage
    mechanism.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们需要在 Kafka 中使用分区，因为它们是并行和分布的核心单元，并帮助 Kafka 横向扩展和分布数据。它们还实现了高吞吐量和容错性，并充当内部存储机制。
- en: It makes sense to note that once a topic is created with a certain number of
    partitions, it’s not proper to change the number of partitions for that topic.
    Instead, you need to create a new topic with the required number of partitions
    and migrate data if needed. Apache Kafka is a huge concept by its nature and if
    you want to learn more, you can check my *Apache Kafka for Distributed Systems*
    course on Udemy ([https://www.udemy.com/course/apache-kafka-for-distributed-systems/](https://www.udemy.com/course/apache-kafka-for-distributed-systems/)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，一旦创建了一个具有特定分区数的主题，就不再适当更改该主题的分区数。相反，您需要创建一个新的主题，并具有所需数量的分区，如果需要，迁移数据。Apache
    Kafka 本身是一个庞大的概念，如果您想了解更多，可以查看我在 Udemy 上的 *Apache Kafka for Distributed Systems*
    课程（[https://www.udemy.com/course/apache-kafka-for-distributed-systems/](https://www.udemy.com/course/apache-kafka-for-distributed-systems/)）。
- en: Configuring Apache Kafka
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Apache Kafka
- en: We talked about theoretical aspects of Apache Kafka and now it is time to implement
    it in practice. You can use the Kafka CLI to interact with Kafka, but we have
    already installed Kafka UI to make our lives easy and not deal with the complexities
    of command lines.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了 Apache Kafka 的理论方面，现在是时候将其付诸实践了。您可以使用 Kafka CLI 与 Kafka 交互，但我们已经安装了 Kafka
    UI 以简化我们的工作，避免处理命令行的复杂性。
- en: 'Our `.env` file defines a topic named `transaction-topic`, and to create it,
    let’s take the following steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `.env` 文件定义了一个名为 `transaction-topic` 的主题，要创建它，请按照以下步骤操作：
- en: Open Docker Desktop. Make sure that all the services are running for this chapter.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Docker Desktop。确保本章所有服务都在运行。
- en: Open your favorite browser and navigate to `http://localhost:9100/`.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您喜欢的浏览器，导航到 `http://localhost:9100/`。
- en: From the dashboard on the left, select **Topics**.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧的仪表板中选择 **主题**。
- en: Click the **Add a Topic** button at the top right and fill in the inputs (*Figure
    7**.3*).
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击右上角的 **添加主题** 按钮，并填写输入（*图 7**.3*）。
- en: '![Figure 7.3: Creating a topic for the broker in Apache Kafka](img/B09148_07_003.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3：在 Apache Kafka 中为代理创建主题](img/B09148_07_003.jpg)'
- en: 'Figure 7.3: Creating a topic for the broker in Apache Kafka'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：在 Apache Kafka 中为代理创建主题
- en: After successfully creation, you will see your topic in the **Topics** list.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 创建成功后，您将在 **主题** 列表中看到您的主题。
- en: So far, we configured Apache Kafka’s topic and created `kafka.service.ts` with
    `kafka.module.ts`. We plan to have fraud functionality in transactions and that
    is why we need to change three more files (`transaction.controller.ts`, `transaction.module.ts`,
    and `transaction.service.ts`) to integrate our new fraud functionality.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经配置了 Apache Kafka 的主题，并使用 `kafka.module.ts` 创建了 `kafka.service.ts`。我们计划在交易中实现欺诈功能，因此我们需要更改另外三个文件（`transaction.controller.ts`、`transaction.module.ts`
    和 `transaction.service.ts`）以集成我们的新欺诈功能。
- en: Adding an asynchronous nature to a transaction microservice
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为事务微服务添加异步特性
- en: 'What we need to do is integrate configuration reading and Kafka functionalities
    into the transaction service. The final version of `transaction.module.ts` will
    look like this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的是将配置读取和 Kafka 功能集成到事务服务中。`transaction.module.ts` 的最终版本将看起来像这样：
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We just added `KafkaService` and `ConfigService`. We plan to inject `KafkaService`
    into the `transaction.service.ts` file and it has a dependency on `ConfigService`.
    That is why we need to add both `KafkaService` and `ConfigService` to the `providers`
    list.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚添加了 `KafkaService` 和 `ConfigService`。我们计划将 `KafkaService` 注入到 `transaction.service.ts`
    文件中，并且它依赖于 `ConfigService`。这就是为什么我们需要将 `KafkaService` 和 `ConfigService` 都添加到 `providers`
    列表中的原因。
- en: 'Let’s switch to the `transaction.service.ts` file itself. The modified version
    of the file is shown here:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们切换到 `transaction.service.ts` 文件本身。文件修改后的版本如下所示：
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you might have already noticed, we injected `KafkaService` and the transaction
    has one more function, called `fraud`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经注意到的，我们注入了 `KafkaService`，并且事务还有一个名为 `fraud` 的额外功能。
- en: 'This asynchronous function, named `fraud`, is designed to handle marking a
    transaction as fraudulent. It fetches the transaction details, verifies its current
    status, updates it to `FRAUD` if valid, potentially sends a notification, and
    returns the updated transaction object. The function takes `id: number` as input,
    representing the unique identifier of the transaction to be flagged as fraudulent.
    The function begins by using `await` `this.findOne(id)` to retrieve the transaction
    data asynchronously from a database. It then checks that the transaction’s current
    status is neither `FRAUD` nor `FAILED` using the strict inequality operator (`!==`).
    This ensures the function doesn’t attempt to mark an already fraudulent or failed
    transaction again. If the status doesn’t meet the criteria, an error is thrown
    with the message `Transaction is not in a valid status` to prevent unexpected
    behavior. Assuming the status check passes (i.e., the transaction isn’t already
    fraudulent or failed), the code proceeds to update the transaction data. It utilizes
    the Prisma library (`this.prisma.transaction.update`) to modify the transaction
    record. The `where` property specifies that the update should target the specific
    transaction with the provided ID.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '这个名为 `fraud` 的异步函数旨在处理将交易标记为欺诈。它获取交易详情，验证其当前状态，如果有效则将其更新为 `FRAUD`，可能发送通知，并返回更新的交易对象。该函数以
    `id: number` 作为输入，表示要标记为欺诈的唯一标识符的交易。函数开始时使用 `await` `this.findOne(id)` 异步从数据库检索交易数据。然后它使用严格不等号运算符
    (`!==`) 检查交易当前的状态既不是 `FRAUD` 也不是 `FAILED`。这确保函数不会再次尝试标记已经欺诈或失败的交易。如果状态不符合标准，将抛出一个错误，错误信息为
    `Transaction is not in a valid status`，以防止意外行为。假设状态检查通过（即交易尚未欺诈或失败），代码将继续更新交易数据。它使用
    Prisma 库 (`this.prisma.transaction.update`) 来修改交易记录。`where` 属性指定更新应针对具有提供的 ID
    的特定交易。'
- en: The `data` property defines the changes to be made. In this case, it sets the
    `status` property of the transaction to `FRAUD`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`data` 属性定义了要进行的更改。在这种情况下，它将事务的 `status` 属性设置为 `FRAUD`。'
- en: The function includes the line `this.kafkaService.send(transaction, null)`.
    This suggests the use of a Kafka message broker to broadcast a notification about
    the fraudulent transaction. The second argument is a key. A message key is an
    optional element you can include with a message in Apache Kafka. It plays an important
    role in how messages are routed and processed within the system. Message keys
    are primarily used for partitioning messages within a topic. Kafka topics are
    further divided into partitions, serving as storage units for distributed data.
    By including a key, you can influence which partition a message gets sent to.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 函数中包含行 `this.kafkaService.send(transaction, null)`。这表明使用了 Kafka 消息代理来广播有关欺诈交易的通告。第二个参数是一个键。消息键是您可以在
    Apache Kafka 中与消息一起包含的可选元素。它在系统内部消息的路由和处理中扮演着重要角色。消息键主要用于在主题内分区消息。Kafka 主题进一步分为分区，作为分布式数据的存储单元。通过包含一个键，您可以影响消息被发送到哪个分区。
- en: Finally, if the status check is passed and the update is successful, the function
    returns the `newTransaction` object. This object contains the updated transaction
    details, including the newly set `FRAUD` status.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果状态检查通过并且更新成功，该函数将返回 `newTransaction` 对象。此对象包含更新的交易详情，包括新设置的 `FRAUD` 状态。
- en: In essence, this function provides a mechanism to flag a transaction as fraudulent,
    considering the current status and sending a notification through Kafka.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，此函数提供了一个机制，用于根据当前状态标记交易为欺诈，并通过 Kafka 发送通知。
- en: 'The final element is the controller. In the transaction controller, we have
    a new endpoint with the following behavior:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个元素是控制器。在交易控制器中，我们有一个新的端点，具有以下行为：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To test everything together, you should do the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要一起测试所有内容，您应该执行以下操作：
- en: Run `npm run start:dev` from the root (the `Ch07``/transactionservice` folder).
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从根目录（`Ch07``/transactionservice` 文件夹）运行 `npm run start:dev`。
- en: Navigate to `localhost:3000/api`.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 `localhost:3000/api`。
- en: We already have default migrated transactions. You can use their IDs to test
    our newly created API or you can create a transaction from scratch and test it.
    Let’s test one of our seed transactions. I’ll use the `id = 1` transaction (*Figure
    7**.4*).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经默认迁移了交易。您可以使用它们的 ID 来测试我们新创建的 API，或者您可以从头创建一个交易并测试它。让我们测试我们的一个种子交易。我将使用
    `id = 1` 的交易 (*图 7**.4*).
- en: '![Figure 7.4: Executing the fraud endpoint](img/B09148_07_004.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4：执行欺诈端点](img/B09148_07_004.jpg)'
- en: 'Figure 7.4: Executing the fraud endpoint'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：执行欺诈端点
- en: 'After successfully executing the fraud endpoint, we will end up with the following
    response:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功执行欺诈端点后，我们将得到以下响应：
- en: '[PRE12]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now let’s open Apache Kafka and check our message. Open `localhost:9100` from
    your favorite browser and go to`transaction-service-topic` and select the **Value**
    section from the **Messages** tab (*Figure 7**.5*):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '现在让我们打开 Apache Kafka 并检查我们的消息。从您喜欢的浏览器中打开 `localhost:9100`，然后转到 `transaction-service-topic`
    并从 **消息** 选项卡中选择 **值** 部分 (*图 7**.5*):'
- en: '![Figure 7.5: Success message in Apache Kafka](img/B09148_07_005.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5：Apache Kafka 中的成功消息](img/B09148_07_005.jpg)'
- en: 'Figure 7.5: Success message in Apache Kafka'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：Apache Kafka 中的成功消息
- en: Great! We’re able to send a message to Apache Kafka and now we need to somehow
    take this message and handle it. Reading messages from the source (it is Apache
    Kafka for us) is called a consuming process and a reader is a consumer.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们能够向 Apache Kafka 发送消息，现在我们需要以某种方式接收并处理这条消息。从源（对我们来说是 Apache Kafka）读取消息称为消费过程，而读取器是消费者。
- en: Adapting an account service to new requirements
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适应新的账户服务需求
- en: 'An account service is the main consumer we need to implement for the given
    context. First, we need to have the ability to interact with Apache Kafka. Just
    copy the already implemented account microservice and continue working on that.
    Navigate to `Ch07/accountService` and run the following command to install the
    `kafkajs` package:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 账户服务是我们需要为给定上下文实现的主要消费者。首先，我们需要能够与 Apache Kafka 交互。只需复制已经实现的账户微服务并继续在该服务上工作。导航到
    `Ch07/accountService` 并运行以下命令来安装 `kafkajs` 包：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we need to develop a separate module to work with Apache Kafka. Apache
    Kafka has its variables (brokers, topics, etc.) and that is why we use a `.env`
    file as we did for the transaction service. Under the `accountService` folder,
    we have `configs/.env`, and we added the following config items:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要开发一个单独的模块来与 Apache Kafka 一起工作。Apache Kafka 有其变量（代理、主题等），这就是为什么我们使用与交易服务相同的
    `.env` 文件。在 `accountService` 文件夹下，我们有 `configs/.env`，并添加了以下配置项：
- en: '[PRE14]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To read the `.env` file, we use a special configuration that lives under `src/config`
    and is called `config.js`. But we need to add the required changes to that file
    to support new key-value pairs. Here is the final version of `config.js`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取 `.env` 文件，我们使用位于 `src/config` 下的特殊配置，并称为 `config.js`。但我们需要向该文件添加必要的更改以支持新的键值对。以下是
    `config.js` 的最终版本：
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We just added additional lines to support `config` elements newly added to the
    `.``env` file.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是添加了额外的行来支持 `.``env` 文件中新添加的 `config` 元素。
- en: So far, we added a configuration reading mechanism and a Kafka package. Now
    it is time to develop a Kafka module to interact with Apache Kafka.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们添加了配置读取机制和 Kafka 包。现在是时候开发一个 Kafka 模块来与 Apache Kafka 交互了。
- en: 'Inside the `src` folder, create a new folder called `modules` and add a new
    file called `kafkamodule.js`. Thisto new requirements” module should have the
    following implementation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `src` 文件夹中，创建一个名为 `modules` 的新文件夹，并添加一个名为 `kafkamodule.js` 的新文件。这个新的“适应需求”模块应该具有以下实现：
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We need the following elements to adapt the account microservice to communicate
    with the transaction service:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要以下元素来使账户微服务能够与交易服务通信：
- en: '`Kafka`: To consume the message from Kafka'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Kafka`: 用于从 Kafka 消费消息'
- en: '`Account`: To interact with the database'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Account`: 用于与数据库交互'
- en: '`path`: To specify the path to read config files'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path`: 用于指定读取配置文件的路径'
- en: '`createConfig`: To retrieve the Kafka configuration'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createConfig`: 用于检索 Kafka 配置'
- en: 'Let’s go through the code step by step to understand what it does:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地查看代码，了解它做了什么：
- en: '`const configPath = path.join(__dirname, ''../../configs/.env'');`: This line
    constructs a path to the `.env` configuration file, which is located two directories
    up from the current directory, and then in the `configs` directory.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`const configPath = path.join(__dirname, ''../../configs/.env'');`：这一行构建到`.env`配置文件的路径，该文件位于当前目录向上两个目录，然后在`configs`目录中。'
- en: '`const appConfig = createConfig(configPath);`: This line calls the `createConfig`
    function with `configPath` as an argument. The `createConfig` function reads the
    configuration file and returns the configuration settings as an object (`appConfig`).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`const appConfig = createConfig(configPath);`：这一行调用`createConfig`函数，并将`configPath`作为参数。`createConfig`函数读取配置文件，并将配置设置作为对象返回（`appConfig`）。'
- en: The following lines create an instance of the Kafka client using the `KafkaJS`
    library. It configures the client with a `clientId` and a list of brokers, both
    of which are obtained from the `appConfig` object.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下行使用`KafkaJS`库创建Kafka客户端的实例。它使用`clientId`和从`appConfig`对象获取的经纪人列表配置客户端。
- en: '[PRE17]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`const consumer = kafka.consumer({ groupId: appConfig.kafka.groupId });`: This
    line creates a new Kafka consumer instance, specifying a `groupId` obtained from
    the `appConfig` object. The `groupId` is used to manage consumer group coordination
    in Kafka.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`const consumer = kafka.consumer({ groupId: appConfig.kafka.groupId });`：这一行创建一个新的Kafka消费者实例，指定从`appConfig`对象获取的`groupId`。`groupId`用于管理Kafka中的消费者组协调。'
- en: We have implemented the main functionality of our module inside the `consumerModule`
    function. This function connects to Kafka and subscribers to the given topic.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在`consumerModule`函数内部实现了我们模块的主要功能。这个函数连接到Kafka并订阅给定的主题。
- en: '`await consumer.connect();`: This line connects the Kafka consumer to the Kafka
    broker.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`await consumer.connect();`：这一行将Kafka消费者连接到Kafka经纪人。'
- en: '`await consumer.subscribe({ topic: appConfig.kafka.topic });`: This line subscribes
    the consumer to the specified Kafka topic, which is obtained from the `appConfig`
    object.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`await consumer.subscribe({ topic: appConfig.kafka.topic });`：这一行将消费者订阅到指定的Kafka主题，该主题从`appConfig`对象获取。'
- en: '`Consumer.run` starts the consumer to listen for messages from the subscribed
    Kafka topic. It defines an asynchronous handler function for each message that
    processes each consumed message. The function takes an object with `topic`, `partition`,
    and `message` properties. For each message, it parses the retrieved message and
    extracts the account ID. The real business rules start here. First, we check whether
    the given account ID exists in our database and that it is not blocked. If the
    account ID exists, then we should update this account, incrementing its count.
    If the increment count is `3`, then the status will be updated to `blocked`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Consumer.run`启动消费者以监听订阅的Kafka主题的消息。它为每个消息定义了一个异步处理函数，该函数处理每个消费的消息。该函数接受一个具有`topic`、`partition`和`message`属性的对象。对于每条消息，它解析检索到的消息并提取账户ID。真正的业务规则从这里开始。首先，我们检查给定的账户ID是否存在于我们的数据库中，并且它没有被阻止。如果账户ID存在，那么我们应该更新这个账户，增加其计数。如果增加的计数是`3`，那么状态将被更新为`blocked`。'
- en: The rest of the code lines are straightforward.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码行都很直接。
- en: 'To use the `kafkamodule.js` file capabilities, we need to import it into `app.js`
    and call it. Here is what `app.js` looks like:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`kafkamodule.js`文件的功能，我们需要将其导入到`app.js`中并调用它。以下是`app.js`的样貌：
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Well, as you might guess, we missed one important piece of information. Yes
    – it is a newly created `count`. To track fraud operations, we need to add a new
    item called `count` to the *account schema*. Open `models/account.js` and add
    the following lines to your schema:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，正如你可能猜到的，我们遗漏了一个重要的信息。是的——它是一个新创建的`count`。为了跟踪欺诈操作，我们需要在*账户模式*中添加一个名为`count`的新项。打开`models/account.js`，并将以下行添加到你的模式中：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We don’t need to change the account service and account controller to use our
    new `count :{}`. It is an implementation detail and a user should not be able
    to interact with this column directly. Everything is ready and we can test our
    service.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要更改账户服务和账户控制器来使用我们新的`count :{}`。这是一个实现细节，用户不应该能够直接与此列交互。一切准备就绪，我们可以测试我们的服务。
- en: Previously, we ran the account microservice without Docker Compose, but now
    we’ve added a `docker-compose.yml` file for it (`Ch07/accountservice/docker-compose.yml`).
    The transaction service already has its own `docker-compose.yml` file that hosts
    Kafka. To test both services together, we must run the `docker-compose.yml` files
    from the `accountservice` and `transactionservice` directories.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们在没有 Docker Compose 的情况下运行账户微服务，但现在我们为它添加了一个 `docker-compose.yml` 文件（`Ch07/accountservice/docker-compose.yml`）。事务服务已经有一个自己的
    `docker-compose.yml` 文件，用于托管 Kafka。为了一起测试这两个服务，我们必须从 `accountservice` 和 `transactionservice`
    目录运行 `docker-compose.yml` 文件。
- en: Testing our microservices together
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一起测试我们的微服务
- en: We should run transaction and account microservices together to test producing
    and consuming processes. First, let’s start with the account microservice. As
    mentioned before, don’t forget to run the `docker-compose.yml` files for both
    services.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该一起运行事务和账户微服务来测试生产和消费过程。首先，让我们从账户微服务开始。如前所述，不要忘记运行两个服务的 `docker-compose.yml`
    文件。
- en: 'To test the newly updated account, follow these steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试新更新的账户，请按照以下步骤操作：
- en: Navigate to `Ch07/accountservice/src` from the terminal.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从终端导航到 `Ch07/accountservice/src`。
- en: Run the account service from the command line using the `node` `index.js` command.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用命令行中的 `node index.js` 命令运行账户服务。
- en: 'Open Postman and, from the new tab, paste the service URL (it is `http://localhost:3001/v1/accounts`
    for us), and for the HTTP method, select `POST`. Select **Body** | **raw**, change
    **Text** to **JSON**, and paste the following:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Postman，从新标签页粘贴服务 URL（对我们来说它是 `http://localhost:3001/v1/accounts`），对于 HTTP
    方法，选择 `POST`。选择 **Body** | **raw**，将 **Text** 更改为 **JSON**，并粘贴以下内容：
- en: '[PRE20]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should get the following response:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下响应：
- en: '[PRE21]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is what it looks like in our database:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据库中，它看起来是这样的：
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Everything is okay for now based on what we have in the account microservice.
    Our main trigger, the producer, is the transaction microservice. Let’s produce
    a message and see whether the account microservice can consume this message or
    not. We still need the account service to run in parallel with transaction microservice,
    and that is why we need to open a new terminal to run the transaction microservice:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 根据账户微服务中的内容，目前一切正常。我们的主要触发器，生产者，是事务微服务。让我们生产一条消息，看看账户微服务是否可以消费这条消息。我们仍然需要账户服务与事务微服务并行运行，这就是为什么我们需要打开一个新的终端来运行事务微服务：
- en: Open a new terminal and navigate to `Ch07/transactionservice`.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的终端并导航到 `Ch07/transactionservice`。
- en: Run `npm run start:dev` to start the transaction microservice.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `npm run start:dev` 以启动事务微服务。
- en: Navigate to `http://localhost:3000/api/` and select `POST /transaction/`.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 `http://localhost:3000/api/` 并选择 `POST /transaction/`.
- en: 'Paste the following JSON to create a new transaction:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下 JSON 粘贴以创建一个新的交易：
- en: '[PRE23]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You should use the account ID created by the account microservice. You will
    get the following response:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该使用账户微服务创建的账户 ID。你将得到以下响应：
- en: '[PRE24]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '{'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{'
- en: '"id": 37,'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"id": 37,'
- en: '"status": "FRAUD",'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"status": "FRAUD",'
- en: '"accountId": "6658ae5284432e40604018d5",'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"accountId": "6658ae5284432e40604018d5",'
- en: '"description": "Optional transaction description",'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"description": "可选的事务描述",'
- en: ……..
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ……..
- en: '}'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE25]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '{'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{'
- en: ….
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ….
- en: '"type": "root",'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"type": "root",'
- en: '"status": "blocked",'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"status": "blocked",'
- en: '"count": 3,'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"count": 3,'
- en: ………
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ………
- en: '}'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '}'
- en: '[PRE26]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: With an understanding of the asynchronous communication technique, you can easily
    apply it to your projects with confidence.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解异步通信技术，你可以自信地将它应用到你的项目中。
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we started our journey by learning the importance of defining
    proper communication between microservices. We mostly use two main communication
    forms: async and sync. Choosing one over another is always a context-dependent
    choice – context is king. Then, we talked about the advantages of asynchronous
    communication. There are multiple ways of implementing asynchronous communication
    and we talked about most of the popular choices. Everything has a price and integrated
    microservices architecture is not an exception. It brings a lot of additional
    complexity we need to take into account and one of them is asynchronous communication.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过学习定义微服务之间适当通信的重要性开始了我们的旅程。我们主要使用两种主要的通信形式：异步和同步。选择其中一种总是基于上下文的选择——上下文是王。然后，我们讨论了异步通信的优点。实现异步通信有多种方式，我们讨论了大多数流行的选择。任何事物都有代价，集成微服务架构也不例外。它带来了许多额外的复杂性，我们需要考虑其中之一就是异步通信。
- en: We talked about Apache Kafka, which helps us to overcome the problem we have.
    We learned about essential concepts such as clusters, brokers, topics, messages,
    and partitions. Our practical examples covered two main microservices. The transaction
    service was our producer, which produces a message, and the account microservice
    was a consumer that consumers that message. Of course, there are a lot of subtopics,
    such as refactoring, exception handling, testing, and deploying, that we haven’t
    covered yet, and the following chapters will cover these in detail.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了Apache Kafka，它帮助我们克服了存在的问题。我们学习了诸如集群、代理、主题、消息和分区等基本概念。我们的实际例子涵盖了两个主要的微服务。事务服务是我们的生产者，它产生消息，而账户微服务是一个消费者，它消费那个消息。当然，还有很多子主题，例如重构、异常处理、测试和部署等，我们还没有涉及，接下来的章节将详细讨论这些内容。
