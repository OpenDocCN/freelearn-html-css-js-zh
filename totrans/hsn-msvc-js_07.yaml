- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are designed to be independent and self-contained. Clearly defined
    communication protocols and APIs ensure these services interact without relying
    on each other’s internal workings. Defining proper communication between microservices
    is important for a well-functioning microservices architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we plan to discuss and learn about another important communication
    mechanism: asynchronous communication between microservices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring asynchronous communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an asynchronous transaction microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adapting an account service to new requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing our microservices together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get into it!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with the chapter, you’ll need an IDE (we prefer Visual Studio
    Code), Postman, Docker, and a browser of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: It is preferable to download the repository from [https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript)
    and open the `Ch07` folder to easily follow the code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up until now, we have developed two simple microservices and for the current
    chapter we plan to extend our transaction microservice to meet the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Every transaction should support the following statuses: `CREATED`, `FAILED`,
    `APPROVED`, `DECLINED`, and `FRAUD`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transaction service should now have a new method that changes the status
    of the given transaction to `FRAUD`. It will update the status of the transaction
    to `FRAUD` and produce a message about the transaction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The account service will consume this message and after *three* fraudulent attempts,
    the account service should read and suspend/block the given account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We plan to use asynchronous communication between microservices and any other
    microservice may use this message for internal purposes. You can check [*Chapter
    2*](B09148_02.xhtml#_idTextAnchor027) for more information about asynchronous
    communication between microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring asynchronous communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can implement asynchronous communication between microservices using various
    patterns and technologies, each suitable for different use cases and requirements.
    Here are some of the common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Message brokers**: Message brokers facilitate asynchronous communication
    by allowing microservices to publish and subscribe to messages. Popular message
    brokers include **RabbitMQ**, which supports multiple messaging protocols and
    patterns such as pub/sub and routing, and **Apache Kafka**, designed for high-throughput
    and fault-tolerant event streaming – one of the best choices for real-time data
    processing. An example of a message broker would be a producer service sending
    a message to a queue or topic and the consumer service subscribing to the queue
    or topic and processing messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event streaming platforms**: Event streaming platforms capture and process
    streams of events. These platforms are particularly useful for real-time analytics
    and data pipeline construction. Popular event streaming platforms include **Apache
    Kafka**, which is often used as both a message broker and an event streaming platform,
    and **Amazon Kinesis**, a managed service for real-time data processing at scale.
    Here is an example: a producer service emits events to a Kafka topic and consumer
    services consume events from the topic and react to them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Publish-Subscribe pattern**: In the pub/sub pattern, messages are published
    to a topic and multiple subscribers can consume these messages asynchronously.
    Popular services that use the pub/sub pattern include **Google Pub/Sub**, a fully
    managed real-time messaging service, and **AWS Simple Notification Service** (**SNS**),
    which allows publishing messages to multiple subscribers. For example, a publisher
    service publishes an event to a topic and the subscriber services receive notifications
    and process the event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task queues**: Task queues are used to distribute tasks to worker services
    asynchronously. This is useful for offloading heavy or time-consuming tasks from
    the main service. Some of the more popular task queues are **Celery**, an asynchronous
    task queue/job queue based on distributed message passing, and **Amazon Simple
    Queue Service** (**SQS**), a fully managed message queue service. Here’s how a
    task queue works: a producer service creates a task and places it in the queue
    and the worker service picks up the task from the queue and processes it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event-driven architecture**: In an event-driven architecture, services communicate
    through events. When something notable happens in one service, it emits an event
    that other services can listen to and act upon. In event-driven architecture an
    event source service publishes an event, and the event listener services react
    to the event and execute their logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WebSockets**: WebSockets allow for full-duplex communication channels over
    a single TCP connection, useful for real-time applications such as chat apps or
    live updates. Here’s an example: the server pushes updates to clients via WebSockets
    and clients receive updates in real time and act upon them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server-Sent Events** (**SSE**): SSE is a server push technology enabling
    servers to push real-time updates to the client once an initial client connection
    is established. Let’s take an example: the server sends events to clients over
    an HTTP connection and clients listen to incoming messages and process them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gRPC with streaming**: gRPC supports bidirectional streaming, allowing both
    client and server to send a sequence of messages using a single connection. gRPC
    works like this: the client and server can continuously exchange streams of messages
    as part of a single RPC call.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this chapter, we will actively use Apache Kafka, an open source, high-performance
    event streaming platform. It is a popular choice for asynchronous communication
    between microservices due to its strengths in enabling a robust and scalable event-driven
    architecture. While we have already talked about how to run services via Docker,
    this chapter will focus on hosting Apache Kafka on Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a quick look at the problems that Apache Kafka solves:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Communication complexity**: In a microservice environment, you have *multiple
    sources* (every API acts as a source) and *multiple targets* (every API can have
    multiple sources to write to). The fact that sources and targets are scaled is
    always accompanied by a communication problem. In this case, the problem is that
    we should solve the complexities created by the source and target rather than
    focus on business requirement implementations. Now you have multiple sources and
    targets, which can create the following issues:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every target requires a different protocol to communicate.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Every target has its data format to work with.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Every different target requires maintenance and support.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In simple terms, say you have a microservice application, and every service
    has its own target. Besides that, every service can have multiple sources, and
    the services can use common sources. Apache Kafka helps you to avoid complex communication
    between microservices.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Communication complexity duplication**: Whenever similar systems are developed;
    we have to rewrite such communication processes again and again. Let’s imagine
    that we are working on several different projects. Although the domain of these
    projects is different, and although they solve different problems at an abstract
    level, the common aspect of these projects is communication complexity. So, it
    means we’re repeating ourselves and trying to resolve the same issue every time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: The system should be able to continue functioning and
    provide reliable data processing and message delivery even in the presence of
    various types of failures, such as hardware failures, network issues, or software
    crashes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High performance**: In most cases, such a communication problem (sources
    - targets) causes the application performance to drop. Regardless of dynamic changes
    in the number of targets and sources in the application, the program should always
    support the high-performance attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: The system should be possible to horizontally scale sources
    and targets. Horizontal scaling, also known as scaling out, is a technique in
    software design for increasing the capacity of a system by adding more machines
    (nodes) to distribute the workload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time communication**: One of the possible target and source communication
    attributes is real-time communication. Depending on the use cases, the system
    should allow real-time data exchange between the source and the target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log and data aggregation**: This is the ability to combine and process logs
    and data in certain aggregates. Log and data aggregation play a crucial role in
    modern software by centralizing and organizing information from various sources,
    making it easier to analyze, troubleshoot, and optimize applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation and processing**: The communication between the target
    and source is not only in the form of data exchange but also information should
    be based on the possibility of transformation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s talk about the infrastructure we need to use to implement our microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an asynchronous transaction microservice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the same transaction microservice we implemented in [*Chapter 6*](B09148_06.xhtml#_idTextAnchor104)
    but with additional changes that will help us add asynchronous behavior to it.
    First, we should prepare our infrastructure. Here is what we will have in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Kafka**: To create loose coupling between microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka UI**: This is a web application designed for managing Apache Kafka
    clusters. It provides a **graphical user interface** (**GUI**) instead of the
    traditional **command-line interface** (**CLI**) for Kafka, making it easier to
    interact with Kafka for many users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zookeeper**: This is open source software that acts as a central coordinator
    for large distributed systems. Think of it as a conductor for an orchestra, keeping
    everything in sync.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PostgreSQL**: To store data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PgAdmin**: A graphical tool to visually see database elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have our `docker-compose.yml` file in our root folder (`Ch07/transactionservice`).
  prefs: []
  type: TYPE_NORMAL
- en: This `docker-compose` file defines a multi-service setup for a PostgreSQL database,
    a PgAdmin instance for managing the database, and a Kafka messaging system with
    Zookeeper for coordination. The services are connected through a custom Docker
    network, `my-app-network`, which enables inter-container communication. For Kafka,
    ensure the correct network settings are configured to avoid connectivity issues,
    especially for multi-network setups where `advertised.listeners` may be needed
    for both internal and external addresses. The PostgreSQL service stores its data
    in a named volume, `postgres_data`, while `PgAdmin` depends on PostgreSQL to be
    up and running. The Kafka and Zookeeper services are set up for message brokering,
    with Kafka UI providing management and monitoring, relying on Zookeeper to maintain
    a distributed system configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the root folder and run the `docker-compose up -d` command to spin
    up the infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Here is how it should look after a successful run (*Figure 7**.1*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: Docker infrastructure](img/B09148_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Docker infrastructure'
  prefs: []
  type: TYPE_NORMAL
- en: After successfully running our docker infrastructure, we are ready to switch
    to our source code to implement our requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to update our transaction service to support additional statuses.
    Open the `schema.prisma` file under the `prisma/migrations` folder and change
    `enum` to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As we already know, one of the responsibilities of Prisma is to isolate us
    from database internals and provide a unique, more understandable language over
    these internals. That is why we have the `.prisma` extension and to map it to
    real SQL, we need to run migration. We already know about the migration steps
    and their impact on your development (check [*Chapter 6*](B09148_06.xhtml#_idTextAnchor104)
    for more detailed information), so in this chapter, we just provide the exact
    command without explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After running the command, you should end up with an additional folder that
    contains `migration.sql` and the folder name is a combination of the generation
    date and the name you provided from the command (*Figure 7**.2*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: Newly generated migration context for statuses](img/B09148_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Newly generated migration context for statuses'
  prefs: []
  type: TYPE_NORMAL
- en: The main functionality we plan to add to the transaction service is fraud functionality.
    This method should change the status of a transaction to `FRAUD` if it is not
    a failed transaction. After updating the status, it should publish a message to
    the broker (Apache Kafka in this case).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Kafka for NestJS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we learned in [*Chapter 6*](B09148_06.xhtml#_idTextAnchor104), NestJS has
    a lot of useful packages to work with different technologies. You don’t need to
    write any of them to integrate them into your project. This applies to Apache
    Kafka also. We don’t need to develop a separate package from scratch for it; just
    run the following command to install the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After successful installation, you will end up with additional changes in your
    `package.json` file. NestJS has a special pattern combination to configure services.
    That is why we first need to create our `kafka` module. As we already learned,
    there is no need to create this file manually. You just need to run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It should generate a folder called `kafka` that contains the `kafka.module.ts`
    file. This module should have `KafkaService` as its provider element, but we don’t
    have a Kafka service. Running the following command will generate `kafka.service.ts`
    and `kafka.service.spec.ts` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We don’t need to work on `kafka.service.spec.ts` and it is up to you to remove
    it. These files are automatically generated test files, and we won’t run any tests
    for this chapter. To make things as simple as possible, we remove it. After running
    the last command, you should realize that `kafka.module.ts` was also automatically
    updated. Here is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The code in `kafka.module.ts` is straightforward and easy to understand due
    to its minimal lines.: A bit later we will talk about the `nestjs/config` package
    also. We will implement the main functionality inside `kafka.service.ts` file.
    Open your `kafka.service.ts` file and replace it with the following code lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s understand what we just did:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Injectable`: This indicates that the class is injectable into other services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OnModuleInit` and `OnModuleDestroy`: These are lifecycle hooks for initialization
    and cleanup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConfigService`: This provides access to environment variables and configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Kafka` and `Producer`: These are classes from the `kafkajs` library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@Injectable()`: This makes the `KafkaService` injectable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`implements OnModuleInit` and `OnModuleDestroy`: This implements the lifecycle
    hooks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`producer`: The Kafka `producer` instance is used for sending messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topic`: This is the pre-configured Kafka topic for message delivery (fetched
    from environment variables).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`configService`: This is the injected instance for accessing configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The constructor of the class fetches Kafka configuration values from environment
    variables:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_CLIENT_ID`: This is the client ID for your application.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_BROKERS`: This is a comma-separated list of Kafka broker addresses.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_TOPIC`: This is the Kafka topic for sending messages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`const kafka = new Kafka({ clientId, brokers });`: This creates a Kafka client
    using the configuration.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`this.producer = kafka.producer({ retry: { retries: 3 } })`: This creates a
    producer instance with a retry configuration for message reliability (set to retry
    three times by default).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onModuleInit`: This connects the Kafka producer when the NestJS module is
    initialized, ensuring the producer is ready to send messages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onModuleDestroy`: This disconnects the Kafka producer when the NestJS module
    is destroyed, releasing resources.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`send`: This takes a value (any) to be sent and an optional key (string) for
    message identification. It constructs a message object with a key and value (serialized
    as JSON) and sends the message to the pre-configured topic using the producer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sensitive information should not be stored directly in kafka.service.ts. For
    this chapter, store configuration settings in a `.env` file locally. However,
    avoid committing this file to version control. For production deployments, consider
    using a secure vault service, like AWS Secrets Manager or Azure Key Vault, to
    manage sensitive configurations securely. From the previous code, it is obvious
    that we store our three main Kafka configurations in a `.env` file. Open your
    `.env` file and add the following lines to the end of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We have already used the `.env` file to configure `postgresql` ([*Chapter 6*](B09148_06.xhtml#_idTextAnchor104)),
    but for this chapter, we need to specify a mechanism that can read .`env` files.
    Another NestJS package, called `config`, will help us to deal with this issue.
    Let’s install it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: That is all. We have imported the package to `kafka.service.js` to work with
    it. Now it is time to talk about Kafka’s essentials. When we produce or consume
    messages, we need to interact with Apache Kafka, and you need to understand some
    basics of Kafka before using it.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster and brokers in Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In production, a Kafka cluster typically consists of multiple brokers, each
    of which stores and manages partitions for assigned topics. Kafka uses ZooKeeper
    (or KRaft in newer versions) to coordinate broker metadata and ensure consistent
    partition distribution across the cluster. A **broker** is synonymous with a **Kafka
    server**. Each broker is a server. The purpose of a broker is to serve data.
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t matter whether it is physical or not; in the end, a broker should
    function as a server. While it is technically possible to have only one broker
    in a cluster, this is usually done only for testing or self-learning purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The purposes of a cluster include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling multiple requests in parallel using multiple brokers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing high throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each broker within a Kafka cluster is also a bootstrap server, containing metadata
    about all other brokers, topics, and partitions. When consumers join a consumer
    group, Kafka's group coordinator uses assignment strategies like Range or RoundRobin
    to assign partitions, ensuring even distribution and balancing the load across
    consumers. This means that when you connect to one broker, you are automatically
    connected to the entire cluster. In most cases, a good starting point is to have
    three brokers. Three brokers in Apache Kafka provide a balance between fault tolerance
    and efficiency. With three brokers, Kafka can replicate data across multiple nodes,
    ensuring high availability even if one broker fails. It also enables a replication
    factor of three, which allows the system to tolerate a broker failure without
    losing data, while avoiding the overhead of managing too many brokers. However,
    in high-load systems, you might end up with hundreds of brokers.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the topic’s configuration, a broker, as a storage type, consists
    of multiple **partitions**. When we create a topic, we define the number of partitions
    under that topic. Kafka, as a distributed system, employs the best algorithm to
    distribute these partitions among brokers.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a Kafka cluster with three brokers. When creating a topic named
    `tracking_accounts` with three partitions, Kafka will attempt to distribute the
    partitions among the three brokers. In the best scenario, this will result in
    one partition per broker. Of course, this depends on various factors, including
    load balancing. You don’t need to intervene; Kafka, as a **distributed framework**,
    automatically manages all these internal operations.
  prefs: []
  type: TYPE_NORMAL
- en: If you have three partitions and four brokers, Kafka will attempt to distribute
    them, assigning one partition to each broker, leaving one broker without a partition.
    But why create more brokers than the partition count? The value becomes apparent
    when you encounter issues with a broker going down. As we know, one of the most
    important attributes of Kafka is its fault tolerance. When a broker fails, Kafka
    automatically recovers using other brokers. The other important question is how
    producers and consumers know which broker to communicate with for reading and
    writing data.
  prefs: []
  type: TYPE_NORMAL
- en: The answer is simple. Since Kafka brokers also act as **bootstrap servers**,
    they possess all the essential information about other servers.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider any producer. Before producing data, the producer sends
    a background request to any broker (it doesn’t matter which one – even the nearest
    broker will do) to retrieve metadata information. This metadata contains all the
    relevant details about other brokers, their topics, partitions, and leader partitions
    (which we will cover in future discussions).
  prefs: []
  type: TYPE_NORMAL
- en: Using this metadata, the producer knows which broker to send data to. We call
    this process **Kafka** **broker discovery**.
  prefs: []
  type: TYPE_NORMAL
- en: Topic and partition concepts in Apache Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The responsibility of the Kafka producer is to produce data. On the other hand,
    the Kafka consumer is a client for your message. The Kafka cluster acts as an
    isolator and *storage* for the producer and consumer. Before producing data, Kafka
    brokers need temporary storage to hold the data. These storage boxes are called
    **topics**.
  prefs: []
  type: TYPE_NORMAL
- en: A topic is a stream of data that acts as a logical isolator over partitions.
  prefs: []
  type: TYPE_NORMAL
- en: The topic is important from the user’s point of view because when reading/writing
    the data, we’re referring mostly to the topic rather than partitions. (Of course,
    when defining partitions in the producing/consuming process, it is mandatory to
    point to the topic name, but in general, it is possible to produce/consume data
    without directly indicating partitions.) The topic concept helps us mere mortals
    to interact with Kafka without worrying about the internal storage mechanism.
    Every topic should have a unique name because the identification process for topics
    is done through their names. You can create as many topics as you want/your business
    requires.
  prefs: []
  type: TYPE_NORMAL
- en: Topics are not a thing that can live in one broker in production systems. Instead,
    using partitions, topics spread out to brokers. This means that, using partitions,
    a topic lives in multiple brokers. It helps Kafka to make a fault-tolerant, scalable,
    and distributed system.
  prefs: []
  type: TYPE_NORMAL
- en: Topics are durable, meaning that the data in them is persisted on disk. This
    makes Kafka a good choice for applications that need to reliably store and process
    data streams.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how about partitions? Under the hood, Kafka uses partitions to store data.
    Every topic in production consists of multiple partitions. Kafka uses the topic
    concept for mainly two purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: To group partitions under one box for storing “one business point” data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To help users interact with Kafka without worrying about the internal structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka uses partitions to achieve parallelism and scalability. This means that
    multiple producers and consumers can work on the same topic at the same time,
    and the data is evenly distributed across the brokers in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: So, why do we need the concept of partitions if we have topics? Well, using
    partitions, Kafka achieves distributive data storage and the **in-sync replica**
    (**ISR**) concept. Partitions help us to distribute topics and achieve fault-tolerant
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Every partition is identified by its ID. Every topic can have as many partitions
    as you want/your business requires. In production, it is very important to define
    the partition count when creating a topic – otherwise, the system will use the
    default configuration for the partition count. This means that, without defining
    the partition count, the system will automatically create the number of partitions
    per topic every time. The partition count should align with business requirements;
    for example, one topic might need forty partitions, while another may need two-hundred.
  prefs: []
  type: TYPE_NORMAL
- en: You can think about partitions as a collection with a stack algorithm. Every
    partition is an array, and their indexes are called **offsets.** A partition has
    a dynamic offset count and there is no fixed size for it. Partitions are dynamically
    extendable, and their sizes can vary within the same topic. Every unit of information
    in a partition is called a message. Consumers can read data in a stacked manner.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka partitions are split into Kafka brokers using a round-robin algorithm.
    This means that each broker in the cluster is assigned an equal number of partitions,
    as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the process of splitting partitions across Kafka brokers also depends on
    the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of partitions**: When you create a Kafka topic, you should specify
    the number of partitions it can have. This number determines how many parallel
    consumers or producers can work with the topic. The number of partitions should
    be chosen based on the expected workload and the level of parallelism required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broker assignment**: The assignment is typically done in a balanced manner
    to ensure an even distribution of partitions across brokers, but it can be influenced
    by partition assignment strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition assignment strategies**: Kafka provides different strategies for
    partition assignment, mainly controlled by the consumer group coordinator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replication factor**: Kafka ensures fault tolerance through data replication
    across multiple brokers. Each partition has a specified replication factor, which
    determines how many copies of the data are maintained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, we need partitions in Kafka because they are a core unit of parallelism
    and distribution and help Kafka to horizontally scale and distribute data. They
    also enable high throughput and fault tolerance and act as an internal storage
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense to note that once a topic is created with a certain number of
    partitions, it’s not proper to change the number of partitions for that topic.
    Instead, you need to create a new topic with the required number of partitions
    and migrate data if needed. Apache Kafka is a huge concept by its nature and if
    you want to learn more, you can check my *Apache Kafka for Distributed Systems*
    course on Udemy ([https://www.udemy.com/course/apache-kafka-for-distributed-systems/](https://www.udemy.com/course/apache-kafka-for-distributed-systems/)).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Apache Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We talked about theoretical aspects of Apache Kafka and now it is time to implement
    it in practice. You can use the Kafka CLI to interact with Kafka, but we have
    already installed Kafka UI to make our lives easy and not deal with the complexities
    of command lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `.env` file defines a topic named `transaction-topic`, and to create it,
    let’s take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Docker Desktop. Make sure that all the services are running for this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open your favorite browser and navigate to `http://localhost:9100/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the dashboard on the left, select **Topics**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Add a Topic** button at the top right and fill in the inputs (*Figure
    7**.3*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.3: Creating a topic for the broker in Apache Kafka](img/B09148_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Creating a topic for the broker in Apache Kafka'
  prefs: []
  type: TYPE_NORMAL
- en: After successfully creation, you will see your topic in the **Topics** list.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we configured Apache Kafka’s topic and created `kafka.service.ts` with
    `kafka.module.ts`. We plan to have fraud functionality in transactions and that
    is why we need to change three more files (`transaction.controller.ts`, `transaction.module.ts`,
    and `transaction.service.ts`) to integrate our new fraud functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Adding an asynchronous nature to a transaction microservice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What we need to do is integrate configuration reading and Kafka functionalities
    into the transaction service. The final version of `transaction.module.ts` will
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We just added `KafkaService` and `ConfigService`. We plan to inject `KafkaService`
    into the `transaction.service.ts` file and it has a dependency on `ConfigService`.
    That is why we need to add both `KafkaService` and `ConfigService` to the `providers`
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s switch to the `transaction.service.ts` file itself. The modified version
    of the file is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you might have already noticed, we injected `KafkaService` and the transaction
    has one more function, called `fraud`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This asynchronous function, named `fraud`, is designed to handle marking a
    transaction as fraudulent. It fetches the transaction details, verifies its current
    status, updates it to `FRAUD` if valid, potentially sends a notification, and
    returns the updated transaction object. The function takes `id: number` as input,
    representing the unique identifier of the transaction to be flagged as fraudulent.
    The function begins by using `await` `this.findOne(id)` to retrieve the transaction
    data asynchronously from a database. It then checks that the transaction’s current
    status is neither `FRAUD` nor `FAILED` using the strict inequality operator (`!==`).
    This ensures the function doesn’t attempt to mark an already fraudulent or failed
    transaction again. If the status doesn’t meet the criteria, an error is thrown
    with the message `Transaction is not in a valid status` to prevent unexpected
    behavior. Assuming the status check passes (i.e., the transaction isn’t already
    fraudulent or failed), the code proceeds to update the transaction data. It utilizes
    the Prisma library (`this.prisma.transaction.update`) to modify the transaction
    record. The `where` property specifies that the update should target the specific
    transaction with the provided ID.'
  prefs: []
  type: TYPE_NORMAL
- en: The `data` property defines the changes to be made. In this case, it sets the
    `status` property of the transaction to `FRAUD`.
  prefs: []
  type: TYPE_NORMAL
- en: The function includes the line `this.kafkaService.send(transaction, null)`.
    This suggests the use of a Kafka message broker to broadcast a notification about
    the fraudulent transaction. The second argument is a key. A message key is an
    optional element you can include with a message in Apache Kafka. It plays an important
    role in how messages are routed and processed within the system. Message keys
    are primarily used for partitioning messages within a topic. Kafka topics are
    further divided into partitions, serving as storage units for distributed data.
    By including a key, you can influence which partition a message gets sent to.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if the status check is passed and the update is successful, the function
    returns the `newTransaction` object. This object contains the updated transaction
    details, including the newly set `FRAUD` status.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, this function provides a mechanism to flag a transaction as fraudulent,
    considering the current status and sending a notification through Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final element is the controller. In the transaction controller, we have
    a new endpoint with the following behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To test everything together, you should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `npm run start:dev` from the root (the `Ch07``/transactionservice` folder).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `localhost:3000/api`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We already have default migrated transactions. You can use their IDs to test
    our newly created API or you can create a transaction from scratch and test it.
    Let’s test one of our seed transactions. I’ll use the `id = 1` transaction (*Figure
    7**.4*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4: Executing the fraud endpoint](img/B09148_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Executing the fraud endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: 'After successfully executing the fraud endpoint, we will end up with the following
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s open Apache Kafka and check our message. Open `localhost:9100` from
    your favorite browser and go to`transaction-service-topic` and select the **Value**
    section from the **Messages** tab (*Figure 7**.5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5: Success message in Apache Kafka](img/B09148_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Success message in Apache Kafka'
  prefs: []
  type: TYPE_NORMAL
- en: Great! We’re able to send a message to Apache Kafka and now we need to somehow
    take this message and handle it. Reading messages from the source (it is Apache
    Kafka for us) is called a consuming process and a reader is a consumer.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting an account service to new requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An account service is the main consumer we need to implement for the given
    context. First, we need to have the ability to interact with Apache Kafka. Just
    copy the already implemented account microservice and continue working on that.
    Navigate to `Ch07/accountService` and run the following command to install the
    `kafkajs` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to develop a separate module to work with Apache Kafka. Apache
    Kafka has its variables (brokers, topics, etc.) and that is why we use a `.env`
    file as we did for the transaction service. Under the `accountService` folder,
    we have `configs/.env`, and we added the following config items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To read the `.env` file, we use a special configuration that lives under `src/config`
    and is called `config.js`. But we need to add the required changes to that file
    to support new key-value pairs. Here is the final version of `config.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We just added additional lines to support `config` elements newly added to the
    `.``env` file.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we added a configuration reading mechanism and a Kafka package. Now
    it is time to develop a Kafka module to interact with Apache Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `src` folder, create a new folder called `modules` and add a new
    file called `kafkamodule.js`. Thisto new requirements” module should have the
    following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We need the following elements to adapt the account microservice to communicate
    with the transaction service:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Kafka`: To consume the message from Kafka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Account`: To interact with the database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: To specify the path to read config files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`createConfig`: To retrieve the Kafka configuration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s go through the code step by step to understand what it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '`const configPath = path.join(__dirname, ''../../configs/.env'');`: This line
    constructs a path to the `.env` configuration file, which is located two directories
    up from the current directory, and then in the `configs` directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`const appConfig = createConfig(configPath);`: This line calls the `createConfig`
    function with `configPath` as an argument. The `createConfig` function reads the
    configuration file and returns the configuration settings as an object (`appConfig`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following lines create an instance of the Kafka client using the `KafkaJS`
    library. It configures the client with a `clientId` and a list of brokers, both
    of which are obtained from the `appConfig` object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`const consumer = kafka.consumer({ groupId: appConfig.kafka.groupId });`: This
    line creates a new Kafka consumer instance, specifying a `groupId` obtained from
    the `appConfig` object. The `groupId` is used to manage consumer group coordination
    in Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have implemented the main functionality of our module inside the `consumerModule`
    function. This function connects to Kafka and subscribers to the given topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`await consumer.connect();`: This line connects the Kafka consumer to the Kafka
    broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`await consumer.subscribe({ topic: appConfig.kafka.topic });`: This line subscribes
    the consumer to the specified Kafka topic, which is obtained from the `appConfig`
    object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Consumer.run` starts the consumer to listen for messages from the subscribed
    Kafka topic. It defines an asynchronous handler function for each message that
    processes each consumed message. The function takes an object with `topic`, `partition`,
    and `message` properties. For each message, it parses the retrieved message and
    extracts the account ID. The real business rules start here. First, we check whether
    the given account ID exists in our database and that it is not blocked. If the
    account ID exists, then we should update this account, incrementing its count.
    If the increment count is `3`, then the status will be updated to `blocked`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of the code lines are straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the `kafkamodule.js` file capabilities, we need to import it into `app.js`
    and call it. Here is what `app.js` looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, as you might guess, we missed one important piece of information. Yes
    – it is a newly created `count`. To track fraud operations, we need to add a new
    item called `count` to the *account schema*. Open `models/account.js` and add
    the following lines to your schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We don’t need to change the account service and account controller to use our
    new `count :{}`. It is an implementation detail and a user should not be able
    to interact with this column directly. Everything is ready and we can test our
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we ran the account microservice without Docker Compose, but now
    we’ve added a `docker-compose.yml` file for it (`Ch07/accountservice/docker-compose.yml`).
    The transaction service already has its own `docker-compose.yml` file that hosts
    Kafka. To test both services together, we must run the `docker-compose.yml` files
    from the `accountservice` and `transactionservice` directories.
  prefs: []
  type: TYPE_NORMAL
- en: Testing our microservices together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should run transaction and account microservices together to test producing
    and consuming processes. First, let’s start with the account microservice. As
    mentioned before, don’t forget to run the `docker-compose.yml` files for both
    services.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the newly updated account, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to `Ch07/accountservice/src` from the terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the account service from the command line using the `node` `index.js` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open Postman and, from the new tab, paste the service URL (it is `http://localhost:3001/v1/accounts`
    for us), and for the HTTP method, select `POST`. Select **Body** | **raw**, change
    **Text** to **JSON**, and paste the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is what it looks like in our database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything is okay for now based on what we have in the account microservice.
    Our main trigger, the producer, is the transaction microservice. Let’s produce
    a message and see whether the account microservice can consume this message or
    not. We still need the account service to run in parallel with transaction microservice,
    and that is why we need to open a new terminal to run the transaction microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new terminal and navigate to `Ch07/transactionservice`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `npm run start:dev` to start the transaction microservice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `http://localhost:3000/api/` and select `POST /transaction/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Paste the following JSON to create a new transaction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should use the account ID created by the account microservice. You will
    get the following response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"id": 37,'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"status": "FRAUD",'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"accountId": "6658ae5284432e40604018d5",'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"description": "Optional transaction description",'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ……..
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ….
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"type": "root",'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"status": "blocked",'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"count": 3,'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ………
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With an understanding of the asynchronous communication technique, you can easily
    apply it to your projects with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we started our journey by learning the importance of defining
    proper communication between microservices. We mostly use two main communication
    forms: async and sync. Choosing one over another is always a context-dependent
    choice – context is king. Then, we talked about the advantages of asynchronous
    communication. There are multiple ways of implementing asynchronous communication
    and we talked about most of the popular choices. Everything has a price and integrated
    microservices architecture is not an exception. It brings a lot of additional
    complexity we need to take into account and one of them is asynchronous communication.'
  prefs: []
  type: TYPE_NORMAL
- en: We talked about Apache Kafka, which helps us to overcome the problem we have.
    We learned about essential concepts such as clusters, brokers, topics, messages,
    and partitions. Our practical examples covered two main microservices. The transaction
    service was our producer, which produces a message, and the account microservice
    was a consumer that consumers that message. Of course, there are a lot of subtopics,
    such as refactoring, exception handling, testing, and deploying, that we haven’t
    covered yet, and the following chapters will cover these in detail.
  prefs: []
  type: TYPE_NORMAL
