<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying Microservices</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we'll cover the following recipes:</p>
<ul>
<li>Configuring your service to run in a container</li>
<li>Running multi-container applications with Docker Compose</li>
<li>Deploying your service on Kubernetes</li>
<li>Test releases with canary deployments</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>The way we deliver software to users has changed dramatically over the years. In the not too distant past, it was common to deploy to production by running a shell script on a collection of servers that pulled an update from some kind of source control repository. The problems with this approach are clear—scaling this out was difficult, bootstrapping servers was error prone, and deployments could easily get stuck in an undesired state, resulting in unpredictable experiences for users.</p>
<p>The advent of configuration management systems, such as <strong>Chef</strong> or <strong>Puppet</strong>, improved this situation somewhat. Instead of having custom bash scripts or commands that ran on remote servers, remote servers could be tagged with a kind of role that instructed them on how to configure and install software. The declarative style of automating configuration was better suited for large-scale software deployments. Server automation tools such as <strong>Fabric</strong> or <strong>Capistrano</strong> were also adopted; they sought to automate the process of pushing code to production, and are still very popular today for applications that do not run in containers.</p>
<p>Containers have revolutionized the way we deliver software. Containers allow developers to package their code with all the dependencies, including libraries, the runtime, OS tools, and configurations. This allows code to be delivered without the need to configure the host server, which dramatically simplifies the process by removing the number of moving pieces.</p>
<p>The process of shipping services in containers has been referred to as <strong>immutable infrastructure</strong>, because once an image is built, it isn't typically changed; instead, new versions of a service result in a new image being built.</p>
<p>Another big change in how software is deployed is the popularization of the twelve-factor methodology (<a href="https://12factor.net/">https://12factor.net/</a>). <strong>Twelve-factor</strong> (or <strong>12f</strong>, as it is commonly written) is a set of guidelines originally written by engineers at Heroku. At their core, twelve-factor apps are designed to be loosely coupled with their environment, resulting in services that can be used along with various logging tools, configuration systems, package management systems, and source control systems. Arguably, the most universally adopted concepts employed by <span>twelve-factor</span> apps are that the configuration is accessed through environment variables and logs are output to standard out. As we saw in the previous chapters, this is how we've integrated with systems such as Vault. These chapters are worth a read, but we've already been following many concepts described in <span>twelve-factor</span> so far in this book.</p>
<p>In this chapter, we'll be discussing containers, orchestration, and scheduling, and various methods for safely shipping changes to users. This is a very active topic, and new techniques are being improvised and discussed, but the recipes in this chapter should serve as a good starting point, especially if you're accustomed to deploying monoliths on virtual machines or bare metal servers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring your service to run in a container</h1>
                </header>
            
            <article>
                
<p>As we know, services are made up of source code and configurations. A service written in Java, for instance, can be packaged as a <strong><span>Java Archive</span></strong> (<strong><span>JAR</span></strong>) file containing compiled class files in Java bytecode, as well as resources such as configuration and properties files. Once packaged, the JAR file can then be executed on any machine running a <strong><span>Java Virtual Machine</span></strong> (<strong><span>JVM</span></strong>).</p>
<p>In order for this to work, however, the machine that we want to run our service on must have a JVM installed. Oftentimes, it must be a specific version of the JVM. Additionally, the machine might need to have some other utilities installed, or it might need access to a shared filesystem. While these are not parts of the service themselves, they do make up what we refer to as the runtime environment of the service.</p>
<p>Linux containers are a technology that allow developers to package an application or service with its complete runtime environment. Containers separate out the runtime for a particular application from the runtime of the host machine that the container is running on.</p>
<p>This makes applications more portable, making it easier to move a service from one environment to another. An engineer can run a service on their laptop, then move it into a preproduction environment, and then into production, without changing the container itself. Containers also allow you to easily run multiple services on the same machine, therefore allowing much more flexibility in how application architectures are deployed and providing opportunities for operational cost optimization.</p>
<p>Docker is a container runtime and set of tools that allows you to create self-contained execution environments for your service. There are other popular container runtimes they are widely used today, but Docker is designed to make containers portable and flexible, making it an ideal choice for building containers for services.</p>
<p>In this recipe, we'll use Docker to create an image that packages our message-service. We'll do this by creating a <kbd>Dockerfile</kbd> file and using the Docker command-line utility to create an image and then run that image as a container.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>The steps for this recipe are as follows:</p>
<ol>
<li class="mce-root"><span>First, open our message-service project from the previous chapters. Create a new file in the root of the project called</span> <kbd>Dockerfile</kbd><span>:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>FROM openjdk:8-jdk-alpine</strong><br/><strong>VOLUME /tmp</strong><br/><strong>EXPOSE 8082</strong><br/><strong>ARG JAR_FILE=build/libs/message-service-1.0-SNAPSHOT.jar</strong><br/><strong>ADD ${JAR_FILE} app.jar</strong><br/><strong>ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]</strong></pre>
<ol start="2">
<li>The <kbd>Dockerfile</kbd> file defines the base image that we'll use to build our message-service image. In this case, we're basing our image off of an Alpine Linux image with OpenJDK 8. Next, we expose the port that our service binds to and define how to run our service after it's packaged as a JAR file. We're now ready to use the <kbd>Dockerfile</kbd> file to build an image. This is done with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker build . -t message-service:0.1.1</strong></pre>
<ol start="3">
<li>You can verify that the preceding command worked by running <kbd>docker images</kbd> and seeing ours listed. Now we're ready to run the message service by executing our service in a container. This is done with the <kbd>docker run</kbd> command. We'll also give it a port mapping and specify the image that we want to use to run our service:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker run -p 0.0.0.0:8082:8082 message-service:0.1.1</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running multi-container applications with Docker Compose</h1>
                </header>
            
            <article>
                
<p>Services rarely run in isolation. A microservice usually connects to a data store of some kind, and could have other runtime dependencies. In order to work on a microservice, it's necessary to run it locally on a developer's machine. Requiring engineers to manually install and manage all the runtime dependencies of a service in order to work on a microservice would be impractical and time consuming. Instead, we need a way to automatically manage runtime service dependencies.</p>
<p>Containers have made services more portable by packaging the runtime environment and configuration with the application code as a shippable artifact. In order to maximize the benefits of using containers for local development, it would be great to be able to declare all the dependencies and run them in separate containers. This is what Docker Compose is designed to do.</p>
<p>Docker Compose uses a declarative YAML configuration file to determine how an application should be executed in multiple containers. This makes it easy to quickly start up a service, a database, and any other runtime dependencies of the service in a way that makes local development especially easy.</p>
<p>In this recipe, we'll follow some of the steps from the previous recipe to create a <kbd>Dockerfile</kbd> file for the authentication-service project. We'll then create a Docker Compose file that specifies MySQL as a dependency of the authentication-service. We'll then look at how to configure our project and run it locally with one container running our application and another running a database server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>For this recipe, you need to perform the following steps:</p>
<ol>
<li>Open the authentication-service project and create a new file called <kbd>Dockerfile</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>FROM openjdk:8-jdk-alpine</strong><br/><strong>VOLUME /tmp</strong><br/><strong>EXPOSE 8082</strong><br/><strong>ARG JAR_FILE=build/libs/authentication-service-1.0-SNAPSHOT.jar</strong><br/><strong>ADD ${JAR_FILE} app.jar</strong><br/><strong>ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]</strong></pre>
<ol start="2">
<li>Docker Compose uses a file called <kbd>docker-compose.yml</kbd> to declare how containerized applications should be run:</li>
</ol>
<pre style="padding-left: 60px"><strong>version: '3'</strong><br/><strong>services:</strong><br/><strong>  authentication:</strong><br/><strong>    build: .</strong><br/><strong>    ports:</strong><br/><strong>     - "8081:8081"</strong><br/><strong>    links:</strong><br/><strong>      - docker-mysql</strong><br/><strong>    environment:</strong><br/><strong>      DATABASE_HOST: 'docker-mysql'</strong><br/><strong>      DATABASE_USER: 'root'</strong><br/><strong>      DATABASE_PASSWORD: 'root'</strong><br/><strong>      DATABASE_NAME: 'user_credentials'</strong><br/><strong>      DATABASE_PORT: 3306</strong><br/><strong>  docker-mysql:</strong><br/><strong>    ports:</strong><br/><strong>      - "3306:3306"</strong><br/><strong>    image: mysql</strong><br/><strong>    restart: always</strong><br/><strong>    environment:</strong><br/><strong>      MYSQL_ROOT_PASSWORD: 'root'</strong><br/><strong>      MYSQL_DATABASE: 'user_credentials'</strong><br/><strong>      MYSQL_ROOT_HOST: '%'</strong></pre>
<ol start="3">
<li>As we'll be connecting to the MySQL server running in the <kbd>docker-mysql</kbd> container, we'll need to modify our authentication-service configuration to use that host when connecting to MySQL:</li>
</ol>
<pre style="padding-left: 60px"><strong>server:</strong><br/><strong>  port: 8081</strong><br/><br/><strong>spring:</strong><br/><strong>  jpa.hibernate.ddl-auto: create</strong><br/><strong>  datasource.url: jdbc:mysql://docker-mysql:3306/user_credentials</strong><br/><strong>  datasource.username: root</strong><br/><strong>  datasource.password: root</strong><br/><br/><strong>hibernate.dialect: org.hibernate.dialect.MySQLInnoDBDialect</strong><br/><br/><strong>secretKey: supers3cr3t</strong></pre>
<ol start="4">
<li>You can now run the authentication-service and MySQL with the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker-compose up</strong><br/><strong>Starting authentication-service_docker-mysql_1 ...</strong></pre>
<ol start="5">
<li>That's it! The authentication-service should now be running locally in a container.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying your service on Kubernetes</h1>
                </header>
            
            <article>
                
<p>Containers make services portable by allowing you to package code, dependencies, and the runtime environment together in one artifact. Deploying containers is generally easier than deploying applications that do not run in containers. The host does not need to have any special configuration or state; it just needs to be able to execute the container runtime. The ability to deploy one or more containers on a single host gave rise to another challenge when managing production environments—scheduling and orchestrating containers to run on specific hosts and manage scaling.</p>
<p>Kubernetes is an open source container orchestration tool. It is responsible for scheduling, managing, and scaling your containerized applications. With Kubernetes, you do not need to worry about deploying your container to one or more specific hosts. Instead, you declare what resources your container needs and let Kubernetes decide how to do the work (what host the container runs on, what services it runs alongside, and so on). Kubernetes grew out of the <strong>Borg paper</strong> (<a href="https://research.google.com/pubs/pub43438.html">https://research.google.com/pubs/pub43438.html</a>), published by engineers at Google, which described how they managed services in Google's data centers using the Borg cluster manager.</p>
<p>Kubernetes was started by Google as an open source project in 2014 and has enjoyed widespread adoption by organizations deploying code in containers.</p>
<p>Installing and managing a Kubernetes cluster is beyond the scope of this book. Luckily, a project called <strong>Minikube</strong> allows you to easily run a single-node Kubernetes cluster on your development machine. Even though the cluster only has one node, the way you interface with Kubernetes when deploying your service is generally the same, so the steps here can be followed for any Kubernetes cluster.</p>
<p>In this recipe, we'll install Minikube, start a single-node Kubernetes cluster, and deploy the <kbd>message-service</kbd> command we've worked with in previous chapters. We'll use the Kubernetes CLI tool (<kbd>kubectl</kbd>) to interface with Minikube.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>For this recipe, you need to go through the following steps:</p>
<ol>
<li>In order to demonstrate how to deploy our service to a <kbd>kubernetes</kbd> cluster, we'll be using a tool called <kbd>minikube</kbd>. The <kbd>minikube</kbd> tool makes it easy to run a single-node <kbd>kubernetes</kbd> cluster on a VM that can be run on a laptop or development machine. Install <kbd>minikube</kbd>. On macOS X, you can use HomeBrew to do this:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ brew install minikube</strong></pre>
<ol start="2">
<li>We'll also be using the <kbd>kubernetes</kbd> CLI tools in this recipe, so install those. On macOS X, using HomeBrew, you can type as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ brew install kubernetes-cli</strong></pre>
<ol start="3">
<li>Now we're ready to start our single-node <kbd>kubernetes</kbd> cluster. You can do this by running <kbd>minikube start</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ minikube start</strong><br/><strong>Starting local Kubernetes v1.10.0 cluster...</strong><br/><strong>Starting VM...</strong><br/><strong>Getting VM IP address...</strong><br/><strong>Moving files into cluster...</strong><br/><strong>Setting up certs...</strong><br/><strong>Connecting to cluster...</strong><br/><strong>Setting up kubeconfig...</strong><br/><strong>Starting cluster components...</strong><br/><strong>Kubectl is now configured to use the cluster.</strong><br/><strong>Loading cached images from config file</strong></pre>
<ol start="4">
<li>Next, set the <kbd>minikube</kbd> cluster up as the default configuration for the <kbd>kubectl</kbd> CLI tool:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ kubectl config use-context minikube</strong><br/><strong>Switched to context "minikube".</strong></pre>
<ol start="5">
<li>Verify that everything is configured properly by running the <kbd>cluster-info</kbd> command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ kubectl cluster-info</strong><br/><strong>Kubernetes master is running at https://192.168.99.100:8443</strong><br/><strong>KubeDNS is running at https://192.168.99.100:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</strong></pre>
<p style="padding-left: 60px">To further debug and diagnose cluster problems, use <kbd>kubectl cluster-info dump</kbd>.</p>
<ol start="6">
<li>You should now be able to launch the <kbd>kubernetes</kbd> dashboard in a browser:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ minikube dashboard</strong><br/><strong>Waiting, endpoint for service is not ready yet...</strong><br/><strong>Opening kubernetes dashboard in default browser...</strong></pre>
<ol start="7">
<li>The <kbd>minikube</kbd> tool uses a number of environment variables to configure the CLI client. Evaluate the environment variables with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ eval $(minikube docker-env)</strong></pre>
<ol start="8">
<li>Next, we'll build the docker image for our service using the <kbd>Dockerfile</kbd> file created in the previous recipe:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker build -t message-service:0.1.1</strong></pre>
<ol start="9">
<li>Finally, run the <kbd>message-service</kbd> command on the <kbd>kubernetes</kbd> cluster, telling <kbd>kubectl</kbd> the correct image to use and the port to expose:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ kubectl run message-service --image=message-service:0.1.1 --port=8082 --image-pull-policy=Never</strong></pre>
<ol start="10">
<li>We can verify that the <kbd>message-service</kbd> command is running in the <kbd>kubernetes</kbd> cluster by listing the pods on the cluster:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ kubectl get pods</strong><br/><strong>NAME READY STATUS RESTARTS AGE</strong><br/><strong>message-service-87d85dd58-svzmj 1/1 Running 0 3s</strong></pre>
<ol start="11">
<li>In order to access the <kbd>message-service</kbd> command, we'll need to expose it as a new service:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ kubectl expose deployment message-service --type=LoadBalancer</strong><br/><strong>service/message-service exposed</strong></pre>
<ol start="12">
<li>We can verify the previous command by listing services on the <kbd>kubernetes</kbd> services:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ kubectl get services</strong><br/><br/><strong>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong><br/><strong>kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 59d</strong><br/><strong>message-service LoadBalancer 10.105.73.177 &lt;pending&gt; 8082:30382/TCP 4s</strong></pre>
<ol start="13">
<li>The <kbd>minikube</kbd> tool has a convenient command for accessing a service running on the <kbd>kubernetes</kbd> cluster. Running the following command will list the URL that the <kbd>message-service</kbd> command is running on:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ minikube service list message-service</strong><br/><strong>|-------------|----------------------|-----------------------------|</strong><br/><strong>| NAMESPACE | NAME | URL |</strong><br/><strong>|-------------|----------------------|-----------------------------|</strong><br/><strong>| default | kubernetes | No node port |</strong><br/><strong>| default | message-service | http://192.168.99.100:30382 |</strong><br/><strong>| kube-system | kube-dns | No node port |</strong><br/><strong>| kube-system | kubernetes-dashboard | http://192.168.99.100:30000 |</strong><br/><strong>|-------------|----------------------|-----------------------------|</strong></pre>
<ol start="14">
<li>Use <kbd>curl</kbd> to try and make a request against the service to verify that it's working. Congratulations! You've deployed the <kbd>message-service</kbd> command on <kbd>kubernetes</kbd>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test releases with canary deployments</h1>
                </header>
            
            <article>
                
<p>Improvements in best practices for deploying have greatly improved the stability of deploys over the years. Automating the repeatable steps, standardizing the way our application interacts with the runtime environment, and packaging our application code with the runtime environment have all made deployments safer and easier than they used to be.</p>
<p>Introducing new code to a production environment is not without risk, however. All the techniques discussed in this chapter help prevent predictable mistakes, but they do nothing to prevent actual software bugs from negatively impacting users of the software we write. Canary deployment is a technique for reducing this risk and increasing confidence in new code that is being deployed to production.</p>
<p>With a canary deployment, you begin by shipping your code to a small percentage of production traffic. You can then monitor metrics, logs, traces, or whatever other tools allow you to observe how your software is working. Once you are confident that things are going as they should, you can gradually increase the percentage of traffic that receives your updated version until all production traffic is being served by the newest release of your service.</p>
<p>The term <strong>canary deployment</strong> comes from a technique that coal miners used to use to protect themselves from carbon monoxide or methane poisoning. By having a canary in the mine, the toxic gases would kill the canary before the miners, giving the miners an early warning sign that they should get out. Similarly, canary deployments allow us to expose a subset of users to risk without impacting the rest of the production environment. Thankfully, no animals have to be harmed when deploying code to production environments.</p>
<p>Canary deployments used to be very difficult to get right. Teams shipping software in this way usually had to come up with some kind of feature-toggling solution that would gate requests to certain versions of the application being deployed. Thankfully, containers have made this much easier, and Kubernetes has made it even more so.</p>
<p>In this recipe, we'll deploy an update to our <kbd>message-service</kbd> application using a canary deployment. As Kubernetes is able to pull images from a Docker container registry, we'll run a registry locally. Normally, you'd use a self-hosted registry or a service such as <em>Docker Hub</em> or <em>Google Container Registry</em>. First, we'll ensure that we have a stable version of the <kbd>message-service</kbd> command running in <kbd>minikube</kbd>, then we'll introduce an update and gradually roll it out to 100% traffic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Go through the following steps to set up a canary deployment:</p>
<ol>
<li>Open the <kbd>message-service</kbd> project we've worked on in the previous recipes. Add the following <kbd>Dockerfile</kbd> file to the root directory of the project:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>FROM </span>openjdk:8-jdk-alpine</strong><br/><strong><span>VOLUME /</span>tmp</strong><br/><strong><span>EXPOSE </span><span>8082<br/></span><span>ARG </span>JAR_FILE=build<span>/</span>libs<span>/</span>message-service-1<span>.0</span><span>-</span>SNAPSHOT.jar</strong><br/><strong><span>ADD </span>${JAR_FILE} app.jar</strong><br/><strong><span>ENTRYPOINT </span>[<span>"java"</span><span>,</span><span>"-Djava.security.egd=file:/dev/./urandom"</span><span>,</span><span>"-jar"</span><span>,</span><span>"/app.jar"</span>]</strong></pre>
<ol start="2">
<li>In order for Kubernetes to know whether the service is running, we need to add a liveness probe endpoint. Open the <kbd>MessageController.java</kbd> file and add a method to respond to GET requests at the <kbd>/ping</kbd> path:</li>
</ol>
<pre style="padding-left: 60px">package com.packtpub.microservices.ch09.message.controllers;<br/><br/>import com.packtpub.microservices.ch09.message.MessageRepository;<br/>import com.packtpub.microservices.ch09.message.clients.SocialGraphClient;<br/>import com.packtpub.microservices.ch09.message.exceptions.MessageNotFoundException;<br/>import com.packtpub.microservices.ch09.message.exceptions.MessageSendForbiddenException;<br/>import com.packtpub.microservices.ch09.message.models.Message;<br/>import com.packtpub.microservices.ch09.message.models.UserFriendships;<br/>import org.springframework.beans.factory.annotation.Autowired;<br/>import org.springframework.http.ResponseEntity;<br/>import org.springframework.scheduling.annotation.Async;<br/>import org.springframework.web.bind.annotation.*;<br/>import org.springframework.web.client.RestTemplate;<br/>import org.springframework.web.servlet.support.ServletUriComponentsBuilder;<br/><br/>import java.net.URI;<br/>import java.util.List;<br/>import java.util.concurrent.CompletableFuture;<br/><br/>@RestController<br/>public class MessageController {<br/><br/>    @Autowired<br/>    private MessageRepository messagesStore;<br/><br/>    @Autowired<br/>    private SocialGraphClient socialGraphClient;<br/><br/>    @RequestMapping(path = "/{id}", method = RequestMethod.GET, produces = "application/json")<br/>    public Message get(@PathVariable("id") String id) throws MessageNotFoundException {<br/>        return messagesStore.get(id);<br/>    }<br/><br/>    @RequestMapping(path = "/ping", method = RequestMethod.GET)<br/>    public String readinessProbe() {<br/>        return "ok";<br/>    }<br/><br/>    @RequestMapping(path = "/", method = RequestMethod.POST, produces = "application/json")<br/>    public ResponseEntity&lt;Message&gt; send(@RequestBody Message message) throws MessageSendForbiddenException {<br/>        List&lt;String&gt; friendships = socialGraphClient.getFriendships(message.getSender());<br/><br/>        if (!friendships.contains(message.getRecipient())) {<br/>            throw new MessageSendForbiddenException("Must be friends to send message");<br/>        }<br/><br/>        Message saved = messagesStore.save(message);<br/>        URI location = ServletUriComponentsBuilder<br/>                .fromCurrentRequest().path("/{id}")<br/>                .buildAndExpand(saved.getId()).toUri();<br/>        return ResponseEntity.created(location).build();<br/>    }<br/><br/>    @RequestMapping(path = "/user/{userId}", method = RequestMethod.GET, produces = "application/json")<br/>    public ResponseEntity&lt;List&lt;Message&gt;&gt; getByUser(@PathVariable("userId") String userId) throws MessageNotFoundException  {<br/>        List&lt;Message&gt; inbox = messagesStore.getByUser(userId);<br/>        if (inbox.isEmpty()) {<br/>            throw new MessageNotFoundException("No messages found for user: " + userId);<br/>        }<br/>        return ResponseEntity.ok(inbox);<br/>    }<br/><br/>    @Async<br/>    public CompletableFuture&lt;Boolean&gt; isFollowing(String fromUser, String toUser) {<br/>        String url = String.format(<br/>                "http://localhost:4567/followings?user=%s&amp;filter=%s",<br/>                fromUser, toUser);<br/><br/>        RestTemplate template = new RestTemplate();<br/>        UserFriendships followings = template.getForObject(url, UserFriendships.class);<br/><br/>        return CompletableFuture.completedFuture(<br/>                followings.getFriendships().isEmpty()<br/>        );<br/>    }<br/>}</pre>
<ol start="3">
<li>Let's start our container registry on port 5000:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker run -d -p 5000:5000 --restart=always --name registry registry:2</strong></pre>
<ol start="4">
<li>As we're using a local repository that is not configured with a valid SSL cert, start <kbd>minikube</kbd> with the ability to pull from insecure repositories:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ minikube start --insecure-registry 127.0.0.1</strong></pre>
<ol start="5">
<li>Build the <kbd>message-service</kbd> docker image, and then push the image to the local container registry with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker build . -t message-service:0.1.1</strong><br/><strong>...</strong><br/><strong>$ docker tag message-service:0.1.1 localhost:5000/message-service</strong><br/><strong>...</strong><br/><strong>$ docker push localhost:5000/message-service</strong></pre>
<ol start="6">
<li>A <strong>Kubernetes Deployment</strong> object describes the desired state for a pod and <kbd>ReplicaSet</kbd>. In our deployment, we'll specify that we want three replicas of our <kbd>message-service</kbd> pod running at all times, and we'll specify the liveness probe that we created a few steps earlier. To create a deployment for our <kbd>message-service</kbd>, create a file called <kbd>deployment.yaml</kbd> with the following contents:</li>
</ol>
<pre style="padding-left: 60px">apiVersion: extensions/v1beta1<br/>kind: Deployment<br/>metadata:<br/>  name: message-service<br/>spec:<br/>  replicas: 3<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: "message-service"<br/>        track: "stable"<br/>    spec:<br/>      containers:<br/>        - name: "message-service"<br/>          image: "localhost:5000/message-service"<br/>          imagePullPolicy: IfNotPresent<br/>          ports:<br/>            - containerPort: 8082<br/>          livenessProbe:<br/>            httpGet:<br/>              path: /ping<br/>              port: 8082<br/>              scheme: HTTP<br/>            initialDelaySeconds: 10<br/>            periodSeconds: 30<br/>            timeoutSeconds: 1</pre>
<ol start="7">
<li>Next, using <kbd>kubectl</kbd>, we'll create our deployment object:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ kubectl create -f deployment.yaml</strong></pre>
<ol start="8">
<li>You can now verify that our deployment is live and that Kubernetes is creating the pod replicas by running <kbd>kubectl get pods</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ kubectl get pods</strong></pre>
<ol start="9">
<li>Now that our application is running in Kubernetes, the next step is to create an update and roll it out to a subset of pods. First, we need to create a new docker image; in this case, we'll call it version 0.1.2 and push it to the local repository:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker build . -t message-service:0.1.2</strong><br/><strong>...</strong><br/><strong>$ docker tag message-service:0.1.2 localhost:5000/message-service</strong><br/><strong>$ docker push localhost:5000/message-service</strong></pre>
<ol start="10">
<li>We can now configure a deployment to run the newest version of our image before rolling it out to the rest of the pods.</li>
</ol>


            </article>

            
        </section>
    </body></html>