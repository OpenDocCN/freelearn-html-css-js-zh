<html><head></head><body>
  <div><h1 class="chapterNumber">12</h1>
    <h1 id="_idParaDest-325" class="chapterTitle">Scalability and Architectural Patterns</h1>
    <p class="normal">In its early days, Node.js was just a non-blocking web server written in C++ and JavaScript and was called web.js. Its creator, Ryan Dahl, soon realized the potential of the platform and started extending it with tools to enable the creation of different types of server-side applications on top of JavaScript and the non-blocking paradigm.</p>
    <p class="normal">The characteristics of Node.js are perfect for the implementation of distributed systems, ranging from a few nodes to thousands of nodes communicating through the network: Node.js was born to be distributed.</p>
    <p class="normal">Unlike in other web platforms, scalability is a topic that gets explored rather quickly in Node.js while developing an application. This is often because of the single-threaded nature of Node.js, which is incapable of exploiting all the resources of a multi-core machine. But this is just one side of the coin. In reality, there are more profound reasons for talking about scalability with Node.js.</p>
    <p class="normal">As we will see in this chapter, scaling an application does not only mean increasing its capacity, enabling it to handle more requests faster: it's also a crucial path to achieving high availability and tolerance to errors.</p>
    <p class="normal">Sometimes, we even refer to scalability when we talk about ways to split the complexity of an application into more manageable pieces. Scalability is a concept with multiple faces, six to be precise, as many as there <a id="_idIndexMarker1093"/>are faces on a cube—the <strong class="keyword">scale cube</strong>.</p>
    <p class="normal">In this chapter, you will learn the following topics:</p>
    <ul>
      <li class="Bullet--PACKT-">Why you should care about scalability</li>
      <li class="Bullet--PACKT-">What the scale cube is and why it is useful to understand scalability</li>
      <li class="Bullet--PACKT-">How to scale by running multiple instances of the same application</li>
      <li class="Bullet--PACKT-">How to leverage a load balancer when scaling an application</li>
      <li class="Bullet--PACKT-">What a service registry is and how it can be used</li>
      <li class="Bullet--PACKT-">Running and scaling Node.js applications using container orchestration platforms like Kubernetes</li>
      <li class="Bullet--PACKT-">How to design a microservice architecture out of a monolithic application</li>
      <li class="Bullet-End--PACKT-">How to integrate a large number of services through the use of some simple architectural patterns</li>
    </ul>
    <h1 id="_idParaDest-326" class="title">An introduction to application scaling</h1>
    <p class="normal">Scalability can be described as the capability of a system to grow and adapt to ever-changing conditions. Scalability<a id="_idIndexMarker1094"/> is not limited to pure technical growth; it is also dependent on the growth of a business and the organization behind it.</p>
    <p class="normal">If you are building the next "unicorn startup" and you expect your product to rapidly reach millions of users worldwide, you will face serious scalability challenges. How is your application going to sustain ever-increasing demand? Is the system going to get slower over time or crash often? How can you store high volumes of data and keep I/O under control? As you hire more people, how can you organize the different teams effectively and make them able to work autonomously, without contention across the different parts of the codebase?</p>
    <p class="normal">Even if you are not working on a high-scale project, that doesn't mean that you will be free from scalability concerns. You will just face different types of scalability challenges. Being unprepared for these challenges might seriously hinder the success of the project and ultimately damage the company behind it. It's important to approach scalability in the context of the specific project and understand the expectations for current and future business needs.</p>
    <p class="normal">Since scalability is such a broad topic, in this chapter, we will focus our attention on discussing the role of Node.js in the context of scalability. We will discuss several useful patterns and architectures used to scale Node.js applications.</p>
    <p class="normal">With these in your toolbelt and a solid understanding of your business context, you will be able to design and implement Node.js applications that can adapt and satisfy your business needs and keep your customers happy.</p>
    <h2 id="_idParaDest-327" class="title">Scaling Node.js applications</h2>
    <p class="normal">We already know that most<a id="_idIndexMarker1095"/> of the workload of a typical Node.js application runs in the context of a single thread. In <em class="chapterRef">Chapter 1</em>, <em class="italic">The Node.js Platform</em>, we learned that this is not necessarily a limitation but rather an advantage, because it allows the application to optimize the usage of the resources necessary to handle concurrent requests, thanks to the non-blocking I/O paradigm. This model works wonderfully for applications handling a moderate number of requests per second (usually a few hundred per second), especially if the application is mostly performing I/O-bound tasks (for example, reading and writing from the filesystem and the network) rather than CPU-bound ones (for example, number crunching and data processing).</p>
    <p class="normal">In any case, assuming we are using commodity hardware, the capacity that a single thread can support is limited. This is regardless of how powerful a server can be, so if we want to use Node.js for high-load applications, the only way is to scale it across multiple processes and machines.</p>
    <p class="normal">However, workload<a id="_idIndexMarker1096"/> is not the only reason to scale a Node.js application. In fact, with the same techniques that allow us to scale workloads, we can obtain other desirable properties such as high <strong class="keyword">availability</strong> and <strong class="keyword">tolerance to failures</strong>. Scalability is also a concept applicable to the size and complexity of an application. In fact, building architectures that can grow as much as needed over time is another important factor when designing software.</p>
    <p class="normal">JavaScript is a tool to be used with caution. The lack of type checking and its many gotchas can be an obstacle to the growth of an application, but with discipline and accurate design, we can turn some of its downsides into precious advantages. With JavaScript, we are often pushed to keep the application simple and to split its components it into small, manageable pieces. This mindset can make it easier to build applications that are distributed and scalable, but also easy to evolve over time.</p>
    <h2 id="_idParaDest-328" class="title">The three dimensions of scalability</h2>
    <p class="normal">When talking about scalability, the first fundamental principle to understand is <strong class="keyword">load distribution</strong>, which is<a id="_idIndexMarker1097"/> the science of splitting the load of an application <a id="_idIndexMarker1098"/>across several processes and machines. There are many ways to achieve this, and the book <em class="italic">The Art of Scalability</em> by Martin L. Abbott and Michael T. Fisher proposes an ingenious model to represent them, called the <strong class="keyword">scale cube</strong>. This model <a id="_idIndexMarker1099"/>describes scalability in terms of the following three dimensions:</p>
    <ul>
      <li class="Bullet--PACKT-"><em class="italic">X</em>-axis — Cloning</li>
      <li class="Bullet--PACKT-"><em class="italic">Y</em>-axis — Decomposing by service/functionality</li>
      <li class="Bullet-End--PACKT-"><em class="italic">Z</em>-axis — Splitting by data partition</li>
    </ul>
    <p class="normal">These three dimensions can be represented as a cube, as shown in <em class="italic">Figure 12.1</em>:</p>
    <figure class="mediaobject"><img src="img/B15729_12_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.1: The scale cube</p>
    <p class="normal">The bottom-left corner of the cube (that is, the intersection between the <em class="italic">X</em>-axis and the <em class="italic">Y</em>-axis) represents the application having all the functionality in a single code base and running on a single<a id="_idIndexMarker1100"/> instance. This is what we generally call a <strong class="keyword">monolithic application</strong>. This is a common situation for applications handling small<a id="_idIndexMarker1101"/> workloads or at the early stages of their development. Given a monolithic application, there are three different strategies for scaling it. By looking at the scale cube, these strategies are represented as growth along the different axes of the cube: <em class="italic">X</em>, <em class="italic">Y</em>, and <em class="italic">Z</em>:</p>
    <ul>
      <li class="Bullet--PACKT-"><strong class="bold-italic">X</strong><strong class="keyword">-axis — Cloning</strong>: The most intuitive evolution of a monolithic, unscaled application is moving<a id="_idIndexMarker1102"/> right along the <em class="italic">X</em>-axis, which is simple, most of the time inexpensive (in terms of development cost), and highly effective. The principle behind this technique is elementary, that is, cloning the same application <em class="italic">n</em> times and letting each instance handle 1/<em class="italic">n</em>th of the workload.</li>
      <li class="Bullet--PACKT-"><strong class="bold-italic">Y</strong><strong class="keyword">-axis — Decomposing by service/functionality</strong>: Scaling along the <em class="italic">Y</em>-axis means decomposing the application<a id="_idIndexMarker1103"/> based on its functionalities, services, or use cases. In this instance, <em class="italic">decomposing</em> means creating different, standalone applications, each with its own codebase, possibly with its own dedicated database, and even with a separate UI.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">For example, a common situation is separating the part of an application responsible for the administration from the public-facing product. Another example is extracting the services responsible for user authentication, thereby creating a dedicated authentication server.</p>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The criteria to split an application by its functionalities depend mostly on its business requirements, the use cases, the data, and many other factors, as we will see later in this<a id="_idIndexMarker1104"/> chapter. Interestingly, this is the scaling dimension with the biggest repercussions, not only on the architecture of an application but also on the way it is managed from a development and an operational perspective. As we will see, <strong class="keyword">microservice</strong> is a term that is most commonly associated with a fine-grained <em class="italic">Y</em>-axis scaling.</p>
      </li>
      <li class="Bullet--PACKT-"><strong class="bold-italic">Z</strong><strong class="keyword">-axis — Splitting by data partition</strong>: The last scaling dimension is the <em class="italic">Z</em>-axis, where the application is split in such a way that each instance is responsible for only a portion of the<a id="_idIndexMarker1105"/> whole data. This is a technique often used in databases, also known as <strong class="keyword">horizontal/vertical partitioning</strong>. In this<a id="_idIndexMarker1106"/> setup, there are multiple instances of the same application, each of them operating on a partition of the data, which is determined using different criteria.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">For example, we could partition the users of an application based on their country (<em class="italic">list partitioning</em>), or based on the starting letter of their surname (<em class="italic">range partitioning</em>), or by letting a hash function decide the partition each user belongs to (<em class="italic">hash partitioning</em>).</p>
        <p class="Bullet-Without-Bullet-Within-Bullet-End--PACKT-">Each partition can then be assigned to a particular instance of the application. The use of data partitions requires each operation to be preceded by a lookup step to determine which instance of the application is responsible for a given datum. As we said, data partitioning is usually applied and handled at the data storage level because its main purpose is overcoming the problems related to handling large monolithic datasets (limited disk space, memory, and network capacity). Applying it at the application level is worth considering only for complex, distributed architectures or for very particular use cases such as, for example, when building applications relying on custom solutions for data persistence, when using databases that don't support partitioning, or when building applications at Google scale. Considering its complexity, scaling an application along the <em class="italic">Z</em>-axis should be taken into consideration only after the <em class="italic">X</em> and <em class="italic">Y</em> axes of the scale cube have been fully exploited.</p>
      </li>
    </ul>
    <p class="normal">In the following sections, we will focus on the two most common and effective techniques used to<a id="_idIndexMarker1107"/> scale Node.js applications, namely, <strong class="keyword">cloning</strong> and <strong class="keyword">decomposing</strong> by functionality/service.</p>
    <h1 id="_idParaDest-329" class="title">Cloning and load balancing</h1>
    <p class="normal">Traditional, multithreaded web servers are usually only scaled horizontally when the resources<a id="_idIndexMarker1108"/> assigned<a id="_idIndexMarker1109"/> to a machine cannot be upgraded any more, or when doing so would involve a higher cost than simply launching another machine. </p>
    <p class="normal">By using multiple threads, traditional web servers can take advantage of all the processing power of a server, using all the available processors and memory. Conversely, Node.js applications, being single-threaded, are usually scaled much sooner compared to traditional web servers. Even in the context of a single machine, we need to find ways to "scale" an application in order to take advantage of all the available resources.</p>
    <div><p class="Information-Box--PACKT-">In Node.js, <strong class="keyword">vertical scaling</strong> (adding more <a id="_idIndexMarker1110"/>resources to a single machine) and <strong class="keyword">horizontal scaling</strong> (adding more machines to the infrastructure) are<a id="_idIndexMarker1111"/> almost equivalent concepts: both, in fact, involve similar techniques to leverage all the available processing power.</p>
    </div>
    <p class="normal">Don't be fooled into thinking about this as a disadvantage. On the contrary, being almost forced to scale has beneficial effects on other attributes of an application, in particular, availability and fault-tolerance. In fact, scaling a Node.js application by cloning is relatively simple and it's often implemented even if there is no need to harvest more resources, just for the purpose of having a redundant, fail-tolerant setup.</p>
    <p class="normal">This also pushes the developer to take into account scalability from the early stages of an application, making sure the application does not rely on any resource that cannot be shared across multiple processes or machines. In fact, an absolute prerequisite to scaling an application is that each instance does not have to store common information on resources that cannot be shared, such as memory or disk. For example, in a web server, storing the session data in memory or on disk is a practice that does not work well with scaling. Instead, using a shared database will ensure that each instance will have access to the same session information, wherever it is deployed.</p>
    <p class="normal">Let's now introduce the most basic mechanism for scaling Node.js applications: the <code class="Code-In-Text--PACKT-">cluster</code> module.</p>
    <h2 id="_idParaDest-330" class="title">The cluster module</h2>
    <p class="normal">In Node.js, the simplest pattern to distribute the load of an application across different instances running on a<a id="_idIndexMarker1112"/> single machine is by using the <code class="Code-In-Text--PACKT-">cluster</code> module, which is part of the core libraries. The <code class="Code-In-Text--PACKT-">cluster</code> module simplifies the forking of new instances of the same application and automatically distributes incoming connections across them, as shown in <em class="italic">Figure 12.2</em>:</p>
    <figure class="mediaobject"><img src="img/B15729_12_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.2: Cluster module schematic</p>
    <p class="normal">The <strong class="keyword">master process</strong> is responsible for spawning a number of processes (<strong class="keyword">workers</strong>), each representing an instance of the application we want to scale. Each incoming connection is then distributed across the cloned workers, spreading the load across them.</p>
    <p class="normal">Since every worker is an independent process, you can use this approach to spawn as many workers as the number of CPUs available in the system. With this approach, you can easily allow a Node.js application to take advantage of all the computing power available in the system.</p>
    <h3 id="_idParaDest-331" class="title">Notes on the behavior of the cluster module</h3>
    <p class="normal">In most systems, the <code class="Code-In-Text--PACKT-">cluster</code> module<a id="_idIndexMarker1113"/> uses an explicit round-robin load balancing algorithm. This algorithm is used inside the master process, which makes sure the requests are evenly distributed across all the workers. Round-robin scheduling is enabled by default on all platforms except Windows, and it can be globally modified by setting the variable <code class="Code-In-Text--PACKT-">cluster.schedulingPolicy</code> and using the constants <code class="Code-In-Text--PACKT-">cluster.SCHED_RR</code> (round robin) or <code class="Code-In-Text--PACKT-">cluster.SCHED_NONE</code> (handled by the operating system).</p>
    <div><p class="Information-Box--PACKT-">The round-robin algorithm distributes the load evenly across the available servers on a rotational basis. The first request is forwarded to the first server, the second to the next server in the list, and so on. When the end of the list is reached, the iteration starts again from the beginning. In the <code class="Code-In-Text--PACKT-">cluster</code> module, the round-robin logic is a little bit <em class="italic">smarter</em> than the traditional implementation. In fact, it is enriched with some extra behaviors that aim to avoid overloading a given worker process.</p>
    </div>
    <p class="normal">When we use the <code class="Code-In-Text--PACKT-">cluster</code> module, every invocation to <code class="Code-In-Text--PACKT-">server.listen()</code> in a worker process is delegated to the master process. This allows the master process to receive all the incoming messages<a id="_idIndexMarker1114"/> and distribute them to the pool of workers. The <code class="Code-In-Text--PACKT-">cluster</code> module makes this delegation process very simple for most use cases, but there are several edge cases in which calling <code class="Code-In-Text--PACKT-">server.listen()</code> in a worker module might not do what you expect:</p>
    <ul>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">server.listen({fd})</code>: If a worker listens using a specific file descriptor, for instance, by invoking <code class="Code-In-Text--PACKT-">server.listen({fd: 17})</code>, this operation might produce unexpected results. File descriptors are mapped at the process level, so if a worker process maps<a id="_idIndexMarker1115"/> a file descriptor, this won't match the same file in the master process. One way to overcome this limitation is to create the file descriptor in the master process and then pass it to the worker process. This way, the worker process can invoke <code class="Code-In-Text--PACKT-">server.listen()</code> using a descriptor that is known to the master.</li>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">server.listen(handle)</code>: Listening using <code class="Code-In-Text--PACKT-">handle</code> objects (<code class="Code-In-Text--PACKT-">FileHandle</code>) explicitly in a worker process will cause the worker to use the supplied handle directly, rather than delegating the operation to the master process.</li>
      <li class="Bullet-End--PACKT-"><code class="Code-In-Text--PACKT-">server.listen(0)</code>: Calling <code class="Code-In-Text--PACKT-">server.listen(0)</code> will generally cause servers to listen on a random port. However, in a cluster, each worker will receive the same "random" port each time they call <code class="Code-In-Text--PACKT-">server.listen(0)</code>. In other words, the port is random only the first time; it will be fixed from the second call on. If you want every worker to listen on a different random port, you have to generate the port numbers by yourself.</li>
    </ul>
    <h3 id="_idParaDest-332" class="title">Building a simple HTTP server</h3>
    <p class="normal">Let's now start working on an example. Let's build a small HTTP server, cloned and load balanced <a id="_idIndexMarker1116"/>using<a id="_idIndexMarker1117"/> the <code class="Code-In-Text--PACKT-">cluster</code> module. First of all, we need an application to scale, and for this example, we don't need too much, just a very basic HTTP server.</p>
    <p class="normal">So, let's create a file called <code class="Code-In-Text--PACKT-">app.js</code> containing the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createServer } from 'http'
const { pid } = process
const server = createServer((req, res) =&gt; {
  // simulates CPU intensive work
  let i = 1e7; while (i &gt; 0) { i-- }
  console.log(`Handling request from ${pid}`)
  res.end(`Hello from ${pid}\n`)
})
server.listen(8080, () =&gt; console.log(`Started at ${pid}`))
</code></pre>
    <p class="normal">The HTTP server we just built responds to any request by sending back a message containing its <strong class="keyword">process identifier</strong> (<strong class="keyword">PID</strong>); this <a id="_idIndexMarker1118"/>is useful for identifying which instance of the application is handling the request. In this version of the application, we have only one process, so the PID that you see in the responses and the logs will always be the same.</p>
    <p class="normal">Also, to simulate some actual CPU work, we perform an empty loop 10 million times: without this, the server load would be almost insignificant and it will be quite hard to draw conclusions from the benchmarks we are going to run.</p>
    <div><p class="Information-Box--PACKT-">The <code class="Code-In-Text--PACKT-">app</code> module we create here is just a simple abstraction for a generic web server. We are not using a web framework like Express or Fastify for simplicity, but feel free to rewrite these examples using your web framework of choice.</p>
    </div>
    <p class="normal">You can now check if all works as expected by running the application as usual and sending a request to <code class="Code-In-Text--PACKT-">http://localhost:8080</code> using either a browser or <code class="Code-In-Text--PACKT-">curl</code>.</p>
    <p class="normal">You can also try to measure the requests <a id="_idIndexMarker1119"/>per second that the server is able to handle on one process. For this purpose, you can use a network benchmarking tool such as <code class="Code-In-Text--PACKT-">autocannon</code> (<a href="http://nodejsdp.link/autocannon">nodejsdp.link/autocannon</a>):</p>
    <pre class="programlisting con"><code class="hljs-con">npx autocannon -c 200 -d 10 http://localhost:8080
</code></pre>
    <p class="normal">The preceding command will load the server with 200 concurrent connections for 10 seconds. As a reference, the result we got on our machine (a 2.5 GHz quad-core Intel Core i7 using Node.js v14) is in the order of 300 transactions per second.</p>
    <div><p class="Information-Box--PACKT-">Please remember that the load tests we will perform in this chapter are intentionally simple and minimal and are provided only for reference and learning purposes. Their results cannot provide a 100% accurate evaluation of the performance of the various techniques we are analyzing. When you are trying to optimize a real production application, make sure to always run your own benchmarks after every change. You might find out that, among the different techniques we are going to illustrate here, some can be more effective than others for your specific application.</p>
    </div>
    <p class="normal">Now that we have a<a id="_idIndexMarker1120"/> simple test web<a id="_idIndexMarker1121"/> application and some reference benchmarks, we are ready to try some techniques to improve the performance of the application.</p>
    <h3 id="_idParaDest-333" class="title">Scaling with the cluster module</h3>
    <p class="normal">Let's now<a id="_idIndexMarker1122"/> update <code class="Code-In-Text--PACKT-">app.js</code> to scale our application<a id="_idIndexMarker1123"/> using the <code class="Code-In-Text--PACKT-">cluster</code> module:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createServer } from 'http'
import { cpus } from 'os'
import cluster from 'cluster'
if (cluster.isMaster) {                                    // (1)
  const availableCpus = cpus()
  console.log(`Clustering to ${availableCpus.length} processes`)
  availableCpus.forEach(() =&gt; cluster.fork())
} else {                                                   // (2)
  const { pid } = process
  const server = createServer((req, res) =&gt; {
    let i = 1e7; while (i &gt; 0) { i-- }
    console.log(`Handling request from ${pid}`)
    res.end(`Hello from ${pid}\n`)
  })
  server.listen(8080, () =&gt; console.log(`Started at ${pid}`))
}
</code></pre>
    <p class="normal">As we can see, using the <code class="Code-In-Text--PACKT-">cluster</code> module requires very little effort. Let's analyze what is happening:</p>
    <ol>
      <li class="numbered">When we launch <code class="Code-In-Text--PACKT-">app.js</code> from the command line, we are actually executing the master process. In this case, the <code class="Code-In-Text--PACKT-">cluster.isMaster</code> variable is set to <code class="Code-In-Text--PACKT-">true</code> and the only work we are required to do is forking the current process using <code class="Code-In-Text--PACKT-">cluster.fork()</code>. In the preceding example, we are starting as many workers as there are logical CPU cores in the system to take advantage of all the available processing power.</li>
      <li class="numbered">When <code class="Code-In-Text--PACKT-">cluster.fork()</code> is executed from the master process, the current module (<code class="Code-In-Text--PACKT-">app.js</code>) is run again, but this time in worker mode (<code class="Code-In-Text--PACKT-">cluster.isWorker</code> is set to <code class="Code-In-Text--PACKT-">true</code>, while <code class="Code-In-Text--PACKT-">cluster.isMaster</code> is <code class="Code-In-Text--PACKT-">false</code>). When the application runs as a worker, it can start doing some actual work. In this case, it starts a new HTTP server.</li>
    </ol>
    <div><p class="Information-Box--PACKT-">It's important to remember that each worker is a different Node.js process with its own event loop, memory space, and loaded modules.</p>
    </div>
    <p class="normal">It's interesting<a id="_idIndexMarker1124"/> to note that the usage of the <code class="Code-In-Text--PACKT-">cluster</code> module is<a id="_idIndexMarker1125"/> based on a recurring pattern, which makes it very easy to run multiple instances of an application:</p>
    <pre class="programlisting code"><code class="hljs-code">if (cluster.isMaster) {
  // fork()
} else {
  // do work
}
</code></pre>
    <div><p class="Information-Box--PACKT-">Under the hood, the <code class="Code-In-Text--PACKT-">cluster.fork()</code> function uses the <code class="Code-In-Text--PACKT-">child_process.fork()</code> API, therefore, we also have a communication channel available between the master and the workers. The worker processes can be accessed from the variable <code class="Code-In-Text--PACKT-">cluster.workers</code>, so broadcasting a message to all of them would be as easy as running the following line of code:</p>
      <pre class="programlisting code"><code class="hljs-code">Object.values(cluster.workers).forEach(worker =&gt; worker.send('Hello from the master'))
</code></pre>
    </div>
    <p class="normal">Now, let's try to run our HTTP server in cluster mode. If our machine has more than one core, we should see a number of workers being started by the master process, one after the other. For example, in a system with four logical cores, the terminal should look like this:</p>
    <pre class="programlisting con"><code class="hljs-con">Started 14107
Started 14099
Started 14102
Started 14101
</code></pre>
    <p class="normal">If we now try to hit our server again using the URL <code class="Code-In-Text--PACKT-">http://localhost:8080</code>, we should notice that each request will return a message with a different PID, which means that these requests have been handled by different workers, confirming that the load is being distributed among them.</p>
    <p class="normal">Now, we can try to load test our server again:</p>
    <pre class="programlisting con"><code class="hljs-con">npx autocannon -c 200 -d 10 http://localhost:8080
</code></pre>
    <p class="normal">This way, we should<a id="_idIndexMarker1126"/> be able to discover the performance<a id="_idIndexMarker1127"/> increase obtained by scaling our application across multiple processes. As a reference, in our machine, we saw a performance increase of about 3.3x (1,000 trans/sec versus 300 trans/sec).</p>
    <h3 id="_idParaDest-334" class="title">Resiliency and availability with the cluster module</h3>
    <p class="normal">Because workers are all separate processes, they can be killed or respawned depending on a program's<a id="_idIndexMarker1128"/> needs, without affecting other workers. As long as there are some workers still alive, the server will continue to accept connections. If no workers are alive, existing connections will be dropped, and new connections will be refused. Node.js does not automatically manage the number of workers; however, it is the application's responsibility to manage the worker pool based on its own needs.</p>
    <p class="normal">As we already mentioned, scaling an application also brings other advantages, in particular, the ability to maintain a certain level of service, even in the presence of malfunctions or crashes. This property is also<a id="_idIndexMarker1129"/> known as <strong class="keyword">resiliency</strong> and it contributes to the availability of a system.</p>
    <p class="normal">By starting multiple instances of the same application, we are creating a redundant system, which means that if one instance goes down for whatever reason, we still have other instances ready to serve requests. This pattern is pretty straightforward to implement using the <code class="Code-In-Text--PACKT-">cluster</code> module. Let's see how it works!</p>
    <p class="normal">Let's take the code from the previous section as a starting point. In particular, let's modify the <code class="Code-In-Text--PACKT-">app.js</code> module so that it crashes after a random interval of time:</p>
    <pre class="programlisting code"><code class="hljs-code">// ...
} else {
  // Inside our worker block
  setTimeout(
    () =&gt; { throw new Error('Ooops') },
    Math.ceil(Math.random() * 3) * 1000
  )
  // ...
</code></pre>
    <p class="normal">With this change in place, our server exits with an error after a random number of seconds between 1 and 3. In a real-life situation, this would eventually cause our application to stop serving requests, unless we use some external tool to monitor its status and restart it automatically. However, if we only have one instance, there may be a non-negligible delay between restarts caused by the startup time of the application. This means that during those restarts, the application is not available. Having multiple instances instead will make sure we always have a backup process to serve an incoming request, even when one of the workers fails.</p>
    <p class="normal">With the <code class="Code-In-Text--PACKT-">cluster</code> module, all we have to do is spawn a new worker as soon as we detect that one is <a id="_idIndexMarker1130"/>terminated with an error code. Let's modify <code class="Code-In-Text--PACKT-">app.js</code> to take this into account:</p>
    <pre class="programlisting code"><code class="hljs-code">// ...
if (cluster.isMaster) {
  // ...
  cluster.on('exit', (worker, code) =&gt; {
    if (code !== 0 &amp;&amp; !worker.exitedAfterDisconnect) {
      console.log(
        `Worker ${worker.process.pid} crashed. ` +
        'Starting a new worker'
      )
      cluster.fork()
    }
  })
} else {
  // ...
}
</code></pre>
    <p class="normal">In the preceding code, as soon as the master process receives an <code class="Code-In-Text--PACKT-">'exit'</code> event, we check whether the process is terminated intentionally or as the result of an error. We do this by checking the status code and the flag <code class="Code-In-Text--PACKT-">worker.exitedAfterDisconnect</code>, which indicates whether the worker was terminated explicitly by the master. If we confirm that the process was terminated because of an error, we start a new worker. It's interesting to note that while the crashed worker gets replaced, the other workers can still serve requests, thus not affecting the availability of the application.</p>
    <p class="normal">To test this assumption, we can try to stress our server again using <code class="Code-In-Text--PACKT-">autocannon</code>. When the stress test completes, we will notice that among the various metrics in the output, there is also an indication of the number of failures. In our case, it is something like this:</p>
    <pre class="programlisting con"><code class="hljs-con">[...]
8k requests in 10.07s, 964 kB read
674 errors (7 timeouts)
</code></pre>
    <p class="normal">This should amount to about 92% availability. Bear in mind that this result can vary a lot as it greatly depends on the number of running instances and how many times they crash during the test, but it should give us a good indicator of how our solution works. The preceding numbers tell us that despite the fact that our application is constantly crashing, we only had 674 failed requests over 8,000 hits.</p>
    <p class="normal">In the example<a id="_idIndexMarker1131"/> scenario that we just built, most of the failing requests will be caused by the interruption of already established connections during a crash. Unfortunately, there is very little we can do to prevent these types of failures, especially when the application terminates because of a crash. Nonetheless, our solution proves to be working and its availability is not bad at all for an application that crashes so often!</p>
    <h3 id="_idParaDest-335" class="title">Zero-downtime restart</h3>
    <p class="normal">A Node.js application might also need to be restarted when we want to release a new version<a id="_idIndexMarker1132"/> to our production<a id="_idIndexMarker1133"/> servers. So, also in this scenario, having multiple instances can help maintain the availability of our application.</p>
    <p class="normal">When we have to intentionally restart an application to update it, there is a small window in which the application restarts and is unable to serve requests. This can be acceptable if we are updating our personal blog, but it's not even an option for a professional <a id="_idIndexMarker1134"/>application with a <strong class="keyword">service-level agreement</strong> (<strong class="keyword">SLA</strong>) or one that is updated very often as part of a continuous delivery process. The solution is to<a id="_idIndexMarker1135"/> implement a <strong class="keyword">zero-downtime restart</strong>, where the code of an application is updated without affecting its availability.</p>
    <p class="normal">With the <code class="Code-In-Text--PACKT-">cluster</code> module, this is, again, a pretty easy task: the pattern involves restarting the workers one at a time. This way, the remaining workers can continue to operate and maintain the services of the application available.</p>
    <p class="normal">Let's add this new feature to our clustered server. All we have to do is add some new code to be executed by the master process:</p>
    <pre class="programlisting code"><code class="hljs-code">import { once } from 'events'
// ...
if (cluster.isMaster) {
  // ...
  process.on('SIGUSR2', async () =&gt; {                        // (1)
    const workers = Object.values(cluster.workers)
    for (const worker of workers) {                          // (2)
      console.log(`Stopping worker: ${worker.process.pid}`)
      worker.disconnect()                                    // (2)
      await once(worker, 'exit')
      if (!worker.exitedAfterDisconnect) continue
      const newWorker = cluster.fork()                       // (4)
      await once(newWorker, 'listening')                     // (5)
    }
  })
} else {
  // ...
}
</code></pre>
    <p class="normal">This is how the preceding code block works:</p>
    <ol>
      <li class="numbered">The restarting of the workers is triggered on receiving the <code class="Code-In-Text--PACKT-">SIGUSR2</code> signal. Note that we are<a id="_idIndexMarker1136"/> using an <code class="Code-In-Text--PACKT-">async</code> function to <a id="_idIndexMarker1137"/>implement the event handler as we will need to perform some asynchronous tasks here.</li>
      <li class="numbered">When a <code class="Code-In-Text--PACKT-">SIGUSR2</code> signal is received, we iterate over all the values of the <code class="Code-In-Text--PACKT-">cluster.workers</code> object. Every element is a <code class="Code-In-Text--PACKT-">worker</code> object that we can use to interact with a given worker currently active in the pool of workers.</li>
      <li class="numbered">The first thing we do for the current worker is invoke <code class="Code-In-Text--PACKT-">worker.disconnect()</code>, which stops the worker gracefully. This means that if the worker is currently handling a request, this won't be interrupted abruptly; instead, it will be completed. The worker exits only after the completion of all inflight requests.</li>
      <li class="numbered">When the terminated process exits, we can spawn a new worker.</li>
      <li class="numbered">We wait for the new worker to be ready and listening for new connections before we proceed with restarting the next worker.</li>
    </ol>
    <div><p class="Information-Box--PACKT-">Since our program makes use of Unix signals, it will not work properly on Windows systems (unless you are using the Windows Subsystem for Linux). Signals are the simplest mechanism to implement our solution. However, this isn't the only one. In fact, other approaches include listening for a command coming from a socket, a pipe, or the standard input.</p>
    </div>
    <p class="normal">Now, we can test our zero-downtime restart by running the application and then sending a <code class="Code-In-Text--PACKT-">SIGUSR2</code> signal. However, we first need to obtain the PID of the master process. The following command can be useful to identify it from the list of all the running processes:</p>
    <pre class="programlisting con"><code class="hljs-con">ps -af
</code></pre>
    <p class="normal">The master process should be the parent of a set of <code class="Code-In-Text--PACKT-">node</code> processes. Once we have the PID we are looking for, we can send the signal to it:</p>
    <pre class="programlisting con"><code class="hljs-con">kill -SIGUSR2 &lt;PID&gt;
</code></pre>
    <p class="normal">Now, the output<a id="_idIndexMarker1138"/> of the application should display<a id="_idIndexMarker1139"/> something like this:</p>
    <pre class="programlisting con"><code class="hljs-con">Restarting workers
Stopping worker: 19389
Started 19407
Stopping worker: 19390
Started 19409
</code></pre>
    <p class="normal">We can try to use <code class="Code-In-Text--PACKT-">autocannon</code> again to verify that we don't have any considerable impact on the availability of our application during the restart of the workers.</p>
    <div><p class="Information-Box--PACKT-"><code class="Code-In-Text--PACKT-">pm2</code> (<a href="http://nodejsdp.link/pm2">nodejsdp.link/pm2</a>) is a small utility, based on <code class="Code-In-Text--PACKT-">cluster</code>, which offers load balancing, process monitoring, zero-downtime restarts, and other goodies.</p>
    </div>
    <h2 id="_idParaDest-336" class="title">Dealing with stateful communications</h2>
    <p class="normal">The <code class="Code-In-Text--PACKT-">cluster</code> module does not work well with stateful communications where the application state is<a id="_idIndexMarker1140"/> not shared between the various instances. This is because different requests belonging to the same stateful session may potentially be handled by a different instance of the application. This is not a problem limited only to the <code class="Code-In-Text--PACKT-">cluster</code> module, but, in general, it applies to any kind of stateless, load balancing algorithm. Consider, for example, the situation described by <em class="italic">Figure 12.3</em>:</p>
    <figure class="mediaobject"><img src="img/B15729_12_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.3: An example issue with a stateful application behind a load balancer</p>
    <p class="normal">The user <strong class="keyword">John</strong> initially sends a request to our application to authenticate himself, but the result of the operation is registered locally (for example, in memory), so only the instance of the application that receives the authentication request (<strong class="keyword">Instance A</strong>) knows that John is successfully<a id="_idIndexMarker1141"/> authenticated. When John sends a new request, the load balancer might forward it to a different instance of the application, which actually doesn't possess the authentication details of John, hence refusing to perform the operation. The application we just described cannot be scaled as it is, but luckily, there are two easy solutions we can apply to solve this problem.</p>
    <h3 id="_idParaDest-337" class="title">Sharing the state across multiple instances</h3>
    <p class="normal">The first option we <a id="_idIndexMarker1142"/>have to scale an application using stateful communications is sharing the state across all the instances.</p>
    <p class="normal">This can be easily achieved <a id="_idIndexMarker1143"/>with a shared datastore, such as, for<a id="_idIndexMarker1144"/> example, a database like PostgreSQL (<a href="http://nodejsdp.link/postgresql">nodejsdp.link/postgresql</a>), MongoDB (<a href="http://nodejsdp.link/mongodb">nodejsdp.link/mongodb</a>), or<a id="_idIndexMarker1145"/> CouchDB (<a href="http://nodejsdp.link/couchdb">nodejsdp.link/couchdb</a>), or, even better, we can use an in-memory store <a id="_idIndexMarker1146"/>such as Redis (<a href="http://nodejsdp.link/redis">nodejsdp.link/redis</a>) or<a id="_idIndexMarker1147"/> Memcached (<a href="http://nodejsdp.link/memcached">nodejsdp.link/memcached</a>).</p>
    <p class="normal"><em class="italic">Figure 12.4</em> outlines this simple and effective solution:</p>
    <figure class="mediaobject"><img src="img/B15729_12_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.4: Application behind a load balancer using a shared data store</p>
    <p class="normal">The only drawback of using a shared store for the communication state is that applying this pattern might require a significant amount of refactoring of the code base. For example, we might be using an existing library that keeps the communication state in memory, so we <a id="_idIndexMarker1148"/>have to figure out how to configure, replace, or reimplement this library to use a shared store.</p>
    <p class="normal">In cases where refactoring might not be feasible, for instance, because of too many changes required or stringent time constraints in making the application more scalable, we can rely on a less invasive solution: <strong class="keyword">sticky load balancing</strong> (or <strong class="keyword">sticky sessions</strong>).</p>
    <h3 id="_idParaDest-338" class="title">Sticky load balancing</h3>
    <p class="normal">The other alternative we have to support stateful communications is having the load balancer<a id="_idIndexMarker1149"/> always routing all of the requests<a id="_idIndexMarker1150"/> associated with a session to the same instance of the application. This technique is also called <strong class="keyword">sticky load balancing</strong>.</p>
    <p class="normal"><em class="italic">Figure 12.5</em> illustrates a simplified scenario involving this technique:</p>
    <figure class="mediaobject"><img src="img/B15729_12_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.5: An example illustrating how sticky load balancing works</p>
    <p class="normal">As we can see from <em class="italic">Figure 12.5</em>, when the load balancer receives a request associated with a new session, it creates a mapping with one particular instance selected by the load balancing algorithm. The next time the load balancer receives a request from that same session, it <a id="_idIndexMarker1151"/>bypasses the load balancing <a id="_idIndexMarker1152"/>algorithm, selecting the application instance that was previously associated with the session. The particular technique we just described involves inspecting the session ID associated with the requests (usually included in a cookie by the application or the load balancer itself).</p>
    <p class="normal">A simpler alternative to associate a stateful connection to a single server is by using the IP address of the client performing the request. Usually, the IP is provided to a hash function that generates an ID representing the application instance designated to receive the request. This technique has the advantage of not requiring the association to be remembered by the load balancer. However, it doesn't work well with devices that frequently change IP, for example, when roaming on different networks.</p>
    <div><p class="Information-Box--PACKT-">Sticky load balancing is not <a id="_idIndexMarker1153"/>supported by default by the <code class="Code-In-Text--PACKT-">cluster</code> module, but it can be added with an npm library called <code class="Code-In-Text--PACKT-">sticky-session</code> (<a href="http://nodejsdp.link/sticky-session">nodejsdp.link/sticky-session</a>).</p>
    </div>
    <p class="normal">One big problem with sticky load balancing is the fact that it nullifies most of the advantages of having a redundant system, where all the instances of the application are the same, and <a id="_idIndexMarker1154"/>where an instance can eventually replace another one that stopped working. For these reasons, it is recommended<a id="_idIndexMarker1155"/> to always try to avoid sticky load balancing and building applications that maintain session state in a shared store. Alternatively, where feasible, you can try to build applications that don't require stateful communications at all; for example, by including the state in the request itself.</p>
    <div><p class="Information-Box--PACKT-">For a real example<a id="_idIndexMarker1156"/> of a library requiring sticky load balancing, we can mention Socket.IO (<a href="http://nodejsdp.link/socket-io">nodejsdp.link/socket-io</a>).</p>
    </div>
    <h2 id="_idParaDest-339" class="title">Scaling with a reverse proxy</h2>
    <p class="normal">The <code class="Code-In-Text--PACKT-">cluster</code> module, although very convenient and simple to use, is not the only option we have to <a id="_idIndexMarker1157"/>scale a Node.js web application. Traditional techniques <a id="_idIndexMarker1158"/>are often preferred because they offer more control and power in highly-available production environments.</p>
    <p class="normal">The alternative to using <code class="Code-In-Text--PACKT-">cluster</code> is to start multiple standalone instances of the same application running on different ports or machines, and then use a <strong class="keyword">reverse proxy</strong> (or gateway) to provide access to those instances, distributing the traffic across them. In this configuration, we don't have a master process distributing requests to a set of workers, but a set of distinct processes running on the same machine (using different ports) or scattered across different machines inside a network. To provide a single access point to our application, we can use a reverse proxy, a special device or service placed between the clients and the instances of our application, which takes any request and forwards it to a destination server, returning the result to the client as if it was itself the origin. In this scenario, the reverse proxy is also used as a load balancer, distributing the requests among the instances of the application.</p>
    <div><p class="Information-Box--PACKT-">For a clear explanation of the differences between a reverse proxy and a forward proxy, you can refer to the Apache HTTP server documentation at <a href="http://nodejsdp.link/forward-reverse">nodejsdp.link/forward-reverse</a>.</p>
    </div>
    <p class="normal"><em class="italic">Figure 12.6</em> shows a typical multi-process, multi-machine configuration with a reverse proxy acting as a load balancer on the front:</p>
    <figure class="mediaobject"><img src="img/B15729_12_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.6: A typical multi-process, multi-machine configuration with a reverse proxy acting as a load balancer</p>
    <p class="normal">For a Node.js application, there<a id="_idIndexMarker1159"/> are many reasons to choose this <a id="_idIndexMarker1160"/>approach in place of the <code class="Code-In-Text--PACKT-">cluster</code> module:</p>
    <ul>
      <li class="Bullet--PACKT-">A reverse proxy can distribute the load across several machines, not just several processes.</li>
      <li class="Bullet--PACKT-">The most popular reverse proxies on the market support sticky load balancing out of the box.</li>
      <li class="Bullet--PACKT-">A reverse proxy can route a request to any available server, regardless of its programming language or platform.</li>
      <li class="Bullet--PACKT-">We can choose more powerful load balancing algorithms.</li>
      <li class="Bullet-End--PACKT-">Many reverse proxies offer additional powerful features such as URL rewrites, caching, SSL termination point, security features (for example, denial-of-service protection), or even the functionality of fully-fledged web servers that can be used to, for example, serve static files.</li>
    </ul>
    <p class="normal">That said, the <code class="Code-In-Text--PACKT-">cluster</code> module could also be easily combined with a reverse proxy if necessary, for example, by using <code class="Code-In-Text--PACKT-">cluster</code> to scale vertically inside a single machine and then using the<a id="_idIndexMarker1161"/> reverse proxy to scale horizontally across<a id="_idIndexMarker1162"/> different nodes.</p>
    <div><p class="Information-Box--PACKT-"><strong class="screenText">Pattern</strong></p>
      <p class="Information-Box--PACKT-">Use a reverse proxy to balance the load of an application across multiple instances running on different ports or machines.</p>
    </div>
    <p class="normal">We have many<a id="_idIndexMarker1163"/> options to implement a load balancer using a reverse proxy. The following is a list of the most popular solutions:</p>
    <ul>
      <li class="Bullet--PACKT-"><strong class="keyword">Nginx</strong> (<a href="http://nodejsdp.link/nginx">nodejsdp.link/nginx</a>): This is<a id="_idIndexMarker1164"/> a web server, reverse proxy, and load balancer, built upon <a id="_idIndexMarker1165"/>the non-blocking I/O model.</li>
      <li class="Bullet--PACKT-"><strong class="keyword">HAProxy</strong> (<a href="http://nodejsdp.link/haproxy">nodejsdp.link/haproxy</a>): This is a<a id="_idIndexMarker1166"/> fast load balance<a id="_idIndexMarker1167"/>r for TCP/HTTP traffic.</li>
      <li class="Bullet--PACKT-"><strong class="keyword">Node.js-based proxies</strong>: There are many solutions for the implementation of reverse proxies <a id="_idIndexMarker1168"/>and load balancers directly in Node.js. This might have advantages and disadvantages, as we will see later.</li>
      <li class="Bullet-End--PACKT-"><strong class="keyword">Cloud-based proxies</strong>: In the era of cloud computing, it's not rare to utilize a load balancer as a service. This <a id="_idIndexMarker1169"/>can be convenient because it requires minimal maintenance, it's usually highly scalable, and sometimes it can support dynamic configurations to enable on-demand scalability.</li>
    </ul>
    <p class="normal">In the next few sections of this chapter, we will analyze a sample configuration using Nginx. Later on, we will work on building our very own load balancer using nothing but Node.js!</p>
    <h3 id="_idParaDest-340" class="title">Load balancing with Nginx</h3>
    <p class="normal">To give you<a id="_idIndexMarker1170"/> an idea <a id="_idIndexMarker1171"/>of how reverse proxies work, we will now build a scalable architecture based on Nginx, but first, we need to install it. We can do that by following the instructions at <a href="http://nodejsdp.link/nginx-install">nodejsdp.link/nginx-install</a>.</p>
    <div><p class="Information-Box--PACKT-">On the latest Ubuntu system, you can quickly install Nginx with the command <code class="Code-In-Text--PACKT-">sudo apt-get install nginx</code>. On macOS, you can use <code class="Code-In-Text--PACKT-">brew</code> (<a href="http://nodejsdp.link/brew">nodejsdp.link/brew</a>): <code class="Code-In-Text--PACKT-">brew install nginx</code>. Note that for the following examples, we will be using the latest version of Nginx available at the time of writing (1.17.10).</p>
    </div>
    <p class="normal">Since we are not going to use <code class="Code-In-Text--PACKT-">cluster</code> to start multiple instances of our server, we need to slightly modify the code of our application so that we can specify the listening port using a command-line argument. This will allow us to launch multiple instances on different ports. Let's consider the main module of our example application (<code class="Code-In-Text--PACKT-">app.js</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">import { createServer } from 'http'
const { pid } = process
const server = createServer((req, res) =&gt; {
  let i = 1e7; while (i &gt; 0) { i-- }
  console.log(`Handling request from ${pid}`)
  res.end(`Hello from ${pid}\n`)
})
const port = Number.parseInt(
  process.env.PORT || process.argv[2]
) || 8080
server.listen(port, () =&gt; console.log(`Started at ${pid}`))
</code></pre>
    <p class="normal">The only<a id="_idIndexMarker1172"/> difference between this version and the first version of our web <a id="_idIndexMarker1173"/>server is that here, we are making the port number configurable through the <code class="Code-In-Text--PACKT-">PORT</code> environment variable or a command-line argument. This is needed because we want to be able to start multiple instances of the server and allow them to listen on different ports.</p>
    <p class="normal">Another important feature that we won't have available without <code class="Code-In-Text--PACKT-">cluster</code> is the automatic restart in case of a crash. Luckily, this is easy to fix by using a dedicated supervisor, that is, an external process that monitors our application and restarts it if necessary. The following are some possible choices:</p>
    <ul>
      <li class="Bullet--PACKT-">Node.js-based supervisors such as <strong class="keyword">forever</strong> (<a href="http://nodejsdp.link/forever">nodejsdp.link/forever</a>) or <strong class="keyword">pm2</strong> (<a href="http://nodejsdp.link/pm2">nodejsdp.link/pm2</a>)</li>
      <li class="Bullet--PACKT-">OS-based monitors such as <strong class="keyword">systemd</strong> (<a href="http://nodejsdp.link/systemd">nodejsdp.link/systemd</a>) or <strong class="keyword">runit</strong> (<a href="http://nodejsdp.link/runit">nodejsdp.link/runit</a>)</li>
      <li class="Bullet--PACKT-">More advanced monitoring solutions such as <strong class="keyword">monit</strong> (<a href="http://nodejsdp.link/monit">nodejsdp.link/monit</a>) or <strong class="keyword">supervisord</strong> (<a href="http://nodejsdp.link/supervisord">nodejsdp.link/supervisord</a>)</li>
      <li class="Bullet-End--PACKT-">Container-based runtimes such as <strong class="keyword">Kubernetes</strong> (<a href="http://nodejsdp.link/kubernetes">nodejsdp.link/kubernetes</a>), <strong class="keyword">Nomad</strong> (<a href="http://nodejsdp.link/nomad">nodejsdp.link/nomad</a>), or <strong class="keyword">Docker Swarm</strong> (<a href="http://nodejsdp.link/swarm">nodejsdp.link/swarm</a>).</li>
    </ul>
    <p class="normal">For this example, we are going to use <code class="Code-In-Text--PACKT-">forever</code>, which is the simplest and most immediate for us to use. We can install it globally by running the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">npm install forever -g
</code></pre>
    <p class="normal">The next step is to start the four instances of our application, all on different ports and supervised by <code class="Code-In-Text--PACKT-">forever</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">forever start app.js 8081
forever start app.js 8082
forever start app.js 8083
forever start app.js 8084
</code></pre>
    <p class="normal">We can check the list of the started processes using the command:</p>
    <pre class="programlisting con"><code class="hljs-con">forever list
</code></pre>
    <div><p class="Information-Box--PACKT-">You can use <code class="Code-In-Text--PACKT-">forever stopall</code> to stop all the Node.js processes previously started with <code class="Code-In-Text--PACKT-">forever</code>. Alternatively, you can use <code class="Code-In-Text--PACKT-">forever stop &lt;id&gt;</code> to stop a specific process from the ones shown with <code class="Code-In-Text--PACKT-">forever list</code>.</p>
    </div>
    <p class="normal">Now, it's time<a id="_idIndexMarker1174"/> to<a id="_idIndexMarker1175"/> configure the Nginx server as a load balancer.</p>
    <p class="normal">First, we need to create a minimal configuration file in our working directory that we will call <code class="Code-In-Text--PACKT-">nginx.conf</code>.</p>
    <div><p class="Information-Box--PACKT-">Note that, because Nginx allows you to run multiple applications behind the same server instance, it is more common to use a global configuration file, which, in Unix systems, is generally located under <code class="Code-In-Text--PACKT-">/usr/local/nginx/conf</code>, <code class="Code-In-Text--PACKT-">/etc/nginx</code> or <code class="Code-In-Text--PACKT-">/usr/local/etc/nginx</code>. Here, by having a configuration file in our working folder, we are taking a simpler approach. This is ok for the sake of this demo as we want to run just one application locally, but we advise you follow the recommended best practices for production deployments.</p>
    </div>
    <p class="normal">Next, let's write the <code class="Code-In-Text--PACKT-">nginx.conf</code> file and apply the following configuration, which is the very minimum required to get a working load balancer for our Node.js processes:</p>
    <pre class="programlisting code"><code class="hljs-code">daemon off;                                                ## (1)
error_log /dev/stderr info;                                ## (2)
events {                                                   ## (3)
  worker_connections 2048;
}
http {                                                     ## (4)
  access_log /dev/stdout;
  upstream my-load-balanced-app {
    server 127.0.0.1:8081;
    server 127.0.0.1:8082;
    server 127.0.0.1:8083;
    server 127.0.0.1:8084;
  }
  server {
    listen 8080;
    location / {
      proxy_pass http://my-load-balanced-app;
    }
  }
}
</code></pre>
    <p class="normal">Let's <a id="_idIndexMarker1176"/>discuss<a id="_idIndexMarker1177"/> this configuration together:</p>
    <ol>
      <li class="numbered">The declaration <code class="Code-In-Text--PACKT-">daemon off</code> allows us to run Nginx as a standalone process using the current unprivileged user and by keeping the process running in the foreground of the current terminal (which allows us to shut it down using Ctrl + C).</li>
      <li class="numbered">We use <code class="Code-In-Text--PACKT-">error_log</code> (and later in the <code class="Code-In-Text--PACKT-">http</code> block, <code class="Code-In-Text--PACKT-">access_log</code>) to stream errors and access logs respectively to the standard output and standard error, so we can read the logs in real time straight from our terminal.</li>
      <li class="numbered">The <code class="Code-In-Text--PACKT-">events</code> block allows us to configure how network connections are managed by Nginx. Here, we are setting the maximum number of simultaneous connections that can be opened by an Nginx worker process to <code class="Code-In-Text--PACKT-">2048</code>.</li>
      <li class="numbered">The <code class="Code-In-Text--PACKT-">http</code> block allows us to define the configuration for a given application. In the <code class="Code-In-Text--PACKT-">upstream my-load-balanced-app</code> section, we are defining the list of backend servers used to handle the network requests. In the <code class="Code-In-Text--PACKT-">server</code> section, we use <code class="Code-In-Text--PACKT-">listen 8080</code> to instruct the server to listen on port <code class="Code-In-Text--PACKT-">8080</code> and finally, we specify the <code class="Code-In-Text--PACKT-">proxy_pass</code> directive, which essentially tells Nginx to forward any request to the server group we defined before (<code class="Code-In-Text--PACKT-">my-load-balanced-app</code>).</li>
    </ol>
    <p class="normal">That's it! Now, we only need to start Nginx using our configuration file with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">nginx -c ${PWD}/nginx.conf
</code></pre>
    <p class="normal">Our system should now be up and running, ready to accept requests and balance the traffic across the four instances of our Node.js application. Simply point your browser to the address <code class="Code-In-Text--PACKT-">http://localhost:8080</code> to see how the traffic is balanced by our Nginx server. You<a id="_idIndexMarker1178"/> can also try again to load test this application<a id="_idIndexMarker1179"/> using <code class="Code-In-Text--PACKT-">autocannon</code>. Since we are still running all the processes in one local machine, your results should not diverge much from what you got when benchmarking the version using the <code class="Code-In-Text--PACKT-">cluster</code> module approach.</p>
    <p class="normal">This example demonstrated how to use Nginx to load balance traffic. For simplicity, we kept everything locally on our machine, but nonetheless, this was a great exercise to get us ready to deploy an application on multiple remote servers. If you want to try to do that, you will essentially have to follow this recipe:</p>
    <ol>
      <li class="numbered">Provision <em class="italic">n</em> backend servers running the Node.js application (running multiple instances with a service monitor like <code class="Code-In-Text--PACKT-">forever</code> or by using the <code class="Code-In-Text--PACKT-">cluster</code> module).</li>
      <li class="numbered">Provision a load balancer machine that has Nginx installed and all the necessary configuration to route the traffic to the <em class="italic">n</em> backend servers. Every process in every server should be listed in the <code class="Code-In-Text--PACKT-">upstream</code> block of your Nginx configuration file using the correct address of the various machines in the network.</li>
      <li class="numbered">Make your load balancer publicly available on the internet by using a public IP and possibly a public domain name.</li>
      <li class="numbered">Try to send some traffic to the load balancer's public address by using a browser or a benchmarking tool like <code class="Code-In-Text--PACKT-">autocannon</code>.</li>
    </ol>
    <div><p class="Information-Box--PACKT-">For simplicity, you can perform all these steps manually by spinning servers through your<a id="_idIndexMarker1180"/> cloud provider admin interface and by using SSH to log in to those. Alternatively, you<a id="_idIndexMarker1181"/> could <a id="_idIndexMarker1182"/>choose tools <a id="_idIndexMarker1183"/>that allow you to automate these tasks by writing <strong class="keyword">infrastructure as code</strong> such as <strong class="keyword">Terraform</strong> (<a href="http://nodejsdp.link/terraform">nodejsdp.link/terraform</a>), <strong class="keyword">Ansible</strong> (<a href="http://nodejsdp.link/ansible">nodejsdp.link/ansible</a>), and <strong class="keyword">Packer</strong> (<a href="http://nodejsdp.link/packer">nodejsdp.link/packer</a>).</p>
    </div>
    <p class="normal">In this example, we used a predefined number of backend servers. In the next section, we will <a id="_idIndexMarker1184"/>explore a technique that allows us to load balance traffic to a dynamic set of backend servers.</p>
    <h2 id="_idParaDest-341" class="title">Dynamic horizontal scaling</h2>
    <p class="normal">One important advantage of modern cloud-based infrastructure is the ability to dynamically adjust the<a id="_idIndexMarker1185"/> capacity of an application based on the current or predicted traffic. This is also known as <strong class="keyword">dynamic scaling</strong>. If implemented<a id="_idIndexMarker1186"/> properly, this practice can reduce the cost of the IT infrastructure enormously while still keeping the application highly available and responsive.</p>
    <p class="normal">The idea is simple: if our application is experiencing a performance degradation caused by a peak in traffic, the system automatically spawns new servers to cope with the increased load. Similarly, if we see that the allocated resources are underutilized, we can shut some servers down to reduce the cost of the running infrastructure. We could also decide to perform scaling operations based on a schedule; for instance, we could shut down some servers during certain hours of the day when we know that the traffic will be lighter, and restart them again just before the peak hours. These mechanisms require the load balancer to always be up-to-date with the current network topology, knowing at any time which server is up.</p>
    <h3 id="_idParaDest-342" class="title">Using a service registry</h3>
    <p class="normal">A common pattern to solve this problem is to use a central repository called a <strong class="keyword">service registry</strong>, which<a id="_idIndexMarker1187"/> keeps track of the running servers<a id="_idIndexMarker1188"/> and the services they provide. </p>
    <p class="normal"><em class="italic">Figure 12.7</em> shows a multiservice architecture with a load balancer on the front, dynamically configured using a service registry:</p>
    <figure class="mediaobject"><img src="img/B15729_12_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.7: A multiservice architecture with a load balancer on the front, dynamically configured using a service registry</p>
    <p class="normal">The architecture in <em class="italic">Figure 12.7</em> assumes the presence of two services, <strong class="keyword">API</strong> and <strong class="keyword">WebApp</strong>. There can be one or many instances of each service, spread across multiple servers.</p>
    <p class="normal">When a request <a id="_idIndexMarker1189"/>to <code class="Code-In-Text--PACKT-">example.com</code> is received, the load <a id="_idIndexMarker1190"/>balancer checks the prefix of the request path. If the prefix is <code class="Code-In-Text--PACKT-">/api</code>, the request is load balanced between the available instances of the <strong class="keyword">API</strong> service. In <em class="italic">Figure 12.7</em>, we have two instances running on the server <code class="Code-In-Text--PACKT-">api1.example.com</code> and one instance running on <code class="Code-In-Text--PACKT-">api2.example.com</code>. For all the other path prefixes, the request is load balanced between the available instances of the <strong class="keyword">WebApp</strong> service. In the diagram, we have only one <strong class="keyword">WebApp</strong> instance, which is running on the server <code class="Code-In-Text--PACKT-">web1.example.com</code>. The load balancer obtains the list of servers and service instances running on every server using the service registry.</p>
    <p class="normal">For this to work in complete automation, each application instance has to register itself to the service registry the moment it comes up online and unregister itself when it stops. This way, the load balancer can always have an up-to-date view of the servers and the services available on the network.</p>
    <div><p class="Information-Box--PACKT-"><strong class="screenText">Pattern (service registry)</strong></p>
      <p class="Information-Box--PACKT-">Use a central repository to store an always up-to-date view of the servers and the services available in a system.</p>
    </div>
    <p class="normal">While this pattern is useful to load balance traffic, it has the added benefit of being able to decouple service instances from the servers on which they are running. We can look at the Service<a id="_idIndexMarker1191"/> Registry pattern as an implementation<a id="_idIndexMarker1192"/> of the Service Locator Design pattern applied to network services.</p>
    <h3 id="_idParaDest-343" class="title">Implementing a dynamic load balancer with http-proxy and Consul</h3>
    <p class="normal">To support a <a id="_idIndexMarker1193"/>dynamic <a id="_idIndexMarker1194"/>network infrastructure, we can use a reverse proxy such as <strong class="keyword">Nginx</strong> or <strong class="keyword">HAProxy</strong>: all we<a id="_idIndexMarker1195"/> need to do is<a id="_idIndexMarker1196"/> update their configuration<a id="_idIndexMarker1197"/> using an <a id="_idIndexMarker1198"/>automated service and then force the load balancer to pick the changes. For Nginx, this can be done using the following command line:</p>
    <pre class="programlisting con"><code class="hljs-con">nginx -s reload
</code></pre>
    <p class="normal">The same result can be achieved with a cloud-based solution, but we have a third and more familiar alternative that makes use of our favorite platform.</p>
    <p class="normal">We all know that Node.js is a great tool for building any sort of network application and, as we said throughout this book, this is exactly one of its main design goals. So, why not build a load balancer using nothing but Node.js? This would give us much more freedom and power and would allow us to implement any sort of pattern or algorithm straight into our custom-built load balancer, including the one we are now going to explore: dynamic load balancing using a service registry. Furthermore, working on this exercise will definitely help us to understand even better how production-grade products such as Nginx and HAProxy actually work.</p>
    <p class="normal">In this example, we are going<a id="_idIndexMarker1199"/> to use <strong class="keyword">Consul</strong> (<a href="http://nodejsdp.link/consul">nodejsdp.link/consul</a>) as the service registry to replicate the multiservice architecture we saw in <em class="italic">Figure 12.7</em>. To do that, we are going to mainly use three npm packages:</p>
    <ul>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">http-proxy</code> (<a href="http://nodejsdp.link/http-proxy">nodejsdp.link/http-proxy</a>): To simplify the creation of a reverse proxy/load balancer in Node.js</li>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">portfinder</code> (<a href="http://nodejsdp.link/portfinder">nodejsdp.link/portfinder</a>): To find a free port in the system</li>
      <li class="Bullet-End--PACKT-"><code class="Code-In-Text--PACKT-">consul</code> (<a href="http://nodejsdp.link/consul-lib">nodejsdp.link/consul-lib</a>): To interact with Consul</li>
    </ul>
    <p class="normal">Let's<a id="_idIndexMarker1200"/> start<a id="_idIndexMarker1201"/> by<a id="_idIndexMarker1202"/> implementing our services. These are<a id="_idIndexMarker1203"/> simple HTTP<a id="_idIndexMarker1204"/> servers like the ones we have used <a id="_idIndexMarker1205"/>so far to test <code class="Code-In-Text--PACKT-">cluster</code> and Nginx, but this time, we want each server to register itself into the service registry the moment it starts.</p>
    <p class="normal">Let's see how this looks (file <code class="Code-In-Text--PACKT-">app.js</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">import { createServer } from 'http'
import consul from 'consul'
import portfinder from 'portfinder'
import { nanoid } from 'nanoid'
const serviceType = process.argv[2]
const { pid } = process
async function main () {
  const consulClient = consul()
  const port = await portfinder.getPortPromise()            // (1)
  const address = process.env.ADDRESS || 'localhost'
  const serviceId = nanoid()
  function registerService () {                             // (2)
    consulClient.agent.service.register({
      id: serviceId,
      name: serviceType,
      address,
      port,
      tags: [serviceType]
    }, () =&gt; {
      console.log(`${serviceType} registered successfully`)
    })
  }
  function unregisterService (err) {                        // (3)
    err &amp;&amp; console.error(err)
    console.log(`deregistering ${serviceId}`)
    consulClient.agent.service.deregister(serviceId, () =&gt; {
      process.exit(err ? 1 : 0)
    })
  }
  process.on('exit', unregisterService)                     // (4)
  process.on('uncaughtException', unregisterService)
  process.on('SIGINT', unregisterService)
  const server = createServer((req, res) =&gt; {               // (5)
    let i = 1e7; while (i &gt; 0) { i-- }
    console.log(`Handling request from ${pid}`)
    res.end(`${serviceType} response from ${pid}\n`)
  })
  server.listen(port, address, () =&gt; {
    registerService()
    console.log(`Started ${serviceType} at ${pid} on port ${port}`)
  })
}
main().catch((err) =&gt; {
  console.error(err)
  process.exit(1)
})
</code></pre>
    <p class="normal">In the<a id="_idIndexMarker1206"/> preceding<a id="_idIndexMarker1207"/> code, there<a id="_idIndexMarker1208"/> are <a id="_idIndexMarker1209"/>some parts that<a id="_idIndexMarker1210"/> deserve<a id="_idIndexMarker1211"/> our attention:</p>
    <ol>
      <li class="numbered">First, we use <code class="Code-In-Text--PACKT-">portfinder.getPortPromise()</code> to discover a free port in the system (by default, <code class="Code-In-Text--PACKT-">portfinder</code> starts to search from port <code class="Code-In-Text--PACKT-">8000</code>). We also allow the user to configure the address based on the environment variable <code class="Code-In-Text--PACKT-">ADDRESS</code>. Finally, we generate a random ID to identify this service using <code class="Code-In-Text--PACKT-">nanoid</code> (<a href="http://nodejsdp.link/nanoid">nodejsdp.link/nanoid</a>).</li>
      <li class="numbered">Next, we declare the <code class="Code-In-Text--PACKT-">registerService()</code> function, which uses the <code class="Code-In-Text--PACKT-">consul</code> library to register a new service in the registry. The service definition needs several attributes: <code class="Code-In-Text--PACKT-">id</code> (a unique identifier for the service), <code class="Code-In-Text--PACKT-">name</code> (a generic name that identifies the service), <code class="Code-In-Text--PACKT-">address</code> and <code class="Code-In-Text--PACKT-">port</code> (to identify how to access the service), and <code class="Code-In-Text--PACKT-">tags</code> (an optional array of tags that can be used to filter and group services). We are using <code class="Code-In-Text--PACKT-">serviceType</code> (which we get from the command-line arguments) to specify the service name and to add a tag. This will allow us to identify all the services of the same type available in the cluster.</li>
      <li class="numbered">At this point, we define a function called <code class="Code-In-Text--PACKT-">unregisterService()</code>, which allows us to remove the service we just registered in Consul.</li>
      <li class="numbered">We use <code class="Code-In-Text--PACKT-">unregisterService()</code> as a cleanup function so that when the program is closed (either intentionally or by accident), the service is unregistered from Consul.</li>
      <li class="numbered">Finally, we start the HTTP server for our service on the port discovered by <code class="Code-In-Text--PACKT-">portfinder</code> and the address configured for the current service. Note that when the server is started, we make sure to invoke the <code class="Code-In-Text--PACKT-">registerService()</code> function to make sure that the service is registered for discovery.</li>
    </ol>
    <p class="normal">With<a id="_idIndexMarker1212"/> this script, we <a id="_idIndexMarker1213"/>will be<a id="_idIndexMarker1214"/> able to start<a id="_idIndexMarker1215"/> and register<a id="_idIndexMarker1216"/> different<a id="_idIndexMarker1217"/> types of applications.</p>
    <p class="normal">Now, it's time to implement the load balancer. Let's do that by creating a new module called <code class="Code-In-Text--PACKT-">loadBalancer.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createServer } from 'http'
import httpProxy from 'http-proxy'
import consul from 'consul'
const routing = [                                            // (1)
  {
    path: '/api',
    service: 'api-service',
    index: 0
  },
  {
    path: '/',
    service: 'webapp-service',
    index: 0
  }
]
const consulClient = consul()                                // (2)
const proxy = httpProxy.createProxyServer()
const server = createServer((req, res) =&gt; {
  const route = routing.find((route) =&gt;                      // (3)
    req.url.startsWith(route.path))
  consulClient.agent.service.list((err, services) =&gt; {       // (4)
    const servers = !err &amp;&amp; Object.values(services)
      .filter(service =&gt; service.Tags.includes(route.service))
    if (err || !servers.length) {
      res.writeHead(502)
      return res.end('Bad gateway')
    }
    route.index = (route.index + 1) % servers.length         // (5)
    const server = servers[route.index]
    const target = `http://${server.Address}:${server.Port}`
    proxy.web(req, res, { target })
  })
})
server.listen(8080, () =&gt; {
  console.log('Load balancer started on port 8080')
})
</code></pre>
    <p class="normal">This is<a id="_idIndexMarker1218"/> how we<a id="_idIndexMarker1219"/> implemented <a id="_idIndexMarker1220"/>our <a id="_idIndexMarker1221"/>Node.js-based <a id="_idIndexMarker1222"/>load<a id="_idIndexMarker1223"/> balancer:</p>
    <ol>
      <li class="numbered">First, we define our load balancer routes. Each item in the <code class="Code-In-Text--PACKT-">routing</code> array contains the <code class="Code-In-Text--PACKT-">service</code> used to handle the requests arriving on the mapped <code class="Code-In-Text--PACKT-">path</code>. The <code class="Code-In-Text--PACKT-">index</code> property will be used to <strong class="keyword">round-robin</strong> the requests of a given service.</li>
      <li class="numbered">We need to instantiate a <code class="Code-In-Text--PACKT-">consul</code> client so that we can have access to the registry. Next, we instantiate an <code class="Code-In-Text--PACKT-">http-proxy</code> server.</li>
      <li class="numbered">In the request handler of the server, the first thing we do is match the URL against our routing table. The result will be a descriptor containing the service name.</li>
      <li class="numbered">We obtain from <code class="Code-In-Text--PACKT-">consul</code> the list of servers implementing the required service. If this list is empty or there was an error retrieving it, then we return an error to the client. We use the <code class="Code-In-Text--PACKT-">Tags</code> attribute to filter all the available services and find the address of the servers that implement the current service type.</li>
      <li class="numbered">At last, we can route the request to its destination. We update <code class="Code-In-Text--PACKT-">route.index</code> to point to the next server in the list, following a round-robin approach. We then use the index to select a server from the list, passing it to <code class="Code-In-Text--PACKT-">proxy.web()</code>, along with the request (<code class="Code-In-Text--PACKT-">req</code>) and the response (<code class="Code-In-Text--PACKT-">res</code>) objects. This will simply forward the request to the server we chose.</li>
    </ol>
    <p class="normal">It is now clear how simple it is to implement a load balancer using only Node.js and a service registry, as well as how much flexibility we can have by doing so.</p>
    <div><p class="Tip--PACKT-">Note that in order to keep the implementation simple, we intentionally left out some interesting optimization opportunities. For instance, in this implementation, we are interrogating <code class="Code-In-Text--PACKT-">consul</code> to get the list of registered services for every single request. This is something that can add a significant overhead, especially if our load balancer receives requests with a high frequency. It would be more efficient to cache the list of services and refresh it on a regular basis (for instance, every 10 seconds). Another optimization could be to use the <code class="Code-In-Text--PACKT-">cluster</code> module to run multiple instances of our load balancer and distribute the load across all the available cores in the machine.</p>
    </div>
    <p class="normal">Now, we should be ready to give our system a try, but first, let's install the Consul server by following the official documentation at <a href="http://nodejsdp.link/consul-install">nodejsdp.link/consul-install</a>.</p>
    <p class="normal">This allows us to start the <a id="_idIndexMarker1224"/>Consul service<a id="_idIndexMarker1225"/> registry on our development<a id="_idIndexMarker1226"/> machine with this simple command line:</p>
    <pre class="programlisting con"><code class="hljs-con">consul agent -dev
</code></pre>
    <p class="normal">Now, we<a id="_idIndexMarker1227"/> are<a id="_idIndexMarker1228"/> ready <a id="_idIndexMarker1229"/>to start the load balancer (using <code class="Code-In-Text--PACKT-">forever</code> to make sure the application is restarted in case of a crash):</p>
    <pre class="programlisting con"><code class="hljs-con">forever start loadBalancer.js
</code></pre>
    <p class="normal">Now, if we try to access some of the services exposed by the load balancer, we will notice that it returns an <code class="Code-In-Text--PACKT-">HTTP 502</code> error, because we didn't start any servers yet. Try it yourself:</p>
    <pre class="programlisting con"><code class="hljs-con">curl localhost:8080/api
</code></pre>
    <p class="normal">The preceding command should return the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">Bad Gateway
</code></pre>
    <p class="normal">The situation will change if we spawn some instances of our services, for example, two <code class="Code-In-Text--PACKT-">api-service</code> and one <code class="Code-In-Text--PACKT-">webapp-service</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">forever start --killSignal=SIGINT app.js api-service
forever start --killSignal=SIGINT app.js api-service
forever start --killSignal=SIGINT app.js webapp-service
</code></pre>
    <p class="normal">Now, the load balancer should automatically see the new servers and start distributing requests across them. Let's try again with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">curl localhost:8080/api
</code></pre>
    <p class="normal">The preceding command should now return this:</p>
    <pre class="programlisting con"><code class="hljs-con">api-service response from 6972
</code></pre>
    <p class="normal">By running this<a id="_idIndexMarker1230"/> again, we should now receive <a id="_idIndexMarker1231"/>a message from another server, confirming that the requests are being distributed evenly among the different servers:</p>
    <pre class="programlisting con"><code class="hljs-con">api-service response from 6979
</code></pre>
    <div><p class="Tip--PACKT-">If you want to see the instances managed by <code class="Code-In-Text--PACKT-">forever</code> and stop some of them you can use the commands <code class="Code-In-Text--PACKT-">forever list</code> and <code class="Code-In-Text--PACKT-">forever stop</code>. To stop all running instances you can use <code class="Code-In-Text--PACKT-">forever stopall</code>. Why don't you try to stop one of the running instances of the <code class="Code-In-Text--PACKT-">api-service</code> to see what happens to the whole application?</p>
    </div>
    <p class="normal">The <a id="_idIndexMarker1232"/>advantages<a id="_idIndexMarker1233"/> of <a id="_idIndexMarker1234"/>this pattern<a id="_idIndexMarker1235"/> are immediate. We can now scale our infrastructure dynamically, on demand, or based on a schedule, and our load balancer will automatically adjust with the new configuration without any extra effort!</p>
    <div><p class="Tip--PACKT-">Consul offers a convenient web UI available at <code class="Code-In-Text--PACKT-">localhost:8500</code> by default. Check it out while playing with this example to see how services appear and disappear <a id="_idIndexMarker1236"/>as they <a id="_idIndexMarker1237"/>get registered or unregistered.</p>
      <p class="Tip--PACKT-">Consul also offers<a id="_idIndexMarker1238"/> a health check feature to monitor registered services. This feature could be integrated<a id="_idIndexMarker1239"/> within <a id="_idIndexMarker1240"/>our example to make <a id="_idIndexMarker1241"/>our infrastructure even more resilient to failures. In fact, if a service does not respond to a health check, it gets automatically removed from the registry and therefore, it won't receive traffic anymore. If you are curious to see how you can implement this feature, you can check out the official documentation for <em class="italic">Checks</em> at <a href="http://nodejsdp.link/consul-checks">nodejsdp.link/consul-checks</a>.</p>
    </div>
    <p class="normal">Now that we know how to perform dynamic load balancing using a load balancer and a service registry, we are ready to explore some interesting alternative approaches, like peer-to-peer load balancing.</p>
    <h2 id="_idParaDest-344" class="title">Peer-to-peer load balancing</h2>
    <p class="normal">Using a reverse proxy is almost a necessity when we want to expose a complex internal network architecture<a id="_idIndexMarker1242"/> to a public network such as the Internet. It helps hide the complexity, providing a single access point that external applications can easily use and rely on. However, if we need to scale a service that is for internal use only, we can have much more flexibility and control.</p>
    <p class="normal">Let's imagine having a service, <strong class="keyword">Service A</strong>, that relies on <strong class="keyword">Service B</strong> to implement its functionality. <strong class="keyword">Service B</strong> is scaled across multiple machines and it's available only in the internal network. What we have learned so far is that <strong class="keyword">Service A</strong> will connect to <strong class="keyword">Service B</strong> using a load balancer, which will distribute the traffic to all the servers implementing <strong class="keyword">Service B</strong>.</p>
    <p class="normal">However, there is an alternative. We can remove the load balancer from the picture and distribute the requests directly from the client (<strong class="keyword">Service A</strong>), which now becomes directly responsible for load balancing its requests across the various instances of <strong class="keyword">Service B</strong>. This is possible only if <strong class="keyword">Server A</strong> knows the details about the servers exposing <strong class="keyword">Service B</strong>, and in an internal network, this is usually known information. With this approach, we are essentially implementing <strong class="keyword">peer-to-peer load balancing</strong>.</p>
    <p class="normal"><em class="italic">Figure 12.8</em> compares the two alternatives we just described:</p>
    <figure class="mediaobject"><img src="img/B15729_12_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.8: Centralized load balancing versus peer-to-peer load balancing</p>
    <p class="normal">This is an extremely<a id="_idIndexMarker1243"/> simple and effective pattern that enables truly distributed communications without bottlenecks or single points of failure. Besides that, it also has the following properties:</p>
    <ul>
      <li class="Bullet--PACKT-">Reduces the infrastructure complexity by removing a network node</li>
      <li class="Bullet--PACKT-">Allows faster communications because messages will travel through one fewer node</li>
      <li class="Bullet-End--PACKT-">Scales better because performances are not limited by what the load balancer can handle</li>
    </ul>
    <p class="normal">On the other hand, by removing the load balancer, we are actually exposing the complexity of its underlying infrastructure. Also, each client has to be smarter by implementing a load balancing algorithm and, possibly, also a way to keep its knowledge of the infrastructure up to date.</p>
    <div><p class="Tip--PACKT-">Peer-to-peer load <a id="_idIndexMarker1244"/>balancing is a pattern used extensively in the ZeroMQ (<a href="http://nodejsdp.link/zeromq">nodejsdp.link/zeromq</a>) library, which we will use in the next chapter.</p>
    </div>
    <p class="normal">In the next section, we <a id="_idIndexMarker1245"/>will showcase an example implementing peer-to-peer load balancing in an HTTP client.</p>
    <h3 id="_idParaDest-345" class="title">Implementing an HTTP client that can balance requests across multiple servers</h3>
    <p class="normal">We already know how to implement a load balancer using only Node.js and distribute incoming requests<a id="_idIndexMarker1246"/> across the available servers, so implementing the same mechanism on the client side should not be that different. All we have to do, in fact, is wrap the client API and augment it with a load balancing mechanism. Take a look at the following module (<code class="Code-In-Text--PACKT-">balancedRequest.js</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">import { request } from 'http'
import getStream from 'get-stream'
const servers = [
  { host: 'localhost', port: 8081 },
  { host: 'localhost', port: 8082 }
]
let i = 0
export function balancedRequest (options) {
  return new Promise((resolve) =&gt; {
    i = (i + 1) % servers.length
    options.hostname = servers[i].host
    options.port = servers[i].port
    request(options, (response) =&gt; {
      resolve(getStream(response))
    }).end()
  })
}
</code></pre>
    <p class="normal">The preceding code is very simple and needs little explanation. We wrapped the original <code class="Code-In-Text--PACKT-">http.request</code> API so that it overrides the <code class="Code-In-Text--PACKT-">hostname</code> and <code class="Code-In-Text--PACKT-">port</code> of the request with those selected from the list of available servers using a round-robin algorithm. Note that, for simplicity, we used the module <code class="Code-In-Text--PACKT-">get-stream</code> (<a href="http://nodejsdp.link/get-stream">nodejsdp.link/get-stream</a>) to "accumulate" the response stream into a buffer that will contain the full response body.</p>
    <p class="normal">The new wrapped API can then be used seamlessly (<code class="Code-In-Text--PACKT-">client.js</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">import { balancedRequest } from './balancedRequest.js'
async function main () {
  for (let i = 0; i &lt; 10; i++) {
    const body = await balancedRequest({
      method: 'GET',
      path: '/'
    })
    console.log(`Request ${i} completed:`, body)
  }
}
main().catch((err) =&gt; {
  console.error(err)
  process.exit(1)
})
</code></pre>
    <p class="normal">To run the<a id="_idIndexMarker1247"/> preceding code, we have to start two instances of the sample server provided:</p>
    <pre class="programlisting con"><code class="hljs-con">node app.js 8081
node app.js 8082
</code></pre>
    <p class="normal">This is followed by the client application we just built:</p>
    <pre class="programlisting con"><code class="hljs-con">node client.js
</code></pre>
    <p class="normal">We should note that each request is sent to a different server, confirming that we are now able to balance the load without a dedicated load balancer!</p>
    <div><p class="Information-Box--PACKT-">An obvious improvement to the wrapper we created previously would be to integrate a service registry directly into the client and obtain the server list dynamically.</p>
    </div>
    <p class="normal">In the next section, we will explore the field of containers and container orchestration and see how, in this specific context, the runtime takes ownership of many scalability<a id="_idIndexMarker1248"/> concerns.</p>
    <h2 id="_idParaDest-346" class="title">Scaling applications using containers</h2>
    <p class="normal">In this section, we will demonstrate how using containers and container orchestration platforms, such<a id="_idIndexMarker1249"/> as Kubernetes, can help us to<a id="_idIndexMarker1250"/> write simpler Node.js applications that can delegate most of the scaling concerns like load balancing, elastic scaling, and high availability to the underlying container platform.</p>
    <p class="normal">Containers and container orchestration platforms constitute a quite broad topic, largely outside the scope of this book. For this reason, here, we aim to provide only some basic examples to get you started with this technology using Node.js. Ultimately, our goal is to encourage you to explore new modern patterns in order to run and scale Node.js applications.</p>
    <h3 id="_idParaDest-347" class="title">What is a container?</h3>
    <p class="normal">A <strong class="keyword">container</strong>, specifically a <strong class="keyword">Linux container</strong>, as <a id="_idIndexMarker1251"/>standardized by the <strong class="keyword">Open Container Initiative</strong> (<strong class="keyword">OCI</strong>) (<a href="http://nodejsdp.link/opencontainers">nodejsdp.link/opencontainers</a>), is defined as "a standard<a id="_idIndexMarker1252"/> unit of <a id="_idIndexMarker1253"/>software that <a id="_idIndexMarker1254"/>packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another."</p>
    <p class="normal">In other words, by using containers, you can seamlessly package and run applications on different machines, from a local development laptop on your desk to a production server in the cloud.</p>
    <p class="normal">Other than being extremely portable, applications running as containers have the advantage of having very little overhead when executed. In fact, containers run almost as fast as running the native application directly on the operating system.</p>
    <p class="normal">In simple terms, you can see a container as a standard unit of software that allows you to define and run an <em class="italic">isolated</em> process directly on a Linux operating system.</p>
    <p class="normal">For their portability and <a id="_idIndexMarker1255"/>performance, containers are considered a huge step forward when compared to <strong class="keyword">virtual machines</strong>.</p>
    <p class="normal">There are different ways and tools to create and run an OCI compliant container for an application. The most <a id="_idIndexMarker1256"/>popular of them is <strong class="keyword">Docker</strong> (<a href="http://nodejsdp.link/docker">nodejsdp.link/docker</a>).</p>
    <p class="normal">You can install Docker in your system by following the instructions for your operating system on the official documentation: <a href="http://nodejsdp.link/docker-docs">nodejsdp.link/docker-docs</a>.</p>
    <h3 id="_idParaDest-348" class="title">Creating and running a container with Docker</h3>
    <p class="normal">Let's rewrite <a id="_idIndexMarker1257"/>our simple<a id="_idIndexMarker1258"/> web <a id="_idIndexMarker1259"/>server application<a id="_idIndexMarker1260"/> with some minor changes (<code class="Code-In-Text--PACKT-">app.js</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">import { createServer } from 'http'
<strong class="hljs-keyword-slc">import</strong><strong class="hljs-slc"> { hostname } </strong><strong class="hljs-keyword-slc">from</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">'os'</strong>
<strong class="hljs-keyword-slc">const</strong><strong class="hljs-slc"> version = </strong><strong class="hljs-number-slc">1</strong>
const server = createServer((req, res) =&gt; {
  let i = 1e7; while (i &gt; 0) { i-- }
  res.end(`Hello from <strong class="hljs-subst-slc">${</strong><strong class="hljs-slc">hostname()</strong><strong class="hljs-subst-slc">}</strong><strong class="hljs-string-slc"> (v</strong><strong class="hljs-subst-slc">${version}</strong><strong class="hljs-string-slc">)</strong>`)
})
server.listen(8080)
</code></pre>
    <p class="normal">Compared to the previous versions of this web server, here, we send the machine hostname and the application version back to the user. If you run this server and make a request, you should get back something like this:</p>
    <pre class="programlisting con"><code class="hljs-con">Hello from my-amazing-laptop.local (v1)
</code></pre>
    <p class="normal">Let's see how we can run this application as a container. The first thing we need to do is create a <code class="Code-In-Text--PACKT-">package.json</code> file for the project:</p>
    <pre class="programlisting code"><code class="hljs-code">{
  "name": "my-simple-app",
  "version": "1.0.0",
  "main": "app.js",
  "type": "module",
  "scripts": {
    "start": "node app.js"
  }
}
</code></pre>
    <p class="normal">In order to <em class="italic">dockerize</em> our application, we need to follow a two-step process:</p>
    <ul>
      <li class="Bullet--PACKT-">Build a container image</li>
      <li class="Bullet-End--PACKT-">Run a container instance from the image</li>
    </ul>
    <p class="normal">To create the <strong class="keyword">container image</strong> for our <a id="_idIndexMarker1261"/>application, we have to define a Dockerfile. A container image (or Docker image) is the actual package and conforms to the OCI standard. It contains all the source code and the necessary dependencies and describes how<a id="_idIndexMarker1262"/> the application must be executed. A <strong class="keyword">Dockerfile</strong> is a file (actually named <code class="Code-In-Text--PACKT-">Dockerfile</code>) that defines the build script used to build a container image for an application. So, without further ado, let's write the Dockerfile for our application:</p>
    <pre class="programlisting code"><code class="hljs-code">FROM node:14-alpine
EXPOSE 8080
COPY app.js package.json /app/
WORKDIR /app
CMD ["npm", "start"]
</code></pre>
    <p class="normal">Our Dockerfile <a id="_idIndexMarker1263"/>is quite<a id="_idIndexMarker1264"/> short, but there are<a id="_idIndexMarker1265"/> a lot of interesting<a id="_idIndexMarker1266"/> things here, so let's discuss them one by one:</p>
    <ul>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">FROM node:14-alpine</code> indicates the base image that we want to use. A base image allows us to build "on top" of an existing image. In this specific case, we are starting from an image that already contains version 14 of Node.js. This means we don't have to be worried about describing how Node.js needs to be packaged into the container image.</li>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">EXPOSE 8080</code> informs Docker that the application will be listening for TCP connections on the port <code class="Code-In-Text--PACKT-">8080</code>.</li>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">COPY app.js package.json /app/</code> copies the files <code class="Code-In-Text--PACKT-">app.js</code> and <code class="Code-In-Text--PACKT-">package.json</code> into the <code class="Code-In-Text--PACKT-">/app</code> folder of the container filesystem. Containers are isolated, so, by default, they can't share files with the host operating system; therefore, we need to copy the project files into the container to be able to access and execute them.</li>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">WORKDIR /app</code> sets the working directory for the container to <code class="Code-In-Text--PACKT-">/app</code>.</li>
      <li class="Bullet-End--PACKT-"><code class="Code-In-Text--PACKT-">CMD ["npm", "start"]</code> specifies the command that is executed to start the application when we run a container from an image. Here, we are just running <code class="Code-In-Text--PACKT-">npm start</code>, which, in turn, will run <code class="Code-In-Text--PACKT-">node app.js</code>, as specified in our <code class="Code-In-Text--PACKT-">package.json</code>. Remember that we are able to run both <code class="Code-In-Text--PACKT-">node</code> and <code class="Code-In-Text--PACKT-">npm</code> in the container only because those two executables are made available through the base image.</li>
    </ul>
    <p class="normal">Now, we can use the Dockerfile to build the container image with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker build .
</code></pre>
    <p class="normal">This command will look for a Dockerfile in the current working directory and execute it to build our image.</p>
    <p class="normal">The <a id="_idIndexMarker1267"/>output<a id="_idIndexMarker1268"/> of this <a id="_idIndexMarker1269"/>command <a id="_idIndexMarker1270"/>should be something like this:</p>
    <pre class="programlisting con"><code class="hljs-con">Sending build context to Docker daemon  7.168kB
Step 1/5 : FROM node:14-alpine
 ---&gt; ea308280893e
Step 2/5 : EXPOSE 8080
 ---&gt; Running in 61c34f4064ab
Removing intermediate container 61c34f4064ab
 ---&gt; 6abfcdf0e750
Step 3/5 : COPY app.js package.json /app/
 ---&gt; 9d498d7dbf8b
Step 4/5 : WORKDIR /app
 ---&gt; Running in 70ea26158cbe
Removing intermediate container 70ea26158cbe
 ---&gt; fc075a421b91
Step 5/5 : CMD ["npm", "start"]
 ---&gt; Running in 3642a01224e8
Removing intermediate container 3642a01224e8
 ---&gt; bb3bd34bac55
Successfully built bb3bd34bac55
</code></pre>
    <div><p class="Information-Box--PACKT-">Note that if you have never used the <code class="Code-In-Text--PACKT-">node:14-alpine</code> image before (or if you have recently wiped your Docker cache), you will also see some additional output, indicating the download of this container image.</p>
    </div>
    <p class="normal">The final hash is the ID of our container image. We can use it to run an instance of the container with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">docker run -it -p 8080:8080 bb3bd34bac55
</code></pre>
    <p class="normal">This command is essentially telling Docker to run the application from image <code class="Code-In-Text--PACKT-">bb3bd34bac55</code> in "interactive mode" (which means that it will not go in the background) and that port <code class="Code-In-Text--PACKT-">8080</code> of the container will be mapped to port <code class="Code-In-Text--PACKT-">8080</code> of the host machine (our operating system).</p>
    <p class="normal">Now, we can access the application at <code class="Code-In-Text--PACKT-">localhost:8080</code>. So, if we use <code class="Code-In-Text--PACKT-">curl</code> to send a request to the web server, we should get a response similar to the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Hello from f2ffa85c8ff8 (v1)
</code></pre>
    <p class="normal">Note that the hostname is now different. This is because every container is running in a sandboxed environment that, by default, doesn't have access to most of the resources in the underlying operating system.</p>
    <p class="normal">At this <a id="_idIndexMarker1271"/>point, you can <a id="_idIndexMarker1272"/>stop the container by<a id="_idIndexMarker1273"/> just pressing Ctrl + C in the<a id="_idIndexMarker1274"/> terminal window where the container is running.</p>
    <div><p class="Information-Box--PACKT-">When building an image, we can use the <code class="Code-In-Text--PACKT-">-t</code> flag to <em class="italic">tag</em> the resulting image. A tag can be used as a more predictable alternative to a generated hash to identify and run container images. For instance, if we want to call our container image <code class="Code-In-Text--PACKT-">hello-web:v1</code>, we can use the following commands:</p>
      <pre class="programlisting con"><code class="hljs-con">docker build -t hello-web:v1 .
docker run -it -p 8080:8080 hello-web:v1
</code></pre>
      <p class="Information-Box--PACKT-">When using tags, you might want to follow the conventional format of <code class="Code-In-Text--PACKT-">image-name:version</code>.</p>
    </div>
    <h3 id="_idParaDest-349" class="title">What is Kubernetes?</h3>
    <p class="normal">We just ran a Node.js application using containers, hooray! Even though this seems like a particularly exciting <a id="_idIndexMarker1275"/>achievement, we have just scratched the surface here. The real power of containers comes out when building more complicated applications. For instance, when building applications composed by multiple independent services that needs to be deployed and coordinated across multiple cloud servers. In this situation, Docker alone is not sufficient anymore. We need a more complex system that allows us to orchestrate all the running container instances over the available machines in our cloud cluster: we need a container orchestration tool.</p>
    <p class="normal">A container orchestration tool has a number of responsibilities:</p>
    <ul>
      <li class="Bullet--PACKT-">It allows us to join multiple cloud servers (nodes) into one logical cluster, where nodes can be added and removed dynamically without affecting the availability of the services running in every node.</li>
      <li class="Bullet--PACKT-">It makes sure that there is no downtime. If a container instance stops or becomes unresponsive to health checks, it will be automatically restarted. Also, if a node in the cluster fails, the workload running in that node will be automatically migrated to another node.</li>
      <li class="Bullet--PACKT-">Provides functionalities to implement service discovery and load balancing.</li>
      <li class="Bullet--PACKT-">Provides orchestrated access to durable storage so that data can be persisted as needed.</li>
      <li class="Bullet--PACKT-">Automatic rollouts and rollbacks of applications with zero downtime.</li>
      <li class="Bullet-End--PACKT-">Secret storage for sensitive data and configuration management systems.</li>
    </ul>
    <p class="normal">One of the most popular container orchestration systems is Kubernetes (<a href="http://nodejsdp.link/kubernetes">nodejsdp.link/kubernetes</a>), originally <a id="_idIndexMarker1276"/>open sourced by Google in 2014. The name Kubernetes originates from the Greek <img src="img/kubernetes.png" style="height: 1.1em" alt=""/>, meaning "helmsman" or "pilot", but also "governor" or more generically, "the one in command". Kubernetes incorporates<a id="_idIndexMarker1277"/> years of experience from Google engineers running workloads in the cloud at scale.</p>
    <p class="normal">One of its peculiarities is the declarative configuration system that allows you to define an "end state" and let the orchestrator figure out the sequence of steps necessary to reach the desired state, without disrupting the stability of the services running on the cluster.</p>
    <p class="normal">The whole idea of Kubernetes configuration revolves around the concept of "objects". An object is an element in your cloud deployment, which can be added, removed, and have its configuration changed over time. Some good examples of Kubernetes objects are:</p>
    <ul>
      <li class="Bullet--PACKT-">Containerized applications</li>
      <li class="Bullet--PACKT-">Resources for the containers (CPU and memory allocations, persistent storage, access to devices such as network interfaces or GPU, and so on)</li>
      <li class="Bullet-End--PACKT-">Policies for the application behavior (restart policies, upgrades, fault-tolerance)</li>
    </ul>
    <p class="normal">A Kubernetes object is a sort of "record of intent", which means that once you create one in a cluster, Kubernetes will constantly monitor (and change, if needed) the state of the object to make sure it stays compliant with the defined expectation.</p>
    <p class="normal">A Kubernetes cluster is generally managed through a <a id="_idIndexMarker1278"/>command-line tool called <code class="Code-In-Text--PACKT-">kubectl</code> (<a href="http://nodejsdp.link/kubectl-install">nodejsdp.link/kubectl-install</a>).</p>
    <p class="normal">There are several ways to create a Kubernetes cluster for development, testing, and production purposes. The easiest way to start experimenting with Kubernetes is through a local single-node cluster, which can be easily<a id="_idIndexMarker1279"/> created by a tool called <code class="Code-In-Text--PACKT-">minikube</code> (<a href="http://nodejsdp.link/minikube-install">nodejsdp.link/minikube-install</a>).</p>
    <p class="normal">Make sure to install both <code class="Code-In-Text--PACKT-">kubectl</code> and <code class="Code-In-Text--PACKT-">minikube</code> on your system, as we will be deploying our sample<a id="_idIndexMarker1280"/> containerized app on a local Kubernetes cluster in the next section!</p>
    <div><p class="Tip--PACKT-">Another great way to learn about Kubernetes is by using the official interactive tutorials (<a href="http://nodejsdp.link/kubernetes-tutorials">nodejsdp.link/kubernetes-tutorials</a>).</p>
    </div>
    <h3 id="_idParaDest-350" class="title">Deploying and scaling an application on Kubernetes</h3>
    <p class="normal">In this section, we<a id="_idIndexMarker1281"/> will be running our simple web server application on a local <code class="Code-In-Text--PACKT-">minikube</code> cluster. So, make sure you have <code class="Code-In-Text--PACKT-">kubectl</code> and <code class="Code-In-Text--PACKT-">minikube</code> correctly installed and started.</p>
    <div><p class="Tip--PACKT-">On macOS and Linux environments, make sure to run <code class="Code-In-Text--PACKT-">minikube start</code> and <code class="Code-In-Text--PACKT-">eval $(minikube docker-env)</code> to initialize the working environment. The second command makes sure that when you use <code class="Code-In-Text--PACKT-">docker</code> and <code class="Code-In-Text--PACKT-">kubectl</code> in your current terminal you will interact with the local Minikube cluster. If you open multiple terminals you should run <code class="Code-In-Text--PACKT-">eval $(minikube docker-env)</code> on every terminal. You can also run <code class="Code-In-Text--PACKT-">minikube dashboard</code> to run a convenient web dashboard that allows you to visualize and interact with all the objects in your cluster.</p>
    </div>
    <p class="normal">The first thing that we want to do is build our Docker image and give it a meaningful name:</p>
    <pre class="programlisting con"><code class="hljs-con">docker build -t hello-web:v1 .
</code></pre>
    <p class="normal">If you have configured your environment correctly, the <code class="Code-In-Text--PACKT-">hello-web</code> image will be available to be used in your local Kubernetes cluster.</p>
    <div><p class="Tip--PACKT-">Using local images is sufficient for local development. When you are ready to go to production, the best option is to publish your images to a Docker container registry such as Docker Hub (<a href="http://nodejsdp.link/docker-hub">nodejsdp.link/docker-hub</a>), Docker Registry (<a href="http://nodejsdp.link/docker-registry">nodejsdp.link/docker-registry</a>), Google Cloud Container Registry (<a href="http://nodejsdp.link/gc-container-registry">nodejsdp.link/gc-container-registry</a>), or Amazon Elastic Container Registry (<a href="http://nodejsdp.link/ecr">nodejsdp.link/ecr</a>). Once you have your images published to a container registry, you can easily deploy your application to different hosts without <a id="_idIndexMarker1282"/>having to rebuild the corresponding images each time.</p>
    </div>
    <h4 class="title">Creating a Kubernetes deployment</h4>
    <p class="normal">Now, in order to run <a id="_idIndexMarker1283"/>an instance of this container in the Minikube cluster, we have to create a <strong class="keyword">deployment</strong> (which is a Kubernetes object) using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">kubectl create deployment hello-web --image=hello-web:v1
</code></pre>
    <p class="normal">This should produce the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">deployment.apps/hello-web created
</code></pre>
    <p class="normal">This command is basically telling Kubernetes to run an instance of the <code class="Code-In-Text--PACKT-">hello-web:v1</code> container as an application called <code class="Code-In-Text--PACKT-">hello-web</code>.</p>
    <p class="normal">You can verify that the deployment is running with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">kubectl get deployments
</code></pre>
    <p class="normal">This should print something like this:</p>
    <pre class="programlisting con"><code class="hljs-con">NAME        READY   UP-TO-DATE   AVAILABLE   AGE
hello-web   1/1     1            1           7s
</code></pre>
    <p class="normal">This table is basically saying that our <code class="Code-In-Text--PACKT-">hello-web</code> deployment is alive and that there is one <strong class="keyword">pod</strong> allocated for it. A pod is a basic unit in Kubernetes and represents a set of containers that have to run together in the same Kubernetes node. Containers in the same pod have shared resources like storage and network. Generally, a pod contains only one container, but it's not uncommon to see more than one container in a pod when these containers are running tightly coupled applications.</p>
    <p class="normal">You can list all the pods running in the cluster with:</p>
    <pre class="programlisting con"><code class="hljs-con">kubectl get pods
</code></pre>
    <p class="normal">This should print something like:</p>
    <pre class="programlisting con"><code class="hljs-con">NAME                         READY   STATUS    RESTARTS   AGE
hello-web-65f47d9997-df7nr   1/1     Running   0          2m19s
</code></pre>
    <p class="normal">Now, in order to be able to access the web server from our local machine, we need to <em class="italic">expose</em> the deployment:</p>
    <pre class="programlisting con"><code class="hljs-con">kubectl expose deployment hello-web --type=LoadBalancer --port=8080
minikube service hello-web
</code></pre>
    <p class="normal">The first command tells Kubernetes to create a <code class="Code-In-Text--PACKT-">LoadBalancer</code> object that exposes the instances of the <code class="Code-In-Text--PACKT-">hello-web</code> app, connecting to port <code class="Code-In-Text--PACKT-">8080</code> of every container.</p>
    <p class="normal">The second command is a <code class="Code-In-Text--PACKT-">minikube</code> helper command that allows us to get the local address to access the load balancer. This command will also open a browser window for you, so now you<a id="_idIndexMarker1284"/> should see the container response in the browser, which should look like this:</p>
    <pre class="programlisting con"><code class="hljs-con">Hello from hello-web-65f47d9997-df7nr (v1)
</code></pre>
    <h4 class="title">Scaling a Kubernetes deployment</h4>
    <p class="normal">Now that our <a id="_idIndexMarker1285"/>application is running and is accessible, let's actually start to experiment with some of the capabilities of Kubernetes. For instance, why not try to scale our application by running five instances instead of just one? This is as easy as running:</p>
    <pre class="programlisting con"><code class="hljs-con">kubectl scale --replicas=5 deployment hello-web
</code></pre>
    <p class="normal">Now, <code class="Code-In-Text--PACKT-">kubectl get deployments</code> should show us the following status:</p>
    <pre class="programlisting con"><code class="hljs-con">NAME        READY   UP-TO-DATE   AVAILABLE   AGE
hello-web   5/5     5            5           9m18s
</code></pre>
    <p class="normal">And <code class="Code-In-Text--PACKT-">kubectl get pods</code> should produce something like this:</p>
    <pre class="programlisting con"><code class="hljs-con">NAME                         READY   STATUS    RESTARTS   AGE
hello-web-65f47d9997-df7nr   1/1     Running   0          9m24s
hello-web-65f47d9997-g98jb   1/1     Running   0          14s
hello-web-65f47d9997-hbdkx   1/1     Running   0          14s
hello-web-65f47d9997-jnfd7   1/1     Running   0          14s
hello-web-65f47d9997-s54g6   1/1     Running   0          14s
</code></pre>
    <p class="normal">If you try to hit the load balancer now, chances are you will see different hostnames as the traffic gets distributed across the available instances. This should be even more apparent if you try to hit the load balancer while putting the application under stress, for instance, by <a id="_idIndexMarker1286"/>running an <code class="Code-In-Text--PACKT-">autocannon</code> load test against the load balancer URL.</p>
    <h4 class="title">Kubernetes rollouts</h4>
    <p class="normal">Now, let's try out another feature of Kubernetes: rollouts. What if we want to release a new version of our app?</p>
    <p class="normal">We can set <code class="Code-In-Text--PACKT-">const version = 2</code> in our <code class="Code-In-Text--PACKT-">app.js</code> file and create a new image:</p>
    <pre class="programlisting con"><code class="hljs-con">docker build -t hello-web:v2 .
</code></pre>
    <p class="normal">At this point, in order to upgrade all the running pods to this new version, we have to run the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">kubectl set image deployment/hello-web hello-web=hello-web:v2 --record
</code></pre>
    <p class="normal">The output of this command should be as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">deployment.apps/hello-web image updated
</code></pre>
    <p class="normal">If everything worked as expected, you should now be able to refresh your browser page and see something like the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Hello from hello-web-567b986bfb-qjvfw (<strong class="hljs-con-slc">v2</strong>)
</code></pre>
    <p class="normal">Note the <strong class="codeHighlighted">v2</strong> flag there.</p>
    <p class="normal">What just happened behind the scenes is that Kubernetes started to roll out the new version of our image by replacing the containers one by one. When a container is replaced, the running instance is stopped gracefully. This way requests that are currently in progress can be completed before the container is shut down.</p>
    <p class="normal">This completes our mini Kubernetes tutorial. The lesson here is that, when using a container orchestrator platform like Kubernetes, we can keep our application code quite simple, as we won't have to include concerns such as scaling to multiple instances or deal with soft rollouts and application restarts. This is the major advantage of this approach.</p>
    <p class="normal">Of course, this simplicity does not come for free. It is paid by having to learn and manage the orchestration platform. If you are running small applications in production, it is probably not worth to incur the complexity and the cost of having to install and manage a container orchestrator platform like Kubernetes. However, if you are serving millions of users every day, there is definitely a lot of value in building and maintaining such a powerful infrastructure.</p>
    <p class="normal">Another interesting observation is that, when running containers in Kubernetes, containers are often considered "disposable," which basically means that they could be killed and restarted at any time. While this might seem like a non-relevant detail, you should actually take this behavior into account and try to keep your applications as stateless as possible. In fact, containers, by default, won't retain any change in the local filesystem, so every time you have to store some persistent information, you will have to rely on external storage mechanisms such as databases or persistent volumes.</p>
    <div><p class="Tip--PACKT-">If you want to clean up your system from the containers you just ran in the preceding examples and stop <code class="Code-In-Text--PACKT-">minikube</code>, you can do so with the following commands:</p>
      <pre class="programlisting con"><code class="hljs-con">kubectl scale --replicas=0 deployment hello-web
kubectl delete -n default service hello-web
minikube stop
</code></pre>
    </div>
    <p class="normal">In the next and last part of this chapter, we will explore some interesting patterns to decompose a<a id="_idIndexMarker1287"/> monolithic application into a set of decoupled microservices, something that is critically important if you have built a monolithic application and are now suffering from scalability issues.</p>
    <h1 id="_idParaDest-351" class="title">Decomposing complex applications</h1>
    <p class="normal">So far in this chapter, we have mainly focused our analysis on the <em class="italic">X</em>-axis of the scale cube. We saw <a id="_idIndexMarker1288"/>how it represents the easiest and most immediate way to distribute the load and scale an application, also improving its availability. In the following section, we are going to focus on the <em class="italic">Y</em>-axis of the scale cube, where applications are scaled by <strong class="keyword">decomposing</strong> them by functionality and service. As we will learn, this technique allows us to scale not only the capacity of an application, but also, and most importantly, its complexity.</p>
    <h2 id="_idParaDest-352" class="title">Monolithic architecture</h2>
    <p class="normal">The term monolithic might make us think of a system without modularity, where all the services of an<a id="_idIndexMarker1289"/> application are interconnected and almost indistinguishable. However, this is not always the case. Often, monolithic systems have a highly modular architecture and a good level of decoupling between their internal components.</p>
    <p class="normal">A perfect example is the Linux OS kernel, which is part of a category called <strong class="keyword">monolithic kernels</strong> (in perfect opposition with its<a id="_idIndexMarker1290"/> ecosystem and the Unix philosophy). Linux has thousands of services and modules that we can load and unload dynamically, even while the system is running. However, they all run in kernel mode, which means that a failure in any of them could bring the entire OS down (have you ever seen a kernel panic?). This approach is opposite to the microkernel architecture, where only the core services of the operating system run in kernel mode, while the rest run in user mode, usually each one with its own process. The main advantage of this approach is that a<a id="_idIndexMarker1291"/> problem in any of these services would more likely cause it to crash in isolation, instead of affecting the stability of the entire system.</p>
    <div><p class="Tip--PACKT-">The Torvalds-Tanenbaum debate on kernel design is probably one of the most famous <em class="italic">flame wars</em> in the history of computer science, where one of the main points of dispute was exactly monolithic versus microkernel design. You can find a web version of the discussion (it originally appeared on Usenet) at <a href="http://nodejsdp.link/torvalds-tanenbaum">nodejsdp.link/torvalds-tanenbaum</a>.</p>
    </div>
    <p class="normal">It's remarkable how these design principles, which are more than 30 years old, can still be applied today and in totally different environments. Modern monolithic applications are comparable to monolithic kernels: if any of their components fail, the entire system is affected, which, translated into Node.js terms, means that all the services are part of the same code base and run in a single process (when not cloned).</p>
    <p class="normal"><em class="italic">Figure 12.9</em> shows an example monolithic architecture:</p>
    <figure class="mediaobject"><img src="img/B15729_12_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.9: Example of a monolithic architecture</p>
    <p class="normal"><em class="italic">Figure 12.9</em> shows the architecture of a typical e-commerce application. Its structure is modular: we have two different frontends, one for the main store and another for the administration interface. Internally, we have a clear separation of the services implemented by the application. Each service is responsible for a specific portion of the application business logic: <strong class="keyword">Products</strong>, <strong class="keyword">Cart</strong>, <strong class="keyword">Checkout</strong>, <strong class="keyword">Search</strong>, and <strong class="keyword">Authentication and Users</strong>. However, the preceding architecture is monolithic since every module is part of the same codebase and runs as part of a single application. A failure in any of its components can potentially tear down the entire online store.</p>
    <p class="normal">Another problem with this type of architecture is the interconnection between its modules; the fact that they all live inside the same application makes it very easy for a developer to build interactions and coupling between modules. For example, consider the use case of when <a id="_idIndexMarker1292"/>a product is being purchased: the <strong class="keyword">Checkout</strong> module has to update the availability of a <strong class="keyword">Product</strong> object, and if those two modules are in the same application, it's too easy for a developer to just obtain a reference to a <strong class="keyword">Product</strong> object and update its availability directly. Maintaining a low coupling between internal modules is very hard in a monolithic application, partly because the boundaries between them are not always clear or properly enforced.</p>
    <p class="normal">A <strong class="keyword">high coupling</strong> is often one<a id="_idIndexMarker1293"/> of the main obstacles to the growth of an application and prevents its scalability in terms of complexity. In fact, an intricate dependency graph means that every part of the system is a liability, it has to be maintained for the entire life of the product, and any change should be carefully evaluated because every component is like a wooden block in a Jenga tower: moving or removing one of them can cause the entire tower to collapse. This often results in building conventions and development processes to cope with the increasing complexity of the project.</p>
    <h2 id="_idParaDest-353" class="title">The microservice architecture</h2>
    <p class="normal">Now, we are going to reveal the most important pattern in Node.js for writing big applications: avoid<a id="_idIndexMarker1294"/> writing big applications. This seems like a trivial statement, but it's an incredibly effective strategy to scale both the complexity and the capacity of a software system. So, what's the alternative to writing big applications? The answer is in the <em class="italic">Y</em>-axis of the scale cube: decomposition and splitting by service and functionality. The idea is to break down an application into its essential components, creating separate, independent applications. It is practically the opposite of a monolithic architecture. This fits perfectly with the Unix philosophy and the Node.js principles we discussed at the beginning of the book; in particular, the motto "make each program do one thing well."</p>
    <p class="normal"><strong class="keyword">Microservice architecture</strong> is, today, the main reference pattern for this type of approach, where a set of self-sufficient services replace big monolithic applications. The prefix "micro" means that the services should be as small as possible, but always within reasonable limits. Don't<a id="_idIndexMarker1295"/> be misled by thinking that creating an architecture with a hundred different applications exposing only one web service is necessarily a good choice. In reality, there is no strict rule on how small or big a service <a id="_idIndexMarker1296"/>should be. It's not the size that<a id="_idIndexMarker1297"/> matters in the design of a microservice <a id="_idIndexMarker1298"/>architecture; instead, it's a combination of different factors, mainly <strong class="keyword">loose coupling</strong>, <strong class="keyword">high cohesion</strong>, and <strong class="keyword">integration complexity</strong>.</p>
    <h3 id="_idParaDest-354" class="title">An example of a microservice architecture</h3>
    <p class="normal">Let's now see what the<a id="_idIndexMarker1299"/> monolithic e-commerce application would look like using a microservice architecture:</p>
    <figure class="mediaobject"><img src="img/B15729_12_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.10: An example implementation of an e-commerce system using the Microservice pattern</p>
    <p class="normal">As we can see from <em class="italic">Figure 12.10</em>, each fundamental component of the e-commerce application is<a id="_idIndexMarker1300"/> now a self-sustaining and independent entity, living in its own context, with its own database. In practice, they are all independent applications exposing a set of related services.</p>
    <p class="normal">The <strong class="keyword">data ownership</strong> of a service<a id="_idIndexMarker1301"/> is an important characteristic of the microservice architecture. This is why the database also has to be split to maintain the proper level of isolation and independence. If a unique shared database is used, it would become much easier for the services to work together; however, this would also introduce a coupling between the services (based on data), nullifying some of the advantages of having different applications.</p>
    <p class="normal">The dashed lines connecting all the nodes tells us that, in some way, they have to communicate and exchange information for the entire system to be fully functional. As the services do not share the same database, there is more communication involved to maintain the<a id="_idIndexMarker1302"/> consistency of the whole system. For example, the <strong class="keyword">Checkout</strong> service needs to know some information about <strong class="keyword">Products</strong>, such as the price and restrictions on shipping, and at the same time, it needs to update the data stored in the <strong class="keyword">Products</strong> service such as the product's availability when the checkout is complete. In <em class="italic">Figure 12.10</em>, we tried to represent the way the nodes communicate generic. Surely, the most popular strategy is using web services, but as we will see later, this is not the only option.</p>
    <div><p class="Information-Box--PACKT-"><strong class="screenText">Pattern (microservice architecture)</strong></p>
      <p class="Information-Box--PACKT-">Split a complex application by creating several small, self-contained services.</p>
    </div>
    <h3 id="_idParaDest-355" class="title">Microservices – advantages and disadvantages</h3>
    <p class="normal">In this section, we are going to highlight some of the advantages and disadvantages of implementing <a id="_idIndexMarker1303"/>a microservice architecture. As we will see, this<a id="_idIndexMarker1304"/> approach promises to bring a radical change in the way we develop our applications, revolutionizing the way we see scalability and complexity, but on the other hand, it introduces new nontrivial challenges.</p>
    <div><p class="Tip--PACKT-">Martin Fowler wrote a great article about microservices that you can find at <a href="http://nodejsdp.link/microservices">nodejsdp.link/microservices</a>.</p>
    </div>
    <h4 class="title">Every service is expendable</h4>
    <p class="normal">The main technical advantage of having each service living in its own application context is that crashes do not propagate to the entire system. The goal is to build truly independent services that are smaller, easier to change, or can even be rebuilt from scratch. If, for example, the <strong class="keyword">Checkout</strong> service of our e-commerce application suddenly crashes because of a serious bug, the rest of the system would continue to work as normal. Some functionality may be affected; for example, the ability to purchase a product, but the rest of the system would continue to work.</p>
    <p class="normal">Also, imagine if we suddenly realized that the database or the programming language we used to implement a component was not a good design decision. In a monolithic application, there would be very little we could do to change things without affecting the entire system. Instead, in a microservice architecture, we could more easily reimplement the entire service from scratch, using a different database or platform, and the rest of the system would not even notice it, as long as the new implementation maintains the<a id="_idIndexMarker1305"/> same interface to the rest of the system.</p>
    <h4 class="title">Reusability across platforms and languages</h4>
    <p class="normal">Splitting a big <a id="_idIndexMarker1306"/>monolithic application into many small services allows us to create independent units that can be reused much more easily. <strong class="keyword">Elasticsearch</strong> (<a href="http://nodejsdp.link/elasticsearch">nodejsdp.link/elasticsearch</a>) is a great example of a reusable search <a id="_idIndexMarker1307"/>service. <strong class="keyword">ORY</strong> (<a href="http://nodejsdp.link/ory">nodejsdp.link/ory</a>) is another example of a reusable open source<a id="_idIndexMarker1308"/> technology that provides a complete authentication and authorization service that can be easily integrated into a microservice architecture.</p>
    <p class="normal">The main advantage of the microservice approach is that the level of information hiding is usually much higher compared to monolithic applications. This is possible because the interactions usually happen through a remote interface such as a web API or a message broker, which makes it much easier to hide implementation details and shield the client from changes in the way the service is implemented or deployed. For example, if all we have to do is invoke a web service, we are shielded from the way the infrastructure behind is scaled, from what programming language it uses, from what database it uses to store its data, and so on. All these decisions can be revisited and adjusted as needed, with potentially no impact on the rest of the system.</p>
    <h4 class="title">A way to scale the application</h4>
    <p class="normal">Going back to the scale cube, it's clear that microservices are equivalent to scaling an application<a id="_idIndexMarker1309"/> along the <em class="italic">Y</em>-axis, so it's already a solution for distributing the load across multiple machines. Also, we should not forget that we can combine microservices with the other two dimensions of the cube to scale the application even further. For example, each service could be cloned to handle more traffic, and the interesting aspect is that they can be scaled independently, allowing better resource management.</p>
    <p class="normal">At this point, it would look like microservices are the solution to all our problems. However, this is far from being true. Let's see the challenges we face using microservices.</p>
    <h4 class="title">The challenges of microservices</h4>
    <p class="normal">Having more nodes to<a id="_idIndexMarker1310"/> manage introduces a higher complexity in terms of integration, deployment, and code sharing: it fixes some of the pains of traditional architectures, but it also opens up many new questions. How do we make the services interact? How can we keep sanity with deploying, scaling, and monitoring such a high number of applications? How can we share and reuse code between services?</p>
    <p class="normal">Fortunately, cloud services and modern DevOps methodologies can provide some answers to those questions, and also, using Node.js can help a lot. Its module system is a perfect companion to share code between different projects. Node.js was made to be a node in a distributed <a id="_idIndexMarker1311"/>system such as those of a microservice architecture.</p>
    <p class="normal">In the following sections, we will introduce some integration patterns that can help with managing and integrating services in a microservice architecture.</p>
    <h2 id="_idParaDest-356" class="title">Integration patterns in a microservice architecture</h2>
    <p class="normal">One of the toughest challenges of microservices is connecting all the nodes to make them <a id="_idIndexMarker1312"/>collaborate. For example, the <strong class="keyword">Cart</strong> service of our e-commerce application would make little sense without some <strong class="keyword">Products</strong> to add, and the <strong class="keyword">Checkout</strong> service would be useless without a list of products to buy (a cart). As we already mentioned, there are also other factors that necessitate an interaction between the various services. For example, the <strong class="keyword">Search</strong> service has to know which <strong class="keyword">Products</strong> are available and must also ensure it keeps its information up to date. The same can be said about the <strong class="keyword">Checkout</strong> service, which has to update the information about <strong class="keyword">Product</strong> availability when a purchase is completed.</p>
    <p class="normal">When designing an integration strategy, it's also important to consider the coupling that it's going to introduce between the services in the system. We should not forget that designing a distributed architecture involves the same practices and principles we use locally when designing a module or subsystem. Therefore, we also need to take into consideration properties such as the reusability and extensibility of the service.</p>
    <h3 id="_idParaDest-357" class="title">The API proxy</h3>
    <p class="normal">The first pattern we are going to show<a id="_idIndexMarker1313"/> makes use of an <strong class="keyword">API proxy</strong> (also commonly identified <a id="_idIndexMarker1314"/>as an <strong class="keyword">API gateway</strong>), a server that proxies the communications between a client and a set of remote APIs. In a microservice architecture, its main purpose is to provide a single access point for multiple API endpoints, but it can also offer load balancing, caching, authentication, and traffic limiting, all of which are features that prove to be very useful to implement a solid API solution.</p>
    <p class="normal">This pattern should not be new to us since we already saw it in action in this chapter when we built the custom load balancer with <code class="Code-In-Text--PACKT-">http-proxy</code> and <code class="Code-In-Text--PACKT-">consul</code>. For that example, our load balancer was exposing only two services, and then, thanks to a service registry, it was able to map a URL path to a service and hence to a list of servers. An API proxy works in the same way; it is essentially a reverse proxy and often also a load balancer, specifically configured to<a id="_idIndexMarker1315"/> handle API requests. <em class="italic">Figure 12.11</em> shows how we can apply such a <a id="_idIndexMarker1316"/>solution to our e-commerce application:</p>
    <figure class="mediaobject"><img src="img/B15729_12_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.11: Using the API Proxy pattern in an e-commerce application</p>
    <p class="normal">From the preceding diagram, it should be clear how an API proxy can hide the complexity of its underlying infrastructure. This is really handy in a microservice infrastructure, as the number of nodes may be high, especially if each service is scaled across multiple machines. The integration achieved by an API proxy is therefore only structural since there is no semantic mechanism. It simply provides a familiar monolithic view of a complex microservice infrastructure.</p>
    <p class="normal">Since the API Proxy pattern essentially abstracts the complexity of connecting to all the different APIs in the system, it might also allow for some freedom to restructure the various services. Maybe, as your requirements change, you will need to split an existing microservice into two or more decoupled microservices or, conversely, you might realize that, in your business context, it's better to join two or more services together. In both cases, the API Proxy pattern will allow you to make all the necessary changes with potentially no<a id="_idIndexMarker1317"/> impact on the upstream systems<a id="_idIndexMarker1318"/> accessing the data through the proxy.</p>
    <div><p class="Tip--PACKT-">The ability to enable incremental change in an architecture over time is a very important characteristic in modern distributed systems. If you are interested in studying this broad subject in greater depth, we recommend the book <em class="italic">Building Evolutionary Architectures</em>: <a href="http://nodejsdp.link/evolutionary-architectures">nodejsdp.link/evolutionary-architectures</a>.</p>
    </div>
    <h3 id="_idParaDest-358" class="title">API orchestration</h3>
    <p class="normal">The pattern <a id="_idIndexMarker1319"/>we are going to <a id="_idIndexMarker1320"/>describe next is probably the most natural and explicit way to integrate and compose a set of services, and it's called <strong class="keyword">API orchestration</strong>. Daniel Jacobson, VP of Engineering for the Netflix API, in one of his blog posts (<a href="http://nodejsdp.link/orchestration-layer">nodejsdp.link/orchestration-layer</a>), defines API<a id="_idIndexMarker1321"/> orchestration as follows:</p>
    <blockquote class="packt_quote"><p>"An API <strong class="keyword">Orchestration Layer</strong> (<strong class="keyword">OL</strong>) is an abstraction layer that takes generically-modeled data elements and/or features and prepares them in a more specific way for a targeted developer or application."</p></blockquote>
    <p class="normal">The "generically modeled elements and/or features" fit the description of a service in a microservice architecture perfectly. The idea is to create an abstraction to connect those bits and pieces to implement new services specific to a particular application.</p>
    <p class="normal">Let's see an example using the e-commerce application. Refer to <em class="italic">Figure 12.12</em>:</p>
    <figure class="mediaobject"><img src="img/B15729_12_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.12: An example usage of an orchestration layer to interact with multiple microservices</p>
    <p class="normal"><em class="italic">Figure 12.12</em> shows how the <strong class="keyword">Store frontend</strong> application uses an orchestration layer to build more<a id="_idIndexMarker1322"/> complex and specific<a id="_idIndexMarker1323"/> features by composing and orchestrating existing services. The described scenario takes, as an example, a hypothetical <code class="Code-In-Text--PACKT-">completeCheckout()</code> service that is invoked the moment a customer clicks the <strong class="keyword">Pay</strong> button at the end of the checkout. </p>
    <p class="normal">The figure shows how <code class="Code-In-Text--PACKT-">completeCheckout()</code> is a composite operation made of three different steps:</p>
    <ol>
      <li class="numbered">First, we complete the transaction by invoking <code class="Code-In-Text--PACKT-">checkoutService/pay</code>.</li>
      <li class="numbered">Then, when the payment is successfully processed, we need to tell the <strong class="keyword">Cart</strong> service that the items were purchased and that they can be removed from the cart. We do that by invoking <code class="Code-In-Text--PACKT-">cartService/delete</code>.</li>
      <li class="numbered">Also, when the payment is complete, we need to update the availability of the products that were just purchased. This is done through <code class="Code-In-Text--PACKT-">productsService/update</code>.</li>
    </ol>
    <p class="normal">As we can see, we took three operations from three different services and we built a new API that coordinates the services to maintain the entire system in a consistent state.</p>
    <p class="normal">Another common operation performed by the <strong class="keyword">API Orchestration Layer</strong> is <strong class="keyword">data aggregation</strong>, or in other words, combining data from different services into a single response. Imagine we wanted to list all the products contained in a cart. In this case, the orchestration <a id="_idIndexMarker1324"/>would need to retrieve the list of product IDs from the <strong class="keyword">Cart</strong> service, and then retrieve the complete<a id="_idIndexMarker1325"/> information about the products from the <strong class="keyword">Products</strong> service. The ways in which we can combine and coordinate services is infinite, but the important pattern to remember is the role of the orchestration layer, which acts as an abstraction between a number of services and a specific application.</p>
    <p class="normal">The orchestration layer is a great candidate for a further functional splitting. It is, in fact, very common to have it implemented as a dedicated, independent service, in which case it takes the name of <strong class="keyword">API Orchestrator</strong>. This practice is perfectly in line with the microservice philosophy.</p>
    <p class="normal"><em class="italic">Figure 12.13</em> shows this further improvement of our architecture:</p>
    <figure class="mediaobject"><img src="img/B15729_12_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.13: An application of the API Orchestrator pattern for our e-commerce example</p>
    <p class="normal">Creating a standalone orchestrator, as shown in the previous figure, can help in decoupling<a id="_idIndexMarker1326"/> the client application (in our case, the <strong class="keyword">Store frontend</strong>) from<a id="_idIndexMarker1327"/> the complexity of the microservice<a id="_idIndexMarker1328"/> infrastructure. This is similar to the API proxy, but there is a crucial difference: an orchestrator performs a <em class="italic">semantic</em> integration of the various services, it's not just a naïve proxy, and it often exposes an API that is different from the one exposed by the underlying services.</p>
    <h3 id="_idParaDest-359" class="title">Integration with a message broker</h3>
    <p class="normal">The Orchestrator <a id="_idIndexMarker1329"/>pattern <a id="_idIndexMarker1330"/>gave us a mechanism to integrate the various services in an explicit way. This has both advantages and disadvantages. It is easy to design, easy to debug, and easy to scale, but unfortunately, it has to have a complete knowledge of the underlying architecture and how each service works. If we were talking about objects instead of architectural nodes, the orchestrator would be an<a id="_idIndexMarker1331"/> anti-pattern called <strong class="keyword">God object</strong>, which defines an object that knows and does too much, which usually results in high coupling, low cohesion, but most importantly, high complexity.</p>
    <p class="normal">The pattern we are now going to show tries to distribute, across the services, the responsibility of synchronizing the information of the entire system. However, the last thing we want to do is create direct relationships between services, which would result in high coupling and a further increase in the complexity of the system, due to the increasing number of interconnections between nodes. The goal is to keep every service decoupled: every<a id="_idIndexMarker1332"/> service<a id="_idIndexMarker1333"/> should be able to work, even without the rest of the services in the system or in combination with new services and nodes.</p>
    <p class="normal">The solution is to use a message broker, a system capable of decoupling the sender from the receiver of a message, allowing us to implement a Centralized Publish/Subscribe pattern. This is, in practice, an implementation of the Observer pattern for distributed systems. We will talk more about this pattern later in <em class="chapterRef">Chapter 13</em>, <em class="italic">Messaging and Integration Patterns</em>. <em class="italic">Figure 12.14</em> shows an example of how this applies to the e-commerce application:</p>
    <figure class="mediaobject"><img src="img/B15729_12_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.14: Using a message broker to distribute events in our e-commerce application</p>
    <p class="normal">As we can see from <em class="italic">Figure 12.14</em>, the client of the <strong class="keyword">Checkout</strong> service, which is the frontend application, does not need to carry out any explicit integration with the other services. </p>
    <p class="normal">All it<a id="_idIndexMarker1334"/> has to do<a id="_idIndexMarker1335"/> is invoke <code class="Code-In-Text--PACKT-">checkoutService/pay</code> to complete the checkout process and take the money from the customer; all the integration work happens in the background:</p>
    <ol>
      <li class="numbered">The <strong class="keyword">Store frontend</strong> invokes the <code class="Code-In-Text--PACKT-">checkoutService/pay</code> operation on the <strong class="keyword">Checkout</strong> service.</li>
      <li class="numbered">When the operation completes, the <strong class="keyword">Checkout</strong> service generates an event, attaching the details of the operation, that is, the <code class="Code-In-Text--PACKT-">cartId</code> and the list of <code class="Code-In-Text--PACKT-">products</code> that were just purchased. The event is published into the message broker. At this point, the <strong class="keyword">Checkout</strong> service does not know who is going to receive the message.</li>
      <li class="numbered">The <strong class="keyword">Cart</strong> service is subscribed to the broker, so it's going to receive the <code class="Code-In-Text--PACKT-">purchased</code> event that was just published by the <strong class="keyword">Checkout</strong> service. The <strong class="keyword">Cart</strong> service reacts by removing the cart identified with the ID contained in the message from its database.</li>
      <li class="numbered">The <strong class="keyword">Products</strong> service was subscribed to the message broker as well, so it receives the same <code class="Code-In-Text--PACKT-">purchased</code> event. It then updates its database based on this new information, adjusting the availability of the products included in the message.</li>
    </ol>
    <p class="normal">This whole process happens without any explicit intervention from external entities such as an orchestrator. The responsibility of spreading the knowledge and keeping information in sync is distributed across the services themselves. There is no <em class="italic">god</em> service that has to know how to move the gears of the entire system, since each service is in charge of its own<a id="_idIndexMarker1336"/> part of<a id="_idIndexMarker1337"/> the integration.</p>
    <p class="normal">The message broker is a fundamental element used to decouple the services and reduce the complexity of their interaction. It might also offer other interesting features, such as persistent message queues and guaranteed ordering of the messages. We will talk more about this in the next chapter.</p>
    <h1 id="_idParaDest-360" class="title">Summary</h1>
    <p class="normal">In this chapter, we learned how to design Node.js architectures that scale both in capacity and complexity. We saw how scaling an application is not only about handling more traffic or reducing the response time, but it's also a practice to apply whenever we want better availability and tolerance to failures. We saw how these properties often are on the same wavelength, and we understood that scaling early is not a bad practice, especially in Node.js, which allows us to do it easily and with few resources.</p>
    <p class="normal">The scale cube taught us that applications can be scaled across three dimensions. Throughout this chapter, we focused on the two most important dimensions, the <em class="italic">X</em>-and <em class="italic">Y</em>-axes, allowing us to discover two essential architectural patterns, namely, load balancing and microservices. You should now know how to start multiple instances of the same Node.js application, how to distribute traffic across them, and how to exploit this setup for other purposes, such as fail tolerance and zero-downtime restarts. We also analyzed how to handle the problem of dynamic and auto-scaled infrastructures. With this, we saw that a service registry can really come in useful for those situations. We learned how to achieve these goals by using plain Node.js, external load balancers like Nginx, and service discovery systems like Consul. We also learned the basics of Kubernetes.</p>
    <p class="normal">At this point, we should have got to grips with some very practical approaches to be able to face scalability much more fearlessly then before.</p>
    <p class="normal">However, cloning and load balancing cover only one dimension of the scale cube, so we moved our analysis to another dimension, studying in more detail what it means to split an application by its constituent services by building a microservice architecture. We saw how microservices enable a complete revolution in how a project is developed and managed, providing a natural way to distribute the load of an application and split its complexity. However, we learned that this also means shifting the complexity from <em class="italic">how to build a big monolithic application</em> to <em class="italic">how to integrate a set of services</em>. This last aspect is where we focused the last part of our analysis, showing some of the architectural solutions to integrate a set of independent services.</p>
    <p class="normal">In the next and last chapter of this book, we will have the chance to complete our <em class="italic">Node.js Design Patterns</em> journey by analyzing the messaging patterns we discussed in this chapter, in addition to more advanced integration techniques that are useful when implementing complex distributed architectures.</p>
    <h1 id="_idParaDest-361" class="title">Exercises</h1>
    <ul>
      <li class="Bullet--PACKT-"><strong class="keyword">12.1 A scalable book library</strong>: Revisit the book library application we built in <em class="chapterRef">Chapter 10</em>, <em class="italic">Universal JavaScript for Web Applications</em>, reconsidering it after what we learned in this chapter. Can you make our original implementation more scalable? Some ideas might be to use the <code class="Code-In-Text--PACKT-">cluster</code> module to run multiple instances of the server, making sure you handle failures by restarting workers that might accidentally die. Alternatively, why not try to run the entire application on Kubernetes?</li>
      <li class="Bullet--PACKT-"><strong class="keyword">12.2 Exploring the </strong><strong class="bold-italic">Z</strong><strong class="keyword">-axis</strong>: Throughout this chapter, we did not show you any examples about how to shard data across multiple instances, but we explored all the necessary patterns to build an application that achieves scalability along the <em class="italic">Z</em>-axis of the scale cube. In this exercise, you are challenged to build a REST API that allows you to get a list of (randomly generated) people whose first name starts with a given letter. You could use a library like <code class="Code-In-Text--PACKT-">faker</code> (<a href="http://nodejsdp.link/faker">nodejsdp.link/faker</a>) to generate a sample of random people, and then you could store this data in different JSON files (or different databases), splitting the data into three different groups. For instance, you might have three groups called A-D, E-P, and Q-Z. <em class="italic">Ada</em> will go in the first group, <em class="italic">Peter</em> in the second, and <em class="italic">Ugo</em> in the third. Now, you can run one or more instances of a web server for every group, but you should expose only one public API endpoint to be able to retrieve all the people whose names starts with a given letter (for instance, <code class="Code-In-Text--PACKT-">/api/people/byFirstName/{letter}</code>). Hint: You could use just a load balancer and map all the possible letters to the respective backend of the instances that are responsible for the associated group. Alternatively, you could create an API orchestration layer that encodes the mapping logic and redirects the traffic accordingly. Can you also throw a service discovery tool into the mix and apply dynamic load balancing, so that groups receiving more traffic can scale as needed?</li>
      <li class="Bullet-End--PACKT-"><strong class="keyword">12.3 Music addiction</strong>: Imagine you have to design the architecture of a service like Spotify or Apple Music. Can you try to design this service as a collection of microservices by applying some of the principles discussed in this chapter? Bonus points if you can actually implement a minimal version of this idea with Node.js! If this turns out to be the next big startup idea and makes you a millionaire, well… don't forget to thank the authors of this book. :)</li>
    </ul>
  </div>
</body></html>