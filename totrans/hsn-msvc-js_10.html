<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-159"><a id="_idTextAnchor160"/>10</h1>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor161"/>Monitoring Microservices</h1>
			<p><strong class="bold">Microservices</strong> have<a id="_idIndexMarker716"/> become a core architectural approach for building scalable and flexible applications, but ensuring their health and performance is just as important as their functionality. Without proper visibility, identifying issues in such a distributed system can be like trying to find a needle in a haystack. Think of monitoring and logging as placing cameras and sensors in different parts of a bustling city, where each microservice is a shop. These tools help you observe how the system is functioning, capture key events, and detect any unusual behavior. By establishing robust logging and monitoring practices, you can quickly pinpoint problems and keep your microservices running smoothly.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Importance of observability</li>
				<li>Introduction to logging</li>
				<li>Centralized logging with the <strong class="bold">Elasticsearch, Logstash, and Kibana</strong> (<strong class="bold">ELK</strong>) stack</li>
			</ul>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor162"/>Technical requirements</h1>
			<p>To follow along with this chapter, we need to have an IDE installed (we prefer Visual Studio Code), Postman, Docker, and a browser of your choice.</p>
			<p>It is preferable to download our repository from <a href="https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10">https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10</a> to easily follow our code snippets.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor163"/>Importance of observability</h1>
			<p>In the world of <a id="_idIndexMarker717"/>software, particularly microservices, <strong class="bold">observability</strong> is crucial. It allows us to gain deep insights into how our system functions by analyzing its outputs. Observability is an important concept in monitoring and understanding systems. It refers to the ability to gain insight into the internal workings of a system by examining its outputs. Let’s try to understand the building blocks of it:</p>
			<ul>
				<li><strong class="bold">Logs</strong>: Logs are detailed records of events that happen within a system. They provide a history of what has occurred, including errors, warnings, and informational messages. Logs can help in identifying and diagnosing issues by showing a step-by-step account of system activities.</li>
				<li><strong class="bold">Metrics</strong>: Metrics are numerical values that represent the performance and behavior of a system. They can include data such as CPU usage, memory consumption, request rates, and error rates. Metrics provide a quantitative measure of the system’s health and performance.</li>
				<li><strong class="bold">Alerts</strong>: Alerts are notifications that are triggered when metrics reach certain thresholds. They are used to inform administrators or operators about potential problems or abnormal behavior in real time, allowing for quick responses to issues.</li>
				<li><strong class="bold">Traces</strong>: Traces provide a detailed view of the flow of requests through a system. They show how requests move from one component to another, highlighting the interactions and dependencies between different parts of the system. Traces help in understanding the path of a request and identifying bottlenecks or points of failure.</li>
			</ul>
			<p>Observability helps in <a id="_idIndexMarker718"/>understanding what is happening inside a system by using logs, metrics, alerts, and traces. Logs give detailed records of events, metrics provide numerical data on performance, alerts notify of potential problems, and traces show the flow of requests. Together, these outputs offer a comprehensive view of a system’s state, aiding in monitoring, troubleshooting, and optimizing performance.</p>
			<p>Now that we’ve covered the concept, let’s dive into the world of logging in microservices.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor164"/>Introduction to logging</h1>
			<p>Have you ever driven a <a id="_idIndexMarker719"/>car with a broken dashboard? The speedometer might be stuck, the fuel gauge unreliable, and warning lights might flicker mysteriously. Without clear information about how the engine is running, it’s difficult to diagnose problems or ensure a safe journey.</p>
			<p>In the world of software, particularly complex systems built with microservices, logging plays a similar role. <strong class="bold">Logs</strong> are detailed<a id="_idIndexMarker720"/> records of events and activities within a system.</p>
			<p>When building your microservices, just thinking about business implementations is not enough. Microservices are, by nature, complex, with many independent services interacting. Logging helps understand individual service behavior and pinpoint issues within a specific service. When things go wrong, logs provide the audit trail to diagnose and fix problems. They help identify errors, dropped requests, or performance bottlenecks. Every microservice application should have a proper logging mechanism.</p>
			<p>Logging microservices is essential for diagnostics, but it comes with challenges such as handling high volumes of distributed logs across different machines and languages, making it harder to aggregate and interpret them. Additionally, missing key details and ensuring sensitive information in logs is securely stored add complexity to managing logs effectively.</p>
			<p>By understanding these challenges, we can implement effective logging strategies to keep our microservices teams talking and our systems running smoothly.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor165"/>Logging levels and node libraries</h2>
			<p>Before practical <a id="_idIndexMarker721"/>examples, we need to understand some basics related to logging and one of<a id="_idIndexMarker722"/> them is <strong class="bold">log levels</strong>. Different log levels are used to categorize the severity or importance of log messages.</p>
			<p><strong class="bold">Error logs</strong> capture<a id="_idIndexMarker723"/> critical issues that need immediate attention, such as crashes or system failures, while <strong class="bold">warning logs</strong> highlight <a id="_idIndexMarker724"/>potential problems that may need investigation. <strong class="bold">Info logs</strong> track <a id="_idIndexMarker725"/>general system operations, <strong class="bold">debug logs</strong> provide <a id="_idIndexMarker726"/>detailed diagnostic information, and <strong class="bold">trace logs</strong> offer the <a id="_idIndexMarker727"/>most granular level of logging for tracking execution flow.</p>
			<p>Of course, you don’t need to implement logging algorithms from scratch. One of the beauties of Node.Js is it provides a cool collection of libraries for us to use. We have different popular log libraries to integrate and use when we build our microservices. You can use <code>winston</code>, <code>pino</code>, <code>morgan</code> (log middleware for  Express.js), <code>bunyan</code>, <code>log4js</code>, and so on when logging your microservices. We will integrate <code>winston</code> and <code>morgan</code> as a logging library for<a id="_idIndexMarker728"/> the current chapter but it is up to you to select one of them.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor166"/>Log formats</h2>
			<p>In Node.js microservices, logging <a id="_idIndexMarker729"/>formats can be categorized into unstructured logging, structured logging, and semi-structured logging. Here is an explanation of each:</p>
			<ul>
				<li><strong class="bold">Unstructured logging</strong>: Unstructured logging<a id="_idIndexMarker730"/> involves writing plain text log messages. This format is straightforward but can be harder to parse and analyze programmatically. Here is an example showing unstructured logging:<pre class="source-code">
const logger = console;
logger.log('Server started on port 3000');
logger.error('Database connection failed: connection
  timeout');
logger.info('User login successful: userId=12345');</pre></li>				<li><code>.csv</code>, <code>.xml</code>, or other formats as well, but the most used format is JSON. This approach makes it easier to search, filter, and analyze logs programmatically. Here is an example showing structured logging:<pre class="source-code">
{
  "level": "error",
  "time": "2024-06-26T12:34:57.890Z",
  "service": "my-microservice",
  "buildInfo": {
    "nodeVersion": "v16.13.0",
    "commitHash": "abc123def456"
  },
  "msg": "Failed to connect to database",
  "eventId": "evt-2000",
  "correlationId": "corr-67890",
  "stack": "Error: Connection timeout\n    at Object.&lt;anonymous&gt; (/path/to/your/file.js:15:19)\n    at Module._compile (internal/modules/cjs/loader.js:999:30)\n    at Module.load (internal/modules/cjs/loader.js:985:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:878:14)\n    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:71:12)\n    at internal/main/run_main_module.js:17:47",
  "source": {
    "file": "/path/to/your/file.js",
    "line": 15,
    "function": "logError"
  }
}</pre></li>				<li><strong class="bold">Semi-structured logging</strong>: It combines <a id="_idIndexMarker732"/>elements of both <a id="_idIndexMarker733"/>unstructured and structured logging. It often involves a consistent pattern or delimiter within plain text logs, making them somewhat easier to parse than completely unstructured logs but not as robust as fully structured logs.</li>
			</ul>
			<p>We explored the importance of logging in microservices, and its challenges, and discussed the different log levels, popular Node.js logging libraries, and how to choose the right logging format <a id="_idIndexMarker734"/>for your microservices. Now, let’s cover best practices for logging.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor167"/>Best practices for logging</h2>
			<p>Effective logging can help <a id="_idIndexMarker735"/>you understand system behavior, diagnose issues, and monitor performance. Here are some essential best practices for logging in Node.js microservices:</p>
			<ul>
				<li><strong class="bold">Use a structured logging format</strong>: Ensure logs are structured (e.g., JSON), making them easily parsed and searchable by log management tools. This facilitates more efficient log analysis and filtering.</li>
				<li><strong class="bold">Include contextual information</strong>: Enrich logs with context such as timestamps, service names, correlation IDs, and user information, enabling better tracing and correlation across microservices.</li>
				<li><strong class="bold">Log at appropriate levels</strong>: Apply suitable log levels (error, warn, info, debug, trace) to categorize log messages based on severity, which helps in filtering logs for relevance and troubleshooting.</li>
				<li><strong class="bold">Avoid logging sensitive information</strong>: Ensure sensitive, data such as passwords and personal details, are redacted or masked before logging to maintain security and compliance.</li>
				<li><strong class="bold">Centralize logs</strong>: Aggregate logs from all microservices in a centralized location using tools such as the ELK stack or cloud-based logging services for streamlined monitoring, analysis, and alerting.</li>
			</ul>
			<p>These practices will help you ensure that your logging is efficient, secure, and scalable, making it easier to monitor system behavior, diagnose issues, and maintain overall performance.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor168"/>Implementing logging in your microservices</h2>
			<p>It is really simple<a id="_idIndexMarker736"/> to implement logging thanks to<a id="_idIndexMarker737"/> the packages of Node.js. In this section, we will use <code>winston</code> and <code>morgan</code> to demonstrate the usage of logging in microservices. Let’s integrate log support into the <code>Account</code> microservice we developed before. To follow this chapter, go to our GitHub repository and download the source code and <code>Ch10</code> using your favorite IDE. We plan to integrate monitoring functionality into our microservice, which we implemented in <a href="B09148_09.xhtml#_idTextAnchor147"><em class="italic">Chapter 9</em></a>. You can just copy the <code>Ch09</code> folder and start to work on it.</p>
			<p>To install the <code>winston</code> and <code>morgan</code> libraries on the account microservice, run the following command from the <code>accountservice</code> folder:</p>
			<pre class="source-code">
npm install -E winston morgan</pre>			<p>Now, our <code>package.json</code> file should contain appropriate versions to use the libraries. Let’s first try to use <code>winston</code> for logging. Create a file called <code>logger.js</code> under the <code>src/log</code> folder <a id="_idIndexMarker738"/>with the <a id="_idIndexMarker739"/>following content:</p>
			<pre class="source-code">
const winston = require('winston');
const logger = winston.createLogger({
    level: process.env.LOG_LEVEL || 'info',
    defaultMeta: {
        service: "account-microservice",
        buildInfo: {
            version: '1.0.0',
            nodeVersion: process.version
        }
    },
    transports:
        [new winston.transports.Console({
            format: winston.format.combine(
                winston.format.colorize(),
                winston.format.simple()
            )
        }),
        new winston.transports.File({
            format: winston.format.combine(
                winston.format.json(),
                winston.format.timestamp()
            ),
            filename: 'combined.log'
        }),
        new winston.transports.File({
            format: winston.format.combine(
                winston.format.json(),
                winston.format.timestamp()
            ),
            filename: 'error.log',
            level: 'error'
        })
        ]
});
module.exports = {
    logger
};</pre>			<p>This code<a id="_idIndexMarker740"/> defines a <code>winston</code> logger in Node.js for an <a id="_idIndexMarker741"/>application named <code>account-microservice</code>. Let’s break down the code step by step:</p>
			<ul>
				<li><code>const winston = require('winston');</code>: This line imports the <code>winston</code> library, which is a popular logging framework for Node.js.</li>
				<li><code>const logger = winston.createLogger({...});</code>: This line creates a new <code>winston</code> logger instance and stores it in the logger constant. The curly braces (<code>{}</code>) contain configuration options for the logger.</li>
				<li><code>level: process.env.LOG_LEVEL || 'info'</code>: This sets the minimum severity level of logs that will be captured. It checks the <code>LOG_LEVEL</code> environment variable first. If that’s not set, it defaults to the <code>'info'</code> level. Levels such as <code>'error'</code>, <code>'warn'</code>, <code>'info'</code>, <code>'debug'</code>, and so on exist, with <code>'error'</code> being the most severe.</li>
				<li><code>defaultMeta</code>: This defines additional information that will be attached to every log message. Here, it includes the service name (<code>account-microservice</code>) and build information (version and <code>nodeVersion</code>).</li>
				<li><code>transports</code>: This configures where the log messages will be sent. Here, it’s an array defining three transports:<ul><li><code>winston.transports.Console</code>: This sends logs to the console (usually your terminal)</li></ul></li>
				<li><code>format: winston.format.combine(...)</code>: This defines how the log message will be<a id="_idIndexMarker742"/> formatted when sent to the<a id="_idIndexMarker743"/> console. It combines two formatters:<ul><li><code>winston.format.colorize()</code>: This adds color to the console output for better readability.</li><li><code>winston.format.simple()</code>: This formats the message in a simple text format.</li></ul></li>
				<li><code>winston.transports.File({ filename: 'combined.log' })</code>: This sends all logs (based on the level setting) to a file named <code>combined.log</code>.</li>
				<li><code>format: winston.format.combine(...)</code>: Similar to the console, it combines formatters:<ul><li><code>winston.format.json()</code>: This formats the message as a JSON object for easier parsing by machines.</li><li><code>winston.format.timestamp()</code>: This adds a timestamp to each log message.</li></ul></li>
				<li><code>winston.transports.File({ filename: 'error.log', level: 'error' })</code>: This sends only error-level logs to a separate file named <code>error.log</code>. It uses the same formatters (<code>json</code> and <code>timestamp</code>).</li>
				<li><code>module.exports =[];: </code>This line makes the created logger (<code>logger</code>)     available for import and use in other parts of your application.</li>
			</ul>
			<p>In summary, this code sets up a comprehensive logging system for our application. It logs messages to both the console and files, with different formatting and filtering based on severity level. This allows us to easily monitor application behavior, debug issues, and analyze logs for further insights.</p>
			<p>Let’s integrate<a id="_idIndexMarker744"/> logging into <code>accountController</code> and <a id="_idIndexMarker745"/>see the result. Here is a simplified version:</p>
			<pre class="source-code">
const accountService = require('../services/account');
const { logger } = require('../log/logger');
const getAccountById = async (req, res) =&gt; {
    logger.info('getAccountById method called', { accountId: req.params.id });
….</pre>			<p>When you call the endpoint that is responsible for delivering the <code>getAccountById</code> method, you will get a log message in your terminal and a <code>combined.log</code> file. We also integrated logging in <code>index.js</code> of our application to see whether everything is OK with the application running:</p>
			<pre class="source-code">
{
  "buildInfo": {
    "nodeVersion": "v20.12.1",
    "version": "1.0.0"
  },
  "level": "info",
  "message": "account service started",
  "port": 3001,
  "service": "account-microservice"
}
{
  "accountId": "6658ae5284432e40604018d5",
  "buildInfo": {
    "nodeVersion": "v20.12.1",
    "version": "1.0.0"
  },
  "level": "info",
  "message": "getAccountById method called",
  "service": "account-microservice"
}</pre>			<p>If you have any errors, you’ll get the error message in your terminal and it will automatically be added to the <code>error.log</code> file.</p>
			<p>In Node.js, particularly when using Express.js for building web applications,  the <code>morgan</code> package is <a id="_idIndexMarker746"/>a <a id="_idIndexMarker747"/>popular tool for streamlining HTTP request logging. It automates the process of capturing and recording information about incoming requests to your application.</p>
			<p>Here’s why you may use it:</p>
			<ul>
				<li><code>morgan</code> eliminates this by automatically capturing data such as the request method, URL, status code, response time, and more. This saves development time and ensures consistent logging.</li>
				<li><code>morgan</code> provides valuable insights into how your application handles requests. This can be crucial for debugging purposes, helping you identify potential issues or performance bottlenecks within your application’s request processing.</li>
				<li><strong class="bold">Monitoring application traffic</strong>:  By reviewing the logs, you can gain a better understanding of your application’s traffic patterns. This can be useful for monitoring overall application health, identifying usage trends, and making informed decisions about scaling or resource allocation.</li>
				<li><code>combined</code>, <code>common</code>, and<code> dev</code>) that cater to different levels of detail. You can also create custom formats to capture specific data points relevant to your application’s needs.</li>
			</ul>
			<p>We’ve already installed the <code>morgan</code> package and it is time to use it. We usually use it as middleware <a id="_idIndexMarker748"/>and here is how to implement<a id="_idIndexMarker749"/> your own <code>morgan</code> middleware. Create a new file called <code>morganmiddleware.j</code>s under the <code>src/middlewares</code> folder. Copy and paste the following inside it:</p>
			<pre class="source-code">
const fs = require('fs');
const path = require('path');
const morgan = require('morgan');
const { logger } = require('../log/logger-logstash');
const morganFormat = JSON.stringify({
    method: ':method',
    url: ':url',
    status: ':status',
    responseTime: ':response-time ms',});
// Path to the combined.log file
const logFilePath = path.join(__dirname,
  '../../combined.log');
// Create a write stream for the log file
const logFileStream = fs.createWriteStream(logFilePath,
  { flags: 'a' });
// Custom message handler function for logging
function messageHandler(message) {
    const parsedMessage = JSON.parse(message.trim());
    // Write log to logstash
    logger.info('Request received for logging',
      parsedMessage);
    // Also write the log to combined.log file
    logFileStream.write(`${message}\n`);
}
// Create morgan middleware with custom format and stream
const morganMiddleware = morgan(morganFormat, {
    stream: {
        write: messageHandler,
    },
});
module.exports = morganMiddleware;</pre>			<p>This code<a id="_idIndexMarker750"/> defines a custom middleware function for logging HTTP<a id="_idIndexMarker751"/> requests in JSON format using the <code>morgan</code> library. The code defines a logging mechanism that uses the <code>morgan</code> middleware to log HTTP requests in a Node.js application. It integrates logging with both a <code>combined.log</code> file and a Logstash server for external log management.</p>
			<p><code>morganFormat</code> is a custom format that logs details such as the HTTP method, URL, status code, and response time for each request. These logs are then handled by a custom <code>messageHandler</code> function.</p>
			<p>In the <code>messageHandler</code>, the incoming log message is parsed from a JSON string into an object. The parsed log is then sent to Logstash using the <code>logger.info</code> function, which is imported from the <code>logger-logstash</code> module. At the same time, the original log message is also written to a local file named <code>combined.log</code>. This is done by creating a write stream to the file using Node.js’s <code>fs</code> module, which appends each new log to the file.</p>
			<p>Finally, the custom <code>morganMiddleware</code> is created using the <code>morgan</code> function, with the logging stream directed to <code>messageHandler</code>. This middleware is then exported to be used in other parts of the application for logging purposes.</p>
			<p>This setup ensures that HTTP request logs are recorded both locally in a file and sent to an external Logstash service for further processing.</p>
			<p>We’re done with middleware functionality and now it is time to apply it. Open <code>app.js</code>, which is where <a id="_idIndexMarker752"/>we have configured our middleware flow and make the following changes:</p>
			<pre class="source-code">
const morganMiddleware = require('./morganmiddleware');
const app = express();
app.use(morganMiddleware);</pre>			<p>Before everything else in the middleware flow, we need to use <code>morganMiddleware</code> and now you can just remove the previous logging functions we did via <code>winston</code>. Run the application and call any endpoint you want. Before running the account microservice, make sure that Docker is running with the appropriate <code>docker-compose</code> file. Don’t forget to run both <code>docker-compose</code> files (<code>accountservice/docker-compose.yml</code> and <code>accountservice/elk-stack/docker-compose.yml</code>).</p>
			<p>Here is the terminal output for logging:</p>
			<div><div><img alt="Figure 10.1: Terminal output for logging" src="img/B09148_10_001.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1: Terminal output for logging</p>
			<p>Check <a id="_idIndexMarker753"/>the <code>combined.log</code> file and terminal<a id="_idIndexMarker754"/> window to see the logs.</p>
			<p>In the next section, we will cover centralized logging.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor169"/>Centralized logging with Elasticsearch, Logstash, and Kibana (ELK) stack</h1>
			<p>In a microservices <a id="_idIndexMarker755"/>architecture, where applications are <a id="_idIndexMarker756"/>broken down into independent, loosely coupled services, <strong class="bold">centralized logging</strong> becomes crucial for effective monitoring and troubleshooting. We have a lot of reasons to use it:</p>
			<ul>
				<li><strong class="bold">Spread out logs</strong>: Normally, logs would be all over the place, on each individual mini-app. Imagine hunting for a problem that jumps between them – like looking for a lost sock in a messy house!</li>
				<li><strong class="bold">See everything at once</strong>: Centralized logging brings all the logs together in one spot, like putting all your socks in a basket. This way, you can easily see how everything is working and if any parts are causing trouble.</li>
				<li><strong class="bold">Fixing problems faster</strong>: With all the logs in one place, it’s like having a super magnifying glass to find issues. You can search through the logs quickly to see what went wrong, saving you time and frustration.</li>
				<li><strong class="bold">Keeping an eye on things</strong>: Centralized logging often works with monitoring tools, like having a dashboard for your socks. This lets you see how well everything is performing and identify any slow spots.</li>
				<li><strong class="bold">Log care made easy</strong>: Having everything in one place makes taking care of the logs much simpler. It’s like having a dedicated sock drawer! Tools can be used to keep things organized, get rid of old logs, and follow any rules you need to follow.</li>
			</ul>
			<p>By using centralized logging, you get a powerful tool to watch over your microservices, fix problems faster, and keep everything running smoothly.</p>
			<p>We have many <a id="_idIndexMarker757"/>different <a id="_idIndexMarker758"/>options to implement centralized logging when building microservices and one of them is the ELK stack.</p>
			<p>The <strong class="bold">ELK stack</strong> is a powerful suite of tools used for centralized logging, real-time search, and data analysis. Here’s a brief overview of each component:</p>
			<ul>
				<li><strong class="bold">Elasticsearch</strong>: This is <a id="_idIndexMarker759"/>a distributed search and analytics engine. We use it to store, search, and analyze large volumes of data quickly and in near real time. Elasticsearch is built on Apache Lucene and provides a RESTful interface for interacting with your data.</li>
				<li><strong class="bold">Logstash</strong>: This is a server-side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to your chosen <em class="italic">stash</em>, such as Elasticsearch. It can handle a variety of data formats and provides a rich set of plugins to perform different transformations and enrichments.</li>
				<li><strong class="bold">Kibana</strong>: This is a data visualization and exploration tool used for analyzing and visualizing the data stored in Elasticsearch. It provides a user-friendly interface for creating dashboards and performing advanced data analysis.</li>
			</ul>
			<p>But how do they work together? Well, Logstash collects and processes the log data from various sources (e.g., server logs, application logs, network logs) and forwards it to Elasticsearch. Elasticsearch indexes and stores the data, making it searchable in near real time. Kibana connects to Elasticsearch and provides the tools necessary to query, visualize, and analyze the data, allowing users to create custom dashboards and reports.</p>
			<p>There are multiple benefits of using the ELK stack:</p>
			<ul>
				<li><strong class="bold">Scalability</strong>: The ELK<a id="_idIndexMarker760"/> stack can scale horizontally, allowing you to handle large volumes of log data.</li>
				<li><strong class="bold">Real-time insights</strong>: Elasticsearch’s real-time search capabilities provide instant insights into your data.</li>
				<li><strong class="bold">Flexibility</strong>: Logstash’s ability to ingest data from various sources and formats makes it highly flexible.</li>
				<li><strong class="bold">Visualization</strong>: Kibana’s rich visualization options enable you to create interactive dashboards for monitoring and analysis.</li>
				<li><strong class="bold">Open source</strong>: The ELK stack is open source, with a large community and a wealth of plugins and extensions.</li>
			</ul>
			<p>As always, we prefer to install tools via Docker and it applies to the ELK stack also. Go to the <code>Ch10/accountservice/elk-stack</code> folder and run the <code>docker-compose.yml</code> file using the <code>docker-compose up -d</code> command. We will not dive into the <a id="_idIndexMarker761"/>details <a id="_idIndexMarker762"/>of <code>docker-compose</code> because we did it in our previous chapters. Simply, we install Elasticsearch, Logstash, and Kibana in the given <code>docker-compose.yml</code> file.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor170"/>A brief introduction to Logstash</h2>
			<p>We can collect and<a id="_idIndexMarker763"/> transform logs using Logstash. We are able to get input from multiple different sources such as logs generated by other applications, plain text, or networks. For log ingestion, we have different approaches that we can follow:</p>
			<ul>
				<li><strong class="bold">Direct transport</strong>: We can configure our application to directly send data to Elasticsearch. Yes, that is an option but not a preferable way of ingesting logs.</li>
				<li><strong class="bold">Write logs to the file</strong>: As we implement in our microservices, it is preferable to implement such types of logging because other applications, such as Logstash, as a separate process, will be able to read, parse, and forward the data to Elasticsearch. It requires more configuration but it is the more robust and preferable way of doing logging for production.</li>
			</ul>
			<p>Logstash configuration is typically written in a configuration file (e.g., <code>logstash.conf</code>). This file consists of three main sections: <code>input</code>, <code>filter</code>, and <code>output</code>. Each section defines different aspects of the data processing pipeline. Here’s a breakdown of each section<a id="_idIndexMarker764"/> and an example configuration:</p>
			<ul>
				<li>The <code>input</code> section defines where <a id="_idIndexMarker765"/>Logstash should collect data <a id="_idIndexMarker766"/>from. This could be from files, syslog, <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>)/ <strong class="bold">User Datagram Protocol</strong> (<strong class="bold">UDP</strong>) ports, or various other sources.</li>
				<li>The <code>filter</code> section is used to process and transform the data. Filters can parse, enrich, and modify the log data. Common filters include <code>grok</code> for pattern matching, <code>mutate</code> for modifying fields, and <code>date</code> for parsing date/time information.</li>
				<li>The <code>output</code> section specifies where the processed data should be sent. This could be Elasticsearch, a file, a message queue, or another destination.</li>
			</ul>
			<p>To see the detailed explanation in action, simply open the <code>logstash.conf</code> file:</p>
			<pre class="console">
input {
  tcp {
    port =&gt; 5000
  }
}
filter {
  json {
    source =&gt; "message"
  }
}
output {
  elasticsearch {
    hosts =&gt; ["elasticsearch:9200"]
    index =&gt; "app-%{+YYYY.MM.dd}"
  }
  stdout { }
}</pre>			<p>Let’s dive into the details<a id="_idIndexMarker767"/> of the given configuration:</p>
			<ul>
				<li><code>tcp { port =&gt; 5000 }</code>: This section defines an <code>input</code> plugin that listens for data coming in over a TCP socket on port <code>5000</code>. Any logs or events sent to this port will be ingested by Logstash.</li></ul></li>
				<li><code>json { source =&gt; "message" }</code>: This <code>filter</code> plugin parses the incoming data, assuming it’s in JSON format, and extracts the value from the field named <code>message</code>. This field is likely where the actual log content resides. By parsing it as JSON, Logstash can understand the structure of the data and make it easier to work with in subsequent processing steps.</li></ul></li>
				<li><code>elasticsearch { hosts =&gt; ["elasticsearch:9200"], index =&gt; "app-%{+YYYY.MM.dd}" }</code>: This <code>output</code> plugin sends the processed data to Elasticsearch, a search and analytics engine optimized for handling large volumes of log data. The host’s option specifies the location of the Elasticsearch instance (presumably running on a machine named <code>elasticsearch</code> with the default port <code>9200</code>).</li></ul></li>
				<li>The <code>index</code> option defines a dynamic index naming pattern. Each day’s logs will be stored in a separate index named <code>app-YYYY.MM.dd</code> (where <code>YYYY</code> represents the year, <code>MM</code> the month, and <code>dd</code> the day). This pattern helps in efficient log management and allows you to easily search for logs from specific dates.</li>
				<li><code>stdout { }</code>: This <code>output</code> plugin simply prints the processed data to the console (standard output) for debugging or monitoring purposes. The empty curly braces (<code>{}</code>) indicate the default configuration for the standard output.</li>
			</ul>
			<p>This Logstash configuration ingests data from TCP source, parses JSON-formatted logs, and then sends them to Elasticsearch for storage and analysis. Daily indexes are created for organized log management. The <code>stdout</code> plugin provides a way to view the processed data during development or troubleshooting.</p>
			<p>Let’s integrate logging <a id="_idIndexMarker768"/>into our account microservice. Create a new file called <code>logger-logstash.js</code> under the <code>accountmicroservice/src/log</code> folder with the following content:</p>
			<pre class="source-code">
const winston = require('winston');
const LogstashTransport = require('winston-logstash
  /lib/winston-logstash-latest.js');
const serviceName = 'account-microservice'
const logstashTransport = new LogstashTransport({
    host: 'localhost',
    port: 5000
})
const logger = winston.createLogger({
    level: 'info',
    format: winston.format.combine(winston
      .format.timestamp(), winston.format.json()),
    defaultMeta: {
        service: serviceName,
        buildInfo: {
            nodeVersion: process.version
        }
    },
    transports: [
        new winston.transports.Console({
            format: winston.format.combine(
                winston.format.colorize(),
                winston.format.simple()
            )
        }),
        logstashTransport
    ]
})
module.exports = {
    logger
};</pre>			<p>We have already<a id="_idIndexMarker769"/> talked about <code>winston</code> configuration and the only new thing here is <code>logstashTransport</code>. We added two <code>transports</code> channel: one for the terminal’s console and the other one for sending logs to <code>logstash</code>. To use the given file with <code>morgan</code>, just change <code>morganmiddleware.js</code>’s logger to the following:</p>
			<pre class="source-code">
const { logger } = require('./log/logger-logstash');</pre>			<p>Now, run our application and go to <code>http://localhost:5601</code>; this is our Kibana. From the left menu, find <strong class="bold">Management</strong> | <strong class="bold">Dev tools</strong>.  Click on the <strong class="bold">Execute</strong> button and you will see the total value with your logs (<em class="italic">Figure 10</em><em class="italic">.2</em>)</p>
			<div><div><img alt="Figure 10.2: Getting logs from Kibana" src="img/B09148_10_002.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2: Getting logs from Kibana</p>
			<p>Now, our logs are<a id="_idIndexMarker770"/> flowing to the ELK stack. You can think of Elasticsearch as a search and analytics engine with a data warehouse capability.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor171"/>A brief introduction to Elasticsearch</h2>
			<p><strong class="bold">Elasticsearch</strong> is a powerhouse <a id="_idIndexMarker771"/>search engine built for speed and scalability. At its core, it’s a distributed system designed to store, search, and analyze large volumes of data in near real time. It is document-oriented and uses JSON.</p>
			<p>Let’s dive deeper into the key attributes of Elasticsearch:</p>
			<ul>
				<li><strong class="bold">Distributed</strong>: Elasticsearch<a id="_idIndexMarker772"/> can store data across multiple nodes (servers) in a cluster. This distribution allows for the following:<ul><li><strong class="bold">Fault tolerance</strong>: If one node fails, other nodes can handle the requests, keeping your search service operational.</li><li><strong class="bold">Horizontal scaling</strong>: You can easily add more nodes to the cluster as your data volume or search traffic grows.</li></ul></li>
				<li><strong class="bold">Scalable</strong>: As mentioned previously, Elasticsearch excels at horizontal scaling. You can add more nodes to the cluster to handle increasing data and search demands. This scalability makes it suitable for large datasets and high search volumes.</li>
				<li><strong class="bold">Search and analytics</strong>: Elasticsearch specializes in full-text search, which analyzes the entire text content of your documents. This allows you to search for keywords, phrases, and even concepts within your data. It also provides powerful analytics capabilities. You can aggregate data, identify trends, and gain insights from your search results.</li>
				<li><strong class="bold">Flexible search</strong>: Elasticsearch offers a wide range of query options. You can search for specific terms, filter results based on various criteria, and perform complex aggregations. This flexibility allows you to tailor your searches to your specific needs and uncover valuable information from your data.</li>
				<li><strong class="bold">Search speed</strong>: Due to its distributed architecture and efficient indexing techniques, Elasticsearch delivers fast search results. This is crucial for applications where <a id="_idIndexMarker773"/>users expect an immediate response to their queries.</li>
			</ul>
			<p>In this section, we provided a brief overview of Elasticsearch, focusing on the core attributes that make it a powerful tool for search and data analysis.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor172"/>A brief introduction to Kibana</h2>
			<p><strong class="bold">Kibana</strong> is the last item<a id="_idIndexMarker774"/> in our ELK stack. It is the visualization layer that complements the data storage and search muscle of Elasticsearch. It’s an open source platform that acts as a window into your Elasticsearch data, allowing you to explore, analyze, and understand it with clear visualizations.</p>
			<p>Kibana has the following interesting possibilities:</p>
			<ul>
				<li><strong class="bold">Visualization powerhouse</strong>: Kibana lets you create interactive dashboards with various charts, graphs, and maps. This visual representation transforms raw data into easily digestible insights.</li>
				<li><strong class="bold">Data exploration</strong>: Kibana provides tools to explore, search, and filter your data within Elasticsearch. You can drill down into specific details and uncover hidden patterns.</li>
				<li><strong class="bold">Sharing insights</strong>: Created dashboards can be shared with others, fostering collaboration and promoting data-driven decision-making.</li>
			</ul>
			<p>There are several compelling reasons to choose Kibana for microservices:</p>
			<ul>
				<li><strong class="bold">Effortless integration</strong>: As part of the ELK Stack (Elasticsearch, Logstash, and Kibana), Kibana integrates<a id="_idIndexMarker775"/> seamlessly with Elasticsearch. This tight integration streamlines the process of visualizing data stored within Elasticsearch.</li>
				<li><strong class="bold">Real-time insights</strong>: Kibana allows you to visualize data in near real-time, providing valuable insights as your data streams in. This is crucial for applications requiring immediate response to changes.</li>
				<li><strong class="bold">Customization options</strong>: Kibana offers a wide range of visualizations and customization options. You can tailor dashboards to fit your specific needs and effectively communicate insights to your audience.</li>
				<li><strong class="bold">Open source and free</strong>: Being open source, Kibana is free to use and offers a vibrant community for support and development.</li>
			</ul>
			<p>Microservices architectures involve multiple, independent services working together. Kibana shines in this environment for several reasons:</p>
			<ul>
				<li><strong class="bold">Monitoring performance</strong>: Visualize key metrics from your microservices on Kibana dashboards to monitor their health and performance. This helps identify bottlenecks and ensure smooth operation.</li>
				<li><strong class="bold">Log analysis</strong>: Centralize and analyze logs from all your microservices within Kibana. This unified view simplifies troubleshooting issues and pinpointing errors across the system.</li>
				<li><strong class="bold">Application insights</strong>: Gain insights into how users interact with your microservices by visualizing usage patterns and trends within Kibana. This data can guide development efforts and improve user experience.</li>
			</ul>
			<p>Learning about the ELK stack, diving into details of Elasticsearch querying and Kibana-related topics, such as<a id="_idIndexMarker776"/> custom dashboards, and working with metrics are beyond the scope of this book and that is why we finish our chapter only with a simple introduction to them.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/>Summary</h1>
			<p>This chapter delved into the crucial aspects of monitoring and logging in microservices architecture, emphasizing the importance of observability in maintaining the health and performance of distributed systems. We began by explaining how observability provides deep insights into system behavior through key components such as logs, metrics, alerts, and traces.</p>
			<p>We then shifted focus to the importance of logging in microservices, which is essential for capturing detailed records of system events, identifying performance bottlenecks, and diagnosing issues in real time. We explored different log levels—error, warning, info, debug, and trace—and discussed how they help categorize log messages based on severity, making troubleshooting more efficient. Additionally, the chapter covered popular logging libraries in Node.js such as <code>winston</code> and <code>morgan</code>.</p>
			<p>Following the theoretical foundation, we demonstrated how to implement logging in a real-world scenario by integrating <code>winston</code> and <code>morgan</code> into the account microservice.</p>
			<p>The chapter then moved on to centralized logging, introducing the powerful ELK stack. We explained how Logstash collects and processes log data, Elasticsearch stores and indexes the data for real-time search, and Kibana visualizes the information through interactive dashboards. By integrating these tools, we established a centralized logging system that simplifies log collection, analysis, and visualization.</p>
			<p>In the next chapter, we will explore how to manage multiple microservices effectively using popular microservices architecture elements. You’ll learn about using an API gateway, which acts as a single entry point to manage requests and direct them to the correct services, as well as organizing data and actions within your system through CQRS and Event Sourcing, two important methods that help handle complex data flows. By the end, you’ll have a clear understanding of building and connecting services in a way that’s efficient and easy to maintain.</p>
		</div>
	</body></html>