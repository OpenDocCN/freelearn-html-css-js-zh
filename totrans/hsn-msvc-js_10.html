<html><head></head><body>
		<div id="_idContainer100">
			<h1 class="chapter-number" id="_idParaDest-159"><a id="_idTextAnchor160"/>10</h1>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor161"/>Monitoring Microservices</h1>
			<p><strong class="bold">Microservices</strong> have<a id="_idIndexMarker716"/> become a core architectural approach for building scalable and flexible applications, but ensuring their health and performance is just as important as their functionality. Without proper visibility, identifying issues in such a distributed system can be like trying to find a needle in a haystack. Think of monitoring and logging as placing cameras and sensors in different parts of a bustling city, where each microservice is a shop. These tools help you observe how the system is functioning, capture key events, and detect any unusual behavior. By establishing robust logging and monitoring practices, you can quickly pinpoint problems and keep your microservices <span class="No-Break">running smoothly.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Importance <span class="No-Break">of observability</span></li>
				<li>Introduction <span class="No-Break">to logging</span></li>
				<li>Centralized logging with the <strong class="bold">Elasticsearch, Logstash, and Kibana</strong> (<span class="No-Break"><strong class="bold">ELK</strong></span><span class="No-Break">) stack</span></li>
			</ul>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor162"/>Technical requirements</h1>
			<p>To follow along with this chapter, we need to have an IDE installed (we prefer Visual Studio Code), Postman, Docker, and a browser of <span class="No-Break">your choice.</span></p>
			<p>It is preferable to download our repository from <a href="https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10">https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript/tree/main/Ch10</a> to easily follow our <span class="No-Break">code snippets.</span></p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor163"/>Importance of observability</h1>
			<p>In the world of <a id="_idIndexMarker717"/>software, particularly microservices, <strong class="bold">observability</strong> is crucial. It allows us to gain deep insights into how our system functions by analyzing its outputs. Observability is an important concept in monitoring and understanding systems. It refers to the ability to gain insight into the internal workings of a system by examining its outputs. Let’s try to understand the building blocks <span class="No-Break">of it:</span></p>
			<ul>
				<li><strong class="bold">Logs</strong>: Logs are detailed records of events that happen within a system. They provide a history of what has occurred, including errors, warnings, and informational messages. Logs can help in identifying and diagnosing issues by showing a step-by-step account of <span class="No-Break">system activities.</span></li>
				<li><strong class="bold">Metrics</strong>: Metrics are numerical values that represent the performance and behavior of a system. They can include data such as CPU usage, memory consumption, request rates, and error rates. Metrics provide a quantitative measure of the system’s health <span class="No-Break">and performance.</span></li>
				<li><strong class="bold">Alerts</strong>: Alerts are notifications that are triggered when metrics reach certain thresholds. They are used to inform administrators or operators about potential problems or abnormal behavior in real time, allowing for quick responses <span class="No-Break">to issues.</span></li>
				<li><strong class="bold">Traces</strong>: Traces provide a detailed view of the flow of requests through a system. They show how requests move from one component to another, highlighting the interactions and dependencies between different parts of the system. Traces help in understanding the path of a request and identifying bottlenecks or points <span class="No-Break">of failure.</span></li>
			</ul>
			<p>Observability helps in <a id="_idIndexMarker718"/>understanding what is happening inside a system by using logs, metrics, alerts, and traces. Logs give detailed records of events, metrics provide numerical data on performance, alerts notify of potential problems, and traces show the flow of requests. Together, these outputs offer a comprehensive view of a system’s state, aiding in monitoring, troubleshooting, and <span class="No-Break">optimizing performance.</span></p>
			<p>Now that we’ve covered the concept, let’s dive into the world of logging <span class="No-Break">in microservices.</span></p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor164"/>Introduction to logging</h1>
			<p>Have you ever driven a <a id="_idIndexMarker719"/>car with a broken dashboard? The speedometer might be stuck, the fuel gauge unreliable, and warning lights might flicker mysteriously. Without clear information about how the engine is running, it’s difficult to diagnose problems or ensure a <span class="No-Break">safe journey.</span></p>
			<p>In the world of software, particularly complex systems built with microservices, logging plays a similar role. <strong class="bold">Logs</strong> are detailed<a id="_idIndexMarker720"/> records of events and activities within <span class="No-Break">a system.</span></p>
			<p>When building your microservices, just thinking about business implementations is not enough. Microservices are, by nature, complex, with many independent services interacting. Logging helps understand individual service behavior and pinpoint issues within a specific service. When things go wrong, logs provide the audit trail to diagnose and fix problems. They help identify errors, dropped requests, or performance bottlenecks. Every microservice application should have a proper <span class="No-Break">logging mechanism.</span></p>
			<p>Logging microservices is essential for diagnostics, but it comes with challenges such as handling high volumes of distributed logs across different machines and languages, making it harder to aggregate and interpret them. Additionally, missing key details and ensuring sensitive information in logs is securely stored add complexity to managing <span class="No-Break">logs effectively.</span></p>
			<p>By understanding these challenges, we can implement effective logging strategies to keep our microservices teams talking and our systems <span class="No-Break">running smoothly.</span></p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor165"/>Logging levels and node libraries</h2>
			<p>Before practical <a id="_idIndexMarker721"/>examples, we need to understand some basics related to logging and one of<a id="_idIndexMarker722"/> them is <strong class="bold">log levels</strong>. Different log levels are used to categorize the severity or importance of <span class="No-Break">log messages.</span></p>
			<p><strong class="bold">Error logs</strong> capture<a id="_idIndexMarker723"/> critical issues that need immediate attention, such as crashes or system failures, while <strong class="bold">warning logs</strong> highlight <a id="_idIndexMarker724"/>potential problems that may need investigation. <strong class="bold">Info logs</strong> track <a id="_idIndexMarker725"/>general system operations, <strong class="bold">debug logs</strong> provide <a id="_idIndexMarker726"/>detailed diagnostic information, and <strong class="bold">trace logs</strong> offer the <a id="_idIndexMarker727"/>most granular level of logging for tracking <span class="No-Break">execution flow.</span></p>
			<p>Of course, you don’t need to implement logging algorithms from scratch. One of the beauties of Node.Js is it provides a cool collection of libraries for us to use. We have different popular log libraries to integrate and use when we build our microservices. You can use <strong class="source-inline">winston</strong>, <strong class="source-inline">pino</strong>, <strong class="source-inline">morgan</strong> (log middleware for  Express.js), <strong class="source-inline">bunyan</strong>, <strong class="source-inline">log4js</strong>, and so on when logging your microservices. We will integrate <strong class="source-inline">winston</strong> and <strong class="source-inline">morgan</strong> as a logging library for<a id="_idIndexMarker728"/> the current chapter but it is up to you to select one <span class="No-Break">of them.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor166"/>Log formats</h2>
			<p>In Node.js microservices, logging <a id="_idIndexMarker729"/>formats can be categorized into unstructured logging, structured logging, and semi-structured logging. Here is an explanation <span class="No-Break">of each:</span></p>
			<ul>
				<li><strong class="bold">Unstructured logging</strong>: Unstructured logging<a id="_idIndexMarker730"/> involves writing plain text log messages. This format is straightforward but can be harder to parse and analyze programmatically. Here is an example showing <span class="No-Break">unstructured logging:</span><pre class="source-code">
const logger = console;
logger.log('Server started on port 3000');
logger.error('Database connection failed: connection
  timeout');
logger.info('User login successful: userId=12345');</pre></li>				<li><strong class="bold">Structured logging</strong>: Structured<a id="_idIndexMarker731"/> logging involves writing logs in a consistent, machine-readable format, such as JSON. You can use <strong class="source-inline">.csv</strong>, <strong class="source-inline">.xml</strong>, or other formats as well, but the most used format is JSON. This approach makes it easier to search, filter, and analyze logs programmatically. Here is an example showing <span class="No-Break">structured logging:</span><pre class="source-code">
{
  "level": "error",
  "time": "2024-06-26T12:34:57.890Z",
  "service": "my-microservice",
  "buildInfo": {
    "nodeVersion": "v16.13.0",
    "commitHash": "abc123def456"
  },
  "msg": "Failed to connect to database",
  "eventId": "evt-2000",
  "correlationId": "corr-67890",
  "stack": "Error: Connection timeout\n    at Object.&lt;anonymous&gt; (/path/to/your/file.js:15:19)\n    at Module._compile (internal/modules/cjs/loader.js:999:30)\n    at Module.load (internal/modules/cjs/loader.js:985:32)\n    at Function.Module._load (internal/modules/cjs/loader.js:878:14)\n    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:71:12)\n    at internal/main/run_main_module.js:17:47",
  "source": {
    "file": "/path/to/your/file.js",
    "line": 15,
    "function": "logError"
  }
}</pre></li>				<li><strong class="bold">Semi-structured logging</strong>: It combines <a id="_idIndexMarker732"/>elements of both <a id="_idIndexMarker733"/>unstructured and structured logging. It often involves a consistent pattern or delimiter within plain text logs, making them somewhat easier to parse than completely unstructured logs but not as robust as fully <span class="No-Break">structured logs.</span></li>
			</ul>
			<p>We explored the importance of logging in microservices, and its challenges, and discussed the different log levels, popular Node.js logging libraries, and how to choose the right logging format <a id="_idIndexMarker734"/>for your microservices. Now, let’s cover best practices <span class="No-Break">for logging.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor167"/>Best practices for logging</h2>
			<p>Effective logging can help <a id="_idIndexMarker735"/>you understand system behavior, diagnose issues, and monitor performance. Here are some essential best practices for logging in <span class="No-Break">Node.js microservices:</span></p>
			<ul>
				<li><strong class="bold">Use a structured logging format</strong>: Ensure logs are structured (e.g., JSON), making them easily parsed and searchable by log management tools. This facilitates more efficient log analysis <span class="No-Break">and filtering.</span></li>
				<li><strong class="bold">Include contextual information</strong>: Enrich logs with context such as timestamps, service names, correlation IDs, and user information, enabling better tracing and correlation <span class="No-Break">across microservices.</span></li>
				<li><strong class="bold">Log at appropriate levels</strong>: Apply suitable log levels (error, warn, info, debug, trace) to categorize log messages based on severity, which helps in filtering logs for relevance <span class="No-Break">and troubleshooting.</span></li>
				<li><strong class="bold">Avoid logging sensitive information</strong>: Ensure sensitive, data such as passwords and personal details, are redacted or masked before logging to maintain security <span class="No-Break">and compliance.</span></li>
				<li><strong class="bold">Centralize logs</strong>: Aggregate logs from all microservices in a centralized location using tools such as the ELK stack or cloud-based logging services for streamlined monitoring, analysis, <span class="No-Break">and alerting.</span></li>
			</ul>
			<p>These practices will help you ensure that your logging is efficient, secure, and scalable, making it easier to monitor system behavior, diagnose issues, and maintain <span class="No-Break">overall performance.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor168"/>Implementing logging in your microservices</h2>
			<p>It is really simple<a id="_idIndexMarker736"/> to implement logging thanks to<a id="_idIndexMarker737"/> the packages of Node.js. In this section, we will use <strong class="source-inline">winston</strong> and <strong class="source-inline">morgan</strong> to demonstrate the usage of logging in microservices. Let’s integrate log support into the <strong class="source-inline">Account</strong> microservice we developed before. To follow this chapter, go to our GitHub repository and download the source code and <strong class="source-inline">Ch10</strong> using your favorite IDE. We plan to integrate monitoring functionality into our microservice, which we implemented in <a href="B09148_09.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>. You can just copy the <strong class="source-inline">Ch09</strong> folder and start to work <span class="No-Break">on it.</span></p>
			<p>To install the <strong class="source-inline">winston</strong> and <strong class="source-inline">morgan</strong> libraries on the account microservice, run the following command from the <span class="No-Break"><strong class="source-inline">accountservice</strong></span><span class="No-Break"> folder:</span></p>
			<pre class="source-code">
npm install -E winston morgan</pre>			<p>Now, our <strong class="source-inline">package.json</strong> file should contain appropriate versions to use the libraries. Let’s first try to use <strong class="source-inline">winston</strong> for logging. Create a file called <strong class="source-inline">logger.js</strong> under the <strong class="source-inline">src/log</strong> folder <a id="_idIndexMarker738"/>with the <a id="_idIndexMarker739"/><span class="No-Break">following content:</span></p>
			<pre class="source-code">
const winston = require('winston');
const logger = winston.createLogger({
    level: process.env.LOG_LEVEL || 'info',
    defaultMeta: {
        service: "account-microservice",
        buildInfo: {
            version: '1.0.0',
            nodeVersion: process.version
        }
    },
    transports:
        [new winston.transports.Console({
            format: winston.format.combine(
                winston.format.colorize(),
                winston.format.simple()
            )
        }),
        new winston.transports.File({
            format: winston.format.combine(
                winston.format.json(),
                winston.format.timestamp()
            ),
            filename: 'combined.log'
        }),
        new winston.transports.File({
            format: winston.format.combine(
                winston.format.json(),
                winston.format.timestamp()
            ),
            filename: 'error.log',
            level: 'error'
        })
        ]
});
module.exports = {
    logger
};</pre>			<p>This code<a id="_idIndexMarker740"/> defines a <strong class="source-inline">winston</strong> logger in Node.js for an <a id="_idIndexMarker741"/>application named <strong class="source-inline">account-microservice</strong>. Let’s break down the code step <span class="No-Break">by step:</span></p>
			<ul>
				<li><strong class="source-inline">const winston = require('winston');</strong>: This line imports the <strong class="source-inline">winston</strong> library, which is a popular logging framework <span class="No-Break">for Node.js.</span></li>
				<li><strong class="source-inline">const logger = winston.createLogger({...});</strong>: This line creates a new <strong class="source-inline">winston</strong> logger instance and stores it in the logger constant. The curly braces (<strong class="source-inline">{}</strong>) contain configuration options for <span class="No-Break">the logger.</span></li>
				<li><strong class="source-inline">level: process.env.LOG_LEVEL || 'info'</strong>: This sets the minimum severity level of logs that will be captured. It checks the <strong class="source-inline">LOG_LEVEL</strong> environment variable first. If that’s not set, it defaults to the <strong class="source-inline">'info'</strong> level. Levels such as <strong class="source-inline">'error'</strong>, <strong class="source-inline">'warn'</strong>, <strong class="source-inline">'info'</strong>, <strong class="source-inline">'debug'</strong>, and so on exist, with <strong class="source-inline">'error'</strong> being the <span class="No-Break">most severe.</span></li>
				<li><strong class="source-inline">defaultMeta</strong>: This defines additional information that will be attached to every log message. Here, it includes the service name (<strong class="source-inline">account-microservice</strong>) and build information (version <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">nodeVersion</strong></span><span class="No-Break">).</span></li>
				<li><strong class="source-inline">transports</strong>: This configures where the log messages will be sent. Here, it’s an array defining <span class="No-Break">three transports:</span><ul><li><strong class="source-inline">winston.transports.Console</strong>: This sends logs to the console (usually <span class="No-Break">your terminal)</span></li></ul></li>
				<li><strong class="source-inline">format: winston.format.combine(...)</strong>: This defines how the log message will be<a id="_idIndexMarker742"/> formatted when sent to the<a id="_idIndexMarker743"/> console. It combines <span class="No-Break">two formatters:</span><ul><li><strong class="source-inline">winston.format.colorize()</strong>: This adds color to the console output for <span class="No-Break">better readability.</span></li><li><strong class="source-inline">winston.format.simple()</strong>: This formats the message in a simple <span class="No-Break">text format.</span></li></ul></li>
				<li><strong class="source-inline">winston.transports.File({ filename: 'combined.log' })</strong>: This sends all logs (based on the level setting) to a file <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">combined.log</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">format: winston.format.combine(...)</strong>: Similar to the console, it <span class="No-Break">combines formatters:</span><ul><li><strong class="source-inline">winston.format.json()</strong>: This formats the message as a JSON object for easier parsing <span class="No-Break">by machines.</span></li><li><strong class="source-inline">winston.format.timestamp()</strong>: This adds a timestamp to each <span class="No-Break">log message.</span></li></ul></li>
				<li><strong class="source-inline">winston.transports.File({ filename: 'error.log', level: 'error' })</strong>: This sends only error-level logs to a separate file named <strong class="source-inline">error.log</strong>. It uses the same formatters (<strong class="source-inline">json</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">timestamp</strong></span><span class="No-Break">).</span></li>
				<li><strong class="source-inline">module.exports =[];: </strong>This line makes the created logger (<strong class="source-inline">logger</strong>)     available for import and use in other parts of <span class="No-Break">your application.</span></li>
			</ul>
			<p>In summary, this code sets up a comprehensive logging system for our application. It logs messages to both the console and files, with different formatting and filtering based on severity level. This allows us to easily monitor application behavior, debug issues, and analyze logs for <span class="No-Break">further insights.</span></p>
			<p>Let’s integrate<a id="_idIndexMarker744"/> logging into <strong class="source-inline">accountController</strong> and <a id="_idIndexMarker745"/>see the result. Here is a <span class="No-Break">simplified version:</span></p>
			<pre class="source-code">
const accountService = require('../services/account');
const { logger } = require('../log/logger');
const getAccountById = async (req, res) =&gt; {
    logger.info('getAccountById method called', { accountId: req.params.id });
….</pre>			<p>When you call the endpoint that is responsible for delivering the <strong class="source-inline">getAccountById</strong> method, you will get a log message in your terminal and a <strong class="source-inline">combined.log</strong> file. We also integrated logging in <strong class="source-inline">index.js</strong> of our application to see whether everything is OK with the <span class="No-Break">application running:</span></p>
			<pre class="source-code">
{
  "buildInfo": {
    "nodeVersion": "v20.12.1",
    "version": "1.0.0"
  },
  "level": "info",
  "message": "account service started",
  "port": 3001,
  "service": "account-microservice"
}
{
  "accountId": "6658ae5284432e40604018d5",
  "buildInfo": {
    "nodeVersion": "v20.12.1",
    "version": "1.0.0"
  },
  "level": "info",
  "message": "getAccountById method called",
  "service": "account-microservice"
}</pre>			<p>If you have any errors, you’ll get the error message in your terminal and it will automatically be added to the <span class="No-Break"><strong class="source-inline">error.log</strong></span><span class="No-Break"> file.</span></p>
			<p>In Node.js, particularly when using Express.js for building web applications,  the <strong class="source-inline">morgan</strong> package is <a id="_idIndexMarker746"/>a <a id="_idIndexMarker747"/>popular tool for streamlining HTTP request logging. It automates the process of capturing and recording information about incoming requests to <span class="No-Break">your application.</span></p>
			<p>Here’s why you may <span class="No-Break">use it:</span></p>
			<ul>
				<li><strong class="bold">Simplified logging</strong>: Manually logging request details can be cumbersome. <strong class="source-inline">morgan</strong> eliminates this by automatically capturing data such as the request method, URL, status code, response time, and more. This saves development time and ensures <span class="No-Break">consistent logging.</span></li>
				<li><strong class="bold">Debugging and analysis</strong>: The logged information from <strong class="source-inline">morgan</strong> provides valuable insights into how your application handles requests. This can be crucial for debugging purposes, helping you identify potential issues or performance bottlenecks within your application’s <span class="No-Break">request processing.</span></li>
				<li><strong class="bold">Monitoring application traffic</strong>:  By reviewing the logs, you can gain a better understanding of your application’s traffic patterns. This can be useful for monitoring overall application health, identifying usage trends, and making informed decisions about scaling or <span class="No-Break">resource allocation.</span></li>
				<li><strong class="bold">Customizable logging</strong>: Morgan offers various predefined logging formats (such as <strong class="source-inline">combined</strong>, <strong class="source-inline">common</strong>, and<strong class="source-inline"> dev</strong>) that cater to different levels of detail. You can also create custom formats to capture specific data points relevant to your <span class="No-Break">application’s needs.</span></li>
			</ul>
			<p>We’ve already installed the <strong class="source-inline">morgan</strong> package and it is time to use it. We usually use it as middleware <a id="_idIndexMarker748"/>and here is how to implement<a id="_idIndexMarker749"/> your own <strong class="source-inline">morgan</strong> middleware. Create a new file called <strong class="source-inline">morganmiddleware.j</strong>s under the <strong class="source-inline">src/middlewares</strong> folder. Copy and paste the following <span class="No-Break">inside it:</span></p>
			<pre class="source-code">
const fs = require('fs');
const path = require('path');
const morgan = require('morgan');
const { logger } = require('../log/logger-logstash');
const morganFormat = JSON.stringify({
    method: ':method',
    url: ':url',
    status: ':status',
    responseTime: ':response-time ms',});
// Path to the combined.log file
const logFilePath = path.join(__dirname,
  '../../combined.log');
// Create a write stream for the log file
const logFileStream = fs.createWriteStream(logFilePath,
  { flags: 'a' });
// Custom message handler function for logging
function messageHandler(message) {
    const parsedMessage = JSON.parse(message.trim());
    // Write log to logstash
    logger.info('Request received for logging',
      parsedMessage);
    // Also write the log to combined.log file
    logFileStream.write(`${message}\n`);
}
// Create morgan middleware with custom format and stream
const morganMiddleware = morgan(morganFormat, {
    stream: {
        write: messageHandler,
    },
});
module.exports = morganMiddleware;</pre>			<p>This code<a id="_idIndexMarker750"/> defines a custom middleware function for logging HTTP<a id="_idIndexMarker751"/> requests in JSON format using the <strong class="source-inline">morgan</strong> library. The code defines a logging mechanism that uses the <strong class="source-inline">morgan</strong> middleware to log HTTP requests in a Node.js application. It integrates logging with both a <strong class="source-inline">combined.log</strong> file and a Logstash server for external <span class="No-Break">log management.</span></p>
			<p><strong class="source-inline">morganFormat</strong> is a custom format that logs details such as the HTTP method, URL, status code, and response time for each request. These logs are then handled by a custom <span class="No-Break"><strong class="source-inline">messageHandler</strong></span><span class="No-Break"> function.</span></p>
			<p>In the <strong class="source-inline">messageHandler</strong>, the incoming log message is parsed from a JSON string into an object. The parsed log is then sent to Logstash using the <strong class="source-inline">logger.info</strong> function, which is imported from the <strong class="source-inline">logger-logstash</strong> module. At the same time, the original log message is also written to a local file named <strong class="source-inline">combined.log</strong>. This is done by creating a write stream to the file using Node.js’s <strong class="source-inline">fs</strong> module, which appends each new log to <span class="No-Break">the file.</span></p>
			<p>Finally, the custom <strong class="source-inline">morganMiddleware</strong> is created using the <strong class="source-inline">morgan</strong> function, with the logging stream directed to <strong class="source-inline">messageHandler</strong>. This middleware is then exported to be used in other parts of the application for <span class="No-Break">logging purposes.</span></p>
			<p>This setup ensures that HTTP request logs are recorded both locally in a file and sent to an external Logstash service for <span class="No-Break">further processing.</span></p>
			<p>We’re done with middleware functionality and now it is time to apply it. Open <strong class="source-inline">app.js</strong>, which is where <a id="_idIndexMarker752"/>we have configured our middleware flow and make the <span class="No-Break">following changes:</span></p>
			<pre class="source-code">
const morganMiddleware = require('./morganmiddleware');
const app = express();
app.use(morganMiddleware);</pre>			<p>Before everything else in the middleware flow, we need to use <strong class="source-inline">morganMiddleware</strong> and now you can just remove the previous logging functions we did via <strong class="source-inline">winston</strong>. Run the application and call any endpoint you want. Before running the account microservice, make sure that Docker is running with the appropriate <strong class="source-inline">docker-compose</strong> file. Don’t forget to run both <strong class="source-inline">docker-compose</strong> files (<strong class="source-inline">accountservice/docker-compose.yml</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">accountservice/elk-stack/docker-compose.yml</strong></span><span class="No-Break">).</span></p>
			<p>Here is the terminal output <span class="No-Break">for logging:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer098">
					<img alt="Figure 10.1: Terminal output for logging" src="image/B09148_10_001.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1: Terminal output for logging</p>
			<p>Check <a id="_idIndexMarker753"/>the <strong class="source-inline">combined.log</strong> file and terminal<a id="_idIndexMarker754"/> window to see <span class="No-Break">the logs.</span></p>
			<p>In the next section, we will cover <span class="No-Break">centralized logging.</span></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor169"/>Centralized logging with Elasticsearch, Logstash, and Kibana (ELK) stack</h1>
			<p>In a microservices <a id="_idIndexMarker755"/>architecture, where applications are <a id="_idIndexMarker756"/>broken down into independent, loosely coupled services, <strong class="bold">centralized logging</strong> becomes crucial for effective monitoring and troubleshooting. We have a lot of reasons to <span class="No-Break">use it:</span></p>
			<ul>
				<li><strong class="bold">Spread out logs</strong>: Normally, logs would be all over the place, on each individual mini-app. Imagine hunting for a problem that jumps between them – like looking for a lost sock in a <span class="No-Break">messy house!</span></li>
				<li><strong class="bold">See everything at once</strong>: Centralized logging brings all the logs together in one spot, like putting all your socks in a basket. This way, you can easily see how everything is working and if any parts are <span class="No-Break">causing trouble.</span></li>
				<li><strong class="bold">Fixing problems faster</strong>: With all the logs in one place, it’s like having a super magnifying glass to find issues. You can search through the logs quickly to see what went wrong, saving you time <span class="No-Break">and frustration.</span></li>
				<li><strong class="bold">Keeping an eye on things</strong>: Centralized logging often works with monitoring tools, like having a dashboard for your socks. This lets you see how well everything is performing and identify any <span class="No-Break">slow spots.</span></li>
				<li><strong class="bold">Log care made easy</strong>: Having everything in one place makes taking care of the logs much simpler. It’s like having a dedicated sock drawer! Tools can be used to keep things organized, get rid of old logs, and follow any rules you need <span class="No-Break">to follow.</span></li>
			</ul>
			<p>By using centralized logging, you get a powerful tool to watch over your microservices, fix problems faster, and keep everything <span class="No-Break">running smoothly.</span></p>
			<p>We have many <a id="_idIndexMarker757"/>different <a id="_idIndexMarker758"/>options to implement centralized logging when building microservices and one of them is the <span class="No-Break">ELK stack.</span></p>
			<p>The <strong class="bold">ELK stack</strong> is a powerful suite of tools used for centralized logging, real-time search, and data analysis. Here’s a brief overview of <span class="No-Break">each component:</span></p>
			<ul>
				<li><strong class="bold">Elasticsearch</strong>: This is <a id="_idIndexMarker759"/>a distributed search and analytics engine. We use it to store, search, and analyze large volumes of data quickly and in near real time. Elasticsearch is built on Apache Lucene and provides a RESTful interface for interacting with <span class="No-Break">your data.</span></li>
				<li><strong class="bold">Logstash</strong>: This is a server-side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to your chosen <em class="italic">stash</em>, such as Elasticsearch. It can handle a variety of data formats and provides a rich set of plugins to perform different transformations <span class="No-Break">and enrichments.</span></li>
				<li><strong class="bold">Kibana</strong>: This is a data visualization and exploration tool used for analyzing and visualizing the data stored in Elasticsearch. It provides a user-friendly interface for creating dashboards and performing advanced <span class="No-Break">data analysis.</span></li>
			</ul>
			<p>But how do they work together? Well, Logstash collects and processes the log data from various sources (e.g., server logs, application logs, network logs) and forwards it to Elasticsearch. Elasticsearch indexes and stores the data, making it searchable in near real time. Kibana connects to Elasticsearch and provides the tools necessary to query, visualize, and analyze the data, allowing users to create custom dashboards <span class="No-Break">and reports.</span></p>
			<p>There are multiple benefits of using the <span class="No-Break">ELK stack:</span></p>
			<ul>
				<li><strong class="bold">Scalability</strong>: The ELK<a id="_idIndexMarker760"/> stack can scale horizontally, allowing you to handle large volumes of <span class="No-Break">log data.</span></li>
				<li><strong class="bold">Real-time insights</strong>: Elasticsearch’s real-time search capabilities provide instant insights into <span class="No-Break">your data.</span></li>
				<li><strong class="bold">Flexibility</strong>: Logstash’s ability to ingest data from various sources and formats makes it <span class="No-Break">highly flexible.</span></li>
				<li><strong class="bold">Visualization</strong>: Kibana’s rich visualization options enable you to create interactive dashboards for monitoring <span class="No-Break">and analysis.</span></li>
				<li><strong class="bold">Open source</strong>: The ELK stack is open source, with a large community and a wealth of plugins <span class="No-Break">and extensions.</span></li>
			</ul>
			<p>As always, we prefer to install tools via Docker and it applies to the ELK stack also. Go to the <strong class="source-inline">Ch10/accountservice/elk-stack</strong> folder and run the <strong class="source-inline">docker-compose.yml</strong> file using the <strong class="source-inline">docker-compose up -d</strong> command. We will not dive into the <a id="_idIndexMarker761"/>details <a id="_idIndexMarker762"/>of <strong class="source-inline">docker-compose</strong> because we did it in our previous chapters. Simply, we install Elasticsearch, Logstash, and Kibana in the given <span class="No-Break"><strong class="source-inline">docker-compose.yml</strong></span><span class="No-Break"> file.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor170"/>A brief introduction to Logstash</h2>
			<p>We can collect and<a id="_idIndexMarker763"/> transform logs using Logstash. We are able to get input from multiple different sources such as logs generated by other applications, plain text, or networks. For log ingestion, we have different approaches that we <span class="No-Break">can follow:</span></p>
			<ul>
				<li><strong class="bold">Direct transport</strong>: We can configure our application to directly send data to Elasticsearch. Yes, that is an option but not a preferable way of <span class="No-Break">ingesting logs.</span></li>
				<li><strong class="bold">Write logs to the file</strong>: As we implement in our microservices, it is preferable to implement such types of logging because other applications, such as Logstash, as a separate process, will be able to read, parse, and forward the data to Elasticsearch. It requires more configuration but it is the more robust and preferable way of doing logging <span class="No-Break">for production.</span></li>
			</ul>
			<p>Logstash configuration is typically written in a configuration file (e.g., <strong class="source-inline">logstash.conf</strong>). This file consists of three main sections: <strong class="source-inline">input</strong>, <strong class="source-inline">filter</strong>, and <strong class="source-inline">output</strong>. Each section defines different aspects of the data processing pipeline. Here’s a breakdown of each section<a id="_idIndexMarker764"/> and an <span class="No-Break">example configuration:</span></p>
			<ul>
				<li>The <strong class="source-inline">input</strong> section defines where <a id="_idIndexMarker765"/>Logstash should collect data <a id="_idIndexMarker766"/>from. This could be from files, syslog, <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>)/ <strong class="bold">User Datagram Protocol</strong> (<strong class="bold">UDP</strong>) ports, or various <span class="No-Break">other sources.</span></li>
				<li>The <strong class="source-inline">filter</strong> section is used to process and transform the data. Filters can parse, enrich, and modify the log data. Common filters include <strong class="source-inline">grok</strong> for pattern matching, <strong class="source-inline">mutate</strong> for modifying fields, and <strong class="source-inline">date</strong> for parsing <span class="No-Break">date/time information.</span></li>
				<li>The <strong class="source-inline">output</strong> section specifies where the processed data should be sent. This could be Elasticsearch, a file, a message queue, or <span class="No-Break">another destination.</span></li>
			</ul>
			<p>To see the detailed explanation in action, simply open the <span class="No-Break"><strong class="source-inline">logstash.conf</strong></span><span class="No-Break"> file:</span></p>
			<pre class="console">
input {
  tcp {
    port =&gt; 5000
  }
}
filter {
  json {
    source =&gt; "message"
  }
}
output {
  elasticsearch {
    hosts =&gt; ["elasticsearch:9200"]
    index =&gt; "app-%{+YYYY.MM.dd}"
  }
  stdout { }
}</pre>			<p>Let’s dive into the details<a id="_idIndexMarker767"/> of the <span class="No-Break">given configuration:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Inputs</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">tcp { port =&gt; 5000 }</strong>: This section defines an <strong class="source-inline">input</strong> plugin that listens for data coming in over a TCP socket on port <strong class="source-inline">5000</strong>. Any logs or events sent to this port will be ingested <span class="No-Break">by Logstash.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Filters</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">json { source =&gt; "message" }</strong>: This <strong class="source-inline">filter</strong> plugin parses the incoming data, assuming it’s in JSON format, and extracts the value from the field named <strong class="source-inline">message</strong>. This field is likely where the actual log content resides. By parsing it as JSON, Logstash can understand the structure of the data and make it easier to work with in subsequent <span class="No-Break">processing steps.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Outputs</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">elasticsearch { hosts =&gt; ["elasticsearch:9200"], index =&gt; "app-%{+YYYY.MM.dd}" }</strong>: This <strong class="source-inline">output</strong> plugin sends the processed data to Elasticsearch, a search and analytics engine optimized for handling large volumes of log data. The host’s option specifies the location of the Elasticsearch instance (presumably running on a machine named <strong class="source-inline">elasticsearch</strong> with the default <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">9200</strong></span><span class="No-Break">).</span></li></ul></li>
				<li>The <strong class="source-inline">index</strong> option defines a dynamic index naming pattern. Each day’s logs will be stored in a separate index named <strong class="source-inline">app-YYYY.MM.dd</strong> (where <strong class="source-inline">YYYY</strong> represents the year, <strong class="source-inline">MM</strong> the month, and <strong class="source-inline">dd</strong> the day). This pattern helps in efficient log management and allows you to easily search for logs from <span class="No-Break">specific dates.</span></li>
				<li><strong class="source-inline">stdout { }</strong>: This <strong class="source-inline">output</strong> plugin simply prints the processed data to the console (standard output) for debugging or monitoring purposes. The empty curly braces (<strong class="source-inline">{}</strong>) indicate the default configuration for the <span class="No-Break">standard output.</span></li>
			</ul>
			<p>This Logstash configuration ingests data from TCP source, parses JSON-formatted logs, and then sends them to Elasticsearch for storage and analysis. Daily indexes are created for organized log management. The <strong class="source-inline">stdout</strong> plugin provides a way to view the processed data during development <span class="No-Break">or troubleshooting.</span></p>
			<p>Let’s integrate logging <a id="_idIndexMarker768"/>into our account microservice. Create a new file called <strong class="source-inline">logger-logstash.js</strong> under the <strong class="source-inline">accountmicroservice/src/log</strong> folder with the <span class="No-Break">following content:</span></p>
			<pre class="source-code">
const winston = require('winston');
const LogstashTransport = require('winston-logstash
  /lib/winston-logstash-latest.js');
const serviceName = 'account-microservice'
const logstashTransport = new LogstashTransport({
    host: 'localhost',
    port: 5000
})
const logger = winston.createLogger({
    level: 'info',
    format: winston.format.combine(winston
      .format.timestamp(), winston.format.json()),
    defaultMeta: {
        service: serviceName,
        buildInfo: {
            nodeVersion: process.version
        }
    },
    transports: [
        new winston.transports.Console({
            format: winston.format.combine(
                winston.format.colorize(),
                winston.format.simple()
            )
        }),
        logstashTransport
    ]
})
module.exports = {
    logger
};</pre>			<p>We have already<a id="_idIndexMarker769"/> talked about <strong class="source-inline">winston</strong> configuration and the only new thing here is <strong class="source-inline">logstashTransport</strong>. We added two <strong class="source-inline">transports</strong> channel: one for the terminal’s console and the other one for sending logs to <strong class="source-inline">logstash</strong>. To use the given file with <strong class="source-inline">morgan</strong>, just change <strong class="source-inline">morganmiddleware.js</strong>’s logger to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
const { logger } = require('./log/logger-logstash');</pre>			<p>Now, run our application and go to <strong class="source-inline">http://localhost:5601</strong>; this is our Kibana. From the left menu, find <strong class="bold">Management</strong> | <strong class="bold">Dev tools</strong>.  Click on the <strong class="bold">Execute</strong> button and you will see the total value with your logs (<span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">)</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer099">
					<img alt="Figure 10.2: Getting logs from Kibana" src="image/B09148_10_002.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2: Getting logs from Kibana</p>
			<p>Now, our logs are<a id="_idIndexMarker770"/> flowing to the ELK stack. You can think of Elasticsearch as a search and analytics engine with a data <span class="No-Break">warehouse capability.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor171"/>A brief introduction to Elasticsearch</h2>
			<p><strong class="bold">Elasticsearch</strong> is a powerhouse <a id="_idIndexMarker771"/>search engine built for speed and scalability. At its core, it’s a distributed system designed to store, search, and analyze large volumes of data in near real time. It is document-oriented and <span class="No-Break">uses JSON.</span></p>
			<p>Let’s dive deeper into the key attributes <span class="No-Break">of Elasticsearch:</span></p>
			<ul>
				<li><strong class="bold">Distributed</strong>: Elasticsearch<a id="_idIndexMarker772"/> can store data across multiple nodes (servers) in a cluster. This distribution allows for <span class="No-Break">the following:</span><ul><li><strong class="bold">Fault tolerance</strong>: If one node fails, other nodes can handle the requests, keeping your search <span class="No-Break">service operational.</span></li><li><strong class="bold">Horizontal scaling</strong>: You can easily add more nodes to the cluster as your data volume or search <span class="No-Break">traffic grows.</span></li></ul></li>
				<li><strong class="bold">Scalable</strong>: As mentioned previously, Elasticsearch excels at horizontal scaling. You can add more nodes to the cluster to handle increasing data and search demands. This scalability makes it suitable for large datasets and high <span class="No-Break">search volumes.</span></li>
				<li><strong class="bold">Search and analytics</strong>: Elasticsearch specializes in full-text search, which analyzes the entire text content of your documents. This allows you to search for keywords, phrases, and even concepts within your data. It also provides powerful analytics capabilities. You can aggregate data, identify trends, and gain insights from your <span class="No-Break">search results.</span></li>
				<li><strong class="bold">Flexible search</strong>: Elasticsearch offers a wide range of query options. You can search for specific terms, filter results based on various criteria, and perform complex aggregations. This flexibility allows you to tailor your searches to your specific needs and uncover valuable information from <span class="No-Break">your data.</span></li>
				<li><strong class="bold">Search speed</strong>: Due to its distributed architecture and efficient indexing techniques, Elasticsearch delivers fast search results. This is crucial for applications where <a id="_idIndexMarker773"/>users expect an immediate response to <span class="No-Break">their queries.</span></li>
			</ul>
			<p>In this section, we provided a brief overview of Elasticsearch, focusing on the core attributes that make it a powerful tool for search and <span class="No-Break">data analysis.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor172"/>A brief introduction to Kibana</h2>
			<p><strong class="bold">Kibana</strong> is the last item<a id="_idIndexMarker774"/> in our ELK stack. It is the visualization layer that complements the data storage and search muscle of Elasticsearch. It’s an open source platform that acts as a window into your Elasticsearch data, allowing you to explore, analyze, and understand it with <span class="No-Break">clear visualizations.</span></p>
			<p>Kibana has the following <span class="No-Break">interesting possibilities:</span></p>
			<ul>
				<li><strong class="bold">Visualization powerhouse</strong>: Kibana lets you create interactive dashboards with various charts, graphs, and maps. This visual representation transforms raw data into easily <span class="No-Break">digestible insights.</span></li>
				<li><strong class="bold">Data exploration</strong>: Kibana provides tools to explore, search, and filter your data within Elasticsearch. You can drill down into specific details and uncover <span class="No-Break">hidden patterns.</span></li>
				<li><strong class="bold">Sharing insights</strong>: Created dashboards can be shared with others, fostering collaboration and promoting <span class="No-Break">data-driven decision-making.</span></li>
			</ul>
			<p>There are several compelling reasons to choose Kibana <span class="No-Break">for microservices:</span></p>
			<ul>
				<li><strong class="bold">Effortless integration</strong>: As part of the ELK Stack (Elasticsearch, Logstash, and Kibana), Kibana integrates<a id="_idIndexMarker775"/> seamlessly with Elasticsearch. This tight integration streamlines the process of visualizing data stored <span class="No-Break">within Elasticsearch.</span></li>
				<li><strong class="bold">Real-time insights</strong>: Kibana allows you to visualize data in near real-time, providing valuable insights as your data streams in. This is crucial for applications requiring immediate response <span class="No-Break">to changes.</span></li>
				<li><strong class="bold">Customization options</strong>: Kibana offers a wide range of visualizations and customization options. You can tailor dashboards to fit your specific needs and effectively communicate insights to <span class="No-Break">your audience.</span></li>
				<li><strong class="bold">Open source and free</strong>: Being open source, Kibana is free to use and offers a vibrant community for support <span class="No-Break">and development.</span></li>
			</ul>
			<p>Microservices architectures involve multiple, independent services working together. Kibana shines in this environment for <span class="No-Break">several reasons:</span></p>
			<ul>
				<li><strong class="bold">Monitoring performance</strong>: Visualize key metrics from your microservices on Kibana dashboards to monitor their health and performance. This helps identify bottlenecks and ensure <span class="No-Break">smooth operation.</span></li>
				<li><strong class="bold">Log analysis</strong>: Centralize and analyze logs from all your microservices within Kibana. This unified view simplifies troubleshooting issues and pinpointing errors across <span class="No-Break">the system.</span></li>
				<li><strong class="bold">Application insights</strong>: Gain insights into how users interact with your microservices by visualizing usage patterns and trends within Kibana. This data can guide development efforts and improve <span class="No-Break">user experience.</span></li>
			</ul>
			<p>Learning about the ELK stack, diving into details of Elasticsearch querying and Kibana-related topics, such as<a id="_idIndexMarker776"/> custom dashboards, and working with metrics are beyond the scope of this book and that is why we finish our chapter only with a simple introduction <span class="No-Break">to them.</span></p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/>Summary</h1>
			<p>This chapter delved into the crucial aspects of monitoring and logging in microservices architecture, emphasizing the importance of observability in maintaining the health and performance of distributed systems. We began by explaining how observability provides deep insights into system behavior through key components such as logs, metrics, alerts, <span class="No-Break">and traces.</span></p>
			<p>We then shifted focus to the importance of logging in microservices, which is essential for capturing detailed records of system events, identifying performance bottlenecks, and diagnosing issues in real time. We explored different log levels—error, warning, info, debug, and trace—and discussed how they help categorize log messages based on severity, making troubleshooting more efficient. Additionally, the chapter covered popular logging libraries in Node.js such as <strong class="source-inline">winston</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">morgan</strong></span><span class="No-Break">.</span></p>
			<p>Following the theoretical foundation, we demonstrated how to implement logging in a real-world scenario by integrating <strong class="source-inline">winston</strong> and <strong class="source-inline">morgan</strong> into the <span class="No-Break">account microservice.</span></p>
			<p>The chapter then moved on to centralized logging, introducing the powerful ELK stack. We explained how Logstash collects and processes log data, Elasticsearch stores and indexes the data for real-time search, and Kibana visualizes the information through interactive dashboards. By integrating these tools, we established a centralized logging system that simplifies log collection, analysis, <span class="No-Break">and visualization.</span></p>
			<p>In the next chapter, we will explore how to manage multiple microservices effectively using popular microservices architecture elements. You’ll learn about using an API gateway, which acts as a single entry point to manage requests and direct them to the correct services, as well as organizing data and actions within your system through CQRS and Event Sourcing, two important methods that help handle complex data flows. By the end, you’ll have a clear understanding of building and connecting services in a way that’s efficient and easy <span class="No-Break">to maintain.</span></p>
		</div>
	</body></html>