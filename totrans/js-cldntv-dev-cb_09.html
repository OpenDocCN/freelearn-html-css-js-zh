<html><head></head><body>
        

                            
                    <h1 class="header-title">Optimizing Performance</h1>
                
            
            
                
<p>In this chapter, the following recipes will be covered:</p>
<ul>
<li class="mce-root">Tuning Function as a Service</li>
<li class="mce-root">Batching requests</li>
<li class="mce-root">Leveraging asynchronous non-blocking IO</li>
<li class="mce-root">Grouping events in stream processors</li>
<li class="mce-root">Autoscaling DynamoDB</li>
<li class="mce-root">Utilizing cache-control</li>
<li class="mce-root">Leveraging session consistency</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction</h1>
                
            
            
                
<p>Cloud-native turns performance testing, tuning, and optimization on their heads. Many of the fully-managed, serverless cloud services that are leveraged have implicit scalability. These services are purchased per request and will automatically scale to meet peak and unexpected demands. For these resources, it is much less necessary to perform upfront performance testing; instead, we optimize for observability, as discussed in <a href="64a4c0f7-3b2d-4638-a52c-f72953ff66d9.xhtml" target="_blank">Chapter 7</a>, <em>Optimizing Observability</em>, and continuously tune based on the information gathered from continuous testing in production. We also leverage continuous deployment to push necessary improvements. This worth-based development approach helps ensure that we are focusing our efforts on the highest value improvements.</p>
<p>Still, there are resources, such as some stream processors and data stores, that rely heavily on explicitly defined batch sizes and read/write capacities. For crucial services, these resources must be sufficiently allocated to ensure peak data processing volumes do not overwhelm them. The recipes in this chapter will therefore focus on performance optimization techniques that are worth applying upfront in the design and development process to help ensure services are not working against themselves.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Tuning Function as a Service</h1>
                
            
            
                
<p>Tuning functions is very different from traditional service tuning because there are so few explicit knobs to turn. There are also many implications of the short life cycle of a function that turns traditional techniques and frameworks into anti-patterns. The following recipe explains a common memory mistake, discusses the cold start implications of traditional language and library choices, and will show you how to package a JavaScript function with <strong>webpack</strong> to minimize download time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch9/tuning-faas --path cncb-tuning-faas</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-tuning-faas</kbd> directory, <kbd>cd cncb-tuning-faas</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-tuning-faas<br/><br/>plugins:<br/>  - <strong>serverless-webpack</strong><br/><br/>provider:<br/>  <strong>memorySize</strong>: 1024 # default<br/>  ...<br/><br/>package:<br/>  <strong>individually</strong>: true<br/>custom:<br/> <strong>webpack</strong>:<br/>   <strong>includeModules</strong>: true<br/><br/>functions:<br/>  save:<br/>    <strong>memorySize</strong>: 512 # function specific<br/>    ...</pre>
<ol start="4">
<li>Review the file named <kbd>webpack.config.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">const slsw = require('serverless-webpack');<br/>const nodeExternals = require("webpack-node-externals");<br/>const path = require('path');<br/><br/>module.exports = {<br/>  entry: <strong>slsw.lib.entries</strong>,<br/>  output: {<br/>    libraryTarget: 'commonjs',<br/>    path: path.join(__dirname, '.webpack'),<br/>    filename: '[name].js'<br/>  },<br/>  optimization: {<br/>    <strong>minimize</strong>: false<br/>  },<br/>  target: 'node',<br/>  mode: slsw.lib.webpack.isLocal ? "development" : "production",<br/>  externals: [<br/>    <strong>nodeExternals</strong>()<br/>  ],<br/>  module: {<br/>    rules: [<br/>      {<br/>        test: /\.js$/,<br/>        use: [<br/>          {<br/>            <strong>loader</strong>: 'babel-loader',<br/>          }<br/>        ],<br/>        include: __dirname,<br/>        exclude: /node_modules/<br/>      }<br/>    ]<br/>  }<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory, note the ZIP file per function, and then unzip each to see the contents.</li>
<li>Deploy the stack, <kbd>npm run dp:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>
<ol start="9">
<li>Review the stack and resources in the AWS Console and note the code size in the Lambda console.</li>
<li>Finally, remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>The most obvious knob we have for tuning <strong>Function as a Service</strong> (<strong>FaaS</strong>) is <kbd>memorySize</kbd> allocation. This setting drives the price calculation as well. Unfortunately, the correlation with price tends to be counter-intuitive and can result in decisions that actually increase costs, while also reducing performance. The way AWS Lambda pricing works is if you double the memory allocation but consequently cut the execution time in half, the price is the same. The corollary is that if you cut memory allocation in half and consequently double the execution time, you are spending the same amount of money for less performance. It works this way because memory allocation actually correlates to the machine instance size that a function is executed on. More memory allocation means that the function will also have a faster CPU and more network IO throughput. In short, <em>do not skimp on memory allocation</em>.</p>
<p>A major concern and source of confusion with Function as a Service is cold start times. For asynchronous functions, such as processing a Kinesis Stream, cold start times are not as concerning. This is because cold start frequency is lower as functions tend to be reused for several hours for each shard. However, minimizing cold start times for synchronous functions behind an API Gateway is very important, because multiple functions are started to accommodate concurrent load. As a result, cold start times could impact the end user experience.</p>
<p>The first thing that impacts cold start time is the size of the function package that must be downloaded to the container. This is one reason that allocating more memory and hence more network IO throughput improves the performance of a function. It is also important to minimize the size of the package that must be downloaded. We will discuss how to use webpack to optimize JavaScript functions shortly.</p>
<p>The next thing that impacts cold start times is the choice of language or runtime. Scripting languages, such as JavaScript and Python, do very little at startup and thus have very little impact on cold start times. Conversely, Java must do work at startup to load classes and prepare the JVM. As the number of classes increases, the impact on cold starts also increases. This leads to the next impact on cold start times: the choice of libraries and frameworks, such as <strong>object relation mapping</strong> (<strong>ORM</strong>) and dependency injection frameworks, and connection pooling libraries. These tend to do a lot of work at startup because they were designed to work in long running servers.</p>
<p>A common issue among FaaS developers using Java is the improvement of cold start times for functions written with Spring and Hibernate; however these tools were not designed for FaaS in the first place. I have programmed in Java for over 20 years, from when it first appeared in the 1990s. I was skeptical about changing to JavaScript at first, but this cookbook is testament to its fit with cloud-native and serverless architecture. It is worth noting, however, that polyglot programming is the best policy; use the right programming language for a specific service, but understand the implications of it when using it with Faas.</p>
<p>To minimize a JavaScript function's package size, we leverage webpack for the same reasons we use it to minimize downloads to browsers. Webpack performs tree shaking, which removes unused code to reduce package size. In the <kbd>serverless.yml</kbd> file, we include the <kbd>serverless-webpack</kbd> plugin and configure it to package functions individually. Packaging functions individually allows us to maximize the benefits of tree shaking. The <kbd>webpack.config.js</kbd> file further controls the packaging process. The <kbd>serverless-webpack</kbd> plugin provides the <kbd>slsw.lib.entries</kbd> utility so that we do not need to duplicate the function names to define all the <kbd>entry</kbd> points. We also turn off the <kbd>minimize</kbd> feature, which uglifies the code. We do this to avoid including source maps for debugging, which significantly increases the package size. We also exclude all of the external libraries in the <kbd>node_modules</kbd> folder and configure the plugin to <kbd>includeModules</kbd>, which includes those that are actually used as runtime. One special exception is the <kbd>aws-sdk</kbd> module, which is never included because it is already available in the function container. The end result is a lean function package that contains only what is necessary.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Batching requests</h1>
                
            
            
                
<p>The design of a stream processor must account for the volume of data it will receive. The data should be processed in real time and the processor should not fall behind. The following recipe demonstrates how to use DynamoDB batch writes to help ensure sufficient throughput.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>Before starting this recipe, you will need an AWS Kinesis Stream, such as the one created in the <em>Creating an event stream</em> recipe.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch9/frp-batching --path cncb-frp-batching</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-frp-batching</kbd> directory, <kbd>cd cncb-frp-batching</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd>.</li>
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToUow)<br/>    .filter(forPurple)<br/>    .ratelimit(Number(process.env.WRITE_CAPACITY) /<br/>      Number(process.env.SHARD_COUNT) /<br/>      Number(process.env.<strong>WRITE_BATCH_SIZE</strong>) / 10, 100)<br/>    .<strong>batch</strong>(Number(process.env.<strong>WRITE_BATCH_SIZE</strong>))<br/>    .map(<strong>batchUow</strong>)<br/>    .<strong>flatMap</strong>(<strong>batchWrite</strong>)<br/>    .collect().toCallback(cb);<br/>};<br/><br/>const <strong>batchUow</strong> = batch =&gt; ({ batch });<br/><br/>const batchWrite = batchUow =&gt; {<br/>  batchUow.params = {<br/>    <strong>RequestItems</strong>: {<br/>      [process.env.TABLE_NAME]: batchUow.batch.map(uow =&gt;<br/>        ({<br/>          PutRequest: {<br/>            Item: uow.event<br/>          }<br/>        })<br/>      )<br/>    },<br/>  };<br/><br/>  ...<br/><br/>  return _(db.<strong>batchWrite</strong>(batchUow.params).promise()<br/>    .then(data =&gt; (<br/>      Object.keys(data.<strong>UnprocessedItems</strong>).length &gt; 0 ?<br/>        Promise.reject(data) :<br/>        batchUow<br/>    ))<br/>    .catch(err =&gt; {<br/>      err.uow = <strong>batchUow</strong>;<br/>      throw err;<br/>    })<br/>  );<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack, <kbd>npm run dp:lcl -- -s $MY_STAGE</kbd>.</li>
<li>Review the stack and resources in the AWS Console.</li>
<li>Invoke the <kbd>simulate</kbd> function with the following command:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -f simulate -r us-east-1 -s $MY_STAGE</strong><br/>[<br/>    {<br/>        "<strong>total</strong>": 3850,<br/>        "orange": 942,<br/>        "<strong>purple</strong>": 952,<br/>        "blue": 1008,<br/>        "green": 948<br/>    }<br/>]</pre>
<ol start="11">
<li>Take a look at the following <kbd>listener</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'event count'<br/></strong>2018-08-04 23:46:53 ... event count: 100<br/>2018-08-04 23:46:57 ... event count: 1000<br/>2018-08-04 23:47:23 ... event count: 250<br/>2018-08-04 23:47:30 ... event count: 1000<br/>2018-08-04 23:47:54 ... event count: 1000<br/>2018-08-04 23:48:18 ... event count: 500<strong><br/><br/></strong><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'Duration'<br/></strong>REPORT ... Duration: 3688.65 ms ...<br/>REPORT ... Duration: 25869.08 ms ...<br/>REPORT ... Duration: 7293.39 ms ...<br/>REPORT ... Duration: 23662.65 ms ...<br/>REPORT ... Duration: 24752.11 ms ...<br/>REPORT ... Duration: 13983.72 ms ...<br/><br/><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'retries'<br/></strong>...<br/>2018-08-04 23:48:20 ... [AWS dynamodb 200 0.031s 0 retries] batchWriteItem({ RequestItems:<br/>   { 'john-cncb-frp-batching-things':<br/>      [ { <strong>PutRequest</strong>:<br/>           { Item:<br/>              { id: { S: '320e6023-9862-11e8-b0f6-01e9feb460f5' },<br/>                type: { S: 'purple' },<br/>                timestamp: { N: '1533440814882' },<br/>                partitionKey: { S: '5' },<br/>                tags: { M: { region: { S: 'us-east-1' } } } } } },<br/>        { <strong>PutRequest</strong>:<br/>           { Item:<br/>              { id: { S: '320e6025-9862-11e8-b0f6-01e9feb460f5' },<br/>                type: { S: 'purple' },<br/>                timestamp: { N: '1533440814882' },<br/>                partitionKey: { S: '1' },<br/>                tags: { M: { region: { S: 'us-east-1' } } } } } },<br/>        ...<br/>        [length]: 10 ] },<br/>  ReturnConsumedCapacity: 'TOTAL',<br/>  ReturnItemCollectionMetrics: 'SIZE' })<br/>...</pre>
<ol start="12">
<li>Finally, remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>If a stream processor receives a batch of 1,000 events, will it execute faster if it has to make 1,000 requests to the database or just 100 requests? The answer of course depends on many variables, but in general, making fewer calls over the network is better because it minimizes the impact of network latency. To this end, services such as DynamoDB and Elasticsearch<em> </em>provide APIs that allow batches of commands to be submitted in a single request. In this recipe, we use DynamoDB's <kbd>batchWrite</kbd> operation. To prepare a batch, we simply add a <kbd>batch</kbd> step to the pipeline and specify the <kbd>WRITE_BATCH_SIZE</kbd>. This performance improvement is very simple to add, but it is important to keep in mind that batching requests increase the rate at which DynamoDB's write capacity is consumed. Therefore, it is necessary to include the <kbd>WRITE_BATCH_SIZE</kbd> in the <kbd>ratelimit</kbd> calculation and increase the write capacity accordingly, as discussed in the <em>Implementing backpressure and rate limiting</em> recipe.</p>
<p>Another important thing to note is that these batch requests are not treated as a single transaction. Some commands may succeed and others may fail in a single request; it is therefore necessary to inspect the response for <kbd>UnprocessedItems</kbd> that needs to be resubmitted. In this recipe, we treat each batch as a <strong>unit of work</strong> (<strong>uow</strong>) and raise a fault for the entire batch, as discussed in the <em>Handling faults</em> recipe. This is a good, safe place to start before tuning the logic to retry only the commands that fail. Note that, ultimately, you would only raise a fault when the maximum number of retries has been attempted.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Leveraging asynchronous non-blocking IO</h1>
                
            
            
                
<p>The design of a stream processor must account for the volume of data it will receive. The data should be processed in real-time and the processor should not fall behind. The following recipe demonstrates how to leverage asynchronous, non-blocking IO to process data in parallel to help ensure sufficient throughput.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch9/frp-async-non-blocking-io --path cncb-frp-async-non-blocking-io</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-frp-async-non-blocking-io</kbd> directory, <kbd>cd cncb-frp-async-non-blocking-io</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd>.</li>
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToUow)<br/>    .filter(forPurple)<br/>    .ratelimit(Number(process.env.WRITE_CAPACITY) /<br/>      Number(process.env.SHARD_COUNT) / <br/>      Number(process.env.<strong>PARALLEL</strong>) / 10, 100)<br/>    .<strong>map</strong>(<strong>put</strong>)<br/>    .<strong>parallel</strong>(Number(process.env.<strong>PARALLEL</strong>))<br/>    .collect().toCallback(cb);<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack <kbd>npm run dp:lcl -- -s $MY_STAGE</kbd>.</li>
<li>Review the stack and resources in the AWS Console.</li>
<li>Invoke the <kbd>simulate</kbd> function with the following command:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -f simulate -r us-east-1 -s $MY_STAGE</strong><br/>[<br/>    {<br/>        "<strong>total</strong>": 4675,<br/>        "blue": 1136,<br/>        "green": 1201,<br/>        "<strong>purple</strong>": 1167,<br/>        "orange": 1171<br/>    }<br/>]</pre>
<ol start="11">
<li>Take a look at the following <kbd>listener</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'event count'<br/></strong>2018-08-05 00:03:05 ... event count: 1675<br/>2018-08-05 00:03:46 ... event count: 1751<br/>2018-08-05 00:04:34 ... event count: 1249<strong><br/><br/></strong><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'Duration'<br/></strong>REPORT ... Duration: 41104.28 ms ...<br/>REPORT ... Duration: 48312.47 ms ...<br/>REPORT ... Duration: 31450.13 ms ...<br/><br/><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'retries'<br/></strong>...<br/>2018-08-05 00:04:58.034 ... [AWS dynamodb 200 0.024s 0 retries] ...<br/>2018-08-05 00:04:58.136 ... [AWS dynamodb 200 0.022s 0 retries] ...<br/>2018-08-05 00:04:58.254 ... [AWS dynamodb 200 0.034s 0 retries] ...<br/>2018-08-05 00:04:58.329 ... [AWS dynamodb 200 0.007s 0 retries] ...<br/>2018-08-05 00:04:58.430 ... [AWS dynamodb 200 0.007s 0 retries] ...<br/>2018-08-05 00:04:58.540 ... [AWS dynamodb 200 0.015s 0 retries] ...<br/>2018-08-05 00:04:58.661 ... [AWS dynamodb 200 0.035s 0 retries] ...<br/>2018-08-05 00:04:58.744 ... [AWS dynamodb 200 0.016s 0 retries] ...<br/>2018-08-05 00:04:58.843 ... [AWS dynamodb 200 0.014s 0 retries] ...<br/>2018-08-05 00:04:58.953 ... [AWS dynamodb 200 0.023s 0 retries] ...<br/>...</pre>
<ol start="12">
<li>Finally, remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p><em>Asynchronous non-blocking IO</em> is extremely valuable for maximizing throughput. Without it, a stream processor will block and do nothing until an external call has completed. This recipe demonstrates how to use a <kbd>parallel</kbd> step to control the number of concurrent calls that can execute. As an example of the impact this can have, I once had a script that read from S3 and would take well over an hour to process, but once I added a <kbd>parallel</kbd> step with a setting of 16, the script executed in just five minutes. The improvement was so significant that Datadog contacted me, almost immediately, to see if we had a runaway process.</p>
<p>To allow concurrent calls, we simply add a <kbd>parallel</kbd> step to the pipeline after an external call step and specify the <kbd>PARALLEL</kbd> amount. This performance improvement is very simple to add, but it is important to keep in mind that parallel requests increase the rate at which DynamoDB's write capacity is consumed. It is therefore necessary to include the <kbd>PARALLEL</kbd> amount in the <kbd>ratelimit</kbd> calculation and increase the write capacity accordingly, as discussed in the <em>Implementing backpressure and rate limiting</em> recipe. Further performance improvements may be achieved by combining parallel execution with grouping and batching.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Grouping events in stream processors</h1>
                
            
            
                
<p>The design of a stream processor must account for the volume of data it will receive. The data should be processed in real-time and the processor should not fall behind. The following recipe demonstrates how grouping related data in a stream can help ensure sufficient throughput.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch9/frp-grouping --path cncb-frp-grouping</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-frp-grouping</kbd> directory, <kbd>cd cncb-frp-grouping</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd>.</li>
</ol>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToUow)<br/>    .filter(forPurple)<br/>    .<strong>group</strong>(uow =&gt; uow.event.<strong>partitionKey</strong>)<br/>    .<strong>flatMap</strong>(<strong>groupUow</strong>)<br/>    .ratelimit(Number(process.env.WRITE_CAPACITY) /<br/>      Number(process.env.SHARD_COUNT) / 10, 100)<br/>    .flatMap(put)<br/>    .collect().toCallback(cb);<br/>};<br/><br/>const <strong>groupUow</strong> = groups =&gt; _(Object.keys(groups).map(key =&gt; ({ <strong>batch</strong>: groups[key]})));<br/><br/>const put = groupUow =&gt; {<br/>  const params = {<br/>    TableName: process.env.TABLE_NAME,<br/>    Item: <strong>groupUow.batch[groupUow.batch.length - 1].event, // last</strong><br/>  };<br/>  ...<br/>  return _(db.put(params).promise()<br/>    .then(() =&gt; groupUow)<br/>  );<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack <kbd>npm run dp:lcl -- -s $MY_STAGE</kbd>.</li>
<li>Review the stack and resources in the AWS Console.</li>
<li>Invoke the <kbd>simulate</kbd> function with the following command:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -f simulate -r us-east-1 -s $MY_STAGE</strong><br/>[<br/>    {<br/>        "<strong>total</strong>": 4500,<br/>        "blue": 1134,<br/>        "green": 1114,<br/>        "<strong>purple</strong>": 1144,<br/>        "orange": 1108<br/>    }<br/>]</pre>
<p class="mce-root"/>
<ol start="11">
<li>Take a look at the following <kbd>listener</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'event count'<br/></strong>2018-08-05 00:28:19 ... event count: 1000<br/>2018-08-05 00:28:20 ... event count: 1000<br/>2018-08-05 00:28:21 ... event count: 650<br/>2018-08-05 00:28:22 ... event count: 1000<br/>2018-08-05 00:28:23 ... event count: 850<strong><br/><br/></strong><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'Duration'<br/></strong>REPORT ... Duration: 759.50 ms ...<br/>REPORT ... Duration: 611.70 ms ...<br/>REPORT ... Duration: 629.91 ms ...<br/>REPORT ... Duration: 612.90 ms ...<br/>REPORT ... Duration: 623.11 ms ...<br/><br/><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'retries'<br/></strong>2018-08-05 00:28:20.197 ... [AWS dynamodb 200 0.112s 0 retries] ...<br/>2018-08-05 00:28:20.320 ... [AWS dynamodb 200 0.018s 0 retries] ...<br/>...<br/>2018-08-05 00:28:23.537 ... [AWS dynamodb 200 0.008s 0 retries] ...<br/>2018-08-05 00:28:23.657 ... [AWS dynamodb 200 0.019s 0 retries] ...</pre>
<ol start="12">
<li>Finally, remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>The <em>Batching requests</em> recipe demonstrates how to minimize the overhead of network IO by batching multiple unrelated commands into a single request. Another way to minimize network IO is to simply reduce the number of commands that need to be executed by grouping related events, and only executing a single command per grouping. For example, we might perform a calculation per group or just sample some of the data. In this recipe, we grouped events by the <kbd>partitionKey</kbd>. We can group events by any data in the events, but the best results are achieved when the grouping is relative to the partition key; this is because the partition key ensures that related events are sent to the same shard.</p>
<p>The <kbd>group</kbd> step makes it straightforward to reduce related events into groups based on the content of the events. For more complicated logic, a <kbd>reduce</kbd> step can be used directly. Next, we map each group to a unit of work (<kbd>groupUow</kbd>) that must succeed or fail together, as discussed in the <em>Handling faults</em> recipe. Finally, as shown in the preceding example, we write the <kbd>last</kbd> event of each group. Note from the logs that grouping results in significantly fewer writes; for this specific run, there were 4,500 events simulated and only 25 writes. Further performance improvements may be achieved by combining grouping with batching and parallel invocations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Autoscaling DynamoDB</h1>
                
            
            
                
<p>Cloud-native, with FaaS and serverless, minimize the amount of effort that is needed to scale the infrastructure that supports the service layer. However, we now need to focus on tuning the stream processors and minimize any throttling of the target data store. The following recipe demonstrates how to use DynamoDB autoscaling to help ensure that enough capacity is allocated to provide sufficient throughput and avoid throttling.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch9/dynamodb-autoscaling --path cncb-dynamodb-autoscaling</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-dynamodb-autoscaling</kbd> directory, <kbd>cd cncb-dynamodb-autoscaling</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-dynamodb-autoscaling<br/><br/>...<br/><br/>plugins:<br/>  - <strong>serverless-dynamodb-autoscaling-plugin</strong><br/><br/>custom:<br/>  <strong>autoscaling</strong>:<br/>    - table: <strong>Table</strong><br/>      <strong>write</strong>:<br/>        minimum: 5<br/>        maximum: 50<br/>        <strong>usage</strong>: 0.6<br/>        <strong>actions</strong>:<br/>          - name: <strong>morning</strong><br/>            minimum: 5<br/>            maximum: 50<br/>            <strong>schedule</strong>: cron(0 6 * * ? *)<br/>          - name: <strong>night</strong><br/>            minimum: 1<br/>            maximum: 1<br/>            schedule: cron(0 0 * * ? *)<br/>      read:<br/>        ...<br/><br/>resources:<br/> Resources:<br/>   <strong>Table</strong>:<br/>     Type: AWS::DynamoDB::Table<br/>     ...</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd>.</li>
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack <kbd>npm run dp:lcl -- -s $MY_STAGE</kbd>.</li>
<li>Review the stack and resources in the AWS Console.</li>
<li>Invoke the <kbd>simulate</kbd> function multiple times with the following command to trigger autoscaling, <kbd>sls invoke -f simulate -r us-east-1 -s $MY_STAGE</kbd>.</li>
<li>Take a look at the following <kbd>listener</kbd> function logs with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'event count'<br/>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter 'Duration'<br/>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter '"1 retries"'<strong><br/></strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter '"2 retries"'<br/>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter '"3 retries"'<br/>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter '"4 retries"'<br/>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter '"5 retries"'<br/>$ sls logs -f listener -r us-east-1 -s $MY_STAGE --filter '"6 retries"'</pre>
<ol start="12">
<li>Watch the Write capacity and Throttled write requests metrics on the DynamoDB Metrics tab in the AWS Console to see the autoscaling increment meet the demand, and then scale down at <kbd>night</kbd> and back up again in the <kbd>morning</kbd>.</li>
<li>Finally, remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In the <em>Implementing backpressure and rate limiting</em> recipe, we see how it is important for stream processors to minimize throttling to maximize throughput. In this chapter, we have discussed techniques to optimize throughput, such as batching, grouping and asynchronous non-blocking requests, which all increase the data store capacity that must be allocated. However, while we do need to ensure that we have sufficient capacity, we also want to minimize wasted capacity, and autoscaling helps us achieve that. Autoscaling can address demand that grows over time to an expected peak, predictable demand, such as known events, and unpredictable demand.</p>
<p>In this recipe, we use the <kbd>serverless-dynamodb-autoscaling-plugin</kbd> to define the <kbd>autoscaling</kbd> policies on a per table basis. For both the <kbd>read</kbd> and <kbd>write</kbd> capacity, we specify the <kbd>minimum</kbd> and <kbd>maximum</kbd> capacity and the desired <kbd>usage</kbd> percentage. This <kbd>usage</kbd> percentage defines the amount of headroom we would like to have so that we can increase capacity early enough to help ensure that additional capacity is allocated before we reach 100 percent utilization and begin to throttle. We can also schedule autoscaling <kbd>actions</kbd> at specific times. In this recipe, we scale down at <kbd>night</kbd> to minimize waste and then scale back up in the <kbd>morning</kbd> before typical demand arrives.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Utilizing cache-control</h1>
                
            
            
                
<p>Autonomous, cloud-native services maintain their own materialized views and store this replicated data in highly-available and extremely performant cloud-native databases. When combined with the performance of an API Gateway and FaaS, it is typically unnecessary to add a traditional caching mechanism to achieve the desired performance for a user-facing, <strong>backend-for-frontend</strong> (<strong>BFF</strong>) service. That being said, this doesn't mean we shouldn't take advantage of the CDN, such as CloudFront, that is already wrapping a service. The following recipe will therefore show you how to utilize cache-control headers and leverage a CDN to improve performance for end users, as well as reduce the load on a service.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch9/cache-control --path cncb-cache-control</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-cache-control</kbd> directory, <kbd>cd cncb-cache-control</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd>.</li>
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>get</strong> = (request, context, callback) =&gt; {<br/>  const response = {<br/>    statusCode: 200,<br/>    headers: {<br/>      '<strong>Cache-Control</strong>': '<strong>max-age=5</strong>',<br/>    },<br/>    body: ...,<br/>  };<br/><br/>  callback(null, response);<br/>};<br/><br/>module.exports.<strong>save</strong> = (request, context, callback) =&gt; {<br/> const response = {<br/>   statusCode: 200,<br/>   headers: {<br/>     '<strong>Cache-Control</strong>': '<strong>no-cache, no-store, must-revalidate</strong>',<br/>   },<br/> };<br/><br/> callback(null, response);<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack, as follows:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/>...<br/>Stack Outputs<br/><strong>ApiDistributionEndpoint: https://d2thj6o092tkou.cloudfront.net</strong><br/>...</pre>
<p>Deploying a CloudFront distribution can often take upwards of 20 minutes.</p>
<ol start="9">
<li>Review the stack and resources in the AWS Console.</li>
<li>Invoke the endpoint shown in the stack output with the following <kbd>curl</kbd> command multiple times to see the difference in performance for the cached results:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl -s -D - -w "Time: %{time_total}" -o /dev/null  https://&lt;see stack output&gt;.cloudfront.net/things/123 | egrep -i 'X-Cache|Time'<br/></strong><br/>X-Cache: Miss from cloudfront<br/>Time: <strong>0.712</strong><br/>$ curl ...<br/>X-Cache: Hit from cloudfront<br/>Time: <strong>0.145<br/><br/>$ curl -v -X POST -H 'Content-Type: application/json' -d '{"name":"thing 1"}' https://&lt;see stack output&gt;.cloudfront.net/things<br/><br/></strong>...<br/>&lt; HTTP/1.1 200 OK<br/>&lt; <strong>Cache-Control: no-cache, no-store, must-revalidate</strong><br/>&lt; X-Cache: Miss from cloudfront<br/>...<strong><br/></strong></pre>
<ol start="11">
<li>Finally, remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this related recipe we focus on the service side of the equation. Cloud-native databases, such as DynamoDB, respond in the low 10s of milliseconds, and the overall latency across AWS API Gateway and AWS Lambda for a BFF service should typically execute in the low 100s of milliseconds. So long as the database capacity is set appropriately and throttling is minimized, it would be hard to make a noticeable improvement in this performance from the end user's perspective. The only way to really improve on this is to not have to make a request at all.</p>
<p>This is a case where cloud-native can really be counter-intuitive. Traditionally, to improve performance, we would need to increase the amount of infrastructure and add an expensive caching layer between the code and the database. In other words, we would need to spend a lot more money to improve performance. However, in this recipe, we are leveraging an extremely low-cost edge cache to both improve performance and lower cost. By adding <kbd>Cache-Control</kbd> headers, such as <kbd>max-age</kbd>, to our responses, we can tell a browser not to repeat a request and also tell the CDN to reuse a response for other users. As a result, we reduce load on the API Gateway and the function and reduce the necessary capacity for the database, which reduces the cost for all of these services. It is also good practice to explicitly control which actions should store <kbd>no-cache</kbd>, for example the <kbd>PUT</kbd>, <kbd>POST</kbd>, and <kbd>DELETE</kbd> methods.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Leveraging session consistency</h1>
                
            
            
                
<p>The design of a cloud-native frontend application must account for the fact that the system should be eventually consistent. For example, in a traditional frontend application, it is not uncommon to save data and then immediately execute a query to retrieve that same data. However, in an eventually consistent system, it is very likely that the query would not find the data on the first try. Instead, cloud-native frontends leverage the fact that single page applications can—at minimum—cache data locally for the duration of the user's session. This approach is referred to as session consistency. The following recipe demonstrates how to use the popular Apollo Client (<a href="https://www.apollographql.com/client">https://www.apollographql.com/client</a>) with ReactJS to improve perceived performance and reduce load on the system by implementing <strong>session consistency</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the <kbd>service</kbd> and <kbd>spa</kbd> projects from the following templates:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch9/session-consistency/spa --path cncb-session-consistency-spa</strong><br/><br/><strong>$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch9/session-consistency/service --path cncb-session-consistency-service</strong></pre>
<ol start="2">
<li>Deploy the <kbd>service</kbd> with the following commands:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ cd ./cncb-session-consistency-service</strong><br/><strong>$ npm install</strong><br/><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong></pre>
<ol start="3">
<li>Navigate to the <kbd>cncb-session-consistency-spa</kbd> directory, <kbd>cd ../cncb-session-consistency-spa</kbd>.</li>
<li>Review the file named <kbd>src/index.js</kbd> with the following content and update the <kbd>uri</kbd> with the value output by the <kbd>service</kbd> stack, as follows:</li>
</ol>
<pre style="padding-left: 30px">...<br/>const client = new <strong>ApolloClient</strong>({<br/>    link: new HttpLink({<br/>        // CHANGE ME<br/>        <strong>uri</strong>: 'https://<em>&lt;API_ID&gt;</em>.execute-api.us-east-1.amazonaws.com/<em>&lt;STAGE&gt;</em>/graphql',<br/>    }),<br/>    cache: new <strong>InMemoryCache</strong>(),<br/>});<br/>...</pre>
<ol start="5">
<li>Review the file named <kbd>src/App.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">...<br/>const <strong>AddThing</strong> = () =&gt; {<br/>  ...<br/>  return (<br/>    &lt;Mutation<br/>      mutation={SAVE_THING}<br/>      <strong>update</strong>={(cache, { data: { <strong>saveThing</strong> } }) =&gt; {<br/>        const { things } = cache.<strong>readQuery</strong>({ query: GET_THINGS });<br/>        cache.<strong>writeQuery</strong>({<br/>          query: GET_THINGS,<br/>          data: { things: { items: things.items.<strong>concat</strong>([<strong>saveThing</strong>]) } }<br/>        });<br/>      }}<br/>    &gt;<br/>      ...<br/>    &lt;/Mutation&gt;<br/>  );<br/>};<br/>...</pre>
<ol start="6">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the app locally with <kbd>npm start</kbd>.</li>
<li>Browse to <kbd>http://localhost:3000</kbd>.</li>
<li>Add and update several things to notice how the query results update.</li>
<li>Finally, remove the service stack once you are finished, as follows:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ cd ../cncb-session-consistency-service</strong><br/><strong>$ npm run rm:lcl -- -s $MY_STAGE</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we use a GraphQL BFF that is similar to the one we created in the <em>Implementing a GraphQL CRUD BFF</em> recipe. The focus here is on the frontend application, which we create with ReactJS and the Apollo Client, and specifically on how to cache our interactions with the service. First, we create the <kbd>ApolloClient</kbd> in the <kbd>src/index.js</kbd> file and initialize it with the endpoint for the service and, most importantly, the <kbd>InMemoryCache</kbd> object.</p>
<p>Next, we implement the user interface in the <kbd>src/App.js</kbd> file. The screen displays a list of things that are returned from the <kbd>things</kbd> query. The Apollo Client will automatically cache the results of the query. The mutation that updates the individual objects will automatically update the cache and thus trigger the screen to re-render. Note that adding new data requires more effort. The <kbd>AddThing</kbd> function uses the mutation's <kbd>update</kbd> feature to keep the cache in sync and trigger a re-render. The <kbd>update</kbd> function receives a reference to the cache and the object that was returned from the mutation. We then call <kbd>readQuery</kbd> to read the query from the cache, append the new object to the query results, and finally update the cache by calling <kbd>writeQuery</kbd>.</p>
<p>The end result is a very low-latency user experience because we are optimizing the number of requests that are performed, the amount of data that is transferred, and the amount of memory that is used. Most importantly, for both new and updated data, we are not throwing away anything that was created on the client side and replacing it with the same, retrieved values—after all, it is just unnecessary work. We already have the data, so why should we throw it away and retrieve it again? We also cannot be certain that the data is consistent on the service side—unless we perform a consistent read that is slower, costs more and, as stated, is unnecessary. Session consistency becomes even more valuable for multi-regional deployments in the event of a regional failure. As we will discuss in <a href="7ddedd13-fcee-4091-8566-02c9814cb782.xhtml" target="_blank">Chapter 10</a>, <em>Deploying to Multiple Regions</em>, eventually consistent, cloud-native systems are very tolerant of regional failure because they are already tolerant of eventual consistency, which can be more protracted during a failover. Therefore, session consistency helps make a regional failure transparent to the end user. For any data that must remain available during a regional failure, we can take session consistency a step further and persist the user session in local storage.</p>


            

            
        
    </body></html>