- en: Deploying Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring your service to run in a container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running multi-container applications with Docker Compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying your service on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test releases with canary deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The way we deliver software to users has changed dramatically over the years.
    In the not too distant past, it was common to deploy to production by running
    a shell script on a collection of servers that pulled an update from some kind
    of source control repository. The problems with this approach are clear—scaling
    this out was difficult, bootstrapping servers was error prone, and deployments
    could easily get stuck in an undesired state, resulting in unpredictable experiences
    for users.
  prefs: []
  type: TYPE_NORMAL
- en: The advent of configuration management systems, such as **Chef** or **Puppet**,
    improved this situation somewhat. Instead of having custom bash scripts or commands
    that ran on remote servers, remote servers could be tagged with a kind of role
    that instructed them on how to configure and install software. The declarative
    style of automating configuration was better suited for large-scale software deployments.
    Server automation tools such as **Fabric** or **Capistrano** were also adopted;
    they sought to automate the process of pushing code to production, and are still
    very popular today for applications that do not run in containers.
  prefs: []
  type: TYPE_NORMAL
- en: Containers have revolutionized the way we deliver software. Containers allow
    developers to package their code with all the dependencies, including libraries,
    the runtime, OS tools, and configurations. This allows code to be delivered without
    the need to configure the host server, which dramatically simplifies the process
    by removing the number of moving pieces.
  prefs: []
  type: TYPE_NORMAL
- en: The process of shipping services in containers has been referred to as **immutable
    infrastructure**, because once an image is built, it isn't typically changed;
    instead, new versions of a service result in a new image being built.
  prefs: []
  type: TYPE_NORMAL
- en: Another big change in how software is deployed is the popularization of the
    twelve-factor methodology ([https://12factor.net/](https://12factor.net/)). **Twelve-factor**
    (or **12f**, as it is commonly written) is a set of guidelines originally written
    by engineers at Heroku. At their core, twelve-factor apps are designed to be loosely
    coupled with their environment, resulting in services that can be used along with
    various logging tools, configuration systems, package management systems, and
    source control systems. Arguably, the most universally adopted concepts employed
    by twelve-factor apps are that the configuration is accessed through environment
    variables and logs are output to standard out. As we saw in the previous chapters,
    this is how we've integrated with systems such as Vault. These chapters are worth
    a read, but we've already been following many concepts described in twelve-factor
    so far in this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll be discussing containers, orchestration, and scheduling,
    and various methods for safely shipping changes to users. This is a very active
    topic, and new techniques are being improvised and discussed, but the recipes
    in this chapter should serve as a good starting point, especially if you're accustomed
    to deploying monoliths on virtual machines or bare metal servers.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring your service to run in a container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, services are made up of source code and configurations. A service
    written in Java, for instance, can be packaged as a **Java Archive** (**JAR**)
    file containing compiled class files in Java bytecode, as well as resources such
    as configuration and properties files. Once packaged, the JAR file can then be
    executed on any machine running a **Java Virtual Machine** (**JVM**).
  prefs: []
  type: TYPE_NORMAL
- en: In order for this to work, however, the machine that we want to run our service
    on must have a JVM installed. Oftentimes, it must be a specific version of the
    JVM. Additionally, the machine might need to have some other utilities installed,
    or it might need access to a shared filesystem. While these are not parts of the
    service themselves, they do make up what we refer to as the runtime environment
    of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Linux containers are a technology that allow developers to package an application
    or service with its complete runtime environment. Containers separate out the
    runtime for a particular application from the runtime of the host machine that
    the container is running on.
  prefs: []
  type: TYPE_NORMAL
- en: This makes applications more portable, making it easier to move a service from
    one environment to another. An engineer can run a service on their laptop, then
    move it into a preproduction environment, and then into production, without changing
    the container itself. Containers also allow you to easily run multiple services
    on the same machine, therefore allowing much more flexibility in how application
    architectures are deployed and providing opportunities for operational cost optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Docker is a container runtime and set of tools that allows you to create self-contained
    execution environments for your service. There are other popular container runtimes
    they are widely used today, but Docker is designed to make containers portable
    and flexible, making it an ideal choice for building containers for services.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll use Docker to create an image that packages our message-service.
    We'll do this by creating a `Dockerfile` file and using the Docker command-line
    utility to create an image and then run that image as a container.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, open our message-service project from the previous chapters. Create
    a new file in the root of the project called `Dockerfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Dockerfile` file defines the base image that we''ll use to build our message-service
    image. In this case, we''re basing our image off of an Alpine Linux image with
    OpenJDK 8\. Next, we expose the port that our service binds to and define how
    to run our service after it''s packaged as a JAR file. We''re now ready to use
    the `Dockerfile` file to build an image. This is done with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify that the preceding command worked by running `docker images`
    and seeing ours listed. Now we''re ready to run the message service by executing
    our service in a container. This is done with the `docker run` command. We''ll
    also give it a port mapping and specify the image that we want to use to run our
    service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Running multi-container applications with Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Services rarely run in isolation. A microservice usually connects to a data
    store of some kind, and could have other runtime dependencies. In order to work
    on a microservice, it's necessary to run it locally on a developer's machine.
    Requiring engineers to manually install and manage all the runtime dependencies
    of a service in order to work on a microservice would be impractical and time
    consuming. Instead, we need a way to automatically manage runtime service dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Containers have made services more portable by packaging the runtime environment
    and configuration with the application code as a shippable artifact. In order
    to maximize the benefits of using containers for local development, it would be
    great to be able to declare all the dependencies and run them in separate containers.
    This is what Docker Compose is designed to do.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose uses a declarative YAML configuration file to determine how an
    application should be executed in multiple containers. This makes it easy to quickly
    start up a service, a database, and any other runtime dependencies of the service
    in a way that makes local development especially easy.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll follow some of the steps from the previous recipe to create
    a `Dockerfile` file for the authentication-service project. We'll then create
    a Docker Compose file that specifies MySQL as a dependency of the authentication-service.
    We'll then look at how to configure our project and run it locally with one container
    running our application and another running a database server.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this recipe, you need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the authentication-service project and create a new file called `Dockerfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker Compose uses a file called `docker-compose.yml` to declare how containerized
    applications should be run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we''ll be connecting to the MySQL server running in the `docker-mysql` container,
    we''ll need to modify our authentication-service configuration to use that host
    when connecting to MySQL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now run the authentication-service and MySQL with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: That's it! The authentication-service should now be running locally in a container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying your service on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers make services portable by allowing you to package code, dependencies,
    and the runtime environment together in one artifact. Deploying containers is
    generally easier than deploying applications that do not run in containers. The
    host does not need to have any special configuration or state; it just needs to
    be able to execute the container runtime. The ability to deploy one or more containers
    on a single host gave rise to another challenge when managing production environments—scheduling
    and orchestrating containers to run on specific hosts and manage scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is an open source container orchestration tool. It is responsible
    for scheduling, managing, and scaling your containerized applications. With Kubernetes,
    you do not need to worry about deploying your container to one or more specific
    hosts. Instead, you declare what resources your container needs and let Kubernetes
    decide how to do the work (what host the container runs on, what services it runs
    alongside, and so on). Kubernetes grew out of the **Borg paper** ([https://research.google.com/pubs/pub43438.html](https://research.google.com/pubs/pub43438.html)),
    published by engineers at Google, which described how they managed services in
    Google's data centers using the Borg cluster manager.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes was started by Google as an open source project in 2014 and has enjoyed
    widespread adoption by organizations deploying code in containers.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and managing a Kubernetes cluster is beyond the scope of this book.
    Luckily, a project called **Minikube** allows you to easily run a single-node
    Kubernetes cluster on your development machine. Even though the cluster only has
    one node, the way you interface with Kubernetes when deploying your service is
    generally the same, so the steps here can be followed for any Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll install Minikube, start a single-node Kubernetes cluster,
    and deploy the `message-service` command we've worked with in previous chapters.
    We'll use the Kubernetes CLI tool (`kubectl`) to interface with Minikube.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this recipe, you need to go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to demonstrate how to deploy our service to a `kubernetes` cluster,
    we''ll be using a tool called `minikube`. The `minikube` tool makes it easy to
    run a single-node `kubernetes` cluster on a VM that can be run on a laptop or
    development machine. Install `minikube`. On macOS X, you can use HomeBrew to do
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also be using the `kubernetes` CLI tools in this recipe, so install
    those. On macOS X, using HomeBrew, you can type as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''re ready to start our single-node `kubernetes` cluster. You can do
    this by running `minikube start`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, set the `minikube` cluster up as the default configuration for the `kubectl`
    CLI tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify that everything is configured properly by running the `cluster-info`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To further debug and diagnose cluster problems, use `kubectl cluster-info dump`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should now be able to launch the `kubernetes` dashboard in a browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `minikube` tool uses a number of environment variables to configure the
    CLI client. Evaluate the environment variables with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll build the docker image for our service using the `Dockerfile` file
    created in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, run the `message-service` command on the `kubernetes` cluster, telling
    `kubectl` the correct image to use and the port to expose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify that the `message-service` command is running in the `kubernetes`
    cluster by listing the pods on the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to access the `message-service` command, we''ll need to expose it
    as a new service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify the previous command by listing services on the `kubernetes`
    services:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `minikube` tool has a convenient command for accessing a service running
    on the `kubernetes` cluster. Running the following command will list the URL that
    the `message-service` command is running on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Use `curl` to try and make a request against the service to verify that it's
    working. Congratulations! You've deployed the `message-service` command on `kubernetes`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test releases with canary deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improvements in best practices for deploying have greatly improved the stability
    of deploys over the years. Automating the repeatable steps, standardizing the
    way our application interacts with the runtime environment, and packaging our
    application code with the runtime environment have all made deployments safer
    and easier than they used to be.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing new code to a production environment is not without risk, however.
    All the techniques discussed in this chapter help prevent predictable mistakes,
    but they do nothing to prevent actual software bugs from negatively impacting
    users of the software we write. Canary deployment is a technique for reducing
    this risk and increasing confidence in new code that is being deployed to production.
  prefs: []
  type: TYPE_NORMAL
- en: With a canary deployment, you begin by shipping your code to a small percentage
    of production traffic. You can then monitor metrics, logs, traces, or whatever
    other tools allow you to observe how your software is working. Once you are confident
    that things are going as they should, you can gradually increase the percentage
    of traffic that receives your updated version until all production traffic is
    being served by the newest release of your service.
  prefs: []
  type: TYPE_NORMAL
- en: The term **canary deployment** comes from a technique that coal miners used
    to use to protect themselves from carbon monoxide or methane poisoning. By having
    a canary in the mine, the toxic gases would kill the canary before the miners,
    giving the miners an early warning sign that they should get out. Similarly, canary
    deployments allow us to expose a subset of users to risk without impacting the
    rest of the production environment. Thankfully, no animals have to be harmed when
    deploying code to production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployments used to be very difficult to get right. Teams shipping software
    in this way usually had to come up with some kind of feature-toggling solution
    that would gate requests to certain versions of the application being deployed.
    Thankfully, containers have made this much easier, and Kubernetes has made it
    even more so.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll deploy an update to our `message-service` application
    using a canary deployment. As Kubernetes is able to pull images from a Docker
    container registry, we'll run a registry locally. Normally, you'd use a self-hosted
    registry or a service such as *Docker Hub* or *Google Container Registry*. First,
    we'll ensure that we have a stable version of the `message-service` command running
    in `minikube`, then we'll introduce an update and gradually roll it out to 100%
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go through the following steps to set up a canary deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `message-service` project we''ve worked on in the previous recipes.
    Add the following `Dockerfile` file to the root directory of the project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In order for Kubernetes to know whether the service is running, we need to
    add a liveness probe endpoint. Open the `MessageController.java` file and add
    a method to respond to GET requests at the `/ping` path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start our container registry on port 5000:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As we''re using a local repository that is not configured with a valid SSL
    cert, start `minikube` with the ability to pull from insecure repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the `message-service` docker image, and then push the image to the local
    container registry with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'A **Kubernetes Deployment** object describes the desired state for a pod and
    `ReplicaSet`. In our deployment, we''ll specify that we want three replicas of
    our `message-service` pod running at all times, and we''ll specify the liveness
    probe that we created a few steps earlier. To create a deployment for our `message-service`,
    create a file called `deployment.yaml` with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, using `kubectl`, we''ll create our deployment object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now verify that our deployment is live and that Kubernetes is creating
    the pod replicas by running `kubectl get pods`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our application is running in Kubernetes, the next step is to create
    an update and roll it out to a subset of pods. First, we need to create a new
    docker image; in this case, we''ll call it version 0.1.2 and push it to the local
    repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We can now configure a deployment to run the newest version of our image before
    rolling it out to the rest of the pods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
