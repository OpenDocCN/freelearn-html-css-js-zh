<html><head></head><body>
  <div><h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-141" class="chapterTitle">Coding with Streams</h1>
    <p class="normal">Streams are one of the most important components and patterns of Node.js. There is a motto in the community that goes, "stream all the things!", and this alone should be enough to describe the role of streams in Node.js. Dominic Tarr, a top contributor to the Node.js community, defines streams as "Node's best and most misunderstood idea." There are different reasons that make Node.js streams so attractive; again, it's not just related to technical properties, such as performance or efficiency, but it's more about their elegance and the way they fit perfectly into the Node.js philosophy.</p>
    <p class="normal">This chapter aims to provide a complete understanding of Node.js streams. The first half of this chapter serves as an introduction to the main ideas, the terminology, and the libraries behind Node.js streams. In the second half, we will cover more advanced topics and, most importantly, we will explore useful streaming patterns that can make your code more elegant and effective in many circumstances.</p>
    <p class="normal">In this chapter, you will learn about the following topics:</p>
    <ul>
      <li class="Bullet--PACKT-">Why streams are so important in Node.js</li>
      <li class="Bullet--PACKT-">Understanding, using, and creating streams</li>
      <li class="Bullet--PACKT-">Streams as a programming paradigm: leveraging their power in many different contexts and not just for I/O</li>
      <li class="Bullet-End--PACKT-">Streaming patterns and connecting streams together in different configurations</li>
    </ul>
    <p class="normal">Without further ado, let's discover together why streams are one of the cornerstones of Node.js.</p>
    <h1 id="_idParaDest-142" class="title">Discovering the importance of streams</h1>
    <p class="normal">In an event-based <a id="_idIndexMarker380"/>platform such as Node.js, the most efficient way to handle I/O is in real time, consuming the input as soon as it is available and sending the output as soon as the application produces it.</p>
    <p class="normal">In this section, we will give you an initial introduction to Node.js streams and their strengths. Please bear in mind that this is only an overview, as a more detailed analysis on how to use and<a id="_idIndexMarker381"/> compose streams will follow later in this chapter.</p>
    <h2 id="_idParaDest-143" class="title">Buffering versus streaming</h2>
    <p class="normal">Almost all the asynchronous APIs that we've seen so far in this book work using <em class="italic">buffer mode</em>. For an input <a id="_idIndexMarker382"/>operation, buffer mode causes all the data coming from a<a id="_idIndexMarker383"/> resource to be collected into a buffer until the operation is completed; it is then passed back to the caller as one single blob of data. The following diagram shows a visual example of this paradigm:</p>
    <figure class="mediaobject"><img src="img/B15729_06_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.1: Buffering</p>
    <p class="normal">In <em class="italic">Figure 6.1</em>, we can see that, at time <em class="italic">t1</em>, some data is received from the resource and saved into the buffer. At time <em class="italic">t2</em>, another data chunk is received—the final one—which completes the read operation, so that, at <em class="italic">t3</em>, the entire buffer is sent to the consumer.</p>
    <p class="normal">On the other side, streams allow us to process the data as soon as it arrives from the resource. This is shown in the following diagram:</p>
    <figure class="mediaobject"><img src="img/B15729_06_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.2: Streaming</p>
    <p class="normal">This time, <em class="italic">Figure 6.2</em> shows you that as soon as each new chunk of data is received from the resource, it is immediately passed to the consumer, who now has the chance to process it straight <a id="_idIndexMarker384"/>away, without waiting for all the data to be collected<a id="_idIndexMarker385"/> in the buffer.</p>
    <p class="normal">But what are the differences between these two approaches? Purely from an efficiency perspective, streams can be more efficient in terms of both space (memory usage) and time (computation clock time). However, Node.js streams have another important advantage: <strong class="keyword">composability</strong>. Let's now see what impact these properties have on the way we design and write our applications.</p>
    <h2 id="_idParaDest-144" class="title">Spatial efficiency</h2>
    <p class="normal">First of all, streams allow us to do things that would not be possible by buffering data and processing it all<a id="_idIndexMarker386"/> at once. For example, consider the case in which we have to read a very big file, let's say, in the order of hundreds of megabytes or even gigabytes. Clearly, using an API that returns a big buffer when the file is completely read is not a good idea. Imagine reading a few of these big files concurrently; our application would easily run out of memory. Besides that, buffers in V8 are limited in size. You cannot allocate more than a few gigabytes of data, so we may hit a wall way before running out of physical memory.</p>
    <div><p class="Tip--PACKT-">The actual maximum size of a buffer changes across platforms and versions of Node.js. If you are<a id="_idIndexMarker387"/> curious to find out what's the limit in bytes in a given platform, you can run this code:</p>
      <pre class="programlisting code"><code class="hljs-code">import buffer from 'buffer'
console.log(buffer.constansts.MAX_LENGTH)
</code></pre>
    </div>
    <h3 id="_idParaDest-145" class="title">Gzipping using a buffered API</h3>
    <p class="normal">To make a concrete example, let's<a id="_idIndexMarker388"/> consider a simple command-line application that compresses<a id="_idIndexMarker389"/> a file using the GZIP format. Using a buffered API, such an application will look like the following in Node.js (error handling is omitted for brevity):</p>
    <pre class="programlisting code"><code class="hljs-code">import { promises as fs } from 'fs'
import { gzip } from 'zlib'
import { promisify } from 'util'
const gzipPromise = promisify(gzip)
const filename = process.argv[2]
async function main () {
  const data = await fs.readFile(filename)
  const gzippedData = await gzipPromise(data)
  await fs.writeFile(`${filename}.gz`, gzippedData)
  console.log('File successfully compressed')
}
main()
</code></pre>
    <p class="normal">Now, we can try to put the preceding code in a file named <code class="Code-In-Text--PACKT-">gzip-buffer.js</code> and then run it with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">node gzip-buffer.js &lt;path to file&gt;
</code></pre>
    <p class="normal">If we choose a file that is big enough (for instance, about 8 GB), we will most likely receive an error message saying that the file that we are trying to read is bigger than the maximum allowed buffer size:</p>
    <pre class="programlisting con"><code class="hljs-con">RangeError [ERR_FS_FILE_TOO_LARGE]: File size (8130792448) is greater than possible Buffer: 2147483647 bytes
</code></pre>
    <p class="normal">That's exactly what<a id="_idIndexMarker390"/> we expected, and it's a symptom of the<a id="_idIndexMarker391"/> fact that we are using the wrong approach.</p>
    <h3 id="_idParaDest-146" class="title">Gzipping using streams</h3>
    <p class="normal">The simplest way we <a id="_idIndexMarker392"/>have to fix our Gzip application and make it work<a id="_idIndexMarker393"/> with big files is to use a streaming API. Let's see how this can be achieved. Let's write a new module with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">// gzip-stream.js
import { createReadStream, createWriteStream } from 'fs'
import { createGzip } from 'zlib'
const filename = process.argv[2]
createReadStream(filename)
  .pipe(createGzip())
  .pipe(createWriteStream(`${filename}.gz`))
  .on('finish', () =&gt; console.log('File successfully compressed'))
</code></pre>
    <p class="normal">"Is that it?" you may ask. Yes! As we said, streams are amazing because of their interface and composability, thus allowing clean, elegant, and concise code. We will see this in a while in more detail, but for now, the important thing to realize is that the program will run smoothly against files of any size and with constant memory utilization. Try it yourself (but consider that compressing a big file may take a while).</p>
    <div><p class="Information-Box--PACKT-">Note that, in the previous example, we omitted error handling for brevity. We will discuss the nuances of proper error handling with streams later in this chapter. Until then, be aware that most examples will be lacking proper error handling.</p>
    </div>
    <h2 id="_idParaDest-147" class="title">Time efficiency</h2>
    <p class="normal">Let's now consider the case <a id="_idIndexMarker394"/>of an application that compresses a file and uploads it to a remote HTTP server, which, in turn, decompresses it and saves it on the filesystem. If the client component of our application was implemented using a buffered API, the upload would start only when the entire file had been read and compressed. On the other hand, the decompression would start on the server only when all the data had been received. A better solution to achieve the same result involves the use of streams. On the client machine, streams allow us to compress and send the data chunks as soon as they are read from the filesystem, whereas on the server, they allow us to decompress every chunk as soon as it is received from the remote peer. Let's demonstrate this by building the application that we mentioned earlier, starting from the server side.</p>
    <p class="normal">Let's create a <a id="_idIndexMarker395"/>module named <code class="Code-In-Text--PACKT-">gzip-receive.js</code> containing the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createServer } from 'http'
import { createWriteStream } from 'fs'
import { createGunzip } from 'zlib'
import { basename, join } from 'path'
const server = createServer((req, res) =&gt; {
  const filename = basename(req.headers['x-filename'])
  const destFilename = join('received_files', filename)
  console.log(`File request received: ${filename}`)
  req
    .pipe(createGunzip())
    .pipe(createWriteStream(destFilename))
    .on('finish', () =&gt; {
      res.writeHead(201, { 'Content-Type': 'text/plain' })
      res.end('OK\n')
      console.log(`File saved: ${destFilename}`)
    })
})
server.listen(3000, () =&gt; console.log('Listening on http://localhost:3000'))
</code></pre>
    <p class="normal">In the preceding example, <code class="Code-In-Text--PACKT-">req</code> is a stream object that is used by the server to receive the request data in chunks from the network. Thanks to Node.js streams, every chunk of data is decompressed and saved to disk as soon as it is received.</p>
    <div><p class="Information-Box--PACKT-"> You might have noticed that, in our server application, we are using <code class="Code-In-Text--PACKT-">basename()</code> to remove any possible path from the name of the received file. This is a security best practice as we want to make sure that the received file is saved exactly within our <code class="Code-In-Text--PACKT-">received_files</code> folder. Without <code class="Code-In-Text--PACKT-">basename()</code>, a malicious user could craft a request that could effectively override system files and inject malicious code into the server machine. Imagine, for instance, what happens if <code class="Code-In-Text--PACKT-">filename</code> is set to <code class="Code-In-Text--PACKT-">/usr/bin/node</code>? In such a case, the attacker could effectively replace our Node.js interpreter with any arbitrary file.</p>
    </div>
    <p class="normal">The client side of our application will go into a module named <code class="Code-In-Text--PACKT-">gzip-send.js</code>, and it looks as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">import { request } from 'http'
import { createGzip } from 'zlib'
import { createReadStream } from 'fs'
import { basename } from 'path'
const filename = process.argv[2]
const serverHost = process.argv[3]
const httpRequestOptions = {
  hostname: serverHost,
  port: 3000,
  path: '/',
  method: 'PUT',
  headers: {
    'Content-Type': 'application/octet-stream',
    'Content-Encoding': 'gzip',
    'X-Filename': basename(filename)
  }
}
const req = request(httpRequestOptions, (res) =&gt; {
  console.log(`Server response: ${res.statusCode}`)
})
createReadStream(filename)
  .pipe(createGzip())
  .pipe(req)
  .on('finish', () =&gt; {
    console.log('File successfully sent')
  })
</code></pre>
    <p class="normal">In the preceding code, we<a id="_idIndexMarker396"/> are again using streams to read the data from the file, and then compressing and sending each chunk as soon as it is read from the filesystem.</p>
    <p class="normal">Now, to try out the application, let's first start the server using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">node gzip-receive.js
</code></pre>
    <p class="normal">Then, we can launch the client by specifying the file to send and the address of the server (for example, <code class="Code-In-Text--PACKT-">localhost</code>):</p>
    <pre class="programlisting con"><code class="hljs-con">node gzip-send.js &lt;path to file&gt; localhost
</code></pre>
    <p class="normal">If we choose a file big enough, we can appreciate how the data flows from the client to the server. But why <a id="_idIndexMarker397"/>exactly is this paradigm—where we have flowing data—more efficient compared to using a buffered API? <em class="italic">Figure 6.3</em> should make this concept easier to grasp:</p>
    <figure class="mediaobject"><img src="img/B15729_06_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.3: Buffering and streaming compared</p>
    <p class="normal">When a file is processed, it goes through a number of sequential stages:</p>
    <ol>
      <li class="numbered">[Client] Read from the filesystem</li>
      <li class="numbered">[Client] Compress the data</li>
      <li class="numbered">[Client] Send it to the server</li>
      <li class="numbered">[Server] Receive from the client</li>
      <li class="numbered">[Server] Decompress the data</li>
      <li class="numbered">[Server] Write the data to disk</li>
    </ol>
    <p class="normal">To complete the processing, we have to go through each stage like in an assembly line, in sequence, until the end. In <em class="italic">Figure 6.3</em>, we can see that, using a buffered API, the process is entirely sequential. To compress the data, we first have to wait for the entire file to be read, then, to send the data, we have to wait for the entire file to be both read and compressed, and so on.</p>
    <p class="normal">Using streams, the assembly line is kicked off as soon as we receive the first chunk of data, without waiting for the entire file to be read. But more amazingly, when the next chunk of data is available, there is no need to wait for the previous set of tasks to be completed; instead, another<a id="_idIndexMarker398"/> assembly line is launched in parallel. This works perfectly because each task that we execute is asynchronous, so it can be parallelized by Node.js. The only constraint is that the order in which the chunks arrive in each stage must be preserved. The internal implementation of Node.js streams takes care of maintaining the order for us.</p>
    <p class="normal">As we can see from <em class="italic">Figure 6.3</em>, the result of using streams is that the entire process takes less time, because we waste no time waiting for all the data to be read and processed all at once.</p>
    <h2 id="_idParaDest-148" class="title">Composability</h2>
    <p class="normal">The code we have seen so far has already given us an overview of how streams can be composed thanks<a id="_idIndexMarker399"/> to the <code class="Code-In-Text--PACKT-">pipe()</code> method, which allows us to connect the different processing units, each being responsible for one single functionality, in perfect Node.js style. This is possible because streams have a uniform interface, and they can understand each other in terms of API. The only prerequisite is that the next stream in the pipeline has to support the data type produced by the previous stream, which can be either binary, text, or even objects, as we will see later in this chapter.</p>
    <p class="normal">To take a look at another demonstration of the power of this property, we can try to add an encryption layer to the <code class="Code-In-Text--PACKT-">gzip-send</code>/<code class="Code-In-Text--PACKT-">gzip-receive</code> application that we built previously.</p>
    <p class="normal">In order to do this, we will need to apply some small changes to both our client and server.</p>
    <h3 id="_idParaDest-149" class="title">Adding client-side encryption</h3>
    <p class="normal">Let's start<a id="_idIndexMarker400"/> with the client:</p>
    <pre class="programlisting code"><code class="hljs-code">// ...
<strong class="hljs-keyword ">import</strong><strong class="hljs-slc"> { createCipheriv, randomBytes } </strong><strong class="hljs-keyword-slc">from</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">'crypto'</strong>       // (1)
const filename = process.argv[2]
const serverHost = process.argv[3]
<strong class="hljs-keyword-slc">const</strong><strong class="hljs-slc"> secret = Buffer.from(process.argv[</strong><strong class="hljs-number-slc">4</strong><strong class="hljs-slc">], </strong><strong class="hljs-string-slc">'hex'</strong><strong class="hljs-slc">)</strong>         // (2)
<strong class="hljs-keyword-slc">const</strong><strong class="hljs-slc"> iv = randomBytes(</strong><strong class="hljs-number-slc">16</strong><strong class="hljs-slc">)</strong>                                 // (3)
// ...
</code></pre>
    <p class="normal">Let's review what we changed here:</p>
    <ol>
      <li class="numbered">First of all, we import the <code class="Code-In-Text--PACKT-">createCipheriv()</code> <code class="Code-In-Text--PACKT-">Transform</code> stream and the <code class="Code-In-Text--PACKT-">randomBytes()</code> function from the <code class="Code-In-Text--PACKT-">crypto</code> module.</li>
      <li class="numbered">We get the server's encryption secret from the command line. We expect the string to be passed as a hexadecimal string, so we read this value and load it in memory using a buffer set to <code class="Code-In-Text--PACKT-">hex</code> mode.</li>
      <li class="numbered">Finally, we generate a random sequence of bytes that we will be using as an initialization vector for the file encryption.</li>
    </ol>
    <p class="normal">Now, we can update <a id="_idIndexMarker401"/>the piece of code responsible for creating the HTTP request:</p>
    <pre class="programlisting code"><code class="hljs-code">const httpRequestOptions = {
  hostname: serverHost,
  headers: {
    'Content-Type': 'application/octet-stream',
    'Content-Encoding': 'gzip',
    'X-Filename': basename(filename),
    <strong class="hljs-string-slc">'X-Initialization-Vector'</strong><strong class="hljs-slc">: iv.toString(</strong><strong class="hljs-string-slc">'hex'</strong><strong class="hljs-slc">)</strong>          // (1)
  }
}
// ...
const req = request(httpRequestOptions, (res) =&gt; {
  console.log(`Server response: ${res.statusCode}`)
})
createReadStream(filename)
  .pipe(createGzip())
  <strong class="hljs-slc">.pipe(createCipheriv(</strong><strong class="hljs-string-slc">'aes192'</strong><strong class="hljs-slc">, secret, iv))</strong>              // (2)
  .pipe(req)
// ...
</code></pre>
    <p class="normal">The main changes here are:</p>
    <ol>
      <li class="numbered">We pass the initialization vector to the server as an HTTP header.</li>
      <li class="numbered">We encrypt the data, just after the Gzip phase.</li>
    </ol>
    <p class="normal">That's all <a id="_idIndexMarker402"/>for the client side.</p>
    <h3 id="_idParaDest-150" class="title">Adding server-side decryption</h3>
    <p class="normal">Let's now refactor <a id="_idIndexMarker403"/>the server. The first thing that we need to do is import some utility functions from the core <code class="Code-In-Text--PACKT-">crypto</code> module, which we can use to generate a random encryption key (the secret):</p>
    <pre class="programlisting code"><code class="hljs-code">// ...
import { createDecipheriv, randomBytes } from 'crypto'
const secret = randomBytes(24)
console.log(`Generated secret: ${secret.toString('hex')}`)
</code></pre>
    <p class="normal">The generated secret is printed to the console as a hex string so that we can share that with our clients.</p>
    <p class="normal">Now, we need to update the file reception logic:</p>
    <pre class="programlisting code"><code class="hljs-code">const server = createServer((req, res) =&gt; {
  const filename = basename(req.headers['x-filename'])
  <strong class="hljs-keyword-slc">const</strong><strong class="hljs-slc"> iv = Buffer.from(</strong>
    <strong class="hljs-slc">req.headers[</strong><strong class="hljs-string-slc">'x-initialization-vector'</strong><strong class="hljs-slc">], </strong><strong class="hljs-string-slc">'hex'</strong><strong class="hljs-slc">)</strong>         // (1)
  const destFilename = join('received_files', filename)
  console.log(`File request received: ${filename}`)
  req
    <strong class="hljs-slc">.pipe(createDecipheriv(</strong><strong class="hljs-string-slc">'aes192'</strong><strong class="hljs-slc">, secret, iv))</strong>          // (2)
    .pipe(createGunzip())
    .pipe(createWriteStream(destFilename))
    // ...
</code></pre>
    <p class="normal">Here, we are applying two changes:</p>
    <ol>
      <li class="numbered">We have to read the encryption <strong class="keyword">initialization vector</strong> (<a href="http://nodejsdp.link/iv">nodejsdp.link/iv</a>) sent by the client.</li>
      <li class="numbered">The first step of our streaming pipeline is now responsible for decrypting the incoming data using the <code class="Code-In-Text--PACKT-">createDecipheriv</code> <code class="Code-In-Text--PACKT-">Transform</code> stream from the <code class="Code-In-Text--PACKT-">crypto</code> module.</li>
    </ol>
    <p class="normal">With very little effort (just a few lines of code), we added an encryption layer to our application; we simply had to use some already available <code class="Code-In-Text--PACKT-">Transform</code> streams (<code class="Code-In-Text--PACKT-">createCipheriv</code> and <code class="Code-In-Text--PACKT-">createDecipheriv</code>) and included them in the stream processing pipelines for the client and the server. In a similar way, we can add and combine other streams, as if we were playing with Lego bricks.</p>
    <p class="normal">The main advantage <a id="_idIndexMarker404"/>of this approach is reusability, but as we can see from the code so far, streams also enable cleaner and more modular code. For these reasons, streams are often used not just to deal with pure I/O, but also as a means to simplify and modularize the code.</p>
    <p class="normal">Now that we have introduced streams, we are ready to explore, in a more structured way, the different types of streams available in Node.js.</p>
    <h1 id="_idParaDest-151" class="title">Getting started with streams</h1>
    <p class="normal">In the previous section, we learned why streams are so powerful, but also that they are everywhere in <a id="_idIndexMarker405"/>Node.js, starting from its core modules. For example, we have seen that the <code class="Code-In-Text--PACKT-">fs</code> module has <code class="Code-In-Text--PACKT-">createReadStream()</code> for reading from a file and <code class="Code-In-Text--PACKT-">createWriteStream()</code> for writing to a file, the HTTP <code class="Code-In-Text--PACKT-">request</code> and <code class="Code-In-Text--PACKT-">response</code> objects are essentially streams, the <code class="Code-In-Text--PACKT-">zlib</code> module allows us to compress and decompress data using a streaming interface and, finally, even the <code class="Code-In-Text--PACKT-">crypto</code> module exposes some useful streaming primitives like <code class="Code-In-Text--PACKT-">createCipheriv</code> and <code class="Code-In-Text--PACKT-">createDecipheriv</code>.</p>
    <p class="normal">Now that we know why streams are so important, let's take a step back and start to explore them in more detail.</p>
    <h2 id="_idParaDest-152" class="title">Anatomy of streams</h2>
    <p class="normal">Every stream in Node.js is an<a id="_idIndexMarker406"/> implementation of one of the four base abstract classes available in the <code class="Code-In-Text--PACKT-">stream</code> core module:</p>
    <ul>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">Readable</code></li>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">Writable</code></li>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">Duplex</code></li>
      <li class="Bullet-End--PACKT-"><code class="Code-In-Text--PACKT-">Transform</code></li>
    </ul>
    <p class="normal">Each <code class="Code-In-Text--PACKT-">stream</code> class is also an instance of <code class="Code-In-Text--PACKT-">EventEmitter</code>. Streams, in fact, can produce several types of event, such as <code class="Code-In-Text--PACKT-">end</code> when a <code class="Code-In-Text--PACKT-">Readable</code> stream has finished reading, <code class="Code-In-Text--PACKT-">finish</code> when a <code class="Code-In-Text--PACKT-">Writable</code> stream has completed writing, or <code class="Code-In-Text--PACKT-">error</code> when something goes wrong.</p>
    <p class="normal">One reason why streams are so flexible is the fact that they can handle not just binary data, but almost any JavaScript value. In fact, they support two operating modes:</p>
    <ul>
      <li class="Bullet--PACKT-"><strong class="keyword">Binary mode</strong>: To stream data<a id="_idIndexMarker407"/> in the form of chunks, such as buffers or strings</li>
      <li class="Bullet-End--PACKT-"><strong class="keyword">Object mode</strong>: To stream data as <a id="_idIndexMarker408"/>a sequence of discrete objects (allowing us to use almost any JavaScript value)</li>
    </ul>
    <p class="normal">These two operating<a id="_idIndexMarker409"/> modes allow us to use streams not just for I/O, but also as a tool to elegantly compose processing units in a functional fashion, as we will see later in this chapter.</p>
    <p class="normal">Let's start our deep dive of Node.js streams by introducing the class of <code class="Code-In-Text--PACKT-">Readable</code> streams.</p>
    <h2 id="_idParaDest-153" class="title">Readable streams</h2>
    <p class="normal">A <code class="Code-In-Text--PACKT-">Readable</code> stream represents a source <a id="_idIndexMarker410"/>of data. In Node.js, it's implemented using the <code class="Code-In-Text--PACKT-">Readable</code> abstract class, which is available in the <code class="Code-In-Text--PACKT-">stream</code> module.</p>
    <h3 id="_idParaDest-154" class="title">Reading from a stream</h3>
    <p class="normal">There are two approaches to<a id="_idIndexMarker411"/> receive the data from a <code class="Code-In-Text--PACKT-">Readable</code> stream: <strong class="keyword">non-flowing </strong>(or <strong class="keyword">paused</strong>)<strong class="keyword"> </strong>and <strong class="keyword">flowing</strong>. Let's analyze these modes in more detail.</p>
    <h4 class="title">The non-flowing mode</h4>
    <p class="normal">The non-flowing or<a id="_idIndexMarker412"/> paused mode is the default pattern<a id="_idIndexMarker413"/> for reading from a <code class="Code-In-Text--PACKT-">Readable</code> stream. It involves attaching a listener to the stream for the <code class="Code-In-Text--PACKT-">readable</code> event, which signals the availability of new data to read. Then, in a loop, we read the data continuously until the internal buffer is emptied. This can be done using the <code class="Code-In-Text--PACKT-">read()</code> method, which synchronously reads from the internal buffer and returns a <code class="Code-In-Text--PACKT-">Buffer</code> object representing the chunk of data. The <code class="Code-In-Text--PACKT-">read()</code> method has the following signature:</p>
    <pre class="programlisting code"><code class="hljs-code">readable.read([size])
</code></pre>
    <p class="normal">Using this approach, the data is imperatively pulled from the stream on demand.</p>
    <p class="normal">To show how this works, let's create a new module named <code class="Code-In-Text--PACKT-">read-stdin.js</code>, which implements a simple program that reads from the standard input (which is also a <code class="Code-In-Text--PACKT-">Readable</code> stream) and echoes everything back to the standard output:</p>
    <pre class="programlisting code"><code class="hljs-code">process.stdin
  .on('readable', () =&gt; {
    let chunk
    console.log('New data available')
    while ((chunk = process.stdin.read()) !== null) {
      console.log(
        `Chunk read (${chunk.length} bytes): "${chunk.toString()}"`
      )
    }
  })
  .on('end', () =&gt; console.log('End of stream'))
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">read()</code> method is a synchronous<a id="_idIndexMarker414"/> operation that pulls <a id="_idIndexMarker415"/>a data chunk from the internal buffers of the <code class="Code-In-Text--PACKT-">Readable</code> stream. The returned chunk is, by default, a <code class="Code-In-Text--PACKT-">Buffer</code> object if the stream is working in binary mode.</p>
    <div><p class="Information-Box--PACKT-">In a <code class="Code-In-Text--PACKT-">Readable</code> stream working in binary mode, we can read strings instead of buffers by calling <code class="Code-In-Text--PACKT-">setEncoding(encoding)</code> on the stream, and providing a valid encoding format (for example, <code class="Code-In-Text--PACKT-">utf8</code>). This approach is recommended when streaming UTF-8 text data as the stream will properly handle multibyte characters, doing the necessary buffering to make sure that no character ends up being split into separate chunks. In other words, every chunk produced by the stream will be a valid UTF-8 sequence of bytes.</p>
      <p class="Information-Box--PACKT-">Note that you can call <code class="Code-In-Text--PACKT-">setEncoding()</code> as many times as you want on a <code class="Code-In-Text--PACKT-">Readable</code> stream, even after you've started consuming the data from the stream. The encoding will be switched dynamically on the next available chunk. Streams are inherently binary; encoding is just a view over the binary data that is emitted by the stream.</p>
    </div>
    <p class="normal">The data is read solely from within the <code class="Code-In-Text--PACKT-">Readable</code> listener, which is invoked as soon as new data is available. The <code class="Code-In-Text--PACKT-">read()</code> method returns <code class="Code-In-Text--PACKT-">null</code> when there is no more data available in the internal buffers; in such a case, we have to wait for another <code class="Code-In-Text--PACKT-">readable</code> event to be fired, telling us that we can read again, or wait for the <code class="Code-In-Text--PACKT-">end</code> event that signals the end of the stream. When a stream is working in binary mode, we can also specify that we are interested in reading a specific amount of data by passing a <code class="Code-In-Text--PACKT-">size</code> value to the <code class="Code-In-Text--PACKT-">read()</code> method. This is particularly useful when implementing network protocols or when parsing specific data formats.</p>
    <p class="normal">Now, we are ready to run the <code class="Code-In-Text--PACKT-">read-stdin.js</code> module and experiment with it. Let's type some characters into the console and then press Enter to see the data echoed back into the standard output. To<a id="_idIndexMarker416"/> terminate the stream and hence <a id="_idIndexMarker417"/>generate a graceful <code class="Code-In-Text--PACKT-">end</code> event, we need to insert an <code class="Code-In-Text--PACKT-">EOF</code> (end-of-file) character (using Ctrl + Z on Windows or Ctrl + D on Linux and macOS).</p>
    <div><p class="Tip--PACKT-">We can also try to connect our program with other processes. This is possible using the pipe operator (<code class="Code-In-Text--PACKT-">|</code>), which redirects the standard output of a program to the standard input of another. For example, we can run a command such as the following:</p>
      <pre class="programlisting con"><code class="hljs-con">cat &lt;path to a file&gt; | node read-stdin.js
</code></pre>
      <p class="Tip--PACKT-">This is an amazing demonstration of how the streaming paradigm is a universal interface that enables our programs to communicate, regardless of the language they are written in.</p>
    </div>
    <h4 class="title">Flowing mode</h4>
    <p class="normal">Another way to read <a id="_idIndexMarker418"/>from a stream is by attaching a listener to the <code class="Code-In-Text--PACKT-">data</code> event. This will switch the stream into using <strong class="keyword">flowing mode</strong>, where the<a id="_idIndexMarker419"/> data is not pulled using <code class="Code-In-Text--PACKT-">read()</code>, but instead is pushed to the <code class="Code-In-Text--PACKT-">data</code> listener as soon as it arrives. For example, the <code class="Code-In-Text--PACKT-">read-stdin.js</code> application that we created earlier will look like this using flowing mode:</p>
    <pre class="programlisting code"><code class="hljs-code">process.stdin
  .on('data', (chunk) =&gt; {
    console.log('New data available')
    console.log(
      `Chunk read (${chunk.length} bytes): "${chunk.toString()}"`
    )
  })
  .on('end', () =&gt; console.log('End of stream'))
</code></pre>
    <p class="normal">Flowing mode offers less flexibility to control the flow of data compared to non-flowing mode. The default operating mode for streams is non-flowing, so to enable flowing mode, it's necessary to attach a listener to the <code class="Code-In-Text--PACKT-">data</code> event or explicitly invoke the <code class="Code-In-Text--PACKT-">resume()</code> method. To temporarily stop the stream from emitting <code class="Code-In-Text--PACKT-">data</code> events, we can invoke the <code class="Code-In-Text--PACKT-">pause()</code> method, causing any incoming data to be cached in the internal buffer. Calling <code class="Code-In-Text--PACKT-">pause()</code> will switch <a id="_idIndexMarker420"/>the stream back to non-flowing mode.</p>
    <h4 class="title">Async iterators</h4>
    <p class="normal"><code class="Code-In-Text--PACKT-">Readable</code> streams are also <a id="_idIndexMarker421"/>async iterators; therefore, we could rewrite our <code class="Code-In-Text--PACKT-">read-stdin.js</code> example as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">async function main () {
  for await (const chunk of process.stdin) {
    console.log('New data available')
    console.log(
      `Chunk read (${chunk.length} bytes): "${chunk.toString()}"`
    )
  }
  console.log('End of stream')
}
main()
</code></pre>
    <p class="normal">We will discuss async iterators in greater detail in <em class="chapterRef">Chapter 9</em>, <em class="italic">Behavioral Design Patterns</em>, so don't worry too much about the syntax in the preceding example for now. What's important to know is that if you need to write a function that consumes an entire <code class="Code-In-Text--PACKT-">Readable</code> stream and returns a <code class="Code-In-Text--PACKT-">Promise</code>, this syntax could come in very handy.</p>
    <h3 id="_idParaDest-155" class="title">Implementing Readable streams</h3>
    <p class="normal">Now that we know<a id="_idIndexMarker422"/> how to read from a stream, the next step is to learn how to implement a new custom <code class="Code-In-Text--PACKT-">Readable</code> stream. To do this, it's necessary to create a new class by inheriting the prototype <code class="Code-In-Text--PACKT-">Readable</code> from the <code class="Code-In-Text--PACKT-">stream</code> module. The concrete stream must provide an implementation of the <code class="Code-In-Text--PACKT-">_read()</code> method, which has the following signature:</p>
    <pre class="programlisting code"><code class="hljs-code">readable._read(size)
</code></pre>
    <p class="normal">The internals of the <code class="Code-In-Text--PACKT-">Readable</code> class will call the <code class="Code-In-Text--PACKT-">_read()</code> method, which, in turn, will start to fill the internal buffer using <code class="Code-In-Text--PACKT-">push()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">readable.push(chunk)
</code></pre>
    <div><p class="Information-Box--PACKT-">Please note that <code class="Code-In-Text--PACKT-">read()</code> is a method called by the stream consumers, while <code class="Code-In-Text--PACKT-">_read()</code> is a method to be implemented by a stream subclass and should never be called directly. The underscore usually indicates that the method is not public and should not be called directly.</p>
    </div>
    <p class="normal">To demonstrate how to implement new <code class="Code-In-Text--PACKT-">Readable</code> streams, we can try to implement a stream that generates random strings. Let's create a new module called <code class="Code-In-Text--PACKT-">random-stream.js</code> that contains the code of our random string generator:</p>
    <pre class="programlisting code"><code class="hljs-code">import { Readable } from 'stream'
import Chance from 'chance'
const chance = new Chance()
export class RandomStream extends Readable {
  constructor (options) {
    super(options)
    this.emittedBytes = 0
  }
  _read (size) {
    const chunk = chance.string({ length: size })          // (1)
    this.push(chunk, 'utf8')                               // (2)
    this.emittedBytes += chunk.length
    if (chance.bool({ likelihood: 5 })) {                  // (3)
      this.push(null)
    }
  }
}
</code></pre>
    <p class="normal">At the top of the file, we <a id="_idIndexMarker423"/>load our dependencies. There is nothing special there, except that we are loading an npm module called <code class="Code-In-Text--PACKT-">chance</code> (<a href="http://nodejsdp.link/chance">nodejsdp.link/chance</a>), which is a library for generating all sorts of random values, from numbers to strings to entire sentences.</p>
    <p class="normal">The next step is to create a new class called <code class="Code-In-Text--PACKT-">RandomStream</code>, which specifies <code class="Code-In-Text--PACKT-">Readable</code> as its parent. In the preceding code, invoking <code class="Code-In-Text--PACKT-">super(options)</code> in the <code class="Code-In-Text--PACKT-">RandomStream</code> constructor will call the constructor of the parent class, allowing us to initialize the stream's internal state.</p>
    <div><p class="Tip--PACKT-">If you have a constructor that only invokes <code class="Code-In-Text--PACKT-">super(options)</code>, you can remove it as you will inherit the parent constructor. Just be careful to remember to call <code class="Code-In-Text--PACKT-">super(options)</code> every time you need to write a custom constructor.</p>
    </div>
    <p class="normal">The possible parameters that can be passed through the <code class="Code-In-Text--PACKT-">options</code> object include the following:</p>
    <ul>
      <li class="Bullet--PACKT-">The <code class="Code-In-Text--PACKT-">encoding</code> argument, which is used to convert buffers into strings (defaults to <code class="Code-In-Text--PACKT-">null</code>)</li>
      <li class="Bullet--PACKT-">A flag to enable object mode (<code class="Code-In-Text--PACKT-">objectMode</code>, defaults to <code class="Code-In-Text--PACKT-">false</code>)</li>
      <li class="Bullet-End--PACKT-">The upper limit of the data stored in the internal buffer, after which no more reading from the source should be done (<code class="Code-In-Text--PACKT-">highWaterMark</code>, defaults to <code class="Code-In-Text--PACKT-">16KB</code>)</li>
    </ul>
    <p class="normal">Okay, now let's explain the <code class="Code-In-Text--PACKT-">_read()</code> method:</p>
    <ol>
      <li class="numbered">The method <a id="_idIndexMarker424"/>generates a random string of length equal to <code class="Code-In-Text--PACKT-">size</code> using <code class="Code-In-Text--PACKT-">chance</code>.</li>
      <li class="numbered">It pushes the string into the internal buffer. Note that since we are pushing strings, we also need to specify the encoding, <code class="Code-In-Text--PACKT-">utf8</code> (this is not necessary if the chunk is simply a binary <code class="Code-In-Text--PACKT-">Buffer</code>).</li>
      <li class="numbered">It terminates the stream randomly, with a likelihood of 5 percent, by pushing <code class="Code-In-Text--PACKT-">null</code> into the internal buffer to indicate an <code class="Code-In-Text--PACKT-">EOF</code> situation or, in other words, the end of the stream.</li>
    </ol>
    <p class="normal">Note that the <code class="Code-In-Text--PACKT-">size</code> argument in the <code class="Code-In-Text--PACKT-">_read()</code> function is an advisory parameter. It's good to honor it and push only the amount of data that was requested by the caller, even though it is not mandatory to do so.</p>
    <div><p class="Information-Box--PACKT-">When we invoke <code class="Code-In-Text--PACKT-">push()</code>, we should check whether it returns <code class="Code-In-Text--PACKT-">false</code>. When that happens, it means that the internal buffer of the receiving stream has reached the <code class="Code-In-Text--PACKT-">highWaterMark</code> limit<a id="_idIndexMarker425"/> and we should stop adding more data to it. This is called <strong class="keyword">backpressure</strong>, and we will be discussing it in more detail in the next section of this chapter.</p>
    </div>
    <p class="normal">That's it for <code class="Code-In-Text--PACKT-">RandomStream</code>, we are now ready to use it. Let's see how to instantiate a <code class="Code-In-Text--PACKT-">RandomStream</code> object and pull some data from it:</p>
    <pre class="programlisting code"><code class="hljs-code">// index.js
import { RandomStream } from './random-stream.js'
const randomStream = new RandomStream()
randomStream
  .on('data', (chunk) =&gt; {
    console.log(`Chunk received (${chunk.length} bytes): ${chunk.toString()}`)
  })
  .on('end', () =&gt; {
    console.log(`Produced ${randomStream.emittedBytes} bytes of random data`)
  })
</code></pre>
    <p class="normal">Now, everything is ready<a id="_idIndexMarker426"/> for us to try our new custom stream. Simply execute the <code class="Code-In-Text--PACKT-">index.js</code> module as usual and watch a random set of strings flowing on the screen.</p>
    <h4 class="title">Simplified construction</h4>
    <p class="normal">For simple custom streams, we<a id="_idIndexMarker427"/> can avoid creating a custom class by using the <code class="Code-In-Text--PACKT-">Readable</code> stream's <em class="italic">simplified construction</em> approach. With this approach, we only need to invoke <code class="Code-In-Text--PACKT-">new Readable(options)</code> and pass a method named <code class="Code-In-Text--PACKT-">read()</code> in the set of options. The <code class="Code-In-Text--PACKT-">read()</code> method here has exactly the same semantic as the <code class="Code-In-Text--PACKT-">_read()</code> method that we saw in the class extension approach. Let's rewrite our <code class="Code-In-Text--PACKT-">RandomStream</code> using the simplified constructor approach:</p>
    <pre class="programlisting code"><code class="hljs-code">import { Readable } from 'stream'
import Chance from 'chance'
const chance = new Chance()
let emittedBytes = 0
const randomStream = new Readable({
  read (size) {
    const chunk = chance.string({ length: size })
    this.push(chunk, 'utf8')
    emittedBytes += chunk.length
    if (chance.bool({ likelihood: 5 })) {
      this.push(null)
    }
  }
})
// now use randomStream instance directly ...
</code></pre>
    <p class="normal">This approach can be particularly useful when you don't need to manage a complicated state and allows you to take advantage of a more succinct syntax. In the previous example, we <a id="_idIndexMarker428"/>created a single instance of our custom stream. If we want to adopt the simplified constructor approach but we need to create multiple instances of the custom stream, we can wrap the initialization logic in a factory function that we can invoke multiple times to create those instances.</p>
    <h4 class="title">Readable streams from iterables</h4>
    <p class="normal">You can easily create<a id="_idIndexMarker429"/> <code class="Code-In-Text--PACKT-">Readable</code> stream instances<a id="_idIndexMarker430"/> from <a id="_idIndexMarker431"/>arrays <a id="_idIndexMarker432"/>or other <strong class="keyword">iterable</strong> objects (that is, <strong class="keyword">generators</strong>, <strong class="keyword">iterators</strong>, and <strong class="keyword">async iterators</strong>) using the <code class="Code-In-Text--PACKT-">Readable.from()</code> helper.</p>
    <p class="normal">In order to get accustomed with this helper, let's look at a simple example where we convert data from an array into a <code class="Code-In-Text--PACKT-">Readable</code> stream:</p>
    <pre class="programlisting code"><code class="hljs-code">import { Readable } from 'stream'
const mountains = [
  { name: 'Everest', height: 8848 },
  { name: 'K2', height: 8611 },
  { name: 'Kangchenjunga', height: 8586 },
  { name: 'Lhotse', height: 8516 },
  { name: 'Makalu', height: 8481 }
]
const mountainsStream = Readable.from(mountains)
mountainsStream.on('data', (mountain) =&gt; {
  console.log(`${mountain.name.padStart(14)}\t${mountain.height}m`)
})
</code></pre>
    <p class="normal">As we can see from this code, the <code class="Code-In-Text--PACKT-">Readable.from()</code> method is quite simple to use: the first argument is an iterable instance (in our case, the <code class="Code-In-Text--PACKT-">mountains</code> array). <code class="Code-In-Text--PACKT-">Readable.from()</code> accepts an additional optional argument that can be used to specify stream options like <code class="Code-In-Text--PACKT-">objectMode</code>.</p>
    <div><p class="Tip--PACKT-">Note that we didn't have to explicitly set <code class="Code-In-Text--PACKT-">objectMode</code> to <code class="Code-In-Text--PACKT-">true</code>. By default, <code class="Code-In-Text--PACKT-">Readable.from()</code> will set <code class="Code-In-Text--PACKT-">objectMode</code> to <code class="Code-In-Text--PACKT-">true</code>, unless this is explicitly opted out by setting it to <code class="Code-In-Text--PACKT-">false</code>. Stream options can be passed as a second argument to the function.</p>
    </div>
    <p class="normal">Running the previous code will produce the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">       Everest    8848m
            K2    8611m
 Kangchenjunga    8586m
        Lhotse    8516m
        Makalu    8481m
</code></pre>
    <div><p class="Tip--PACKT-">Try not to instantiate large arrays in memory. Imagine if, in the previous example, we wanted to list all the mountains in the world. There are about 1 million mountains, so if we were to load all of them in an array upfront, we would allocate a quite significant amount of memory. Even if we then consume the data in the array<a id="_idIndexMarker433"/> through a <code class="Code-In-Text--PACKT-">Readable</code> stream, all the data has already been preloaded, so we are effectively voiding the memory efficiency of streams. It's always preferable to load and consume the data in chunks, and you could do so by using native streams such as <code class="Code-In-Text--PACKT-">fs.createReadStream</code>, by building a custom stream, or by using <code class="Code-In-Text--PACKT-">Readable.from</code> with lazy iterables such as generators, iterators, or async iterators. We will see some examples of the latter approach in <em class="chapterRef">Chapter 9</em>, <em class="italic">Behavioral Design Patterns</em>.</p>
    </div>
    <h2 id="_idParaDest-156" class="title">Writable streams</h2>
    <p class="normal">A <code class="Code-In-Text--PACKT-">Writable</code> stream <a id="_idIndexMarker434"/>represents a data destination. Imagine, for instance, a file on the filesystem, a database table, a socket, the standard error, or the standard output interface. In Node.js, it's implemented using the <code class="Code-In-Text--PACKT-">Writable</code> abstract class, which is available in the <code class="Code-In-Text--PACKT-">stream</code> module.</p>
    <h3 id="_idParaDest-157" class="title">Writing to a stream</h3>
    <p class="normal">Pushing some data<a id="_idIndexMarker435"/> down a <code class="Code-In-Text--PACKT-">Writable</code> stream is a straightforward business; all we have to do is use the <code class="Code-In-Text--PACKT-">write()</code> method, which has the following signature:</p>
    <pre class="programlisting code"><code class="hljs-code">writable.write(chunk, [encoding], [callback])
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">encoding</code> argument is optional and can be specified if <code class="Code-In-Text--PACKT-">chunk</code> is a string (it defaults to <code class="Code-In-Text--PACKT-">utf8</code>, and it's ignored if <code class="Code-In-Text--PACKT-">chunk</code> is a buffer). The <code class="Code-In-Text--PACKT-">callback</code> function, on the other hand, is called when the chunk is flushed into the underlying resource and is optional as well.</p>
    <p class="normal">To signal that no more data will be written to the stream, we have to use the <code class="Code-In-Text--PACKT-">end()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">writable.end([chunk], [encoding], [callback])
</code></pre>
    <p class="normal">We can provide a final chunk of data through the <code class="Code-In-Text--PACKT-">end()</code> method; in this case, the <code class="Code-In-Text--PACKT-">callback</code> function is equivalent to registering a listener to the <code class="Code-In-Text--PACKT-">finish</code> event, which is fired when all the data written in the stream has been flushed into the underlying resource.</p>
    <p class="normal">Now, let's show how this works <a id="_idIndexMarker436"/>by creating a small HTTP server that outputs a random sequence of strings:</p>
    <pre class="programlisting code"><code class="hljs-code">// entropy-server.js
import { createServer } from 'http'
import Chance from 'chance'
const chance = new Chance()
const server = createServer((req, res) =&gt; {
  res.writeHead(200, { 'Content-Type': 'text/plain' })     // (1)
  while (chance.bool({ likelihood: 95 })) {                // (2)
    res.write(`${chance.string()}\n`)                      // (3)
  }
  res.end('\n\n')                                          // (4)
  res.on('finish', () =&gt; console.log('All data sent'))     // (5)
})
server.listen(8080, () =&gt; {
  console.log('listening on http://localhost:8080')
})
</code></pre>
    <p class="normal">The HTTP server that we created writes into the <code class="Code-In-Text--PACKT-">res</code> object, which is an instance of <code class="Code-In-Text--PACKT-">http.ServerResponse</code> and also a <code class="Code-In-Text--PACKT-">Writable</code> stream. What happens is explained as follows:</p>
    <ol>
      <li class="numbered">We first write the head of the HTTP response. Note that <code class="Code-In-Text--PACKT-">writeHead()</code> is not a part of the <code class="Code-In-Text--PACKT-">Writable</code> interface; in fact, it's an auxiliary method exposed by the <code class="Code-In-Text--PACKT-">http.ServerResponse</code> class and is specific to the HTTP protocol.</li>
      <li class="numbered">We start a loop that terminates with a likelihood of 5% (we instruct <code class="Code-In-Text--PACKT-">chance.bool()</code> to return <code class="Code-In-Text--PACKT-">true</code> 95% of the time).</li>
      <li class="numbered">Inside the loop, we write a random string into the stream.</li>
      <li class="numbered">Once we are out of the loop, we call <code class="Code-In-Text--PACKT-">end()</code> on the stream, indicating that no more data will be written. Also, we provide a final string containing two new line characters to be written into the stream before ending it.</li>
      <li class="numbered">Finally, we register a listener for the <code class="Code-In-Text--PACKT-">finish</code> event, which will be fired when all the data has been flushed into the underlying socket.</li>
    </ol>
    <p class="normal">To test the server, we can open a browser at the address <code class="Code-In-Text--PACKT-">http://localhost:8080</code> or use <code class="Code-In-Text--PACKT-">curl</code> from the terminal as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">curl localhost:8080
</code></pre>
    <p class="normal">At this point, the server should start sending random strings to the HTTP client that you chose (please bear in <a id="_idIndexMarker437"/>mind that some browsers might buffer the data, and the streaming behavior might not be apparent).</p>
    <h3 id="_idParaDest-158" class="title">Backpressure</h3>
    <p class="normal">Similar to a liquid flowing in a real piping system, Node.js streams can also suffer from bottlenecks, where <a id="_idIndexMarker438"/>data is written faster than the stream can consume it. The mechanism to cope with this problem involves buffering the incoming data; however, if the stream doesn't give any feedback to the writer, we may incur a situation where more and more data is accumulated in the internal buffer, leading to undesired levels of memory usage.</p>
    <p class="normal">To prevent this from happening, <code class="Code-In-Text--PACKT-">writable.write()</code> will return <code class="Code-In-Text--PACKT-">false</code> when the internal buffer exceeds the <code class="Code-In-Text--PACKT-">highWaterMark</code> limit. In <code class="Code-In-Text--PACKT-">Writable</code> streams, the <code class="Code-In-Text--PACKT-">highWaterMark</code> property is the limit of the internal buffer size, beyond which the <code class="Code-In-Text--PACKT-">write()</code> method starts returning <code class="Code-In-Text--PACKT-">false</code>, indicating that the application should now stop writing. When the buffer is emptied, the <code class="Code-In-Text--PACKT-">drain</code> event is emitted, communicating that it's safe to start writing again. This <a id="_idIndexMarker439"/>mechanism is called <strong class="keyword">backpressure</strong>.</p>
    <p class="normal">Backpressure is an advisory mechanism. Even if <code class="Code-In-Text--PACKT-">write()</code> returns <code class="Code-In-Text--PACKT-">false</code>, we could ignore this signal and continue writing, making the buffer grow indefinitely. The stream won't be blocked automatically when the <code class="Code-In-Text--PACKT-">highWaterMark</code> threshold is reached; therefore, it is recommended to always be mindful and respect the backpressure.</p>
    <div><p class="Information-Box--PACKT-">The mechanism described in this section is similarly applicable to <code class="Code-In-Text--PACKT-">Readable</code> streams. In fact, backpressure exists in <code class="Code-In-Text--PACKT-">Readable</code> streams too, and it's triggered when the <code class="Code-In-Text--PACKT-">push()</code> method, which is invoked inside <code class="Code-In-Text--PACKT-">_read()</code>, returns <code class="Code-In-Text--PACKT-">false</code>. However, that's a problem specific to stream implementers, so we usually have to deal with it less frequently.</p>
    </div>
    <p class="normal">We can quickly demonstrate how to take into account the backpressure of a <code class="Code-In-Text--PACKT-">Writable</code> stream by modifying the <code class="Code-In-Text--PACKT-">entropy-server.js</code> module that we created previously:</p>
    <pre class="programlisting code"><code class="hljs-code">// ...
const server = createServer((req, res) =&gt; {
  res.writeHead(200, { 'Content-Type': 'text/plain' })
  function generateMore () {                                // (1)
    while (chance.bool({ likelihood: 95 })) {
      const randomChunk = chance.string({                   // (2)
        length: (16 * 1024) - 1
      })
      const shouldContinue = res.write(`${randomChunk}\n`)  // (3)
      if (!shouldContinue) {
        console.log('back-pressure')
        return res.once('drain', generateMore)
      }
    }
    res.end('\n\n')
  }
  generateMore()
  res.on('finish', () =&gt; console.log('All data sent'))
})
// ...
</code></pre>
    <p class="normal">The most important steps of the previous code can be summarized as follows:</p>
    <ol>
      <li class="numbered">We wrapped the main logic in a function called <code class="Code-In-Text--PACKT-">generateMore()</code>.</li>
      <li class="numbered">To increase the <a id="_idIndexMarker440"/>chances of receiving some backpressure, we increased the size of the data chunk to 16 KB minus 1 byte, which is very close to the default <code class="Code-In-Text--PACKT-">highWaterMark</code> limit.</li>
      <li class="numbered">After writing a chunk of data, we check the return value of <code class="Code-In-Text--PACKT-">res.write()</code>. If we receive <code class="Code-In-Text--PACKT-">false</code>, it means that the internal buffer is full and we should stop sending more data. When this happens, we exit the function and register another cycle of writes using <code class="Code-In-Text--PACKT-">generateMore()</code> for when the <code class="Code-In-Text--PACKT-">drain</code> event is emitted.</li>
    </ol>
    <p class="normal">If we now try to run the server again, and then generate a client request with <code class="Code-In-Text--PACKT-">curl</code>, there is a high probability that there will be some backpressure, as the server produces data at a very high rate, faster than the underlying socket can handle.</p>
    <h3 id="_idParaDest-159" class="title">Implementing Writable streams</h3>
    <p class="normal">We can implement a <a id="_idIndexMarker441"/>new <code class="Code-In-Text--PACKT-">Writable</code> stream by inheriting the class <code class="Code-In-Text--PACKT-">Writable</code> and providing an implementation for the <code class="Code-In-Text--PACKT-">_write()</code> method. Let's try to do it immediately while discussing the details along the way.</p>
    <p class="normal">Let's build a <code class="Code-In-Text--PACKT-">Writable</code> stream that receives objects in the following format:</p>
    <pre class="programlisting code"><code class="hljs-code">{
  path: &lt;path to a file&gt;
  content: &lt;string or buffer&gt;
}
</code></pre>
    <p class="normal">For each one of these objects, our stream has to save the <code class="Code-In-Text--PACKT-">content</code> property into a file created at the given <code class="Code-In-Text--PACKT-">path</code>. We can immediately see that the inputs of our stream are objects, and not<a id="_idIndexMarker442"/> strings or buffers. This means that our stream has to work in object mode.</p>
    <p class="normal">Let's call the module <code class="Code-In-Text--PACKT-">to-file-stream.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { Writable } from 'stream'
import { promises as fs } from 'fs'
import { dirname } from 'path'
import mkdirp from 'mkdirp-promise'
export class ToFileStream extends Writable {
  constructor (options) {
    super({ ...options, objectMode: true })
  }
  _write (chunk, encoding, cb) {
    mkdirp(dirname(chunk.path))
      .then(() =&gt; fs.writeFile(chunk.path, chunk.content))
      .then(() =&gt; cb())
      .catch(cb)
  }
}
</code></pre>
    <p class="normal">We created a new class for our new stream, which extends <code class="Code-In-Text--PACKT-">Writable</code> from the <code class="Code-In-Text--PACKT-">stream</code> module.</p>
    <p class="normal">We had to invoke the parent constructor to initialize its internal state; we also needed to make sure that the <code class="Code-In-Text--PACKT-">options</code> object specifies that the stream works in object mode (<code class="Code-In-Text--PACKT-">objectMode: true</code>). Other options accepted by <code class="Code-In-Text--PACKT-">Writable</code> are as follows:</p>
    <ul>
      <li class="Bullet--PACKT-"><code class="Code-In-Text--PACKT-">highWaterMark</code> (the default is 16 KB): This controls the backpressure limit.</li>
      <li class="Bullet-End--PACKT-"><code class="Code-In-Text--PACKT-">decodeStrings</code> (defaults to <code class="Code-In-Text--PACKT-">true</code>): This enables the automatic decoding of strings into binary buffers before passing them to the <code class="Code-In-Text--PACKT-">_write()</code> method. This option is ignored in object mode.</li>
    </ul>
    <p class="normal">Finally, we provided an implementation for the <code class="Code-In-Text--PACKT-">_write()</code> method. As you can see, the method accepts a data chunk and an encoding (which makes sense only if we are in binary mode and the stream option <code class="Code-In-Text--PACKT-">decodeStrings</code> is set to <code class="Code-In-Text--PACKT-">false</code>). Also, the method accepts a callback function (<code class="Code-In-Text--PACKT-">cb</code>), which needs to be invoked when the operation completes; it's not necessary to pass the result of the operation but, if needed, we can still pass an error<a id="_idIndexMarker443"/> that will cause the stream to emit an <code class="Code-In-Text--PACKT-">error</code> event.</p>
    <p class="normal">Now, to try the stream that we just built, we can create a new module and perform some write operations against the stream:</p>
    <pre class="programlisting code"><code class="hljs-code">import { join } from 'path'
import { ToFileStream } from './to-file-stream.js'
const tfs = new ToFileStream()
tfs.write({
  path: join('files', 'file1.txt'), content: 'Hello' })
tfs.write({
  path: join('files', 'file2.txt'), content: 'Node.js' })
tfs.write({
  path: join('files', 'file3.txt'), content: 'streams' })
tfs.end(() =&gt; console.log('All files created'))
</code></pre>
    <p class="normal">Here, we created and used our first custom <code class="Code-In-Text--PACKT-">Writable</code> stream. Run the new module as usual and check its output. You will see that after the execution, three new files will be created within a new folder called <code class="Code-In-Text--PACKT-">files</code>.</p>
    <h4 class="title">Simplified construction</h4>
    <p class="normal">As we saw for <code class="Code-In-Text--PACKT-"><a id="_idIndexMarker444"/></code><code class="Code-In-Text--PACKT-">Readable</code> streams, <code class="Code-In-Text--PACKT-">Writable</code> streams also offer a simplified construction mechanism. If we were to rewrite our <code class="Code-In-Text--PACKT-">ToFileStream</code> using the simplified construction for <code class="Code-In-Text--PACKT-">Writable</code> streams, it would look like this:</p>
    <pre class="programlisting code"><code class="hljs-code">// ...
const tfs = new Writable({
  objectMode: true,
  write (chunk, encoding, cb) {
    mkdirp(dirname(chunk.path))
      .then(() =&gt; fs.writeFile(chunk.path, chunk.content))
      .then(() =&gt; cb())
      .catch(cb)
  }
})
// ...
</code></pre>
    <p class="normal">With this approach, we are simply using the <code class="Code-In-Text--PACKT-">Writable</code> constructor and passing a <code class="Code-In-Text--PACKT-">write()</code> function that<a id="_idIndexMarker445"/> implements the custom logic of our <code class="Code-In-Text--PACKT-">Writable</code> instance. Note that with this approach, the <code class="Code-In-Text--PACKT-">write()</code> function doesn't have an underscore in the name. We can also pass other construction options like <code class="Code-In-Text--PACKT-">objectMode</code>.</p>
    <h2 id="_idParaDest-160" class="title">Duplex streams</h2>
    <p class="normal">A <code class="Code-In-Text--PACKT-">Duplex</code> stream is a stream that is both <code class="Code-In-Text--PACKT-">Readable</code> and <code class="Code-In-Text--PACKT-">Writable</code>. It is useful when we want to describe an<a id="_idIndexMarker446"/> entity that is both a data source and a data destination, such as network sockets, for example. <code class="Code-In-Text--PACKT-">Duplex</code> streams inherit the methods of both <code class="Code-In-Text--PACKT-">stream.Readable</code> and <code class="Code-In-Text--PACKT-">stream.Writable</code>, so this is nothing new to us. This means that we can <code class="Code-In-Text--PACKT-">read()</code> or <code class="Code-In-Text--PACKT-">write()</code> data, or listen for both <code class="Code-In-Text--PACKT-">readable</code> and <code class="Code-In-Text--PACKT-">drain</code> events.</p>
    <p class="normal">To create a custom <code class="Code-In-Text--PACKT-">Duplex</code> stream, we have to provide an implementation for both <code class="Code-In-Text--PACKT-">_read()</code> and <code class="Code-In-Text--PACKT-">_write()</code>. The <code class="Code-In-Text--PACKT-">options</code> object passed to the <code class="Code-In-Text--PACKT-">Duplex()</code> constructor is internally forwarded to both the <code class="Code-In-Text--PACKT-">Readable</code> and <code class="Code-In-Text--PACKT-">Writable</code> constructors. The options are the same as those we already discussed in the previous sections, with the addition of a new one called <code class="Code-In-Text--PACKT-">allowHalfOpen</code> (defaults to <code class="Code-In-Text--PACKT-">true</code>) that, if set to <code class="Code-In-Text--PACKT-">false</code>, will cause both parts (<code class="Code-In-Text--PACKT-">Readable</code> and <code class="Code-In-Text--PACKT-">Writable</code>) of the stream to end if only one of them does.</p>
    <div><p class="Information-Box--PACKT-">If we need to have a <code class="Code-In-Text--PACKT-">Duplex</code> stream working in object mode on one side and binary mode on the other, we can use the options <code class="Code-In-Text--PACKT-">readableObjectMode</code> and <code class="Code-In-Text--PACKT-">writableObjectMode</code> independently.</p>
    </div>
    <h2 id="_idParaDest-161" class="title">Transform streams</h2>
    <p class="normal"><code class="Code-In-Text--PACKT-">Transform</code> streams are a <a id="_idIndexMarker447"/>special kind of <code class="Code-In-Text--PACKT-">Duplex</code> stream that are specifically designed to handle data transformations. Just to give you a few concrete examples, the functions <code class="Code-In-Text--PACKT-">zlib.createGzip()</code> and <code class="Code-In-Text--PACKT-">crypto.createCipheriv()</code> that we discussed at the beginning of this chapter create <code class="Code-In-Text--PACKT-">Transform</code> streams for compression and encryption, respectively.</p>
    <p class="normal">In a simple <code class="Code-In-Text--PACKT-">Duplex</code> stream, there is no immediate relationship between the data read from the stream and the data written into it (at least, the stream is agnostic to such a relationship). Think about a TCP socket, which just sends and receives data to and from the remote peer; the socket is not aware of any relationship between the input and output. <em class="italic">Figure 6.4</em> illustrates the data flow in a <code class="Code-In-Text--PACKT-">Duplex</code> stream:</p>
    <figure class="mediaobject"><img src="img/B15729_06_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.4: Duplex stream schematic representation</p>
    <p class="normal">On the other<a id="_idIndexMarker448"/> hand, <code class="Code-In-Text--PACKT-">Transform</code> streams apply some kind of transformation to each chunk of data that they receive from their <code class="Code-In-Text--PACKT-">Writable</code> side, and then make the transformed data available on their <code class="Code-In-Text--PACKT-">Readable</code> side. <em class="italic">Figure 6.5</em> shows how the data flows in a <code class="Code-In-Text--PACKT-">Transform</code> stream:</p>
    <figure class="mediaobject"><img src="img/B15729_06_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.5: Transform stream schematic representation</p>
    <p class="normal">From the outside, the interface of a <code class="Code-In-Text--PACKT-">Transform</code> stream is exactly like that of a <code class="Code-In-Text--PACKT-">Duplex</code> stream. However, when we want to build a new <code class="Code-In-Text--PACKT-">Duplex</code> stream, we have to provide both the <code class="Code-In-Text--PACKT-">_read()</code> and <code class="Code-In-Text--PACKT-">_write()</code> methods, while for implementing a new <code class="Code-In-Text--PACKT-">Transform</code> stream, we have to fill in another pair of methods: <code class="Code-In-Text--PACKT-">_transform()</code> and <code class="Code-In-Text--PACKT-">_flush()</code>.</p>
    <p class="normal">Let's see how to create a new <code class="Code-In-Text--PACKT-">Transform</code> stream with an example.</p>
    <h3 id="_idParaDest-162" class="title">Implementing Transform streams</h3>
    <p class="normal">Let's implement a <code class="Code-In-Text--PACKT-">Transform</code> stream<a id="_idIndexMarker449"/> that replaces all the occurrences of a given string. To do this, we have to create a new module named <code class="Code-In-Text--PACKT-">replaceStream.js</code>. Let's jump directly to the implementation:</p>
    <pre class="programlisting code"><code class="hljs-code">import { Transform } from 'stream'
export class ReplaceStream extends Transform {
  constructor (searchStr, replaceStr, options) {
    super({ ...options })
    this.searchStr = searchStr
    this.replaceStr = replaceStr
    this.tail = ''
  }
  _transform (chunk, encoding, callback) {
    const pieces = (this.tail + chunk).split(this.searchStr)  // (1)
    const lastPiece = pieces[pieces.length - 1]               // (2)
    const tailLen = this.searchStr.length - 1
    this.tail = lastPiece.slice(-tailLen)
    pieces[pieces.length - 1] = lastPiece.slice(0, -tailLen)
    this.push(pieces.join(this.replaceStr))                   // (3)
    callback()
  }
  _flush (callback) {
    this.push(this.tail)
    callback()
  }
}
</code></pre>
    <p class="normal">In this example, we created a new class extending the <code class="Code-In-Text--PACKT-">Transform</code> base class. The constructor of the class accepts three arguments: <code class="Code-In-Text--PACKT-">searchStr</code>, <code class="Code-In-Text--PACKT-">replaceStr</code>, and <code class="Code-In-Text--PACKT-">options</code>. As you can imagine, they allow us to define the text to match and the string to use as a replacement, plus an <code class="Code-In-Text--PACKT-">options</code> object for advanced configuration of the underlying <code class="Code-In-Text--PACKT-">Transform</code> stream. We also initialize an internal <code class="Code-In-Text--PACKT-">tail</code> variable, which will be used later by the <code class="Code-In-Text--PACKT-">_transform()</code> method.</p>
    <p class="normal">Now, let's analyze the <code class="Code-In-Text--PACKT-">_transform()</code> method, which is the core of our new class. The <code class="Code-In-Text--PACKT-">_transform()</code> method has practically the same signature as the <code class="Code-In-Text--PACKT-">_write()</code> method of the <code class="Code-In-Text--PACKT-">Writable</code> stream, but instead of writing data into an underlying resource, it pushes it into the internal read buffer using <code class="Code-In-Text--PACKT-">this.push()</code>, exactly as we would do in the <code class="Code-In-Text--PACKT-">_read()</code> method of a <code class="Code-In-Text--PACKT-">Readable</code> stream. This shows how the two sides of a <code class="Code-In-Text--PACKT-">Transform</code> stream are connected.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">_transform()</code> method of <code class="Code-In-Text--PACKT-">ReplaceStream</code> implements the core of our algorithm. To search for and replace a string in a buffer is an easy task; however, it's a totally different story when the data is streaming, and possible matches might be distributed across multiple chunks. The procedure followed by the code can be explained as follows:</p>
    <ol>
      <li class="numbered">Our algorithm splits the data in memory (<code class="Code-In-Text--PACKT-">tail</code> data and the current <code class="Code-In-Text--PACKT-">chunk</code>) using <code class="Code-In-Text--PACKT-">searchStr</code> as a separator.</li>
      <li class="numbered">Then, it takes the last item of the array generated by the operation and extracts the last <code class="Code-In-Text--PACKT-">searchString.length - 1</code> characters. The result is saved in the <code class="Code-In-Text--PACKT-">tail</code> variable and will be prepended to the next chunk of data.</li>
      <li class="numbered">Finally, all the pieces resulting from <code class="Code-In-Text--PACKT-">split()</code> are joined together using <code class="Code-In-Text--PACKT-">replaceStr</code> as a separator and pushed into the internal buffer.</li>
    </ol>
    <p class="normal">When the stream<a id="_idIndexMarker450"/> ends, we might still have some content in the <code class="Code-In-Text--PACKT-">tail</code> variable not pushed into the internal buffer. That's exactly what the <code class="Code-In-Text--PACKT-">_flush()</code> method is for; it is invoked just before the stream is ended, and this is where we have one final chance to finalize the stream or push any remaining data before completely ending the stream.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">_flush()</code> method only takes in a callback, which we have to make sure to invoke when all the operations are complete, causing the stream to be terminated. With this, we have completed our <code class="Code-In-Text--PACKT-">ReplaceStream</code> class.</p>
    <p class="normal">Now, it's time to try the new stream. Let's create a script that writes some data into the stream and then reads the transformed result:</p>
    <pre class="programlisting code"><code class="hljs-code">import { ReplaceStream } from './replace-stream.js'
const replaceStream = new ReplaceStream('World', 'Node.js')
replaceStream.on('data', chunk =&gt; console.log(chunk.toString()))
replaceStream.write('Hello W')
replaceStream.write('orld!')
replaceStream.end()
</code></pre>
    <p class="normal">To make life a little bit harder for our stream, we spread the search term (which is <code class="Code-In-Text--PACKT-">World</code>) across two different chunks, then, using the flowing mode, we read from the same stream, logging each transformed chunk. Running the preceding program should produce the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">Hel
lo Node.js
!
</code></pre>
    <div><p class="Information-Box--PACKT-">Please note <a id="_idIndexMarker451"/>that the preceding output is broken into multiple lines because we are using <code class="Code-In-Text--PACKT-">console.log()</code> to print it out. This allows us to demonstrate that our implementation is able to replace string matches correctly, even when the matching text spans multiple chunks of data.</p>
    </div>
    <h4 class="title">Simplified construction</h4>
    <p class="normal">Unsurprisingly, even<a id="_idIndexMarker452"/> <code class="Code-In-Text--PACKT-">Transform</code> streams support simplified construction. At this point, we should have developed an instinct for how this API might look, so let's get our hands dirty straight away and rewrite the previous example using this approach:</p>
    <pre class="programlisting code"><code class="hljs-code">const searchStr = 'World'
const replaceStr = 'Node.js'
let tail = ''
const replaceStream = new Transform({
  defaultEncoding: 'utf8',
  transform (chunk, encoding, cb) {
    const pieces = (tail + chunk).split(searchStr)
    const lastPiece = pieces[pieces.length - 1]
    const tailLen = searchStr.length - 1
    tail = lastPiece.slice(-tailLen)
    pieces[pieces.length - 1] = lastPiece.slice(0, -tailLen)
    this.push(pieces.join(replaceStr))
    cb()
  },
  flush (cb) {
    this.push(tail)
    cb()
  }
})
// now write to replaceStream ...
</code></pre>
    <p class="normal">As expected, simplified construction works by directly instantiating a new <code class="Code-In-Text--PACKT-">Transform</code> object and passing our specific transformation logic through the <code class="Code-In-Text--PACKT-">transform()</code> and <code class="Code-In-Text--PACKT-">flush()</code> functions<a id="_idIndexMarker453"/> directly through the <code class="Code-In-Text--PACKT-">options</code> object. Note that <code class="Code-In-Text--PACKT-">transform()</code> and <code class="Code-In-Text--PACKT-">flush()</code> don't have a prepended underscore here.</p>
    <h3 id="_idParaDest-163" class="title">Filtering and aggregating data with Transform streams</h3>
    <p class="normal">As we mentioned in the previous section, <code class="Code-In-Text--PACKT-">Transform</code> streams are the perfect building blocks for<a id="_idIndexMarker454"/> implementing data transformation<a id="_idIndexMarker455"/> pipelines. In the previous section, we illustrated an example of a <code class="Code-In-Text--PACKT-">Transform</code> stream that can replace words in a stream of text. But <code class="Code-In-Text--PACKT-">Transform</code> streams can be used to implement other types of data transformation as well. For instance, it's quite common to use <code class="Code-In-Text--PACKT-">Transform</code> streams to implement data filtering and data aggregation.</p>
    <p class="normal">Just to make a practical example, let's imagine we are asked by a Fortune 500 company to analyze a big file containing all the sales for the year 2020. The company wants us to use <code class="Code-In-Text--PACKT-">data.csv</code>, a sales report in CSV format, to calculate the total profit for the sales made in Italy.</p>
    <p class="normal">For simplicity, let's imagine the sales data that is stored in the CSV file contains three fields per line: item type, country of sale, and profit. So, such a file could look like this:</p>
    <pre class="programlisting code"><code class="hljs-code">type,country,profit
Household,Namibia,597290.92
Baby Food,Iceland,808579.10
Meat,Russia,277305.60
Meat,Italy,413270.00
Cereal,Malta,174965.25
Meat,Indonesia,145402.40
Household,Italy,728880.54
[... many more lines]
</code></pre>
    <p class="normal">Now, it's clear that we have to find all the records that have "Italy" as <code class="Code-In-Text--PACKT-">country</code> and, in the process, sum up the <code class="Code-In-Text--PACKT-">profit</code> value of the matching lines into a single number.</p>
    <p class="normal">In order to process a CSV file in a <a id="_idIndexMarker456"/>streaming fashion, we can use the excellent <code class="Code-In-Text--PACKT-">csv-parse</code> module (<a href="http://nodejsdp.link/csv-parse">nodejsdp.link/csv-parse</a>).</p>
    <p class="normal">If we assume for a moment that we have already implemented our custom streams to filter and aggregate the data, a possible solution to this task might look like this:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createReadStream } from 'fs'
import parse from 'csv-parse'
import { FilterByCountry } from './filter-by-country.js'
import { SumProfit } from './sum-profit.js'
const csvParser = parse({ columns: true })
createReadStream('data.csv')             // (1)
  .pipe(csvParser)                       // (2)
  .pipe(new FilterByCountry('Italy'))    // (3)
  .pipe(new SumProfit())                 // (4)
  .pipe(process.stdout)                  // (5)
</code></pre>
    <p class="normal">The streaming <a id="_idIndexMarker457"/>pipeline proposed here consists of<a id="_idIndexMarker458"/> five steps:</p>
    <ol>
      <li class="numbered">We read the source CSV file as a stream.</li>
      <li class="numbered">We use the <code class="Code-In-Text--PACKT-">csv-parse</code> library to parse every line of the document as a CSV record. For every line, this stream will emit an object containing the properties <code class="Code-In-Text--PACKT-">type</code>, <code class="Code-In-Text--PACKT-">country</code>, and <code class="Code-In-Text--PACKT-">profit</code>.</li>
      <li class="numbered">We filter all the records by country, retaining only the records that match the country "Italy." All the records that don't match "Italy" are dropped, which means that they will not be passed to the other steps in the pipeline. Note that this is one of the custom <code class="Code-In-Text--PACKT-">Transform</code> streams that we have to implement.</li>
      <li class="numbered">For every record, we accumulate the profit. This stream will eventually emit one single string, which represents the value of the total profit for products sold in Italy. This value will be emitted by the stream only when all the data from the original file has been completely processed. Note that this is the second custom <code class="Code-In-Text--PACKT-">Transform</code> stream that we have to implement to complete this project.</li>
      <li class="numbered">Finally, the data emitted from the previous step is displayed in the standard output.</li>
    </ol>
    <p class="normal">Now, let's implement the <code class="Code-In-Text--PACKT-">FilterByCountry</code> stream:</p>
    <pre class="programlisting code"><code class="hljs-code">import { Transform } from 'stream'
export class FilterByCountry extends Transform {
  constructor (country, options = {}) {
    options.objectMode = true
    super(options)
    this.country = country
  }
  _transform (record, enc, cb) {
    if (record.country === this.country) {
      this.push(record)
    }
    cb()
  }
}
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">FilterByCountry</code> is a custom <code class="Code-In-Text--PACKT-">Transform</code> stream. We can see that the constructor accepts an argument called <code class="Code-In-Text--PACKT-">country</code>, which allows us to specify the country name we want to filter on. In the constructor, we also set the stream to run in <code class="Code-In-Text--PACKT-">objectMode</code> because we know it will be used to process objects (records from the CSV file).</p>
    <p class="normal">In the <code class="Code-In-Text--PACKT-">_transform</code> method, we check if the country of the current record matches the country specified <a id="_idIndexMarker459"/>at construction time. If it's a match, then <a id="_idIndexMarker460"/>we pass the record on to the next stage of the pipeline by calling <code class="Code-In-Text--PACKT-">this.push()</code>. Regardless of whether the record matches or not, we need to invoke <code class="Code-In-Text--PACKT-">cb()</code> to indicate that the current record has been successfully processed and that the stream is ready to receive another record.</p>
    <div><p class="Information-Box--PACKT-"><strong class="screenText">Pattern: Transform filter</strong></p>
      <p class="Information-Box--PACKT-">Invoke <code class="Code-In-Text--PACKT-">this.push()</code> in a conditional way to allow only some data to reach the next stage of the pipeline.</p>
    </div>
    <p class="normal">Finally, let's implement the <code class="Code-In-Text--PACKT-">SumProfit</code> filter:</p>
    <pre class="programlisting code"><code class="hljs-code">import { Transform } from 'stream'
export class SumProfit extends Transform {
  constructor (options = {}) {
    options.objectMode = true
    super(options)
    this.total = 0
  }
  _transform (record, enc, cb) {
    this.total += Number.parseFloat(record.profit)
    cb()
  }
  _flush (cb) {
    this.push(this.total.toString())
    cb()
  }
}
</code></pre>
    <p class="normal">This stream needs to run in <code class="Code-In-Text--PACKT-">objectMode</code> as well, because it will receive objects representing records from the CSV file. Note that, in the constructor, we also initialize an instance variable called <code class="Code-In-Text--PACKT-">total</code> and we set its value to <code class="Code-In-Text--PACKT-">0</code>.</p>
    <p class="normal">In the <code class="Code-In-Text--PACKT-">_transform()</code> method, we process every record and use the current <code class="Code-In-Text--PACKT-">profit</code> value to increase the <code class="Code-In-Text--PACKT-">total</code>. It's important to note that this time, we are not calling <code class="Code-In-Text--PACKT-">this.push()</code>. This means that no value is emitted while the data is flowing through the stream. We still <a id="_idIndexMarker461"/>need to call <code class="Code-In-Text--PACKT-">cb()</code>, though, to indicate that<a id="_idIndexMarker462"/> the current record has been processed and the stream is ready to receive another one.</p>
    <p class="normal">In order to emit the final result when all the data has been processed, we have to define a custom flush behavior using the <code class="Code-In-Text--PACKT-">_flush()</code> method. Here, we finally call <code class="Code-In-Text--PACKT-">this.push()</code> to emit a string representation of the resulting <code class="Code-In-Text--PACKT-">total</code> value. Remember that <code class="Code-In-Text--PACKT-">_flush()</code> is automatically invoked before the stream is closed.</p>
    <div><p class="Information-Box--PACKT-"><strong class="screenText">Pattern: Streaming aggregation</strong></p>
      <p class="Information-Box--PACKT-">Use <code class="Code-In-Text--PACKT-">_transform()</code> to process the data and accumulate the partial result, then call <code class="Code-In-Text--PACKT-">this.push()</code> only in the <code class="Code-In-Text--PACKT-">_flush()</code> method to emit the result when all the data has been processed.</p>
    </div>
    <p class="normal">This completes our example. Now, you can grab the CSV file from the code repository and execute this program to see what the total profit for Italy is. No surprise it's going to be a lot of money since we are talking about the profit of a Fortune 500 company!</p>
    <h2 id="_idParaDest-164" class="title">PassThrough streams</h2>
    <p class="normal">There is a fifth type <a id="_idIndexMarker463"/>of stream that is worth mentioning: <code class="Code-In-Text--PACKT-">PassThrough</code>. This type of stream is a special type of <code class="Code-In-Text--PACKT-">Transform</code> that outputs every data chunk without applying any transformation.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">PassThrough</code> is possibly the most underrated type of stream, but there are actually several circumstances in which it can be a very valuable tool in our toolbelt. For instance, <code class="Code-In-Text--PACKT-">PassThrough</code> streams can be useful for observability or to implement late piping and lazy stream patterns.</p>
    <h3 id="_idParaDest-165" class="title">Observability</h3>
    <p class="normal">If we want to observe how<a id="_idIndexMarker464"/> much data is flowing through one or more streams, we could do so by attaching a <code class="Code-In-Text--PACKT-">data</code> event listener to a <code class="Code-In-Text--PACKT-">PassThrough</code> instance and then piping this instance in a given point of a stream pipeline. Let's a see a simplified example to be able to appreciate this concept:</p>
    <pre class="programlisting code"><code class="hljs-code">import { PassThrough } from 'stream'
let bytesWritten = 0
const monitor = new PassThrough()
monitor.on('data', (chunk) =&gt; {
  bytesWritten += chunk.length
})
monitor.on('finish', () =&gt; {
  console.log(`${bytesWritten} bytes written`)
})
monitor.write('Hello!')
monitor.end()
</code></pre>
    <p class="normal">In this example, we are creating a new instance of <code class="Code-In-Text--PACKT-">PassThrough</code> and using the <code class="Code-In-Text--PACKT-">data</code> event to count how many bytes are flowing through the stream. We also use the <code class="Code-In-Text--PACKT-">finish</code> event to dump the total amount to the console. Finally we write some data directly into the stream using <code class="Code-In-Text--PACKT-">write()</code> and <code class="Code-In-Text--PACKT-">end()</code>. This is just an illustrative example; in a more realistic scenario, we would be piping our <code class="Code-In-Text--PACKT-">monitor</code> instance in a given point of a stream pipeline. For instance, if we wanted to monitor how many bytes are written to disk in our first file compression example of this chapter, we could easily achieve that by doing something like this:</p>
    <pre class="programlisting code"><code class="hljs-code">createReadStream(filename)
  .pipe(createGzip())
  <strong class="hljs-slc">.pipe(monitor)</strong>
  .pipe(createWriteStream(`${filename}.gz`))
</code></pre>
    <p class="normal">The beauty of this approach is that we didn't have to touch any of the other existing streams in the pipeline, so if we need to observe other parts of the pipeline (for instance, imagine we want to know the number of bytes of the uncompressed data), we can move <code class="Code-In-Text--PACKT-">monitor</code> around<a id="_idIndexMarker465"/> with very little effort.</p>
    <div><p class="Information-Box--PACKT-">Note that you could implement an alternative version of the <code class="Code-In-Text--PACKT-">monitor</code> stream by using a custom transform stream instead. In such a case, you would have to make sure that the received chunks are pushed without any modification or delay, which is something that a <code class="Code-In-Text--PACKT-">PassThrough</code> stream would do automatically for you. Both approaches are equally valid, so pick the approach that feels more natural to you.</p>
    </div>
    <h3 id="_idParaDest-166" class="title">Late piping</h3>
    <p class="normal">In some circumstances, we might have to use APIs that accept a stream as an input parameter. This is<a id="_idIndexMarker466"/> generally not a big deal because we already know how to create and use streams. However, it may get a little bit more complicated if the data we want to read or write through the stream is available after we've called the given API.</p>
    <p class="normal">To visualize this scenario in more concrete terms, let's imagine that we have to use an API that gives us the following function to upload a file to a data storage service:</p>
    <pre class="programlisting code"><code class="hljs-code">function upload (filename, contentStream) {
  // ...
}
</code></pre>
    <div><p class="Tip--PACKT-">This function is effectively a simplified version of what is commonly available in the SDK of file storage services like Amazon Simple Storage Service (S3) or Azure Blob Storage service. Often, those libraries will provide the user with a more flexible function that can receive the content data in different formats (for example, a string, a buffer, or a <code class="Code-In-Text--PACKT-">Readable</code> stream).</p>
    </div>
    <p class="normal">Now, if we want to upload a file from the filesystem, this is a trivial operation, and we can do something like this:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createReadStream } from 'fs'
upload('a-picture.jpg', createReadStream('/path/to/a-picture.jpg'))
</code></pre>
    <p class="normal">But what if we want to do some processing to the file stream before the upload. For instance, let's say we want to compress or encrypt the data? Also, what if we have to do this transformation asynchronously after the <code class="Code-In-Text--PACKT-">upload</code> function has been called?</p>
    <p class="normal">In such cases, we can provide a <code class="Code-In-Text--PACKT-">PassThrough</code> stream to the <code class="Code-In-Text--PACKT-">upload()</code> function, which will effectively act as a placeholder. The internal implementation of <code class="Code-In-Text--PACKT-">upload()</code> will immediately try to consume data from it, but there will be no data available in the stream until we actually write to it. Also, the stream won't be considered complete until we close it, so the <code class="Code-In-Text--PACKT-">upload()</code> function<a id="_idIndexMarker467"/> will have to wait for data to flow through the <code class="Code-In-Text--PACKT-">PassThrough</code> instance to initiate the upload.</p>
    <p class="normal">Let's see a possible command-line script that uses this approach to upload a file from the filesystem and also <a id="_idIndexMarker468"/>compresses it using the <strong class="keyword">Brotli</strong> compression. We are going to assume that the third-party <code class="Code-In-Text--PACKT-">upload()</code> function is provided in a file called <code class="Code-In-Text--PACKT-">upload.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createReadStream } from 'fs'
import { createBrotliCompress } from 'zlib'
import { PassThrough } from 'stream'
import { basename } from 'path'
import { upload } from './upload.js'
const filepath = process.argv[2]                           // (1)
const filename = basename(filepath)
const contentStream = new PassThrough()                    // (2)
upload(`${filename}.br`, contentStream)                    // (3)
  .then((response) =&gt; {
    console.log(`Server response: ${response.data}`)
  })
  .catch((err) =&gt; {
    console.error(err)
    process.exit(1)
  })
createReadStream(filepath)                                 // (4)
  .pipe(createBrotliCompress())
  .pipe(contentStream)
</code></pre>
    <div><p class="Information-Box--PACKT-">In this book's repository, you will find a complete implementation of this example that allows you to upload files to an HTTP server that you can run locally.</p>
    </div>
    <p class="normal">Let's review what's happening in the previous example:</p>
    <ol>
      <li class="numbered">We get the path to the file we want to upload from the first command-line argument and use <code class="Code-In-Text--PACKT-">basename</code> to extrapolate the filename from the given path.</li>
      <li class="numbered">We create a placeholder for our content stream as a <code class="Code-In-Text--PACKT-">PassThrough</code> instance.</li>
      <li class="numbered">Now, we invoke the <code class="Code-In-Text--PACKT-">upload</code> function by passing our filename (with the added <code class="Code-In-Text--PACKT-">.br</code> suffix, indicating that it is using the Brotli compression) and the placeholder content stream.</li>
      <li class="numbered">Finally, we create a pipeline by chaining a filesystem <code class="Code-In-Text--PACKT-">Readable</code> stream, a Brotli compression <code class="Code-In-Text--PACKT-">Transform</code> stream, and finally our content stream as the destination.</li>
    </ol>
    <p class="normal">When this code is executed, the upload will start as soon as we invoke the <code class="Code-In-Text--PACKT-">upload()</code> function (possibly<a id="_idIndexMarker469"/> establishing a connection to the remote server), but the data will start to flow only later, when our pipeline is initialized. Note that our pipeline will also close the <code class="Code-In-Text--PACKT-">contentStream</code> when the processing completes, which will indicate to the <code class="Code-In-Text--PACKT-">upload()</code> function that all the content has been fully consumed.</p>
    <div><p class="Information-Box--PACKT-"><strong class="screenText">Pattern</strong></p>
      <p class="Information-Box--PACKT-">Use a <code class="Code-In-Text--PACKT-">PassThrough</code> stream when you need to provide a placeholder for data that will be read or written in the future.</p>
    </div>
    <p class="normal">We can also use this pattern to transform the interface of the <code class="Code-In-Text--PACKT-">upload()</code> function. Instead of accepting a <code class="Code-In-Text--PACKT-">Readable</code> stream as input, we can make it return a Writeable stream, which can then be used to provide the data we want to upload:</p>
    <pre class="programlisting code"><code class="hljs-code">function createUploadStream (filename) {
  // ...
  // returns a writable stream that can be used to upload data
}
</code></pre>
    <p class="normal">If we were tasked to implement this function, we could achieve that in a very elegant way by using a <code class="Code-In-Text--PACKT-">PassThrough</code> instance, as in the following example implementation:</p>
    <pre class="programlisting code"><code class="hljs-code">function createUploadStream (filename) {
  const connector = new PassThrough()
  upload(filename, connector)
  return connector
}
</code></pre>
    <p class="normal">In the preceding code, we are using a <code class="Code-In-Text--PACKT-">PassThrough</code> stream as a connector. This stream becomes a perfect abstraction for a case where the consumer of the library can write data at any later stage.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">createUploadStream()</code> function can then be used as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">const upload = createUploadStream('a-file.txt')
upload.write('Hello World')
upload.end()
</code></pre>
    <div><p class="Information-Box--PACKT-">This book's repository also contains an HTTP upload example that adopts this alternative pattern.</p>
    </div>
    <h2 id="_idParaDest-167" class="title">Lazy streams</h2>
    <p class="normal">Sometimes, we need to create<a id="_idIndexMarker470"/> a large number of streams at the same time, for example, to pass them to a function for further processing. A typical example is <a id="_idIndexMarker471"/>when using <code class="Code-In-Text--PACKT-">archiver</code> (<a href="http://nodejsdp.link/archiver">nodejsdp.link/archiver</a>), a package for creating archives such as TAR and ZIP. The <code class="Code-In-Text--PACKT-">archiver</code> package allows you to create an archive from a set of streams, representing the files to add. The problem is that if we want to pass a large number of streams, such as from files on the filesystem, we would likely get an <code class="Code-In-Text--PACKT-">EMFILE, too many open files</code> error. This is because functions like <code class="Code-In-Text--PACKT-">createReadStream()</code> from the <code class="Code-In-Text--PACKT-">fs</code> module will actually open a file descriptor every time a new stream is created, even before you start to read from those streams.</p>
    <p class="normal">In more generic terms, creating a stream instance might initialize expensive operations straight away (for example, open a file or a socket, initialize a connection to a database, and so on), even before we actually start to use such a stream. This might not be desirable if you are creating a large number of stream instances for later consumption.</p>
    <p class="normal">In these cases, you might want to delay the expensive initialization until you actually need to consume data from the stream.</p>
    <p class="normal">It is possible to achieve this by using a library like <code class="Code-In-Text--PACKT-">lazystream</code> (<a href="http://nodejsdp.link/lazystream">nodejsdp.link/lazystream</a>). This library <a id="_idIndexMarker472"/>allows you to effectively create proxies for actual stream instances, where the proxied instance is not created until some piece of code is actually starting to consume data from the proxy.</p>
    <p class="normal">In the following example, <code class="Code-In-Text--PACKT-">lazystream</code> allows us to create a lazy <code class="Code-In-Text--PACKT-">Readable</code> stream for the special Unix file <code class="Code-In-Text--PACKT-">/dev/urandom</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import lazystream from 'lazystream'
const lazyURandom = new lazystream.Readable(function (options) {
  return fs.createReadStream('/dev/urandom')
})
</code></pre>
    <p class="normal">The function we pass as a parameter to <code class="Code-In-Text--PACKT-">new lazystream.Readable()</code> is effectively a factory function that generates the proxied stream when necessary.</p>
    <p class="normal">Behind the scenes, <code class="Code-In-Text--PACKT-">lazystream</code> is implemented using a <code class="Code-In-Text--PACKT-">PassThrough</code> stream that, only when its <code class="Code-In-Text--PACKT-">_read()</code> method is invoked for the first time, creates the proxied instance by invoking the factory function, and pipes the generated stream into the <code class="Code-In-Text--PACKT-">PassThrough</code> itself. The code that consumes the stream is totally agnostic of the proxying that is happening here, and it will consume the data as if it was flowing directly from the <code class="Code-In-Text--PACKT-">PassThrough</code> stream. <code class="Code-In-Text--PACKT-">lazystream</code> implements a similar utility to create lazy <code class="Code-In-Text--PACKT-">Writable</code> streams as well.</p>
    <p class="normal">Creating lazy <code class="Code-In-Text--PACKT-">Readable</code> and <code class="Code-In-Text--PACKT-">Writable</code> streams from scratch could be an interesting exercise that is left to you. If you get stuck, have a look at the source code of <code class="Code-In-Text--PACKT-">lazystream</code> for inspiration<a id="_idIndexMarker473"/> on how to implement this pattern.</p>
    <p class="normal">In the next section, we will discuss the <code class="Code-In-Text--PACKT-">.pipe()</code> method in greater detail and also see other ways to connect different streams to form a processing pipeline.</p>
    <h2 id="_idParaDest-168" class="title">Connecting streams using pipes</h2>
    <p class="normal">The concept of Unix pipes<a id="_idIndexMarker474"/> was invented by Douglas Mcllroy. This<a id="_idIndexMarker475"/> enabled the output of a program to be connected to the input of the next. Take a look at the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">echo Hello World! | sed s/World/Node.js/g
</code></pre>
    <p class="normal">In the preceding command, <code class="Code-In-Text--PACKT-">echo</code> will write <code class="Code-In-Text--PACKT-">Hello World!</code> to its standard output, which is then redirected to the standard input of the <code class="Code-In-Text--PACKT-">sed</code> command (thanks to the pipe <code class="Code-In-Text--PACKT-">|</code> operator). Then, <code class="Code-In-Text--PACKT-">sed</code> replaces any occurrence of <code class="Code-In-Text--PACKT-">World</code> with <code class="Code-In-Text--PACKT-">Node.js</code> and prints the result to its standard output (which, this time, is the console).</p>
    <p class="normal">In a similar way, Node.js streams can be connected using the <code class="Code-In-Text--PACKT-">pipe()</code> method of the <code class="Code-In-Text--PACKT-">Readable</code> stream, which has the following interface:</p>
    <pre class="programlisting code"><code class="hljs-code">readable.pipe(writable, [options])
</code></pre>
    <p class="normal">Very intuitively, the <code class="Code-In-Text--PACKT-">pipe()</code> method takes the data that is emitted from the <code class="Code-In-Text--PACKT-">readable</code> stream and pumps it into the provided <code class="Code-In-Text--PACKT-">writable</code> stream. Also, the <code class="Code-In-Text--PACKT-">writable</code> stream is ended automatically when the <code class="Code-In-Text--PACKT-">readable</code> stream emits an <code class="Code-In-Text--PACKT-">end</code> event (unless we specify <code class="Code-In-Text--PACKT-">{end: false}</code> as <code class="Code-In-Text--PACKT-">options</code>). The <code class="Code-In-Text--PACKT-">pipe()</code> method returns the <code class="Code-In-Text--PACKT-">writable</code> stream passed in the first argument, allowing us to create chained invocations if such a stream is also <code class="Code-In-Text--PACKT-">Readable</code> (such as a <code class="Code-In-Text--PACKT-">Duplex</code> or <code class="Code-In-Text--PACKT-">Transform</code> stream).</p>
    <p class="normal">Piping two streams together will create <em class="italic">suction</em>, which allows the data to flow automatically to the <code class="Code-In-Text--PACKT-">writable</code> stream, so there is no need to call <code class="Code-In-Text--PACKT-">read()</code> or <code class="Code-In-Text--PACKT-">write()</code>, but most importantly, there is no need to control the backpressure anymore, because it's automatically taken care of.</p>
    <p class="normal">To provide a quick example, we can create a new module that takes a text stream from the standard input, applies the <em class="italic">replace</em> transformation discussed earlier when we built our custom <code class="Code-In-Text--PACKT-">ReplaceStream</code>, and then pushes the data back to the standard output:</p>
    <pre class="programlisting code"><code class="hljs-code">// replace.js
import { ReplaceStream } from './replace-stream.js'
process.stdin
  .pipe(new ReplaceStream(process.argv[2], process.argv[3]))
  .pipe(process.stdout)
</code></pre>
    <p class="normal">The preceding program pipes the data that comes from the standard input into an instance of <code class="Code-In-Text--PACKT-">ReplaceStream</code> and then back to the standard output. Now, to try this small application, we <a id="_idIndexMarker476"/>can leverage a Unix pipe to redirect some data<a id="_idIndexMarker477"/> into its standard input, as shown in the following example:</p>
    <pre class="programlisting con"><code class="hljs-con">echo Hello World! | node replace.js World Node.js
</code></pre>
    <p class="normal">This should produce the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">Hello Node.js!
</code></pre>
    <p class="normal">This simple example demonstrates that streams (and in particular, text streams) are a universal interface and that pipes are the way to compose and interconnect all these interfaces almost magically.</p>
    <h3 id="_idParaDest-169" class="title">Pipes and error handling</h3>
    <p class="normal">The <code class="Code-In-Text--PACKT-">error</code> events are not <a id="_idIndexMarker478"/>propagated automatically through the pipeline when using <code class="Code-In-Text--PACKT-">pipe()</code>. Take, for example, this code fragment:</p>
    <pre class="programlisting code"><code class="hljs-code">stream1
  .pipe(stream2)
  .on('error', () =&gt; {})
</code></pre>
    <p class="normal">In the preceding pipeline, we will catch only the errors coming from <code class="Code-In-Text--PACKT-">stream2</code>, which is the stream that we attached the listener to. This means that, if we want to catch any error generated from <code class="Code-In-Text--PACKT-">stream1</code>, we have to attach another error listener directly to it, which will make our example look like this:</p>
    <pre class="programlisting code"><code class="hljs-code">stream1
  .on('error', () =&gt; {})
  .pipe(stream2)
  .on('error', () =&gt; {})
</code></pre>
    <p class="normal">This is clearly not ideal, especially when dealing with pipelines with a significant number of steps. To make this worse, in the event of an error, the failing stream is only unpiped from the pipeline. The failing stream is not properly destroyed, which might leave dangling resources (for example, file descriptors, connections, and so on) and leak memory. A more robust (yet inelegant) implementation of the preceding snippet might look like this:</p>
    <pre class="programlisting code"><code class="hljs-code">function handleError (err) {
  console.error(err)
  stream1.destroy()
  stream2.destroy()
}
stream1
  .on('error', handleError)
  .pipe(stream2)
  .on('error', handleError)
</code></pre>
    <p class="normal">In this example, we registered a handler for the <code class="Code-In-Text--PACKT-">error</code> event for both <code class="Code-In-Text--PACKT-">stream1</code> and <code class="Code-In-Text--PACKT-">stream2</code>. When an error happens, our <code class="Code-In-Text--PACKT-">handleError()</code> function is invoked, and we can log the error and destroy every stream in the pipeline. This allows us to ensure that all the allocated resources are properly released, and the error is handled gracefully.</p>
    <h3 id="_idParaDest-170" class="title">Better error handling with pipeline()</h3>
    <p class="normal">Handling errors<a id="_idIndexMarker479"/> manually in a pipeline is not just cumbersome, but also <a id="_idIndexMarker480"/>error-prone—definitely something we should avoid if we can!</p>
    <p class="normal">Luckily, the core <code class="Code-In-Text--PACKT-">stream</code> package offers us an excellent utility function that can make building pipelines a much safer and more enjoyable practice, which is the <code class="Code-In-Text--PACKT-">pipeline()</code> helper function.</p>
    <p class="normal">In a nutshell, you can use the <code class="Code-In-Text--PACKT-">pipeline()</code> function as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">pipeline(stream1, stream2, stream3, ... , cb)
</code></pre>
    <p class="normal">This helper pipes every stream passed in the arguments list to the next one. For each stream, it will also register a proper <code class="Code-In-Text--PACKT-">error</code> and <code class="Code-In-Text--PACKT-">close</code> listeners. This way, all the streams are properly destroyed when the pipeline completes successfully or when it's interrupted by an error. The last argument is an optional callback that will be called when the stream finishes. If it finishes because of an error, the callback will be invoked with the given error as the first argument.</p>
    <p class="normal">In order to build up some practice with this helper, let's write a simple command-line script that implements the following pipeline:</p>
    <ul>
      <li class="Bullet--PACKT-">Reads a Gzip data stream from the standard input</li>
      <li class="Bullet--PACKT-">Decompresses the data</li>
      <li class="Bullet--PACKT-">Makes all the text uppercase</li>
      <li class="Bullet--PACKT-">Gzips the resulting data</li>
      <li class="Bullet-End--PACKT-">Sends the data back to the standard output</li>
    </ul>
    <p class="normal">Let's call <a id="_idIndexMarker481"/>this<a id="_idIndexMarker482"/> module <code class="Code-In-Text--PACKT-">uppercasify-gzipped.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createGzip, createGunzip } from 'zlib'          // (1)
import { Transform, pipeline } from 'stream'
const uppercasify = new Transform({                      // (2)
  transform (chunk, enc, cb) {
    this.push(chunk.toString().toUpperCase())
    cb()
  }
})
pipeline(                                                // (3)
  process.stdin,
  createGunzip(),
  uppercasify,
  createGzip(),
  process.stdout,
  (err) =&gt; {                                             // (4)
    if (err) {
      console.error(err)
      process.exit(1)
    }
  }
)
</code></pre>
    <p class="normal">In this example:</p>
    <ol>
      <li class="numbered">We are importing the necessary dependencies from <code class="Code-In-Text--PACKT-">zlib</code> and the <code class="Code-In-Text--PACKT-">stream</code> modules.</li>
      <li class="numbered">We create a simple <code class="Code-In-Text--PACKT-">Transform</code> stream that makes every chunk uppercase.</li>
      <li class="numbered">We define our pipeline, where we list all the stream instances in order.</li>
      <li class="numbered">We add a callback to monitor the completion of the stream. In the event of an error, we print the error in the standard error interface, and we exit with error code <code class="Code-In-Text--PACKT-">1</code>.</li>
    </ol>
    <p class="normal">The pipeline will start automatically by consuming data from the standard input and producing data for the standard output.</p>
    <p class="normal">We could test our script with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">echo 'Hello World!' | gzip | node uppercasify-gzipped.js | gunzip
</code></pre>
    <p class="normal">This should produce the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">HELLO WORLD!
</code></pre>
    <p class="normal">If we try to<a id="_idIndexMarker483"/> remove the <code class="Code-In-Text--PACKT-">gzip</code> step from the preceding sequence of <a id="_idIndexMarker484"/>commands, our script will fail with an error similar to the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Error: unexpected end of file
    at Zlib.zlibOnError [as onerror] (zlib.js:180:17) {
  errno: -5,
  code: 'Z_BUF_ERROR'
}
</code></pre>
    <p class="normal">This error is raised by the stream created with the <code class="Code-In-Text--PACKT-">createGunzip()</code> function, which is responsible for decompressing the data. If the data is not actually gzipped, the decompression algorithm won't be able to process the data and it will fail. In such a case, <code class="Code-In-Text--PACKT-">pipeline()</code> will take care of cleaning up after the error and destroy all the streams in the pipeline.</p>
    <div><p class="Tip--PACKT-">The <code class="Code-In-Text--PACKT-">pipeline()</code> function can be easily <em class="italic">promisified</em> by using the <code class="Code-In-Text--PACKT-">promisify()</code> helper from the core <code class="Code-In-Text--PACKT-">util</code> module.</p>
    </div>
    <p class="normal">Now that we have built a solid understanding of Node.js streams, we are ready to move into some more involved stream patterns like control flow and advanced piping patterns.</p>
    <h1 id="_idParaDest-171" class="title">Asynchronous control flow patterns with streams</h1>
    <p class="normal">Going through the examples that we have presented so far, it should be clear that streams can be<a id="_idIndexMarker485"/> useful not only to handle I/O, but also as an elegant programming pattern that can be used to process any kind of data. But the advantage<a id="_idIndexMarker486"/>s do not end at its simple appearance; streams can also be leveraged to turn "asynchronous control flow" into "<strong class="keyword">flow control</strong>," as we will see in this section.</p>
    <h2 id="_idParaDest-172" class="title">Sequential execution</h2>
    <p class="normal">By default, streams will <a id="_idIndexMarker487"/>handle data in sequence. For example, the <code class="Code-In-Text--PACKT-">_transform()</code> function of a <code class="Code-In-Text--PACKT-">Transform</code> stream will never be invoked with the next chunk of data until the previous invocation completes by calling <code class="Code-In-Text--PACKT-">callback()</code>. This is an important property of streams, crucial for processing each chunk in the right order, but it can also be exploited to turn streams into an elegant alternative to the traditional control flow patterns.</p>
    <p class="normal">Some code is always better than too much explanation, so let's work on an example to demonstrate how we can use streams to execute asynchronous tasks in sequence. Let's create a function that concatenates a set of files received as input, making sure to honor the order in which they are provided. Let's create a new module called <code class="Code-In-Text--PACKT-">concat-files.js</code> and define its contents as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createWriteStream, createReadStream } from 'fs'
import { Readable, Transform } from 'stream'
export function concatFiles (dest, files) {
  return new Promise((resolve, reject) =&gt; {
    const destStream = createWriteStream(dest)
    Readable.from(files)                                    // (1)
      .pipe(new Transform({                                 // (2)
        objectMode: true,
        transform (filename, enc, done) {
          const src = createReadStream(filename)
          src.pipe(destStream, { end: false })
          src.on('error', done)
          src.on('end', done)                               // (3)
        }
      }))
      .on('error', reject)
      .on('finish', () =&gt; {                                 // (4)
        destStream.end()
        resolve()
      })
  })
}
</code></pre>
    <p class="normal">The preceding function implements a sequential iteration over the <code class="Code-In-Text--PACKT-">files</code> array by transforming it into a stream. The algorithm can be explained as follows:</p>
    <ol>
      <li class="numbered">First, we use <code class="Code-In-Text--PACKT-">Readable.from()</code> to create a <code class="Code-In-Text--PACKT-">Readable</code> stream from the <code class="Code-In-Text--PACKT-">files</code> array. This stream operates in object mode (the default setting for streams created with <code class="Code-In-Text--PACKT-">Readable.from()</code>) and it will emit filenames: every chunk is a string indicating the path to a file. The order of the chunks respects the order of the files in the <code class="Code-In-Text--PACKT-">files</code> array.</li>
      <li class="numbered">Next, we create a custom <code class="Code-In-Text--PACKT-">Transform</code> stream to handle each file in the sequence. Since we are receiving strings, we set the option <code class="Code-In-Text--PACKT-">objectMode</code> to <code class="Code-In-Text--PACKT-">true</code>. In our transformation<a id="_idIndexMarker488"/> logic, for each file, we create a <code class="Code-In-Text--PACKT-">Readable</code> stream to read the file content and pipe it into <code class="Code-In-Text--PACKT-">destStream</code> (a <code class="Code-In-Text--PACKT-">Writable</code> stream for the destination file). We make sure not to close <code class="Code-In-Text--PACKT-">destStream</code> after the source file finishes reading by specifying <code class="Code-In-Text--PACKT-">{ end: false }</code> in the <code class="Code-In-Text--PACKT-">pipe()</code> options.</li>
      <li class="numbered">When all the contents of the source file have been piped into <code class="Code-In-Text--PACKT-">destStream</code>, we invoke the <code class="Code-In-Text--PACKT-">done</code> function to communicate the completion of the current processing, which is necessary to trigger the processing of the next file.</li>
      <li class="numbered">When all the files have been processed, the <code class="Code-In-Text--PACKT-">finish</code> event is fired; we can finally end <code class="Code-In-Text--PACKT-">destStream</code> and invoke the <code class="Code-In-Text--PACKT-">cb()</code> function of <code class="Code-In-Text--PACKT-">concatFiles()</code>, which signals the completion of the whole operation.</li>
    </ol>
    <p class="normal">We can now try to use the little module we just created. Let's do that in a new file, called <code class="Code-In-Text--PACKT-">concat.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { concatFiles } from './concat-files.js'
async function main () {
  try {
    await concatFiles(process.argv[2], process.argv.slice(3))
  } catch (err) {
    console.error(err)
    process.exit(1)
  }
  console.log('All files concatenated successfully')
}
main()
</code></pre>
    <p class="normal">We can now run the preceding program by passing the destination file as the first command-line argument, followed by a list of files to concatenate; for example:</p>
    <pre class="programlisting con"><code class="hljs-con">node concat.js all-together.txt file1.txt file2.txt
</code></pre>
    <p class="normal">This should create a new file called <code class="Code-In-Text--PACKT-">all-together.txt</code> containing, in order, the contents of <code class="Code-In-Text--PACKT-">file1.txt</code> and <code class="Code-In-Text--PACKT-">file2.txt</code>.</p>
    <p class="normal">With the <code class="Code-In-Text--PACKT-">concatFiles()</code> function, we were able to obtain an asynchronous sequential iteration using only streams. This is an elegant and compact solution that enriches our toolbelt, along <a id="_idIndexMarker489"/>with the techniques we already explored in <em class="chapterRef">Chapter 4</em>, <em class="italic">Asynchronous Control Flow Patterns with Callbacks</em>, and <em class="chapterRef">Chapter 5</em>, <em class="italic">Asynchronous Control Flow Patterns with Promises and Async/Await</em>.</p>
    <div><p class="Information-Box--PACKT-"><strong class="screenText">Pattern</strong></p>
      <p class="Information-Box--PACKT-">Use a stream, or combination of streams, to easily iterate over a set of asynchronous tasks in sequence.</p>
    </div>
    <p class="normal">In the next section, we will discover how to use Node.js streams to implement unordered parallel task execution.</p>
    <h2 id="_idParaDest-173" class="title">Unordered parallel execution</h2>
    <p class="normal">We just saw that streams process each data chunk in sequence, but sometimes, this can be a bottleneck<a id="_idIndexMarker490"/> as we would not make the most of the concurrency of Node.js. If we have to execute a slow asynchronous operation for every data chunk, it can be advantageous to parallelize the execution and speed up the overall process. Of course, this pattern can only be applied if there is no relationship between each chunk of data, which might happen frequently for object streams, but very rarely for binary streams.</p>
    <div><p class="Information-Box--PACKT-"><strong class="screenText">Caution</strong></p>
      <p class="Information-Box--PACKT-">Unordered parallel streams cannot be used when the order in which the data is processed is important.</p>
    </div>
    <p class="normal">To parallelize the execution of a <code class="Code-In-Text--PACKT-">Transform</code> stream, we can apply the same patterns that we learned in <em class="chapterRef">Chapter 4</em>, <em class="italic">Asynchronous Control Flow Patterns with Callbacks</em>, but with some adaptations<a id="_idIndexMarker491"/> to get them working with streams. Let's see how this works.</p>
    <h3 id="_idParaDest-174" class="title">Implementing an unordered parallel stream</h3>
    <p class="normal">Let's immediately <a id="_idIndexMarker492"/>demonstrate how to implement an unordered parallel stream with an example. Let's create a module called <code class="Code-In-Text--PACKT-">parallel-stream.js</code> and define a generic <code class="Code-In-Text--PACKT-">Transform</code> stream that executes a given transform function in parallel:</p>
    <pre class="programlisting code"><code class="hljs-code">import { Transform } from 'stream'
export class ParallelStream extends Transform {
  constructor (userTransform, opts) {                    // (1)
    super({ objectMode: true, ...opts })
    this.userTransform = userTransform
    this.running = 0
    this.terminateCb = null
  }
  _transform (chunk, enc, done) {                        // (2)
    this.running++
    this.userTransform(
      chunk,
      enc,
      this.push.bind(this),
      this._onComplete.bind(this)
    )
    done()
  }
  _flush (done) {                                        // (3)
    if (this.running &gt; 0) {
      this.terminateCb = done
    } else {
      done()
    }
  }
  _onComplete (err) {                                    // (4)
    this.running--
    if (err) {
      return this.emit('error', err)
    }
    if (this.running === 0) {
      this.terminateCb &amp;&amp; this.terminateCb()
    }
  }
}
</code></pre>
    <p class="normal">Let's analyze this new class step by step:</p>
    <ol>
      <li class="numbered">As you can see, the constructor accepts a <code class="Code-In-Text--PACKT-">userTransform()</code> function, which is then saved as an instance variable. We invoke the parent constructor and for convenience, we enable the object mode by default.</li>
      <li class="numbered">Next, it is the <code class="Code-In-Text--PACKT-">_transform()</code> method's turn. In this method, we execute the <code class="Code-In-Text--PACKT-">userTransform()</code> function and then increment the count of running tasks. Finally, we notify the <code class="Code-In-Text--PACKT-">Transform</code> stream that the current transformation step is complete by invoking <code class="Code-In-Text--PACKT-">done()</code>. The trick for<a id="_idIndexMarker493"/> triggering the processing of another item in parallel is exactly this. We are not waiting for the <code class="Code-In-Text--PACKT-">userTransform()</code> function to complete before invoking <code class="Code-In-Text--PACKT-">done()</code>; instead, we do it immediately. On the other hand, we provide a special callback to <code class="Code-In-Text--PACKT-">userTransform()</code>, which is the <code class="Code-In-Text--PACKT-">this._onComplete()</code> method. This allows us to get notified when the execution of <code class="Code-In-Text--PACKT-">userTransform()</code> completes.</li>
      <li class="numbered">The <code class="Code-In-Text--PACKT-">_flush()</code> method is invoked just before the stream terminates, so if there are still tasks running, we can put the release of the <code class="Code-In-Text--PACKT-">finish</code> event on hold by not invoking the <code class="Code-In-Text--PACKT-">done()</code> callback immediately. Instead, we assign it to the <code class="Code-In-Text--PACKT-">this.terminateCallback</code> variable.</li>
      <li class="numbered">To understand how the stream is then properly terminated, we have to look into the <code class="Code-In-Text--PACKT-">_onComplete()</code> method. This last method is invoked every time an asynchronous task completes. It checks whether there are any more tasks running and, if there are none, it invokes the <code class="Code-In-Text--PACKT-">this.terminateCallback()</code> function, which will cause the stream to end, releasing the <code class="Code-In-Text--PACKT-">finish</code> event that was put on hold in the <code class="Code-In-Text--PACKT-">_flush()</code> method.</li>
    </ol>
    <p class="normal">The <code class="Code-In-Text--PACKT-">ParallelStream</code> class we just built allows us to easily create a <code class="Code-In-Text--PACKT-">Transform</code> stream that executes its tasks in parallel, but there is a caveat: it does not preserve the order of the items as they are received. In fact, asynchronous operations can complete and push data at any time, regardless of when they are started. We immediately understand that this property does<a id="_idIndexMarker494"/> not play well with binary streams where the order of data usually matters, but it can surely be useful with some types of object streams.</p>
    <h3 id="_idParaDest-175" class="title">Implementing a URL status monitoring application</h3>
    <p class="normal">Now, let's apply our <code class="Code-In-Text--PACKT-">ParallelStream</code> to a concrete example. Let's imagine that we want to build a simple<a id="_idIndexMarker495"/> service to monitor the status of a big list of URLs. Let's imagine all these URLs are contained in a single file and are newline separated.</p>
    <p class="normal">Streams can offer a very efficient and elegant solution to this problem, especially if we use our <code class="Code-In-Text--PACKT-">ParallelStream</code> class to parallelize the checking of the URLs.</p>
    <p class="normal">Let's build this simple application immediately in a new module called <code class="Code-In-Text--PACKT-">check-urls.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { pipeline } from 'stream'
import { createReadStream, createWriteStream } from 'fs'
import split from 'split'
import superagent from 'superagent'
import { ParallelStream } from './parallel-stream.js'
pipeline(
  createReadStream(process.argv[2]),                       // (1)
  split(),                                                 // (2)
  new ParallelStream(                                      // (3)
    async (url, enc, push, done) =&gt; {
      if (!url) {
        return done()
      }
      try {
        await superagent.head(url, { timeout: 5 * 1000 })
        push(`${url} is up\n`)
      } catch (err) {
        push(`${url} is down\n`)
      }
      done()
    }
  ),
  createWriteStream('results.txt'),                        // (4)
  (err) =&gt; {
    if (err) {
      console.error(err)
      process.exit(1)
    }
    console.log('All urls have been checked')
  }
)
</code></pre>
    <p class="normal">As we can see, with streams, our code looks very elegant and straightforward: everything is contained in a single streaming pipeline. Let's see how it works:</p>
    <ol>
      <li class="numbered">First, we create a <code class="Code-In-Text--PACKT-">Readable</code> stream from the file given as input.</li>
      <li class="numbered">We pipe the contents of the input file through <code class="Code-In-Text--PACKT-">split</code> (<a href="http://nodejsdp.link/split">nodejsdp.link/split</a>), a <code class="Code-In-Text--PACKT-">Transform</code> stream that ensures each line is emitted in a different chunk.</li>
      <li class="numbered">Then, it's time to use our <code class="Code-In-Text--PACKT-">ParallelStream</code> to check the URL. We do this by sending a <code class="Code-In-Text--PACKT-">head</code> request and waiting for a response. When the operation completes, we<a id="_idIndexMarker496"/> push the result down the stream.</li>
      <li class="numbered">Finally, all the results are piped into a file, <code class="Code-In-Text--PACKT-">results.txt</code>.</li>
    </ol>
    <p class="normal">Now, we can run the <code class="Code-In-Text--PACKT-">check-urls.js</code> module with a command such as this:</p>
    <pre class="programlisting con"><code class="hljs-con">node check-urls.js urls.txt
</code></pre>
    <p class="normal">Here, the file <code class="Code-In-Text--PACKT-">urls.txt</code> contains a list of URLs (one per line); for example:</p>
    <pre class="programlisting code"><code class="hljs-code">https://mario.fyi
https://loige.co
http://thiswillbedownforsure.com
</code></pre>
    <p class="normal">When the command finishes running, we will see that a file, <code class="Code-In-Text--PACKT-">results.txt</code>, was created. This contains the results of the operation; for example:</p>
    <pre class="programlisting code"><code class="hljs-code">http://thiswillbedownforsure.com is down
https://mario.fyi is up
https://loige.co is up
</code></pre>
    <p class="normal">There is a good probability that the order in which the results are written is different from the order in which the URLs were specified in the input file. This is clear evidence that our stream executes its tasks in parallel, and it does not enforce any order between the various data <a id="_idIndexMarker497"/>chunks in the stream.</p>
    <div><p class="Information-Box--PACKT-">For the sake of curiosity, we might want to try replacing <code class="Code-In-Text--PACKT-">ParallelStream</code> with a normal <code class="Code-In-Text--PACKT-">Transform</code> stream and compare the behavior and performance of the two (you might want to do this as an exercise). Using <code class="Code-In-Text--PACKT-">Transform</code> directly is way slower, because each URL is checked in sequence, but on the other hand the order of the results in the file <code class="Code-In-Text--PACKT-">results.txt</code> is preserved.</p>
    </div>
    <p class="normal">In the next section, we will see how to extend this pattern to limit the number of parallel tasks running at a given time.</p>
    <h2 id="_idParaDest-176" class="title">Unordered limited parallel execution</h2>
    <p class="normal">If we try to run the <code class="Code-In-Text--PACKT-">check-urls.js</code> application against a file that contains thousands or millions of URLs, we will surely run into issues. Our application will create an uncontrolled<a id="_idIndexMarker498"/> number of connections all at once, sending a considerable amount of data in parallel, and potentially undermining the stability of the application and the availability of the entire system. As we already know, the solution to keep the load and resource usage under control is to limit the concurrency of the parallel tasks.</p>
    <p class="normal">Let's see how this works with streams by creating a <code class="Code-In-Text--PACKT-">limited-parallel-stream.js</code> module, which is an adaptation of <code class="Code-In-Text--PACKT-">parallel-stream.js</code> we created in the previous section.</p>
    <p class="normal">Let's see what it looks like, starting from its constructor (we will highlight the changed parts):</p>
    <pre class="programlisting code"><code class="hljs-code">export class <strong class="hljs-title-slc">LimitedParallelStream</strong> extends Transform {
  constructor (<strong class="hljs-slc">concurrency</strong>, userTransform, opts) {
    super({ ...opts, objectMode: true })
    <strong class="hljs-built_in-slc">this</strong><strong class="hljs-slc">.concurrency = concurrency</strong>
    this.userTransform = userTransform
    this.running = 0
    <strong class="hljs-built_in-slc">this</strong><strong class="hljs-slc">.continueCb = </strong><strong class="hljs-literal-slc">null</strong>
    <strong class="hljs-built_in-slc">this</strong><strong class="hljs-slc">.terminateCb = </strong><strong class="hljs-literal-slc">null</strong>
  }
// ...
</code></pre>
    <p class="normal">We need a <code class="Code-In-Text--PACKT-">concurrency</code> limit to be taken as input, and this time, we are going to save two callbacks, one for any pending <code class="Code-In-Text--PACKT-">_transform</code> method (<code class="Code-In-Text--PACKT-">continueCb</code>—more on this next) and another one for the callback of the <code class="Code-In-Text--PACKT-">_flush</code> method (<code class="Code-In-Text--PACKT-">terminateCb</code>).</p>
    <p class="normal">Next is the <code class="Code-In-Text--PACKT-">_transform()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">  _transform (chunk, enc, done) {
    this.running++
    this.userTransform(
      chunk,
      enc,
      this.push.bind(this),
      this._onComplete.bind(this)
    )
    <strong class="hljs-keyword-slc">if</strong><strong class="hljs-slc"> (</strong><strong class="hljs-built_in-slc">this</strong><strong class="hljs-slc">.running &lt; </strong><strong class="hljs-built_in-slc">this</strong><strong class="hljs-slc">.concurrency) {</strong>
      <strong class="hljs-slc">done()</strong>
    <strong class="hljs-slc">} </strong><strong class="hljs-keyword-slc">else</strong><strong class="hljs-slc"> {</strong>
      <strong class="hljs-built_in-slc">this</strong><strong class="hljs-slc">.continueCb = done</strong>
    <strong class="hljs-slc">}</strong>
  }
</code></pre>
    <p class="normal">This time, in the <code class="Code-In-Text--PACKT-">_transform()</code> method, we have to check whether we have any free execution slots before we can invoke <code class="Code-In-Text--PACKT-">done()</code> and trigger the processing of the next item. If we have<a id="_idIndexMarker499"/> already reached the maximum number of concurrently running streams, we can simply save the <code class="Code-In-Text--PACKT-">done()</code> callback in the <code class="Code-In-Text--PACKT-">continueCb</code> variable so that it can be invoked as soon as a task finishes.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">_flush()</code> method remains exactly the same as in the <code class="Code-In-Text--PACKT-">ParallelStream</code> class, so let's move directly to implementing the <code class="Code-In-Text--PACKT-">_onComplete()</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">  _onComplete (err) {
    this.running--
    if (err) {
      return this.emit('error', err)
    }
    <strong class="hljs-keyword-slc">const</strong><strong class="hljs-slc"> tmpCb = </strong><strong class="hljs-built_in-slc">this</strong><strong class="hljs-slc">.continueCb</strong>
    <strong class="hljs-built_in-slc">this</strong><strong class="hljs-slc">.continueCb = </strong><strong class="hljs-literal-slc">null</strong>
    <strong class="hljs-slc">tmpCb &amp;&amp; tmpCb()</strong>
    if (this.running === 0) {
      this.terminateCb &amp;&amp; this.terminateCb()
    }
  }
</code></pre>
    <p class="normal">Every time a task completes, we invoke any saved <code class="Code-In-Text--PACKT-">continueCb()</code> that will cause the stream to unblock, triggering the processing of the next item.</p>
    <p class="normal">That's it for<a id="_idIndexMarker500"/> the <code class="Code-In-Text--PACKT-">LimitedParallelStream</code> class. We can now use it in the <code class="Code-In-Text--PACKT-">check-urls.js</code> module in place of <code class="Code-In-Text--PACKT-">ParallelStream</code> and have the concurrency of our tasks limited to the value that we set.</p>
    <h2 id="_idParaDest-177" class="title">Ordered parallel execution</h2>
    <p class="normal">The parallel streams that we created previously may shuffle the order of the emitted data, but there are<a id="_idIndexMarker501"/> situations where this is not acceptable. Sometimes, in fact, it is necessary to have each chunk emitted in the same order in which it was received. However, not all hope is lost: we can still run the transform function in parallel; all we have to do is to sort the data emitted by each task so that it follows the same order in which the data was received.</p>
    <p class="normal">This technique involves the use of a buffer to reorder the chunks while they are emitted by each running task. For brevity, we are not going to provide an implementation of such a stream, as it's quite<a id="_idIndexMarker502"/> verbose for the scope of this book. What we are going to do instead is reuse one of the available packages on npm built for this specific <a id="_idIndexMarker503"/>purpose, that is, <code class="Code-In-Text--PACKT-">parallel-transform</code> (<a href="http://nodejsdp.link/parallel-transform">nodejsdp.link/parallel-transform</a>).</p>
    <p class="normal">We can quickly check the behavior of an ordered parallel execution by modifying our existing <code class="Code-In-Text--PACKT-">check-urls</code> module. Let's say that we want our results to be written in the same order as the URLs in the input file, while executing our checks in parallel. We can do this using <code class="Code-In-Text--PACKT-">parallel-transform</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">//...
import parallelTransform from 'parallel-transform'
pipeline(
  createReadStream(process.argv[2]),
  split(),
  parallelTransform(4, async function (url, done) {
    if (!url) {
      return done()
    }
    console.log(url)
    try {
      await request.head(url, { timeout: 5 * 1000 })
      this.push(`${url} is up\n`)
    } catch (err) {
      this.push(`${url} is down\n`)
    }
    done()
  }),
  createWriteStream('results.txt'),
  (err) =&gt; {
    if (err) {
      console.error(err)
      process.exit(1)
    }
    console.log('All urls have been checked')
  }
)
</code></pre>
    <p class="normal">In the example here, <code class="Code-In-Text--PACKT-">parallelTransform()</code> creates a <code class="Code-In-Text--PACKT-">Transform</code> stream in object mode that executes our transformation logic with a maximum concurrency of 4. If we try to run this new version of <code class="Code-In-Text--PACKT-">check-urls.js</code>, we will now see that the <code class="Code-In-Text--PACKT-">results.txt</code> file lists the results in the<a id="_idIndexMarker504"/> same order as the URLs appear in the input file. It is important to see that, even though the order of the output is the same as the input, the asynchronous tasks still run in parallel and can possibly complete in any order.</p>
    <div><p class="Information-Box--PACKT-">When using the ordered parallel execution pattern, we need to be aware of slow items blocking the pipeline or growing the memory indefinitely. In fact, if there is an item that requires a very long time to complete, depending on the implementation of the pattern, it will either cause the buffer containing the pending ordered results to grow indefinitely or the entire processing to block until the slow item completes. With the first strategy, we are optimizing for performance, while with the second, we get predictable memory usage. <code class="Code-In-Text--PACKT-">parallel-transform</code> implementation opts for predictable memory utilization and maintains an internal buffer that will not grow more than the specified maximum concurrency.</p>
    </div>
    <p class="normal">With this, we conclude our analysis of the asynchronous control flow patterns with streams. Next, we are going to focus on some piping patterns.</p>
    <h1 id="_idParaDest-178" class="title">Piping patterns</h1>
    <p class="normal">As in real-life plumbing, Node.js streams can also be piped together by following different <a id="_idIndexMarker505"/>patterns. We can, in fact, merge the flow of two different streams into one, split the flow of one stream into two or more pipes, or redirect the flow based on a condition. In this section, we are going to explore the most important plumbing patterns that can be applied to Node.js streams.</p>
    <h2 id="_idParaDest-179" class="title">Combining streams</h2>
    <p class="normal">In this chapter, we have stressed the fact that streams provide a simple infrastructure to modularize and reuse<a id="_idIndexMarker506"/> our code, but there is one last piece missing in this puzzle: what if we want to modularize and reuse an entire pipeline? What if we want to combine multiple streams so that they look like one from the outside? The following figure shows what this means:</p>
    <figure class="mediaobject"><img src="img/B15729_06_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.6: Combining streams</p>
    <p class="normal">From <em class="italic">Figure 6.6</em>, we should already get a hint of how this works:</p>
    <ul>
      <li class="Bullet--PACKT-">When we write into the combined stream, we are actually writing into the first stream of the pipeline.</li>
      <li class="Bullet-End--PACKT-">When we read from the combined stream, we are actually reading from the last stream of the pipeline.</li>
    </ul>
    <p class="normal">A combined stream is usually a <code class="Code-In-Text--PACKT-">Duplex</code> stream, which is built by connecting the first stream to its <code class="Code-In-Text--PACKT-">Writable</code> side and the last stream to its <code class="Code-In-Text--PACKT-">Readable</code> side.</p>
    <div><p class="Information-Box--PACKT-">To create a <code class="Code-In-Text--PACKT-">Duplex</code> stream out of two different streams, one <code class="Code-In-Text--PACKT-">Writable</code> and one <code class="Code-In-Text--PACKT-">Readable</code>, we can <a id="_idIndexMarker507"/>use an npm<a id="_idIndexMarker508"/> module such as <code class="Code-In-Text--PACKT-">duplexer2</code> (<a href="http://nodejsdp.link/duplexer2">nodejsdp.link/duplexer2</a>) or <code class="Code-In-Text--PACKT-">duplexify</code> (<a href="http://nodejsdp.link/duplexify">nodejsdp.link/duplexify</a>).</p>
    </div>
    <p class="normal">But that's not enough. In fact, another important characteristic of a combined stream is that it has to capture and propagate all the errors that are emitted from any stream inside the pipeline. As we already mentioned, any error event is not automatically propagated down the pipeline when we use <code class="Code-In-Text--PACKT-">pipe()</code>, and we should explicitly attach an error listener to each stream. We saw that we could use the <code class="Code-In-Text--PACKT-">pipeline()</code> helper function to overcome the limitations of <code class="Code-In-Text--PACKT-">pipe()</code> with error management, but the issue with both <code class="Code-In-Text--PACKT-">pipe()</code> and the <code class="Code-In-Text--PACKT-">pipeline()</code> helper is that the two functions return only the last stream of the pipeline, so we only get the (last) <code class="Code-In-Text--PACKT-">Readable</code> component and not the (first) <code class="Code-In-Text--PACKT-">Writable</code> <a id="_idIndexMarker509"/>component. </p>
    <p class="normal">We can verify this very easily with the following snippet of code:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createReadStream, createWriteStream } from 'fs'
import { Transform, pipeline } from 'stream'
import { strict as assert } from 'assert'
const streamA = createReadStream('package.json')
const streamB = new Transform({
  transform (chunk, enc, done) {
    this.push(chunk.toString().toUpperCase())
    done()
  }
})
const streamC = createWriteStream('package-uppercase.json')
const pipelineReturn = pipeline(
  streamA,
  streamB,
  streamC,
  () =&gt; {
    // handle errors here
  })
assert.strictEqual(streamC, pipelineReturn) // valid
const pipeReturn = streamA.pipe(streamB).pipe(streamC)
assert.strictEqual(streamC, pipeReturn) // valid
</code></pre>
    <p class="normal">From the preceding code, it should be clear that with just <code class="Code-In-Text--PACKT-">pipe()</code> or <code class="Code-In-Text--PACKT-">pipeline()</code>, it would not be trivial to construct a combined stream.</p>
    <p class="normal">To recap, a combined stream has two major advantages:</p>
    <ul>
      <li class="Bullet--PACKT-">We can redistribute it as a black box by hiding its internal pipeline.</li>
      <li class="Bullet-End--PACKT-">We have simplified error management, as we don't have to attach an error listener to each stream in the pipeline, but just to the combined stream itself.</li>
    </ul>
    <p class="normal">Combining streams is a <a id="_idIndexMarker510"/>pretty common practice, so if we don't have any particular need, we might just want to reuse an existing library such as <code class="Code-In-Text--PACKT-">pumpify</code> (<a href="http://nodejsdp.link/pumpify">nodejsdp.link/pumpify</a>).</p>
    <p class="normal">This library offers a very simple interface. In fact, all you have to do to obtain a combined stream is to call <code class="Code-In-Text--PACKT-">pumpify()</code>, passing all the streams you want in your pipeline. This is very similar to the signature of <code class="Code-In-Text--PACKT-">pipeline()</code>, except that there's no callback:</p>
    <pre class="programlisting code"><code class="hljs-code">const combinedStream = pumpify(streamA, streamB, streamC)
</code></pre>
    <p class="normal">When we do<a id="_idIndexMarker511"/> something like this, <code class="Code-In-Text--PACKT-">pumpify</code> will create a pipeline out of our streams, return a new combined stream that abstracts away the complexity of our pipeline, and provide the advantages discussed previously.</p>
    <div><p class="Tip--PACKT-">If you are curious to see what it takes to build a library like <code class="Code-In-Text--PACKT-">pumpify</code>, you should check its source code on GitHub (<a href="http://nodejsdp.link/pumpify-gh">nodejsdp.link/pumpify-gh</a>). One interesting fact is that, internally, <code class="Code-In-Text--PACKT-">pumpify</code> uses <code class="Code-In-Text--PACKT-">pump</code> (<a href="http://nodejsdp.link/pump">nodejsdp.link/pump</a>), a module that was born before the Node.js <code class="Code-In-Text--PACKT-">pipeline()</code> helper. <code class="Code-In-Text--PACKT-">pump</code> is effectively the module that inspired the development of <code class="Code-In-Text--PACKT-">pipeline()</code>. If you compare their source code, you will find out that, unsurprisingly, the two modules have a lot in common.</p>
    </div>
    <h3 id="_idParaDest-180" class="title">Implementing a combined stream</h3>
    <p class="normal">To illustrate a simple<a id="_idIndexMarker512"/> example of combining streams, let's consider the case of the following two <code class="Code-In-Text--PACKT-">Transform</code> streams:</p>
    <ul>
      <li class="Bullet--PACKT-">One that both compresses and encrypts the data</li>
      <li class="Bullet-End--PACKT-">One that both decrypts and decompresses the data</li>
    </ul>
    <p class="normal">Using a library such as <code class="Code-In-Text--PACKT-">pumpify</code>, we can easily build these streams (in a file called <code class="Code-In-Text--PACKT-">combined-streams.js</code>) by combining some of the streams that we already have available from the core libraries:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createGzip, createGunzip } from 'zlib'
import {
  createCipheriv,
  createDecipheriv,
  scryptSync
} from 'crypto'
import pumpify from 'pumpify'
function createKey (password) {
  return scryptSync(password, 'salt', 24)
}
export function createCompressAndEncrypt (password, iv) {
  const key = createKey(password)
  const combinedStream = pumpify(
    createGzip(),
    createCipheriv('aes192', key, iv)
  )
  combinedStream.iv = iv
  return combinedStream
}
export function createDecryptAndDecompress (password, iv) {
  const key = createKey(password)
  return pumpify(
    createDecipheriv('aes192', key, iv),
    createGunzip()
  )
}
</code></pre>
    <p class="normal">We can now use these<a id="_idIndexMarker513"/> combined streams as if they were black boxes, for example, to create a small application that archives a file by compressing and encrypting it. Let's do that in a new module named <code class="Code-In-Text--PACKT-">archive.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createReadStream, createWriteStream } from 'fs'
import { pipeline } from 'stream'
import { randomBytes } from 'crypto'
import { createCompressAndEncrypt } from './combined-streams.js'
const [,, password, source] = process.argv
const iv = randomBytes(16)
const destination = `${source}.gz.enc`
pipeline(
  createReadStream(source),
  createCompressAndEncrypt(password, iv),
  createWriteStream(destination),
  (err) =&gt; {
    if (err) {
      console.error(err)
      process.exit(1)
    }
    console.log(`${destination} created with iv: ${iv.toString('hex')}`)
  }
)
</code></pre>
    <p class="normal">Note how we don't have to worry about how many steps there are within <code class="Code-In-Text--PACKT-">archiveFile</code>. In fact, we just<a id="_idIndexMarker514"/> treat it as a single stream within our current pipeline. This makes our combined stream easily reusable in other contexts.</p>
    <p class="normal">Now, to run the <code class="Code-In-Text--PACKT-">archive</code> module, simply specify a password and a file in the command-line arguments:</p>
    <pre class="programlisting con"><code class="hljs-con">node archive.js mypassword /path/to/a/file.txt
</code></pre>
    <p class="normal">This command will create a file called <code class="Code-In-Text--PACKT-">/path/to/a/file.txt.gz.enc</code> and it will print the generated initialization vector to the console.</p>
    <p class="normal">Now, as an exercise, you could use the <code class="Code-In-Text--PACKT-">createDecryptAndDecompress()</code> function to create a similar script that takes a password, an initialization vector, and an archived file and unarchives it.</p>
    <div><p class="Information-Box--PACKT-">In real-life applications, it is generally preferable to include the initialization vector as part of the encrypted data, rather than requiring the user to pass it around. One way to implement this is by having the first 16 bytes emitted by the archive stream to be representing the initialization vector. The unarchive utility would need to be updated accordingly to consume the first 16 bytes before starting to process the data in a streaming fashion. This approach would add some additional complexity, which is outside the scope of this example, therefore we opted for a simpler solution. Once you feel comfortable with streams, we encourage you to try to implement as an exercise a solution where the initialization vector doesn't have to be passed around by the user.</p>
    </div>
    <p class="normal">With this example, we have clearly demonstrated how important it is to combine streams. From one side, it allows us to create reusable compositions of streams, and from the other, it simplifies the error management of a pipeline.</p>
    <h2 id="_idParaDest-181" class="title">Forking streams</h2>
    <p class="normal">We can perform a <em class="italic">fork</em> of a stream<a id="_idIndexMarker515"/> by piping a single <code class="Code-In-Text--PACKT-">Readable</code> stream into multiple <code class="Code-In-Text--PACKT-">Writable</code> streams. This is useful when we want to send the same data to different destinations; for example, two different sockets or two different files. It can also be used when we want to perform different transformations on the same data, or when we want to split the data based on some criteria. If you are familiar with the Unix command <code class="Code-In-Text--PACKT-">tee</code> (<a href="http://nodejsdp.link/tee">nodejsdp.link/tee</a>), this is exactly the same concept applied to Node.js streams!</p>
    <p class="normal"><em class="italic">Figure 6.7</em> gives us a graphical representation of this pattern:</p>
    <figure class="mediaobject"><img src="img/B15729_06_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.7: Forking a stream</p>
    <p class="normal">Forking a stream in <a id="_idIndexMarker516"/>Node.js is quite easy, but there are a few caveats to keep in mind. Let's start by discussing this pattern with an example. It will be easier to appreciate the caveats of this pattern once we have an example at hand.</p>
    <h3 id="_idParaDest-182" class="title">Implementing a multiple checksum generator</h3>
    <p class="normal">Let's create a small <a id="_idIndexMarker517"/>utility that outputs both the <code class="Code-In-Text--PACKT-">sha1</code> and <code class="Code-In-Text--PACKT-">md5</code> hashes of a given file. Let's call this new module <code class="Code-In-Text--PACKT-">generate-hashes.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createReadStream, createWriteStream } from 'fs'
import { createHash } from 'crypto'
const filename = process.argv[2]
const sha1Stream = createHash('sha1').setEncoding('hex')
const md5Stream = createHash('md5').setEncoding('hex')
const inputStream = createReadStream(filename)
inputStream
  .pipe(sha1Stream)
  .pipe(createWriteStream(`${filename}.sha1`))
inputStream
  .pipe(md5Stream)
  .pipe(createWriteStream(`${filename}.md5`))
</code></pre>
    <p class="normal">Very simple, right? The <code class="Code-In-Text--PACKT-">inputStream</code> variable is piped into <code class="Code-In-Text--PACKT-">sha1Stream</code> on one side and <code class="Code-In-Text--PACKT-">md5Stream</code> on the other. There are a few things to note that happen behind the scenes:</p>
    <ul>
      <li class="Bullet--PACKT-">Both <code class="Code-In-Text--PACKT-">md5Stream</code> and <code class="Code-In-Text--PACKT-">sha1Stream</code> will be ended automatically when <code class="Code-In-Text--PACKT-">inputStream</code> ends, unless we specify <code class="Code-In-Text--PACKT-">{ end: false }</code> as an option when invoking <code class="Code-In-Text--PACKT-">pipe()</code>.</li>
      <li class="Bullet--PACKT-">The two forks of the stream will receive the same data chunks, so we must be very careful when performing side-effect operations on the data, as that would affect every stream that we are sending data to.</li>
      <li class="Bullet--PACKT-">Backpressure will work out of the box; the flow coming from <code class="Code-In-Text--PACKT-">inputStream</code> will go as fast as the slowest branch of the fork. In other words, if one destination pauses the source stream to handle backpressure for a long time, all the other destinations <a id="_idIndexMarker518"/>will be waiting as well. Also, one destination blocking indefinitely will block the entire pipeline!</li>
      <li class="Bullet--PACKT-">If we pipe to an additional stream after we've started consuming the data at source (async piping), the new stream will only receive new chunks of data. In those cases, we can use a <code class="Code-In-Text--PACKT-">PassThrough</code> instance as a placeholder to collect all the data from the moment we start consuming the stream. Then, the <code class="Code-In-Text--PACKT-">PassThrough</code> stream can be read at any future time without the risk of losing any data. Just be aware that this approach might generate backpressure and block the entire pipeline, as discussed in the previous point.</li>
    </ul>
    <h2 id="_idParaDest-183" class="title">Merging streams</h2>
    <p class="normal">Merging is the<a id="_idIndexMarker519"/> opposite operation to forking and involves piping a set of <code class="Code-In-Text--PACKT-">Readable</code> streams into a single <code class="Code-In-Text--PACKT-">Writable</code> stream, as shown in <em class="italic">Figure 6.8</em>:</p>
    <figure class="mediaobject"><img src="img/B15729_06_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.8: Merging streams</p>
    <p class="normal">Merging multiple streams into one is, in general, a simple operation; however, we have to pay attention to the way we handle the <code class="Code-In-Text--PACKT-">end</code> event, as piping using the default options (whereby <code class="Code-In-Text--PACKT-">{ end: true }</code>) causes the destination stream to end as soon as one of the sources ends. This can often<a id="_idIndexMarker520"/> lead to an error, as the other active sources continue to write to an already terminated stream. </p>
    <p class="normal">The solution to this problem is to use the option <code class="Code-In-Text--PACKT-">{ end: false }</code> when piping multiple sources to a single destination and then invoke <code class="Code-In-Text--PACKT-">end()</code> on the destination only when all the sources have completed reading.</p>
    <h3 id="_idParaDest-184" class="title">Merging text files</h3>
    <p class="normal">To make a simple example, let's implement a small program that takes an output path and an arbitrary<a id="_idIndexMarker521"/> number of text files, and then merges the lines of every file into the destination file. Our new module is going to be called <code class="Code-In-Text--PACKT-">merge-lines.js</code>. Let's define its contents, starting from some initialization steps:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createReadStream, createWriteStream } from 'fs'
import split from 'split'
const dest = process.argv[2]
const sources = process.argv.slice(3)
</code></pre>
    <p class="normal">In the preceding code, we are just loading all the dependencies and initializing the variables that contain the name of the destination (<code class="Code-In-Text--PACKT-">dest</code>) file and all the source files (<code class="Code-In-Text--PACKT-">sources</code>).</p>
    <p class="normal">Next, we will create the destination stream:</p>
    <pre class="programlisting code"><code class="hljs-code">const destStream = createWriteStream(dest)
</code></pre>
    <p class="normal">Now, it's time to initialize the source streams:</p>
    <pre class="programlisting code"><code class="hljs-code">let endCount = 0
for (const source of sources) {
  const sourceStream = createReadStream(source, { highWaterMark: 16 })
  sourceStream.on('end', () =&gt; {
    if (++endCount === sources.length) {
      destStream.end()
      console.log(`${dest} created`)
    }
  })
  sourceStream
    .pipe(split((line) =&gt; line + '\n'))
    <strong class="hljs-slc">.pipe(destStream, { </strong><strong class="hljs-attr-slc">end</strong><strong class="hljs-slc">: </strong><strong class="hljs-literal-slc">false</strong><strong class="hljs-slc"> })</strong>
}
</code></pre>
    <p class="normal">In the preceding code, we created a <code class="Code-In-Text--PACKT-">Readable</code> stream for every source file. Then, for each source stream, we attached an <code class="Code-In-Text--PACKT-">end</code> listener, which will terminate the destination stream only when all the files have been read completely. Finally, we piped every source stream to <code class="Code-In-Text--PACKT-">split()</code>, a <code class="Code-In-Text--PACKT-">Transform</code> stream that makes sure that we produce a chunk for every line of text, and<a id="_idIndexMarker522"/> finally, we piped the results to our destination stream. This is when the real merge happens. We are piping multiple source streams into one single destination.</p>
    <p class="normal">We can now execute this code with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">node merge-lines.js &lt;destination&gt; &lt;source1&gt; &lt;source2&gt; &lt;source3&gt; ...
</code></pre>
    <p class="normal">If you run this code with large enough files, you will notice that the destination file will contain lines that are randomly intermingled from all the source files (a low <code class="Code-In-Text--PACKT-">highWaterMark</code> of 16 bytes makes this property even more apparent). This kind of behavior can be acceptable in some types of object streams and some text streams split by line (as in our current example), but it is often undesirable when dealing with most binary streams.</p>
    <p class="normal">There is one variation of the pattern that allows us to merge streams in order; it consists of consuming the source streams one after the other. When the previous one ends, the next one starts emitting chunks (it is like <em class="italic">concatenating</em> the output of all the sources). As always, on npm, we can find some packages that also deal with this situation. One of them is <code class="Code-In-Text--PACKT-">multistream</code> (<a href="https://npmjs.org/package/multistream">https://npmjs.org/package/multistream</a>).</p>
    <h2 id="_idParaDest-185" class="title">Multiplexing and demultiplexing</h2>
    <p class="normal">There is a <a id="_idIndexMarker523"/>particular variation of the merge stream pattern in which we don't really <a id="_idIndexMarker524"/>want to just join multiple streams together but, instead, use a shared channel to deliver the data of a set of streams. This is a conceptually different operation because the source streams remain logically separated inside the shared channel, which allows us to split the stream again once the data reaches the other end of the shared channel. <em class="italic">Figure 6.9</em> clarifies this situation:</p>
    <figure class="mediaobject"><img src="img/B15729_06_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.9: Multiplexing and demultiplexing streams</p>
    <p class="normal">The operation of combining multiple streams (in this case, also known as channels) to allow transmission over<a id="_idIndexMarker525"/> a single stream is called <strong class="keyword">multiplexing</strong>, while the opposite operation, namely reconstructing the original streams from the data received from a<a id="_idIndexMarker526"/> shared stream, is called <strong class="keyword">demultiplexing</strong>. The <em class="italic">devices</em> that perform these operations are called <strong class="keyword">multiplexer</strong> (or <strong class="keyword">mux</strong>) and <strong class="keyword">demultiplexer</strong> (or <strong class="keyword">demux</strong>), respectively. This is a widely studied area in <a id="_idIndexMarker527"/>computer science and telecommunications in general, as it is one<a id="_idIndexMarker528"/> of the foundations of almost any type of communication media such as telephony, radio, TV, and, of course, the Internet itself. For the scope of this book, we will not go too far with the explanations, as this is a vast topic.</p>
    <p class="normal">What we want to demonstrate in this section is how it's possible to use a shared Node.js stream to transmit multiple logically separated streams that are then separated again at the other end of the shared stream.</p>
    <h3 id="_idParaDest-186" class="title">Building a remote logger</h3>
    <p class="normal">Let's use an example to drive our discussion. We want a small program that starts a child process <a id="_idIndexMarker529"/>and redirects both its standard output and standard error to a remote server, which, in turn, saves the two streams in two separate files. So, in this case, the shared medium is a TCP connection, while the two channels to be multiplexed are the <code class="Code-In-Text--PACKT-">stdout</code> and <code class="Code-In-Text--PACKT-">stderr</code> of a child process. We will leverage a technique called <strong class="keyword">packet switching</strong>, the same <a id="_idIndexMarker530"/>technique that is used by protocols such as IP, TCP, and UDP. Packet switching involves wrapping the data into <em class="italic">packets</em>, allowing us to specify various meta information that's useful for multiplexing, routing, controlling the flow, checking for corrupted data, and so on. The protocol that we are implementing in our example is very minimalist. We wrap our data into simple packets, as illustrated in <em class="italic">Figure 6.10</em>:</p>
    <figure class="mediaobject"><img src="img/B15729_06_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.10: Bytes structure of the data packet for our remote logger</p>
    <p class="normal">As shown in <em class="italic">Figure 6.10</em>, the packet contains the actual data, but also a header (<em class="italic">Channel ID</em> + <em class="italic">Data length</em>), which will make it possible to differentiate the data of each stream and enable the demultiplexer to<a id="_idIndexMarker531"/> route the packet to the right channel.</p>
    <h4 class="title">Client side – multiplexing</h4>
    <p class="normal">Let's start to build our application from the client side. With a lot of creativity, we will call the<a id="_idIndexMarker532"/> module <code class="Code-In-Text--PACKT-">client.js.</code> This represents the part of the application that is responsible for starting a child process and multiplexing its streams.</p>
    <p class="normal">So, let's start by defining the module. First, we need some dependencies:</p>
    <pre class="programlisting code"><code class="hljs-code">import { fork } from 'child_process'
import { connect } from 'net'
</code></pre>
    <p class="normal">Now, let's implement a function that performs the multiplexing of a list of sources:</p>
    <pre class="programlisting code"><code class="hljs-code">function multiplexChannels (sources, destination) {
  let openChannels = sources.length
  for (let i = 0; i &lt; sources.length; i++) {
    sources[i]
      .on('readable', function () {                           // (1)
        let chunk
        while ((chunk = this.read()) !== null) {
          const outBuff = Buffer.alloc(1 + 4 + chunk.length)  // (2)
          outBuff.writeUInt8(i, 0)
          outBuff.writeUInt32BE(chunk.length, 1)
          chunk.copy(outBuff, 5)
          console.log(`Sending packet to channel: ${i}`)
          destination.write(outBuff)                          // (3)
        }
      })
      .on('end', () =&gt; {                                      // (4)
        if (--openChannels === 0) {
          destination.end()
        }
      })
  }
}
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">multiplexChannels()</code> function takes in, as input, the source streams to be multiplexed and the destination channel, and then it performs the following steps:</p>
    <ol>
      <li class="numbered">For each source stream, it registers a listener for the <code class="Code-In-Text--PACKT-">readable</code> event, where we read the data from the stream using the non-flowing mode.</li>
      <li class="numbered">When a chunk is read, we wrap it into a packet that contains, in order, 1 byte (<code class="Code-In-Text--PACKT-">UInt8</code>) for the channel ID, 4 bytes (<code class="Code-In-Text--PACKT-">UInt32BE</code>) for the packet size, and then the actual data.</li>
      <li class="numbered">When the packet is ready, we write it into the destination stream.</li>
      <li class="numbered">Finally, we register a listener for the <code class="Code-In-Text--PACKT-">end</code> event so that we can terminate the destination stream<a id="_idIndexMarker533"/> when all the source streams have ended.</li>
    </ol>
    <div><p class="Information-Box--PACKT-">Our protocol is to be able to multiplex up to 256 different source streams because we only have 1 byte to identify the channel.</p>
    </div>
    <p class="normal">Now, the last part of our client becomes very easy:</p>
    <pre class="programlisting code"><code class="hljs-code">const socket = connect(3000, () =&gt; {                       // (1)
  const child = fork(                                      // (2)
    process.argv[2],
    process.argv.slice(3),
    { silent: true }
  )
  multiplexChannels([child.stdout, child.stderr], socket)  // (3)
})
</code></pre>
    <p class="normal">In this last code fragment, we perform the following operations:</p>
    <ol>
      <li class="numbered">We create a new TCP client connection to the address <code class="Code-In-Text--PACKT-">localhost:3000</code>.</li>
      <li class="numbered">We start the child process by using the first command-line argument as the path, while we provide the rest of the <code class="Code-In-Text--PACKT-">process.argv</code> array as arguments for the child process. We specify the option <code class="Code-In-Text--PACKT-">{silent: true}</code> so that the child process does not inherit <code class="Code-In-Text--PACKT-">stdout</code> and <code class="Code-In-Text--PACKT-">stderr</code> of the parent.</li>
      <li class="numbered">Finally, we take <code class="Code-In-Text--PACKT-">stdout</code> and <code class="Code-In-Text--PACKT-">stderr</code> of the child process and we multiplex them into the socket's <code class="Code-In-Text--PACKT-"><a id="_idIndexMarker534"/></code><code class="Code-In-Text--PACKT-">Writable</code> stream using the <code class="Code-In-Text--PACKT-">mutiplexChannels()</code> function.</li>
    </ol>
    <h4 class="title">Server side – demultiplexing</h4>
    <p class="normal">Now, we can take care of creating the server side of the application (<code class="Code-In-Text--PACKT-">server.js</code>), where we demultiplex th<a id="_idIndexMarker535"/>e streams from the remote connection and pipe them into two different files. </p>
    <p class="normal">Let's start by creating a function called <code class="Code-In-Text--PACKT-">demultiplexChannel()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">import { createWriteStream } from 'fs'
import { createServer } from 'net'
function demultiplexChannel (source, destinations) {
  let currentChannel = null
  let currentLength = null
  source
    .on('readable', () =&gt; {                                  // (1)
      let chunk
      if (currentChannel === null) {                         // (2)
        chunk = source.read(1)
        currentChannel = chunk &amp;&amp; chunk.readUInt8(0)
      }
      if (currentLength === null) {                          // (3)
        chunk = source.read(4)
        currentLength = chunk &amp;&amp; chunk.readUInt32BE(0)
        if (currentLength === null) {
          return null
        }
      }
      chunk = source.read(currentLength)                     // (4)
      if (chunk === null) {
        return null
      }
      console.log(`Received packet from: ${currentChannel}`)
      destinations[currentChannel].write(chunk)              // (5)
      currentChannel = null
      currentLength = null
    })
    .on('end', () =&gt; {                                       // (6)
      destinations.forEach(destination =&gt; destination.end())
      console.log('Source channel closed')
    })
}
</code></pre>
    <p class="normal">The preceding code might look complicated, but it is not. Thanks to the features of Node.js <code class="Code-In-Text--PACKT-">Readable</code> streams, we can easily implement the demultiplexing of our little protocol as follows:</p>
    <ol>
      <li class="numbered">We start reading from the stream using the non-flowing mode.</li>
      <li class="numbered">First, if we have not read the channel ID yet, we try to read 1 byte from the stream and then<a id="_idIndexMarker536"/> transform it into a number.</li>
      <li class="numbered">The next step is to read the length of the data. We need 4 bytes for that, so it's possible (even if unlikely) that we don't have enough data in the internal buffer, which will cause the <code class="Code-In-Text--PACKT-">this.read()</code> invocation to return <code class="Code-In-Text--PACKT-">null</code>. In such a case, we simply interrupt the parsing and retry at the next <code class="Code-In-Text--PACKT-">readable</code> event.</li>
      <li class="numbered">When we can finally also read the data size, we know how much data to pull from the internal buffer, so we try to read it all.</li>
      <li class="numbered">When we read all the data, we can write it to the right destination channel, making sure that we reset the <code class="Code-In-Text--PACKT-">currentChannel</code> and <code class="Code-In-Text--PACKT-">currentLength</code> variables (these will be used to parse the next packet).</li>
      <li class="numbered">Lastly, we make sure to end all the destination channels when the source channel ends.</li>
    </ol>
    <p class="normal">Now that we can demultiplex the source stream, let's put our new function to work:</p>
    <pre class="programlisting code"><code class="hljs-code">const server = createServer((socket) =&gt; {
  const stdoutStream = createWriteStream('stdout.log')
  const stderrStream = createWriteStream('stderr.log')
  demultiplexChannel(socket, [stdoutStream, stderrStream])
})
server.listen(3000, () =&gt; console.log('Server started'))
</code></pre>
    <p class="normal">In the preceding code, we first start a TCP server on port <code class="Code-In-Text--PACKT-">3000</code>; then, for each connection that we receive, we create two <code class="Code-In-Text--PACKT-">Writable</code> streams pointing to two different files: one for the <a id="_idIndexMarker537"/>standard output and the other for the standard error. These are our destination channels. Finally, we use <code class="Code-In-Text--PACKT-">demultiplexChannel()</code> to demultiplex the <code class="Code-In-Text--PACKT-">socket</code> stream into <code class="Code-In-Text--PACKT-">stdoutStream</code> and <code class="Code-In-Text--PACKT-">stderrStream</code>.</p>
    <h4 class="title">Running the mux/demux application</h4>
    <p class="normal">Now, we are ready<a id="_idIndexMarker538"/> to try our new mux/demux<a id="_idIndexMarker539"/> application, but first, let's create a small Node.js program to produce some sample output; let's call it <code class="Code-In-Text--PACKT-">generate-data.js</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">console.log('out1')
console.log('out2')
console.error('err1')
console.log('out3')
console.error('err2')
</code></pre>
    <p class="normal">Okay; now, we are ready to try our remote logging application. First, let's start the server:</p>
    <pre class="programlisting con"><code class="hljs-con">node server.js
</code></pre>
    <p class="normal">Then, we'll start the client by providing the file that we want to start as a child process:</p>
    <pre class="programlisting con"><code class="hljs-con">node client.js generateData.js
</code></pre>
    <p class="normal">The client will run almost immediately, but at the end of the process, the standard input and standard output of the <code class="Code-In-Text--PACKT-">generate-data.js</code> application will have traveled through one single TCP connection and then, on the server, be demultiplexed into two separate files.</p>
    <div><p class="Information-Box--PACKT-">Please make a note that, as we are using <code class="Code-In-Text--PACKT-">child_process.fork()</code> (<a href="http://nodejsdp.link/fork">nodejsdp.link/fork</a>), our client will only be able to launch other Node.js modules.</p>
    </div>
    <h3 id="_idParaDest-187" class="title">Multiplexing and demultiplexing object streams</h3>
    <p class="normal">The example<a id="_idIndexMarker540"/> that we have just shown demonstrates how to multiplex and <a id="_idIndexMarker541"/>demultiplex a binary/text stream, but it's worth mentioning that the same rules apply to object streams. The biggest difference is that when using objects, we already have a way to transmit the data using atomic messages (the objects), so multiplexing would be as easy as setting a <code class="Code-In-Text--PACKT-">channelID</code> property in each object. Demultiplexing would simply involve reading the <code class="Code-In-Text--PACKT-">channelID</code> property and routing each object toward the right destination stream.</p>
    <p class="normal">Another pattern involving only demultiplexing is routing the data coming from a source depending on some condition. With this pattern, we can implement complex flows, such as the one shown in <em class="italic">Figure 6.11</em>:</p>
    <figure class="mediaobject"><img src="img/B15729_06_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.11: Demultiplexing an object stream</p>
    <p class="normal">The demultiplexer <a id="_idIndexMarker542"/>used in the system in <em class="italic">Figure 6.11</em> takes a stream of <a id="_idIndexMarker543"/>objects representing <em class="italic">animals</em> and distributes each of them to the right destination stream based on the class of the animal: <em class="italic">reptiles</em>, <em class="italic">amphibians</em>, or <em class="italic">mammals</em>.</p>
    <p class="normal">Using the same principle, we can also implement an <code class="Code-In-Text--PACKT-">if...else</code> statement for streams. For some<a id="_idIndexMarker544"/> inspiration, take a look at the <code class="Code-In-Text--PACKT-">ternary-stream</code> package (<a href="http://nodejsdp.link/ternary-stream">nodejsdp.link/ternary-stream</a>), which allows us to do exactly that.</p>
    <h1 id="_idParaDest-188" class="title">Summary</h1>
    <p class="normal">In this chapter, we have shed some light on Node.js streams and some of their most common use cases. We learned why streams are so acclaimed by the Node.js community and we mastered their basic functionality, enabling us to discover more and navigate comfortably in this new world. We analyzed some advanced patterns and started to understand how to connect streams in different configurations, grasping the importance of interoperability, which is what makes streams so versatile and powerful.</p>
    <p class="normal">If we can't do something with one stream, we can probably do it by connecting other streams together, and this works great with the <em class="italic">one thing per module</em> philosophy. At this point, it should be clear that streams are not just a <em class="italic">good to know</em> feature of Node.js; they are an essential part—a crucial pattern to handle binary data, strings, and objects. It's not by chance that we dedicated an entire chapter to them.</p>
    <p class="normal">In the next few chapters, we will focus on the traditional object-oriented design patterns. But don't be fooled; even though JavaScript is, to some extent, an object-oriented language, in Node.js, the functional or hybrid approach is often preferred. Get rid of every prejudice before reading the next chapters.</p>
    <h1 id="_idParaDest-189" class="title">Exercises</h1>
    <ul>
      <li class="Bullet--PACKT-"><strong class="keyword">6.1 Data compression efficiency</strong>: Write a command-line script that takes a file as input and compresses it using the different algorithms available in the <code class="Code-In-Text--PACKT-">zlib</code> module (Brotli, Deflate, Gzip). You want to produce a summary table that compares the algorithm's compression time and compression efficiency on the given file. Hint: This could be a good use case for the fork pattern, but remember that we made some important performance considerations when we discussed it earlier in this chapter.</li>
      <li class="Bullet--PACKT-"><strong class="keyword">6.2 Stream data processing</strong>: On Kaggle, you can find a lot of interesting data sets, such as the London Crime Data (<a href="http://nodejsdp.link/london-crime">nodejsdp.link/london-crime</a>). You can download the data in CSV format and build a stream processing script that analyzes the data and tries to answer the following questions:<ul>
          <li class="Bullet-Within-Bullet--PACKT-">Did the number of crimes go up or down over the years?</li>
          <li class="Bullet-Within-Bullet--PACKT-">What are the most dangerous areas of London?</li>
          <li class="Bullet-Within-Bullet--PACKT-">What is the most common crime per area?</li>
          <li class="Bullet-Within-Bullet--PACKT-">What is the least common crime?</li>
        </ul>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">Hint: You can use a combination of <code class="Code-In-Text--PACKT-">Transform</code> streams and <code class="Code-In-Text--PACKT-">PassThrough</code> streams to parse and observe the data as it is flowing. Then, you can build in-memory aggregations for the data, which can help you answer the preceding questions. Also, you don't need to do everything in one pipeline; you could build very specialized pipelines (for example, one per question) and use the fork pattern to distribute the parsed data across them.</p>
      </li>
      <li class="Bullet--PACKT-"><strong class="keyword">6.3 File share over TCP</strong>: Build a client and a server to transfer files over TCP. Extra points if you add a layer of encryption on top of that and if you can transfer multiple files at once. Once you have your implementation ready, give the client code and your IP address to a friend or a colleague, then ask them to send you some files! Hint: You could use mux/demux to receive multiple files at once.</li>
      <li class="Bullet--PACKT-"><strong class="keyword">6.4 Animations with </strong><strong class="codeHighlighted">Readable</strong><strong class="keyword"> streams</strong>: Did you know you can create amazing terminal animations with just <code class="Code-In-Text--PACKT-">Readable</code> streams? Well, to understand what we are talking about here, try to run <code class="Code-In-Text--PACKT-">curl parrot.live</code> in your terminal and see what happens! If you think that this is cool, why don't you try to create something similar? Hint: If you need some help with figuring out how to implement this, you can check out the actual source code of <code class="Code-In-Text--PACKT-">parrot.live</code> by simply accessing its URL through your browser.</li>
    </ul>
  </div>
</body></html>