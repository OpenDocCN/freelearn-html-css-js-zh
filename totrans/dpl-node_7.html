<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Deploying and Maintaining</h1></div></div></div><p>In this book, we have seen the advantages of composing applications out of well defined components. This assembly process involves the installation of many support systems, from the operating system your application will run on, to the version of Node you will support, to the various npm modules, testing frameworks, profiling tools, and other subsystems that power an application. It is likely that you have been doing all this on a single machine—manually starting and stopping servers, updating configurations, and altering application code. Are you adding a new module? Stop the server, add the module, and restart the server.</p><p>In a production environment, this sort of ad hoc development is almost impossible, and it remains tedious regardless. How can this process be automated and streamlined so that altering the number of servers being balanced or incrementally pushing out new deployments can be done with minimum work, thus making life simpler for the folks responsible for operations?</p><p>In this chapter, we will learn about the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Automating the deployment of applications, including a look at the differences between continuous integration, delivery, and deployment</li><li class="listitem" style="list-style-type: disc">Using Git to track local changes and triggering deployment actions via webhooks when appropriate</li><li class="listitem" style="list-style-type: disc">Using Vagrant to synchronize your local development environment with a deployed production server</li><li class="listitem" style="list-style-type: disc">Provisioning a server with Ansible</li><li class="listitem" style="list-style-type: disc">Implementing continuous integration and deployment using Jenkins and working through a complete example of how to automate builds and deploys when the source code changes</li><li class="listitem" style="list-style-type: disc">Maintaining npm packages and dependency trees, outlining how to track version changes, and keeping your deployed applications up to date</li></ul></div><p>Note that application deployment is a complex topic with many dimensions that are often considered within unique sets of needs. This chapter is intended as an introduction to some of the technologies and themes you will encounter. Also, note that the scaling issues discussed in <a class="link" href="ch03.html" title="Chapter 3. Scaling Node">Chapter 3</a>, <em>Scaling Node</em>, are part and parcel of deployment. Also, our discussion in <a class="link" href="ch02.html" title="Chapter 2. Installing and Virtualizing Node Servers">Chapter 2</a>, <em>Installing and Virtualizing Node Servers</em>, is relevant here. You may want to revisit those topics while working through the following deployment scenarios.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Using GitHub webhooks</h1></div></div></div><p>At the most basic level, deployment involves automatically validating, preparing, and releasing new code into production environments. One of the simplest ways to set up a deployment strategy is to trigger releases whenever changes are committed to a Git repository through the <a id="id857" class="indexterm"/>use of <strong>webhooks</strong>. Paraphrasing the GitHub documentation, webhooks <em>provide a way for notifications to be delivered to an external web server whenever certain actions occur on a repository</em>.</p><p>In <a class="link" href="ch02.html" title="Chapter 2. Installing and Virtualizing Node Servers">Chapter 2</a>, <em>Installing and Virtualizing Node Servers</em>, we saw a simplified example of this process, where <a id="id858" class="indexterm"/>pushing changes to a Heroku instance caused your production build to automatically update. One problem with this simple solution is that no validation was performed—if you pushed bad code, your production server would blindly run bad code. In this section, we'll use GitHub webhooks to create a simple continuous deployment workflow, adding more realistic checks and balances.</p><p>We'll build a local development environment that lets developers work with a clone of the production server code, make changes, and see the results of those changes immediately. As this <a id="id859" class="indexterm"/>local <strong>development</strong> build uses the same repository as<a id="id860" class="indexterm"/> the <strong>production</strong> build, the build process for a chosen environment is simple to configure, and multiple production and/or development <em>boxes</em> can be created with no special effort.</p><p>The first step is to create<a id="id861" class="indexterm"/> a GitHub (<a class="ulink" href="http://www.github.com">www.github.com</a>) account if you don't already have one. Basic accounts are free and easy to set up.</p><p>Now, let's look at how GitHub webhooks work.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec69"/>Enabling webhooks</h2></div></div></div><p>Create a <a id="id862" class="indexterm"/>new folder and insert the following <code class="literal">package.json</code> file:</p><div><pre class="programlisting">{
  "name": "express-webhook",
  "main": "server.js",
  "dependencies": {
    "express": "~4.0.0",
    "body-parser": "^1.12.3"
  }
}</pre></div><p>This ensures<a id="id863" class="indexterm"/> that Express 4.x is installed and includes the <code class="literal">body-parser </code>package, which is used to handle POST data. Next, create a basic server called <code class="literal">server.js</code>:</p><div><pre class="programlisting">var express   = require('express');
var app       = express();
var bodyParser   = require('body-parser');
var port      = process.env.PORT || 8082;

app.use(bodyParser.json());
app.get('/', function(req, res) {
  res.send('Hello World!');
});
app.post('/webhook', function(req, res) {
  //  We'll add this next
});
app.listen(port);
console.log('Express server listening on port ' + port);</pre></div><p>Enter the folder you've created, and build and run the server with <code class="literal">npm install; npm start</code>. Visit <code class="literal">localhost:8082/</code> and you should see <strong>"Hello World!"</strong> in your browser.</p><p>Whenever any file changes in a given repository, we want GitHub to push information about the change to <code class="literal">/webhook</code>. So, the first step is to create a GitHub repository for the Express server mentioned in the code. Go to your GitHub account and create a new repository with the name <code class="literal">'express-webhook'</code>. The following screenshot shows this:</p><div><img src="img/1403OS_07_03.jpg" alt="Enabling webhooks"/></div><p>Once<a id="id864" class="indexterm"/> the repository is created, enter your local repository folder and run the following commands:</p><div><pre class="programlisting">
<strong>git init</strong>
<strong>git add .</strong>
<strong>git commit -m "first commit"</strong>
<strong>git remote add origin git@github.com:&lt;your username&gt;/express-webhook</strong>
</pre></div><p>You should now have a new GitHub repository and a local linked version. The next step is to configure this repository to broadcast the push event on the repository. Navigate to the following URL:</p><p>
<code class="literal">https://github.com/&lt;your_username&gt;/express-webhook/settings</code>
</p><p>From here, navigate to <strong>Webhooks &amp; Services</strong> | <strong>Add webhook</strong> (you may need to enter your password again). You should now see the following screen:</p><div><img src="img/1403OS_07_04.jpg" alt="Enabling webhooks"/></div><p>This is <a id="id865" class="indexterm"/>where you set up webhooks. Note that the <code class="literal">push</code> event is already set as default, and, if asked, you'll want to disable SSL verification for now. GitHub needs a target URL to use POST on change events. If you have your local repository in a location that is already web accessible, enter that now, remembering to append the <code class="literal">/webhook</code> route, as in <a class="ulink" href="http://www.example.com/webhook">http://www.example.com/webhook</a>.</p><p>If you are building on a local machine or on another limited network, you'll need to create a secure tunnel that GitHub can use. A free service to do this can be found at <a class="ulink" href="http://localtunnel.me/">http://localtunnel.me/</a>. Follow the instructions on that page, and use the custom URL provided to <a id="id866" class="indexterm"/>configure your webhook.</p><div><div><h3 class="title"><a id="note47"/>Note</h3><p>Other good forwarding services<a id="id867" class="indexterm"/> can be found at <a class="ulink" href="https://forwardhq.com/">https://forwardhq.com/</a> and <a class="ulink" href="https://meetfinch.com/">https://meetfinch.com/</a>.</p></div></div><p>Now that webhooks are enabled, the next step is to test the system by triggering a push event. Create a new file called <code class="literal">readme.md</code> (add whatever you'd like to it), save it, and then run the following commands:</p><div><pre class="programlisting">
<strong>git add readme.md</strong>
<strong>git commit -m "testing webhooks"</strong>
<strong>git push origin master</strong>
</pre></div><p>This <a id="id868" class="indexterm"/>will push changes to your GitHub repository. Return to the <strong>Webhooks &amp; Services</strong> section for the <code class="literal">express-webhook</code> repository on GitHub. You should see something like this:</p><div><img src="img/1403OS_07_05.jpg" alt="Enabling webhooks"/></div><p>This is a good thing! GitHub noticed your push and attempted to deliver information about the changes to the webhook endpoint you set, but the delivery failed as we haven't configured the <code class="literal">/webhook</code> route yet—that's to be expected. Inspect the failed delivery payload by clicking on the last attempt—you should see a large JSON file. In that payload, you'll find something like this:</p><div><pre class="programlisting">  "committer": {
    "name": "Sandro Pasquali",
    "email": "spasquali@gmail.com",
    "username": "sandro-pasquali"
  },
  "added": [
    "readme.md"
  ],
  "removed": [],
  "modified": []</pre></div><p>It should now be clear what sort of information GitHub will pass along whenever a push event happens. You can now configure the <code class="literal">/webhook</code> route in the demonstration Express server to parse this data and do something with that information, such as sending an e-mail to an administrator. For example, use the following code:</p><div><pre class="programlisting">app.post('/webhook', function(req, res) {
  console.log(req.body);
});</pre></div><p>The next time your webhook fires, the entire JSON payload will be displayed.</p><p>Let's<a id="id869" class="indexterm"/> take this to another level, breaking down the autopilot application to see how webhooks can be used to create a build/deploy system.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec70"/>Implementing a build/deploy system using webhooks</h2></div></div></div><p>To demonstrate<a id="id870" class="indexterm"/> how to build a <a id="id871" class="indexterm"/>webhook-powered deployment system, we're going to use a starter kit for application development. Go ahead and use <a id="id872" class="indexterm"/>fork on the repository at <a class="ulink" href="https://github.com/sandro-pasquali/autopilot.git">https://github.com/sandro-pasquali/autopilot.git</a>. You now have a copy of the<a id="id873" class="indexterm"/> <strong>autopilot</strong> repository, which includes scaffolding for common Gulp tasks, tests, an Express server, and a deploy system that we're now going to explore.</p><p>The autopilot application implements special features depending on whether you are running it in production or in development. While autopilot is a little too large and complex to fully document here, we're going to take a look at how major components of the system are designed and implemented so that you can build your own or augment existing systems. Here's what we will examine:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to create webhooks on GitHub programmatically</li><li class="listitem" style="list-style-type: disc">How to catch and read webhook payloads</li><li class="listitem" style="list-style-type: disc">How to use payload data to clone, test, and integrate changes</li><li class="listitem" style="list-style-type: disc">How to use PM2 to safely manage and restart servers when code changes</li></ul></div><p>If you haven't already used fork on the autopilot repository, do that now. Clone the autopilot repository onto a server or someplace else where it is web-accessible. Follow the instructions on how to connect and push to the fork you've created on GitHub, and get familiar with how to pull and push changes, commit changes, and so on.</p><div><div><h3 class="title"><a id="note48"/>Note</h3><p>PM2 delivers<a id="id874" class="indexterm"/> a basic deploy system that you might consider for your project (<a class="ulink" href="https://github.com/Unitech/PM2/blob/master/ADVANCED_README.md#deployment">https://github.com/Unitech/PM2/blob/master/ADVANCED_README.md#deployment</a>).</p></div></div><p>Install the cloned autopilot repository with <code class="literal">npm install; npm start</code>. Once npm has installed dependencies, an interactive CLI application will lead you through the configuration process. Just hit the <em>Enter</em> key for all the questions, which will set defaults for a local development build (we'll build in production later). Once the configuration is complete, a new development server process controlled by PM2 will have been spawned. You'll see it listed in the PM2 manifest under <code class="literal">autopilot-dev</code> in the following screenshot:</p><div><img src="img/1403OS_07_02.jpg" alt="Implementing a build/deploy system using webhooks"/></div><p>You will make changes in the <code class="literal">/source</code> directory of this development build. When you eventually have a production server in place, you will use <code class="literal">git push</code> on the local changes to push them to the autopilot repository on GitHub, triggering a webhook. GitHub will use <code class="literal">POST</code> on the information about the change to an Express route that we will define on our <a id="id875" class="indexterm"/>server, which will trigger<a id="id876" class="indexterm"/> the build process. The build runner will <code class="literal">pull</code> your changes from GitHub into a temporary directory, install, build, and test the changes, and if all is well, it will replace the relevant files in your deployed repository. At this point, PM2 will restart, and your changes will be immediately available.</p><p>Schematically, the flow looks like this:</p><div><img src="img/1403OS_07_01.jpg" alt="Implementing a build/deploy system using webhooks"/></div><p>To <a id="id877" class="indexterm"/>create webhooks on GitHub <a id="id878" class="indexterm"/>programmatically, you will need to create an access token. The following diagram explains the steps from A to B to C:</p><div><img src="img/1403OS_07_09.jpg" alt="Implementing a build/deploy system using webhooks"/></div><p>We're going to use the Node library at <a class="ulink" href="https://github.com/mikedeboer/node-github">https://github.com/mikedeboer/node-github</a> to access GitHub. We'll use this package to create hooks on GitHub using the access token you've just created.</p><p>Once<a id="id879" class="indexterm"/> you have an access token, creating<a id="id880" class="indexterm"/> a webhook is easy:</p><div><pre class="programlisting">var GitHubApi = require("github");

github.authenticate({
  type: "oauth",
  token: &lt;your token&gt;
});
github.repos.createHook({
  "user": &lt;your github username&gt;,
  "repo": &lt;github repo name&gt;,
  "name": "web",
  "secret": &lt;any secret string&gt;,
  "active": true,
  "events": [
    "push"
  ],
  "config": {
    "url": "http://yourserver.com/git-webhook",
    "content_type": "json"
  }
}, function(err, resp) {
  ...
});</pre></div><p>Autopilot performs this on startup, removing the need for you to manually create a hook.</p><p>Now, we are listening for changes. As we saw previously, GitHub will deliver a payload indicating what has been added, what has been deleted, and what has changed. The next step for the autopilot system is to integrate these changes.</p><p>It is important to remember that, when you use webhooks, you do not have control over how often GitHub will send changesets—if more than one person on your team can push, there is no predicting when those pushes will happen. The autopilot system uses Redis to manage a queue of requests, executing them in order. You will need to manage multiple changes in a way. For now, let's look at a straightforward way to build, test, and integrate changes.</p><p>In your <a id="id881" class="indexterm"/>code bundle, visit <code class="literal">autopilot/swanson/push.js</code>. This is a process runner on which fork has been used by <code class="literal">buildQueue.js</code> in<a id="id882" class="indexterm"/> that same folder. The following information is passed to it:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The URL of the GitHub repository that we will clone</li><li class="listitem" style="list-style-type: disc">The directory to clone that repository into (<code class="literal">&lt;temp directory&gt;</code>/<code class="literal">&lt;commit hash&gt;</code>)</li><li class="listitem" style="list-style-type: disc">The changeset</li><li class="listitem" style="list-style-type: disc">The location of the production repository that will be changed</li></ul></div><p>Go ahead and read through the code. Using a few shell scripts, we will clone the changed repository and build it using the same commands you're used to—<code class="literal">npm install</code>, <code class="literal">npm test</code>, and so on. If the application builds without errors, we need only run through the changeset and replace the old files with the changed files.</p><p>The final step is to restart our production server so that the changes reach our users. Here is where the real power of PM2 comes into play.</p><p>When the autopilot system is run in production, PM2 creates a cluster of servers (similar to the Node <code class="literal">cluster</code> module). This is important as it allows us to restart the production server incrementally. As we restart one server node in the cluster with the newly pushed content, the other clusters continue to serve old content. This is essential to keeping a zero-downtime production running.</p><p>Hopefully, the autopilot implementation will give you a few ideas on how to improve this process and customize it to your own needs.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec38"/>Synchronizing local and deployed builds</h1></div></div></div><p>One of the most important (and often difficult) parts of the deployment process is ensuring that <a id="id883" class="indexterm"/>the environment an application is being developed, built, and tested within perfectly simulates the environment that application will be deployed into. In this section, you'll learn how to emulate, or virtualize, the environment your deployed application will run within using Vagrant. After demonstrating how this setup can simplify your <em>local</em> development process, we'll use Ansible to provision a remote instance on DigitalOcean.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec71"/>Developing locally with Vagrant</h2></div></div></div><p>For a long <a id="id884" class="indexterm"/>while, developers would work <a id="id885" class="indexterm"/>directly on running servers or cobble together their own version of the <a id="id886" class="indexterm"/>production environment locally, often writing ad hoc scripts and tools to smoothen their development process. This is no longer necessary in a world of virtual machines. In this section, we will learn how to use Vagrant to emulate a production environment within your development environment, advantageously giving you a realistic <em>box</em> to work on testing code for production and isolating your development process from your local machine processes.</p><p>By definition, Vagrant is used to create a virtual box emulating a production environment. So, we need to install Vagrant, a virtual machine, and a machine image. Finally, we'll <a id="id887" class="indexterm"/>need to write the configuration and <a id="id888" class="indexterm"/>provisioning scripts for our environment.</p><p>Go <a id="id889" class="indexterm"/>to <a class="ulink" href="http://www.vagrantup.com/downloads">http://www.vagrantup.com/downloads</a> and install the right Vagrant version<a id="id890" class="indexterm"/> for your box. Do the same with VirtualBox here at <a class="ulink" href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a>.</p><p>You now need to add a box to run. For this example, we're going to use Centos 7.0, but you can choose whichever you'd prefer. Create a new folder for this project, enter it, and run the following command:</p><div><pre class="programlisting">vagrant box add chef/centos-7.0</pre></div><div><div><h3 class="title"><a id="note49"/>Note</h3><p>Usefully, the creators of Vagrant, HashiCorp, provide a search service for Vagrant <a id="id891" class="indexterm"/>boxes at <a class="ulink" href="https://atlas.hashicorp.com/boxes/search">https://atlas.hashicorp.com/boxes/search</a>.</p></div></div><p>You will be prompted to choose your virtual environment provider—select <code class="literal">virtualbox</code>. All relevant files and machines will now be downloaded. Note that these boxes are very large and may take time to download.</p><p>You'll now create a configuration file for Vagrant called <code class="literal">Vagrantfile</code>. As with npm, the <code class="literal">init</code> command quickly sets up a base file. Additionally, we'll need to inform Vagrant of the box we'll be using:</p><div><pre class="programlisting">
<strong>vagrant init chef/centos-7.0</strong>
</pre></div><p>Vagrantfile is written in Ruby and defines the Vagrant environment. Open it up now and scan it. There is a lot of commentary, and it makes a useful read. Note the <code class="literal">config.vm.box = "chef/centos-7.0"</code> line, which was inserted during the initialization process.</p><p>Now you can start Vagrant:</p><div><pre class="programlisting">
<strong>vagrant up</strong>
</pre></div><p>If everything went as expected, your box has been booted within Virtualbox. To confirm that your box is running, use the following code:</p><div><pre class="programlisting">
<strong>vagrant ssh</strong>
</pre></div><p>If you see a prompt, you've just set up a virtual machine. You'll see that you are in the typical home directory of a CentOS environment.</p><p>To destroy your box, run <code class="literal">vagrant destroy</code>. This deletes the virtual machine by cleaning up captured resources. However, the next <code class="literal">vagrant up</code> command will need to do a lot of work to rebuild. If you simply want to shut down your machine, use <code class="literal">vagrant halt</code>.</p><p>Vagrant is <a id="id892" class="indexterm"/>useful as a virtualized, production-like <a id="id893" class="indexterm"/>environment for developers to work within. To that end, it must be configured to emulate a production environment. In other words, your box must be provisioned by telling Vagrant how it should be configured and what software should be installed whenever <code class="literal">vagrant up</code> is run.</p><p>One strategy for provisioning is to create a shell script that configures our server directly and point the Vagrant provisioning process to that script. Add the following line to Vagrantfile:</p><div><pre class="programlisting">config.vm.provision "shell", path: "provision.sh"</pre></div><p>Now, create that file with the following contents in the folder hosting Vagrantfile:</p><div><pre class="programlisting"># install nvm
curl https://raw.githubusercontent.com/creationix/nvm/v0.24.1/install.sh | bash
# restart your shell with nvm enabled
source ~/.bashrc
# install the latest Node.js
nvm install 0.12
# ensure server default version
nvm alias default 0.12</pre></div><p>Destroy any running Vagrant boxes. Run Vagrant again, and you will notice in the output the execution of the commands in our provisioning shell script.</p><p>When this has been completed, enter your Vagrant box as the root (Vagrant boxes are automatically assigned the root password "vagrant"):</p><div><pre class="programlisting">
<strong>vagrant ssh</strong>
<strong>su</strong>
</pre></div><p>You will see that Node v0.12.x is installed:</p><div><pre class="programlisting">
<strong>node -v</strong>
</pre></div><div><div><h3 class="title"><a id="note50"/>Note</h3><p>It's standard to allow password-less sudo for the <em>Vagrant</em> user. Run <code class="literal">visudo</code> and add the following line to the <code class="literal">sudoers</code> configuration file:</p><div><pre class="programlisting">vagrant ALL=(ALL) NOPASSWD: ALL</pre></div></div></div><p>Typically, when you are developing applications, you'll be modifying files in a project directory. You might bind a directory in your Vagrant box to a local code editor and develop in that way. Vagrant offers a simpler solution. Within your VM, there is a <code class="literal">/vagrant</code> folder that maps to the folder that Vagrantfile exists within, and these two folders are<a id="id894" class="indexterm"/> automatically synced. So, if you add <a id="id895" class="indexterm"/>the <code class="literal">server.js</code> file to the right folder on your local machine, that file will also show up in your VM's <code class="literal">/vagrant </code>folder.</p><p>Go ahead and create a new <code class="literal">test</code> file either in your local folder or in your VM's <code class="literal">/vagrant</code> folder. You'll see that file synchronized to both locations regardless of where it was originally created.</p><p>Let's clone our <code class="literal">express-webhook</code> repository from earlier in this chapter into our Vagrant box. Add the following lines to provision.sh:</p><div><pre class="programlisting">
<strong># install various packages, particularly for git</strong>
<strong>yum groupinstall "Development Tools" -y</strong>
<strong>yum install gettext-devel openssl-devel perl-CPAN perl-devel zlib-devel -y</strong>
<strong>yum install git -y</strong>
<strong># Move to shared folder, clone and start server</strong>
<strong>cd /vagrant</strong>
<strong>git clone https://github.com/sandro-pasquali/express-webhook</strong>
<strong>cd express-webhook</strong>
<strong>npm i; npm start</strong>
</pre></div><p>Add the following to Vagrantfile, which will map port <code class="literal">8082</code> on the Vagrant box (a guest port representing the port our hosted application listens on) to port <code class="literal">8000</code> on our host machine:</p><div><pre class="programlisting">config.vm.network "forwarded_port", guest: 8082, host: 8000</pre></div><p>Now, we need to restart the Vagrant box (loading this new configuration) and re-provision it:</p><div><pre class="programlisting">
<strong>vagrant reload</strong>
<strong>vagrant provision</strong>
</pre></div><p>This will take a while as <code class="literal">yum</code> installs various dependencies. When provisioning is complete, you should see this as the last line:</p><div><pre class="programlisting">
<strong>==&gt; default: Express server listening on port 8082</strong>
</pre></div><p>Remembering that we bound the guest port <code class="literal">8082</code> to the host port <code class="literal">8000</code>, go to your browser and navigate to <code class="literal">localhost:8000</code>. You should see <strong>"Hello World!"</strong> displayed.</p><p>Also note that in our provisioning script, we cloned to the (shared) <code class="literal">/vagrant</code> folder. This<a id="id896" class="indexterm"/> means the clone of <code class="literal">express-webhook </code>should be <a id="id897" class="indexterm"/>visible in the current folder, which will allow you to work on the more easily accessible codebase, knowing it will be automatically synchronized with the version on your Vagrant box.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec72"/>Provisioning with Ansible</h2></div></div></div><p>Configuring your machines <em>by hand</em>, as we've done previously, doesn't scale well. For one, it can<a id="id898" class="indexterm"/> be overly difficult to set and manage<a id="id899" class="indexterm"/> environment variables. Also, writing your own provisioning scripts is error-prone and no longer necessary given the existence of provisioning tools, such as Ansible.</p><p>With Ansible, we can define server environments using an organized syntax rather than ad hoc scripts, making it easier to distribute and modify configurations. Let's recreate the <code class="literal">provision.sh</code> script <a id="id900" class="indexterm"/>developed earlier using Ansible <strong>playbooks</strong>:</p><div><blockquote class="blockquote"><p><em>Playbooks are Ansible's configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce or a set of steps in a general IT process.</em></p></blockquote></div><p>Playbooks are expressed in<a id="id901" class="indexterm"/> the <strong>YAML</strong> format (a human-readable data serialization language). To start with, we're going to change Vagrantfile's provisioner to Ansible. First, create the following subdirectories in your Vagrant folder:</p><div><pre class="programlisting">provisioning
  common
    tasks</pre></div><p>These will be explained as we proceed through the Ansible setup.</p><p>Next, create the following configuration file and name it <code class="literal">ansible.cfg</code>:</p><div><pre class="programlisting">[defaults]
roles_path = provisioning
log_path = ./ansible.log</pre></div><p>This indicates that <a id="id902" class="indexterm"/>Ansible <strong>roles</strong> can be found in the <code class="literal">/provisioning</code> folder, and that we want to keep a provisioning log in <code class="literal">ansible.log</code>. Roles are used to organize tasks and other functions into reusable files. These will be explained shortly.</p><p>Modify the <code class="literal">config.vm.provision</code> definition to the following:</p><div><pre class="programlisting">    config.vm.provision "ansible" do |ansible|
    ansible.playbook = "provisioning/server.yml"
    ansible.verbose = "vvvv"
    end</pre></div><p>This tells <a id="id903" class="indexterm"/>Vagrant to defer to Ansible for<a id="id904" class="indexterm"/> provisioning instructions, and that we want the provisioning process to be verbose—we want to get feedback when the provisioning step is running. Also, we can see that the playbook definition, <code class="literal">provisioning/server.yml</code>, is expected to exist. Create that file now:</p><div><pre class="programlisting">---
- hosts: all
  sudo: yes
  roles:
    - common
  vars:
    env:
      user: 'vagrant'
    nvm:
      version: '0.24.1'
      node_version: '0.12'
    build:
      repo_path: 'https://github.com/sandro-pasquali'
      repo_name: 'express-webhook'</pre></div><p>Playbooks can contain very complex rules. This simple file indicates that we are going to provision all available hosts using a single role called <code class="literal">common</code>. In more complex deployments, an inventory of IP addresses could be set under <code class="literal">hosts</code>, but, here, we just want to use a general setting for our one server. Additionally, the provisioning step will be provided with certain environment variables following the forms <code class="literal">env.user</code>, <code class="literal">nvm.node_version</code>, and so on. These variables will come into play when we define the <code class="literal">common</code> role, which will be to provision our Vagrant server with the programs necessary to build, clone, and deploy <code class="literal">express-webhook</code>. Finally, we assert that Ansible should run as an administrator (<code class="literal">sudo</code>) by default—this is necessary for the <code class="literal">yum</code> package manager on CentOS.</p><p>We're now ready to define the <code class="literal">common</code> role. With Ansible, folder structures are important and are implied by the playbook. In our case, Ansible expects the role location (<code class="literal">./provisioning</code>, as defined in <code class="literal">ansible.cfg</code>) to contain the <code class="literal">common</code> folder (reflecting the <code class="literal">common</code> role given in the playbook), which itself must contain a <code class="literal">tasks</code> folder containing a <code class="literal">main.yml</code> file. These last two naming conventions are specific and required.</p><p>The final step is creating the <code class="literal">main.yml</code> file in <code class="literal">provisioning/common/tasks</code>. First, we replicate the <code class="literal">yum</code> package loaders (see the file in your code bundle for the full list):</p><div><pre class="programlisting">
<strong>---</strong>
<strong>- name: Install necessary OS programs</strong>
<strong>  yum: name={{ item }} state=installed</strong>
<strong>  with_items:</strong>
<strong>    - autoconf</strong>
<strong>    - automake</strong>
<strong>    ...</strong>
<strong>    - git</strong>
</pre></div><p>Here, we <a id="id905" class="indexterm"/>see a few benefits of Ansible. A <a id="id906" class="indexterm"/>human-readable description of <code class="literal">yum </code>tasks is provided to a looping structure that will install every item in the list. Next, we run the nvm installer, which simply executes the auto-installer for nvm:</p><div><pre class="programlisting">
<strong>- name: Install nvm</strong>
<strong>  sudo: no</strong>
<strong>  shell: "curl https://raw.githubusercontent.com/creationix/nvm/v{{ nvm.version }}/install.sh | bash"</strong>
</pre></div><p>Note that, here, we're overriding the playbook's <code class="literal">sudo</code> setting. This can be done on a per-task basis, which gives us the freedom to move between different permission levels while provisioning. We are also able to execute shell commands while at the same time interpolating variables:</p><div><pre class="programlisting">
<strong>- name: Update .bashrc</strong>
<strong>  sudo: no</strong>
<strong>  lineinfile: &gt;</strong>
<strong>    dest="/home/{{ env.user }}/.bashrc"</strong>
<strong>    line="source /home/{{ env.user }}/.nvm/nvm.sh"</strong>
</pre></div><p>Ansible provides extremely useful tools for file manipulation, and we will see here a very common one—updating the <code class="literal">.bashrc</code> file for a user. The <code class="literal">lineinfile</code> directive makes the addition of aliases, among other things, straightforward.</p><p>The remainder of the commands follow a similar pattern to implement, in a structured way, the provisioning directives we need for our server. All the files you will need are in your code bundle in the <code class="literal">vagrant/with_ansible</code> folder. Once you have them installed, run <code class="literal">vagrant up</code> to see Ansible in action.</p><p>One of the strengths of Ansible is the way it handles contexts. When you start your Vagrant build, you will notice that Ansible gathers facts, as shown in the following screenshot:</p><div><img src="img/1403OS_07_06.jpg" alt="Provisioning with Ansible"/></div><p>Simply put, Ansible analyzes the context it is working in and only executes what is necessary to <a id="id907" class="indexterm"/>execute. If one of your tasks has <a id="id908" class="indexterm"/>already been run, the next time you try <code class="literal">vagrant provision</code>, that task will not run again. This is <em>not</em> true for shell scripts! In this way, editing playbooks and reprovisioning does not consume time redundantly changing what has already been changed.</p><p>Ansible is a powerful tool that can be used for provisioning and much more complex deployment tasks. One of its great strengths is that it can run remotely—unlike most other tools, Ansible uses SSH to connect to remote servers and run operations. There is no need to install it on your production boxes. You are encouraged to browse the Ansible documentation<a id="id909" class="indexterm"/> at <a class="ulink" href="http://docs.ansible.com/index.html">http://docs.ansible.com/index.html</a> to learn more.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec39"/>Integrating, delivering, and deploying</h1></div></div></div><p>In this chapter, we've been looking at using deployment systems that encourage agile development, generally facilitating safe delivery into production environments of code updates in near real time. Variations in how deployments can be structured and/or understood, which usually depend on factors such as team size and management structure, are<a id="id910" class="indexterm"/> common. A brief summary of each of the three<a id="id911" class="indexterm"/> typical <a id="id912" class="indexterm"/>categories, <strong>continuous integration</strong>, <strong>continuous delivery</strong>, and <strong>continuous deployment</strong>, will be provided in the following sections. Finally, we'll set up a build/deploy system for a Node application using Jenkins, a CI server, configured to automatically deploy changes to a Heroku server.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec73"/>Continuous integration</h2></div></div></div><p>Continuous integration <a id="id913" class="indexterm"/>is the process of merging changes into a master branch continuously (typically, several times a day). The goal of CI is to make errors impatient and noisy, arriving early and failing loudly, rather than emerging later from much larger and more complex <strong>bulk</strong> merges comprising several days or weeks of work. Unit tests are typically run here. Note that an updated integration branch is not necessarily continuously deployed, though it may be. The goal is to keep a master branch fresh, current, and ready to be deployed when necessary.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec74"/>Continuous delivery</h2></div></div></div><p>"Delivery" is the<a id="id914" class="indexterm"/> key word here. In environments where all changes must be tested/vetted by a quality assurance team or some other group of stakeholders prior to being released, changes are delivered and reviewed as they are proposed. While continuous delivery does not preclude delivery into production, the general goal is to deliver new code where it can be subjected to further functional tests, tests of business logic, and so on, prior to it reaching real customers.</p><p>This test environment should be equivalent to the production environment and, when tests pass, there should be confidence that the changes will also be deployable to production. Because this stage is typically understood as preceding deployment, it is often described as the <em>staging environment</em>.</p><p>Staged changes are normally deployable in one step, a single system command, or the click of a button in a GUI.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec75"/>Continuous deployment</h2></div></div></div><p>Continuous deployment is the aggressive, optimistic strategy of building your application in a way <a id="id915" class="indexterm"/>such that it can be released into production at any time, typically as soon as it passes certain automated tests. This strategy generally leads to many releases per day and requires that the validation pipeline, which changes move through, is as close to production-like as possible.</p><p>Because there is limited (or nonexistent) oversight of the code being released, continuous post-release inspection of application performance is normal. That is, trust but verify: push changes into production after automated testing, but regularly check whether your visitor counts are dropping, response times are rising, or other metrics are behaving abnormally.</p><p>While similar to continuous delivery, the two should not be confused.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec76"/>Building and deploying with Jenkins</h2></div></div></div><p>You've learned<a id="id916" class="indexterm"/> how to use GitHub webhooks<a id="id917" class="indexterm"/> to trigger a build process whenever new code is pushed to a repository. From pulling and testing a changed repository to notifying a chat server that a new build has occurred, Jenkins helps you to trigger deployment workflows. As your deployment needs become more complex than simply testing a single branch, the benefits of a more powerful CI tool become apparent. Jenkins provides tools to manage build permissions, task scheduling, triggering deploys, displaying build logs, and more. Let's deploy an application using Jenkins.</p><p>To install <a id="id918" class="indexterm"/>Jenkins, run the installer for your environment that can be found at <a class="ulink" href="http://jenkins-ci.org/">http://jenkins-ci.org/</a>. There are also services that allow you to install Jenkins in the "cloud", but we're going to build a local service. Upon successful installation, a browser will open up with the Jenkins "home page" UI, as shown here:</p><div><img src="img/1403OS_07_07.jpg" alt="Building and deploying with Jenkins"/></div><p>You will use this<a id="id919" class="indexterm"/> Jenkins <strong>dashboard</strong> often as you manage builds.</p><p>Note that Jenkins will, by default, run on port <code class="literal">8080</code>. You will, as with webhooks, need to map this location to a web-accessible URL directly, via proxy, via forwarding, or in some other way. Move to <strong>Manage Jenkins</strong> | <strong>Configure System</strong> and find the <strong>Jenkins Location</strong> section. Add the Jenkins URL, as shown in the following screenshot:</p><div><img src="img/1403OS_07_08.jpg" alt="Building and deploying with Jenkins"/></div><p>If you<a id="id920" class="indexterm"/> are running Jenkins on <code class="literal">localhost</code>, jump<a id="id921" class="indexterm"/> back to earlier in this chapter when we discussed using forwarding services, such as <a class="ulink" href="http://localtunnel.me/">http://localtunnel.me/</a>.</p><div><div><h3 class="title"><a id="note51"/>Note</h3><p>You may be warned about an unsecured Jenkins instance. This is a valid complaint! While we will not set up authentication, you should do so in any real production environment. It isn't hard. Visit <strong>Manage Jenkins</strong> | <strong>Configure Global Security</strong> to do so and/or visit <a class="ulink" href="https://wiki.jenkins-ci.org/display/JENKINS/Securing+Jenkins">https://wiki.jenkins-ci.org/display/JENKINS/Securing+Jenkins</a>.</p></div></div><p>The next thing to do is configure Jenkins to work with Node.js and GitHub. From the dashboard, navigate to <strong>Manage Jenkins</strong> | <strong>Manage Plugins</strong> | <strong>Available</strong>. You should see a list of available plugins, from which you will search for and install <em>NodeJS Plugin</em> and <em>GitHub Plugin</em>. This may take a while as these plugins, and their dependencies, are installed. If any of the installs prompt you to restart Jenkins, you will find instructions on how to do that in the installs list provided further on in this section.</p><p>The key integration that we'll have to do is with GitHub. In a new browser window, visit your GitHub account and generate a new access token.</p><p>Copy the generated key. You will now give Jenkins this access token so that it can perform operations on GitHub on your behalf, in particular around webhooks. Return to <strong>Manage Jenkins</strong> | <strong>Configure</strong>, and add this OAuth token and your user information to the <strong>GitHub Web Hook</strong> section, as shown here:</p><div><img src="img/1403OS_07_10.jpg" alt="Building and deploying with Jenkins"/></div><p>Run <strong>Test Credential</strong> to ensure that Jenkins can connect to GitHub using the token you've provided.</p><p>Finally, we <a id="id922" class="indexterm"/>need to provide our GitHub credentials<a id="id923" class="indexterm"/> to Jenkins so that it can pull our repository when changes happen. Navigate to <strong>Credentials</strong> and click on <strong>Global credentials</strong>. Select <strong>Username with Password</strong> and add your credentials, which will ensure that you give these credentials a useful name (you'll need to refer to these credentials later).</p><p>Because you have already built your own webhook-powered CI system, it may already be apparent to you why Jenkins is being configured in this way. In the end, we are configuring Jenkins to respond to push events on a GitHub repository, pull the changed repository, and automatically build it. To that end, we will need to provision Jenkins so that it is configured with Node and can, therefore, build Node repositories.</p><p>Navigate to <strong>Configure System</strong> and add a NodeJS installation, as shown here:</p><div><img src="img/1403OS_07_11.jpg" alt="Building and deploying with Jenkins"/></div><p>You will now configure the Node environment that Jenkins will use. You should match that environment with the environment your production servers will run in. Click on <strong>Add NodeJS</strong> and follow the instructions. You can select <strong>Install automatically</strong> and, when presented <a id="id924" class="indexterm"/>with installer options, select <strong>Install from nodejs.org</strong>. Make sure that you add any global npm packages you need—tools such<a id="id925" class="indexterm"/> as gulp, pm2, mocha, and others that are necessary to your build environment.</p><p>If you would rather manage the install yourself, just use the "Run Shell command" option and something like the following command, adding any global installs you'd like:</p><div><pre class="programlisting">
<strong>curl https://raw.githubusercontent.com/creationix/nvm/v0.24.1/install.sh | bash; nvm install 0.12; nvm alias default 0.12; npm install gulp -g</strong>
</pre></div><p>Remember to save your changes!</p><p>We're almost done configuring Jenkins for CI. The last step is to create a build project. Navigate to <strong>New Item</strong>, add a useful item name in the <strong>Item name</strong> field, select <strong>Freestyle project</strong>, and click on <strong>OK</strong>. Now, navigate to <strong>Source Code Management</strong>, select <strong>Git</strong>, add a GitHub repository name, select the credentials to access that repository, click on <strong>Save</strong>, and you'll be ready to build, as shown in the following screenshot:</p><div><img src="img/1403OS_07_12.jpg" alt="Building and deploying with Jenkins"/></div><p>Return to the Jenkins dashboard, and you'll see your build project listed. Click on the name, and select <strong>Build Now</strong> from the menu on the left-hand side. If all goes well, you'll see a build history table quickly populate, as shown here:</p><div><img src="img/1403OS_07_13.jpg" alt="Building and deploying with Jenkins"/></div><p>Click <a id="id926" class="indexterm"/>on the number and, if all is well, you'll see<a id="id927" class="indexterm"/> information on your build, indicating <em>no changes</em> (you have just pulled off a masterstroke), some information about the Git revision number, and so on. Now, the real test—make a change to your GitHub repository, either by pushing a change or simply editing a file using GitHub's editing tools. If you return to the dashboard, you will see that Jenkins has added a new build to <strong>Build Queue</strong>; shortly the build will complete, and you'll see the changes you've just made listed in your project's build history. You've just created a CI environment for your project!</p><p>Now, we need to deploy. We'll use Heroku to deploy, but feel free to try your provider of choice—as long as it <em>speaks</em> Git, Jenkins will be able to push your repository.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec77"/>Deploying to Heroku</h2></div></div></div><p>It might be useful to return to <a class="link" href="ch02.html" title="Chapter 2. Installing and Virtualizing Node Servers">Chapter 2</a>, <em>Installing and Virtualizing Node Servers</em>, and refresh<a id="id928" class="indexterm"/> your memory about how to build on Heroku. At the very least, you will need to install Heroku Toolbelt and authenticate. Once you are connected to Heroku via the toolbelt, clone the <code class="literal">express-webhook</code> repository we created earlier and enter that folder. Now, run <code class="literal">heroku create</code> to build a machine on Heroku. You should receive both a URL and a Git endpoint resembling the following:</p><div><pre class="programlisting">
<strong>https://floating-shelf-4947.herokuapp.com/ | https://git.heroku.com/floating-shelf-4947.git</strong>
<strong>Git remote heroku added</strong>
</pre></div><p>Now, it is time to push something for that server to run. Execute the following command to push the <code class="literal">express-webhook</code> application to Heroku:</p><div><pre class="programlisting">
<strong>git push heroku master</strong>
</pre></div><p>The express-webhook application is now deployed to Heroku. Heroku will have automatically built and started the application. Go ahead and visit the URL we received before in a browser. The next step is to use Jenkins to automatically deploy to Heroku whenever <a id="id929" class="indexterm"/>you make changes to the application repository.</p><p>You are now connected to two Git repositories, which you can see by running <code class="literal">git remote -v</code>:</p><div><pre class="programlisting">
<strong>heroku  https://git.heroku.com/floating-shelf-4947.git (fetch)</strong>
<strong>heroku  https://git.heroku.com/floating-shelf-4947.git (push)</strong>
<strong>origin  https://github.com/sandro-pasquali/express-webhook (fetch)</strong>
<strong>origin  https://github.com/sandro-pasquali/express-webhook (push)</strong>
</pre></div><p>The <code class="literal">origin</code> URL is our GitHub repository, and <code class="literal">heroku</code> represents the Git repository maintained by Heroku. We'll synchronize these two via Jenkins.</p><p>As Jenkins will eventually be doing the pushing for us, we need to give it permission to access your Heroku box. What we're going to do is generate a key pair for the <code class="literal">jenkins</code> user and associate these local SSH keys with Heroku, allowing Jenkins to perform pushes and so on. Log in as the<code class="literal"> jenkins</code> user, and run the following two commands:</p><div><pre class="programlisting">
<strong>ssh-keygen -t rsa</strong>
<strong>heroku keys:add ~/.ssh/id_rsa.pub</strong>
</pre></div><p>Jenkins can now authenticate with Heroku. All that is left to do is inform Jenkins about the Heroku repository and to instruct Jenkins to deploy to Heroku whenever it is informed, via the webhook we configured earlier, that changes have been made.</p><p>Return to your Jenkins project, click on <strong>Configure</strong>, and add the Heroku Git endpoint as another repository to the <strong>Source Code Management</strong> section by clicking on <strong>Add Repository</strong>. Fill in the <strong>Repository URL</strong> field to match the one you received earlier:</p><div><img src="img/1403OS_07_14.jpg" alt="Deploying to Heroku"/></div><p>Note that <a id="id930" class="indexterm"/>you will <em>not</em> fill in <strong>Credentials</strong> as we've earlier linked Jenkins to Heroku using SSH keys.</p><p>Now, click on the "Advanced" button underneath the new repository, and give it a name—you'll need this for the next step. Here we use <strong>heroku</strong>, but it can be anything:</p><div><img src="img/1403OS_07_15.jpg" alt="Deploying to Heroku"/></div><p>Now, Jenkins has been made aware of our GitHub repo and our Heroku repo. The final step is to configure Jenkins to push GitHub changes to Heroku.</p><p>Scroll down to <strong>Post-build Actions</strong> in your Jenkins project. Click on <strong>Add post-build action</strong> and select <strong>Git publisher</strong>. Fill out the form provided exactly as shown here:</p><div><img src="img/1403OS_07_16.jpg" alt="Deploying to Heroku"/></div><p>We are <a id="id931" class="indexterm"/>telling Jenkins to push to the<code class="literal"> master</code> branch of the <code class="literal">express-webhook</code> GitHub repository to <code class="literal">heroku</code> after each successful build. This is the deploy step. Save your changes—you're done!</p><p>To test that everything is working, modify the default route of <code class="literal">server.js </code>in your local clone of <code class="literal">express-webhook</code> such that it produces a different message, and push that change to GitHub. If you return to the Jenkins dashboard, you will soon see something like the following progress indicator on the build status of your project:</p><div><img src="img/1403OS_07_17.jpg" alt="Deploying to Heroku"/></div><p>If all goes well, your project will be listed on the dashboard as having been successfully built. If you refresh your Heroku URL, you will also see the changes you've made. Congratulations on successfully setting up continuous deployment for your project!</p><p>Now that you have the structure set up for CI and deployment, start adding tests and other build steps and run them either in your Node environment or using the many Jenkins tools available to you. Happy building!</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec40"/>Package maintenance</h1></div></div></div><p>JavaScript itself does not provide a native package management system; npm does this job for Node applications. A good package management strategy is, therefore, a key part of a good<a id="id932" class="indexterm"/> deployment strategy.</p><p>Packages offer the benefit of encapsulation. Running packages are accessible only through the API they've exported. This isolation reduces the number of potential bugs in a system, thus guarding the core functionality from accidental alteration. However, given that (opaque) packages can themselves require other packages as dependencies, the full dependency graph of an application can be difficult for a developer to easily see. For example, if the functionality of a package you have implemented suddenly changes, how do you debug it? Is the error in the package? Is it in one of its dependent packages?</p><p>Understanding <em>what is going on</em> in your npm dependency graph is essential when you are deploying Node applications. In this section, we will look at ways to stay up to date on package updates, use Git to manage private packages, track the health of an <em>entire</em> dependency graph, and look at best practices to set version rules in your application's <code class="literal">package.json</code> file.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec78"/>Understanding Semver</h2></div></div></div><p>
<strong>Semantic Versioning</strong> (<strong>Semver</strong>) is simply a set of rules that have been proposed to govern how<a id="id933" class="indexterm"/> dependencies in <a id="id934" class="indexterm"/>a system are declared. Npm enforces<a id="id935" class="indexterm"/> these rules in its package manager, so understanding how they govern dependency management is what will be discussed here.</p><p>Take for example the following npm package file:</p><div><pre class="programlisting">"devDependencies": {
  "browserify": "^6.1.0",
  "gulp": "~3.8.8",
  "foobar": " &gt;=1.2.3 &lt;1.3.0"
}</pre></div><p>Each dependency is given a version number corresponding to a version in the npm repository. Some of these numbers are further modified by tokens, for example, a caret (<code class="literal">^</code>) or a tilde (<code class="literal">~</code>), as well as version ranges. Let's look at what each segment of semantically versioned numbers signify and how various tokens are used to modulate those segments.</p><p>A version number is broken into three segments, which are shown here:</p><div><img src="img/1403OS_07_18.jpg" alt="Understanding Semver"/></div><p>Semver <a id="id936" class="indexterm"/>concretely describes<a id="id937" class="indexterm"/> allowable package version ranges as well as implying the current stability or state of a package—whether the package is stable, whether it is mature, and so on. The numbering proceeds in order: 1.0.1 precedes 1.0.2, which precedes 2.0.0.</p><p>The significance of the changes that Semver describes proceeds from left to right, where a change in the major version of a package typically describes changes that break compatibility with lower versions—2.0 is not compatible with 1.0. According to <a class="ulink" href="http://semver.org">semver.org</a>, you <a id="id938" class="indexterm"/>should use version numbers in this way:</p><div><blockquote class="blockquote"><p><em>"Given a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards-compatible manner, and PATCH version when you make backwards-compatible bug fixes."</em></p></blockquote></div><p>Then, Semver allows you to set acceptable range limits on the versions of dependencies in your application with an eye toward providing useful indications of the level of impact implied by <a id="id939" class="indexterm"/>version changes. Some common usage examples are given here:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">"3" indicates that only the major version (3) must be satisfied, ignoring minor or patch values—3.0.0, 3.6.3, and 3.99.99 are all acceptable.</li><li class="listitem" style="list-style-type: disc">"3.4.5" indicates that <em>only </em>that version is acceptable, with no variation.</li><li class="listitem" style="list-style-type: disc">"&lt;, &lt;=, &gt; and &gt;=" range comparators work as expected in many programming languages and can be used to set controlled ranges. &gt;= 3.0.1 &lt;= 3.2.1 accepts 3.0.2 and 3.1.9 but not 3.0.0 or 3.2.2.</li><li class="listitem" style="list-style-type: disc">1.3.4 &gt;= 3.0.1 &lt;= 3.2.1 accepts the version range as described in the preceding point <em>or</em> the 1.3.4 version.</li><li class="listitem" style="list-style-type: disc">Being equivalent to &gt;= 0.0.0, "*" indicates that <em>any</em> version is acceptable.</li><li class="listitem" style="list-style-type: disc">Hyphen ranges (-) describe inclusive sets. The hyphen range 1.0.0 - 2.0.0 matches any package with a major version of 1.</li><li class="listitem" style="list-style-type: disc">x-ranges provide a shorthand for minor and patch ranges; 1.2.x is equivalent to &gt;= 1.2.0 &lt;= 1.3.0 and 1.x is equivalent to &gt;= 1.0.0 &lt;= 2.0.0.</li><li class="listitem" style="list-style-type: disc">Tilde(~) ranges<a id="id940" class="indexterm"/> allow patch-level changes if a minor version is specified and minor-level changes if it is not. ~1.3.2 is equivalent to &gt;= 1.3.2 &lt; 1.4.0, ~1.3 is equivalent to &gt;= 1.3.0 &lt; 1.4.0, and ~1 is equivalent to &gt;= 1.0.0 &lt; 2.0.0.</li><li class="listitem" style="list-style-type: disc">Caret(^) ranges allow changes that do not modify the leftmost nonzero digit. ^1.2.0 is equivalent to &gt;= 1.2.0 &lt;= 2.0.0, ^0.2.1 is equivalent to &gt;= 0.2.1 &lt;= 0.3.0, and ^0.0.2 is equivalent to &gt;= 0.0.2 &lt; 0.0.3.</li></ul></div><div><div><h3 class="title"><a id="note52"/>Note</h3><p>For more <a id="id941" class="indexterm"/>details, visit <a class="ulink" href="https://github.com/npm/node-semver">https://github.com/npm/node-semver</a> and <a class="ulink" href="https://docs.npmjs.com/misc/semver">https://docs.npmjs.com/misc/semver</a>. A useful tool to check versions for specific packages against Semver tuples can be found at <a class="ulink" href="http://semver.npmjs.com/">http://semver.npmjs.com/</a>.</p></div></div><p>As we<a id="id942" class="indexterm"/> saw when we were using<a id="id943" class="indexterm"/> the <code class="literal">npm install &lt;packagename&gt; --save</code> construct, npm defaults to caret prefixing—npm will assign the newly installed dependency a version of <code class="literal">^&lt;latest version&gt;</code> in <code class="literal">package.json</code>. If you'd like to have a default tilde prefix, use <code class="literal">npm config set save-prefix="~"</code>.</p><p>Another important feature of Semver for maintainability is prerelease tags. These tags allow you to release a package version that is not ready for production (prerelease), which you might do in order to get it in the hands of other people on your team, beta testers, and so on, while ensuring that the default version will be installed on a "normal" install.</p><p>When you publish an npm package, you can use the <code class="literal">--tag</code> argument to tag that release. The published package is now no longer tagged as "latest" but as whichever tag you've assigned it. Let's say we tagged<a id="id944" class="indexterm"/> the <strong>alpha.7</strong> package (and changed the version field of the package with <code class="literal">npm version &lt;version&gt;-alpha.7</code>).</p><p>Now, consider the case where that package is being listed as a dependency somewhere in <em>userland</em>:</p><div><pre class="programlisting">
<strong>"my-package" : "&gt;=1.03-alpha.1"</strong>
</pre></div><p>When this package is installed, npm will install the alpha.7 package—Semver ranges would apply as alpha.7 is greater than alpha.1.</p><p>Let's define our package in this way:</p><div><pre class="programlisting">
<strong>"my-package" : "&gt;=1.03"</strong>
</pre></div><p>In the preceding case, the alpha.7 package will <em>not</em> be installed. In this way, we can see that by<a id="id945" class="indexterm"/> the Semver rule, prerelease<a id="id946" class="indexterm"/> tags only apply if the comparator (what you've set as the version of the package) also contains a prerelease tag. In this way, you can safely release experimental breaking changes in tagged packages as only someone who is fully aware of the tag name (and its alpha nature) would do the work required to be done to use it, while others continue to use production versions.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec79"/>Managing packages with npm</h2></div></div></div><p>One of the <a id="id947" class="indexterm"/>most important (and tricky) application <a id="id948" class="indexterm"/>management strategies you will deploy is choosing packages and updating package versions. In this section, good strategies to maintain your npm packages will be discussed—how to keep your packages up to date, how to lock dependencies, how to load packages from Git repositories rather than npm, and so on.</p><p>Generally, you'll want to balance the relative safety of the rigid Semver constraints with the need to stay as up to date as possible with the latest version of an important package and to keep your dependency tree predictable and clean. Developing a good strategy here will help with application maintenance.</p><p>Take a look<a id="id949" class="indexterm"/> at the following six aspects of package maintenance:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Maintaining awareness of the full npm dependency tree</li><li class="listitem" style="list-style-type: disc">Tracking divergence between the latest version and the installed version of a package</li><li class="listitem" style="list-style-type: disc">Removing unused packages defined in your package file</li><li class="listitem" style="list-style-type: disc">Ensuring that all needed dependencies are installed</li><li class="listitem" style="list-style-type: disc">Ensuring that the dependencies you need are the ones you have</li><li class="listitem" style="list-style-type: disc">Using private or other modules not held in the npm repository</li></ul></div><p>Other package management systems enforce the rule that a single version of a package exists across all dependencies; npm does not. Packages typically require other packages, so multiple versions of the same package can enter into an npm build. An application may have A and B dependencies, with the A package requiring version 1.0.1 of the C package, and with the B package requiring version 2.0.1 of the C package.</p><p>Think about what it means to say that, on every npm install, there is limited (often barely thought out) control over the package versions inserted into a dependency tree—there is no guarantee that your application will run the same code at any given time. What's been installed at one moment may fundamentally change if you reinstall one hour later or even one second later. That's an extraordinary level of risk to introduce into production systems—similar to a software manager being indifferent to who makes changes, where, or when.</p><p>The<a id="id950" class="indexterm"/> first step is getting a full breakdown of what has been<a id="id951" class="indexterm"/> installed. Use <code class="literal">npm ls</code> for this, which returns something like the following:</p><div><pre class="programlisting">
<strong>...</strong>
<strong>├─┬ mocha@1.21.5</strong>
<strong>│ ├── commander@2.3.0</strong>
<strong>│ ├─┬ debug@2.0.0</strong>
<strong>│ │ └── ms@0.6.2</strong>
<strong>│ ├── diff@1.0.8</strong>
<strong>│ ├── escape-string-regexp@1.0.2</strong>
<strong>│ ├─┬ glob@3.2.3</strong>
<strong>│ │ ├── graceful-fs@2.0.3</strong>
<strong>│ │ ├── inherits@2.0.1</strong>
<strong>│ │ └─┬ minimatch@0.2.14</strong>
<strong>│ │   ├── lru-cache@2.5.0</strong>
<strong>│ │   └── sigmund@1.0.0</strong>
<strong>│ ├── growl@1.8.1</strong>
<strong>...</strong>
</pre></div><p>If you want this tree to be represented as JSON, use the <code class="literal">--json</code> flag: <code class="literal">npm ls --json</code>. To include the contents of each package's <code class="literal">description</code> field in the output, use <code class="literal">npm ls --long</code>. You can use <code class="literal">npm ls -g</code> to get this tree for <em>globally</em> installed packages. If you'd just like to know which packages are installed globally, try <code class="literal">ls `npm root -g`</code>.</p><p>Keeping up to date on the current versions of the installed packages is something you should be doing regularly. It doesn't take long for the version of a package to become outdated. Npm provides the <code class="literal">npm outdated</code> tool for this purpose (here, it is used with the <code class="literal">--long</code> "extended information" argument). The following screenshot shows this:</p><div><img src="img/1403OS_07_19.jpg" alt="Managing packages with npm"/></div><p>Here, we see that the <code class="literal">package.json</code> file within the <code class="literal">node_modules/redis</code> folder of our application is at version 0.8.2 (<strong>current</strong>), that the <strong>latest</strong> version is 0.12.1, and that the <strong>wanted</strong> Semver for <em>redis</em> in the root <code class="literal">package.json</code> file will match up to version 0.12.1. This indicates that it has been quite a while since <code class="literal">npm install</code> was run within this application. A <a id="id952" class="indexterm"/>very useful global tool to perform these sorts of checks is <code class="literal">npm-check</code> (<a class="ulink" href="https://github.com/dylang/npm-check">https://github.com/dylang/npm-check</a>), which delivers more detailed information, as shown in the following screenshot:</p><div><img src="img/1403OS_07_20.jpg" alt="Managing packages with npm"/></div><p>Additionally, this <a id="id953" class="indexterm"/>tool offers an interactive UI that will <a id="id954" class="indexterm"/>automatically update the packages you choose.</p><p>Another type of residue that accumulates over time is unused packages. These can be installed in <code class="literal">node_modules</code> but no longer linked, or these can be defined for a package but not required anywhere in the application's code.</p><p>To remove packages that are <em>installed</em> but no longer listed in <code class="literal">package.json</code>, you can use <code class="literal">npm prune</code>. Note that this is simply a technique for cleaning up the <code class="literal">node_modules</code> folder within an individual package's folder; it is not a smart, global tool to remove unused packages across the entire tree.</p><p>The <code class="literal">dependency-check</code> module (<a class="ulink" href="https://github.com/maxogden/dependency-check">https://github.com/maxogden/dependency-check</a>) is<a id="id955" class="indexterm"/> another tool to find unnecessary packages. Assuming <a id="id956" class="indexterm"/>that such an unused dependency exists, dependency-check will find it:</p><div><pre class="programlisting">
<strong>dependency-check package.json --unused</strong>
<strong>Fail! Modules in package.json not used in code: express</strong>
</pre></div><p>Conversely, packages may be required in the application code but not listed in a package file. This happens occasionally, when a necessary package is installed during development but not saved to <code class="literal">package.json</code>, possibly because the user forgets to use the <code class="literal">--save</code> option or for some other reason. The <code class="literal">dependency-check</code> command will walk all files in your codebase and find such cases, as shown here:</p><div><pre class="programlisting">
<strong>dependency-check package.json</strong>
<strong>Fail! Dependencies not listed in package.json: express</strong>
</pre></div><p>Note <a id="id957" class="indexterm"/>that it is expected that the entry point to your application<a id="id958" class="indexterm"/> is listed in <code class="literal">package.json</code> as <code class="literal">dependency-check</code> needs to know where your application tree is rooted. You should, therefore, ensure that your packages all have a <code class="literal">main</code> attribute pointing to an existing file. If you need to add further files to check, use the <code class="literal">--entry</code> argument as follows:</p><div><pre class="programlisting">
<strong>dependency-check package.json --entry a.js b.js [...]</strong>
</pre></div><p>To have a <code class="literal">main</code> entry point to your application is an important general practice that you should follow.</p><p>One final tool that can help speed up your npm builds is <code class="literal">npm dedupe</code>. When triggered, npm attempts to reduce the number of redundant package installs, "flattening" the tree somewhat, and, therefore, reducing install time. Consider this dependency tree:</p><div><pre class="programlisting">A
└─┬ B
│ └── C
└─┬ D
  └── C</pre></div><p>Here, the A package depends on the B and D packages, which each depend on the C package. Normally, C would be installed twice, once for each parent. However, if the Semver that B and D use to target C matches a single version of C, npm will reduce the tree in a way that both B and D pull from pull from the same, single, installed version of C. Note that Semver rules still apply—npm will not break version requirements solely to reduce the number of installs required.</p><p>It should be clear that many of the tools we've been looking at would fit nicely into a build/deploy process, issuing a warning if, for example, a given package is not used or is out of date. npm is<a id="id959" class="indexterm"/> itself an npm package (<a class="ulink" href="https://github.com/npm/npm">https://github.com/npm/npm</a>)—try using npm programmatically within your build process to perform some of these checks.</p><div><div><div><div><h3 class="title"><a id="ch07lvl3sec37"/>Designing a dependency tree</h3></div></div></div><p>All dependencies are not created equal. Some are necessary when in development mode but are not<a id="id960" class="indexterm"/> meaningful in production. The location and <a id="id961" class="indexterm"/>versions of dependencies can also vary as you may not always use packages in the npm repository, or you may want to use specialized versions.</p><p>There are<a id="id962" class="indexterm"/> three types of dependencies used in npm package files: <strong>dependencies</strong>, <strong>devDependencies</strong>, and <strong>peerDependencies</strong>. Let's look at the <a id="id963" class="indexterm"/>differences.</p><p>Simple dependencies <a id="id964" class="indexterm"/>are likely what you're most familiar with. These dependencies are <em>always</em> installed, regardless of context. You should place dependencies that <em>must</em> exist in this collection, typically the packages your production build will need.</p><p>When you are developing and building, you will often use tools, such as Mocha or gulp. Once a validated build is ready to be placed in production, however, there is no need for those packages to accompany it. The packages you do not need in production should be placed in the devDependencies collection. While npm will always install both dependencies and devDependencies, you can (and should) exclude devDependencies from the deploy install using the <code class="literal">--production</code> flag, which is as follows:</p><div><pre class="programlisting">
<strong>npm install --production</strong>
</pre></div><p>Usefully, if you run the <code class="literal">npm config set production</code> command, the <code class="literal">~/.npmrc</code> file will be updated such that all future installs will automatically set the <code class="literal">--production</code> flag. For example, your provisioner can do this configuration.</p><p>Finally, peerDependencies deals with the case of plugins. You're familiar with various Grunt plugins. While these are loaded via the npm ecosystem, they need their host program (Grunt) in order to function. You might think that each of these plugins should just <code class="literal">require('grunt')</code>—but which version of Grunt? Any one of these plugins may depend on a specific version of its host program, but those host programs are also direct dependencies of the package. So, consider this declaration:</p><div><pre class="programlisting">"dependencies": {
  "grunt": "1.2.3",
  "gulp-plugin": "1.0.0" // requires grunt@2.0.0
}</pre></div><p>The preceding declaration leads to a dangerous conflict:</p><div><pre class="programlisting">└── grunt@1.2.3
└─┬ gulp-plugin@1.0.0
  └── grunt@2.0.0</pre></div><p>So, peerDependencies should be used in plugin-type packages that have specific host-program needs, allowing the plugin to "carry along" their needed host. If npm attempts to install a different version of that host program, an error is thrown. This, of course, leads to another problem—any given plugin can cause an install to fail if its required host program is not version-compatible with the one the main application is demanding. The complexities<a id="id965" class="indexterm"/> of peerDependencies remain an ongoing discussion in the Node community (<a class="ulink" href="https://github.com/npm/npm/issues/5080">https://github.com/npm/npm/issues/5080</a>).</p><p>As mentioned, npm does not put many limits on package versions, allowing multiple versions of the<a id="id966" class="indexterm"/> same package to exist simultaneously<a id="id967" class="indexterm"/> and, indeed, for versions (and, therefore, package functionality) to change unexpectedly.</p><p>One way to secure your application's state is to lock a dependency tree using <code class="literal">npm shrinkwrap</code>. This command will trigger npm to generate the <code class="literal">npm-shrinkwrap.json</code> file containing explicit references to specific versions. The file generated contains definitions such as the following:</p><div><pre class="programlisting">"moment": {
  "version": "2.8.4",
  "from": "moment@^2.8.3",
  "resolved": "https://registry.npmjs.org/moment/-/moment-2.8.4.tgz"
},
"node-uuid": {
  "version": "1.4.2",
  "from": "node-uuid@^1.4.1",
  "resolved": "https://registry.npmjs.org/node-uuid/-/node-uuid-1.4.2.tgz"
}</pre></div><p>It should be clear how this syntax ensures that future installs will be identical. Note that this is a heavy-handed approach that you probably don't need very often. However, in production situations where you are deploying identical code across multiple machines, shrinkwrapped "bundles" may be exactly what you need.</p><p>Another option to ensure visibility in the behavior of your packages is to control them in their entirety. You are able to link dependencies to Git repositories, either public or private. For example, you can load Express directly from its GitHub repository:</p><div><pre class="programlisting">dependencies : {
  "express" : "strongloop/express"
}</pre></div><p>npm assumes GitHub, so you are able to use the compressed syntax, as shown in the preceding code.</p><p>You can also link to a private Git repository using <code class="literal">https/oauth</code>:</p><div><pre class="programlisting">
<strong>"package-name": "git+https://&lt;github_token&gt;:x-oauth-basic@github.com/&lt;user&gt;/&lt;repo&gt;.git"</strong>
</pre></div><p>You can also use SSH as follows:</p><div><pre class="programlisting">
<strong>"package-name": "git+ssh://git@github.com/&lt;user&gt;/&lt;repo&gt;.git"</strong>
</pre></div><p>The <a id="id968" class="indexterm"/>npm package manager is an essential part of the <a id="id969" class="indexterm"/>Node ecosystem, and Node applications are typically composed of dozens, even hundreds, of packages. Developing a strategy around package management is an important consideration if you plan to release and maintain a large-scale Node application.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec41"/>Summary</h1></div></div></div><p>In this chapter, you learned how to deploy a local build into a production-ready environment. The powerful Git webhook tool was demonstrated as a way of creating a continuous integration environment, and this knowledge was carried forward into the creation of a full build/deploy pipeline that connected a GitHub repository to a Heroku deployment via a CI environment configured using Jenkins. We also covered the semantic versioning system that npm uses and even how to use Semver, npm methods, and some helper libraries to keep our package trees clean and predictable.</p><p>From basic JavaScript programs to the deployment of full applications, in this book, we took a tour of Node's design and goals. We've worked through ways in which Node's event-driven architecture influences how we design networked software by building on the foundational concept of streams. With an eye toward the creation of fast, deployable systems, we worked through virtualization strategies, compiler optimizations, load balancing, and vertical and horizontal scaling strategies. Additionally, the power of composing software out of small, focused programs was considered by introducing the power of micro services, interprocess messaging, and queues as one way to build distributed systems.</p><p>Keeping in mind that software is written by fallible humans, we also covered strategies for testing and maintaining running applications, learning to expect failure, and planning for it, with the help of both native and third-party logging and monitoring tools. We learned debugging techniques and optimization strategies aimed at reducing bottlenecks at the local and network levels and also how to find their source when they inevitably appear. With the goal of making development simpler, we looked at how to make effective use of integration tools and versioning systems, provision virtual machines and test with headless browsers, enable developers to work freely and take risks, and push changes with the confidence that smart deployment strategies confer. Constructing a smart build pipeline, you learned about the power of full-stack JavaScript, transpilation, live updates, and continuous testing and integration.</p><p>You are encouraged to modify and extend the example code to improve it or, otherwise, change it to your needs. The hope is that, as you come to appreciate the power of Node.js, the npm ecosystem, and open source software, you will begin to naturally design your applications so that they will require few changes when pushed into production, and that you will share your discoveries so that others can do the same thing.</p></div></body></html>