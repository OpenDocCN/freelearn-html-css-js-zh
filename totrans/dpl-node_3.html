<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Scaling Node</h1></div></div></div><p>Like <em>concurrency </em>and <em>parallelism</em>, <em>scalability</em> and <em>performance </em>are not the same thing.</p><div><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><em>"The terms "performance" and "scalability" are commonly used interchangeably, but the two are distinct: performance measures the speed with which a single request can be executed, while scalability measures the ability of a request to maintain its performance under increasing load. For example, the performance of a request may be reported as generating a valid response within three seconds, but the scalability of the request measures the request's ability to maintain that three-second response time as the user load increases."</em></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<em>Pro Java EE 5, Steve Haines</em></td></tr></table></div><p>It is not unusual for a reviewer to assert that Node cannot scale across cores and is, therefore, unable to optimize <a id="id226" class="indexterm"/>performance on a given machine. This belief is based on two false impressions—that Node is<a id="id227" class="indexterm"/> "not good at" CPU-intensive tasks and that it <em>cannot scale</em> because its process can only leverage a single core. These claims are often stretched further into assertions about how Node's claim of being nonblocking is false, primarily by imagining locked threads and underutilized hardware.</p><p>Scalable applications remain responsive under increasing load. Scalable applications imply that more nodes can be added to, and removed from, a system depending on fluctuations in both client connections and resource needs (such as more memory or storage space). Node aims to make it easy to conceptualize, describe, and implement scalable networked applications. The primary focus is on creating a toolkit to build structures out of many nodes connected through evented network streams communicating through standard protocols. Distributed systems are concerned with failure more than with performance and the question that arises is: how can we swap, add, and remove nodes intelligently within a running system?</p><div><div><h3 class="title"><a id="note12"/>Note</h3><p>Solving the <a id="id228" class="indexterm"/>
<strong>C10K problem</strong>, which is<em> the problem of optimizing network sockets to handle a large number of clients at the same time</em> (<a class="ulink" href="https://en.wikipedia.org/wiki/C10k_problem">https://en.wikipedia.org/wiki/C10k_problem</a>) is a key design goal for<a id="id229" class="indexterm"/> many modern application tools and environments, including Node.</p></div></div><p>We will look at two common scaling strategies—vertical and horizontal scaling. Vertical scaling (<em>scaling up</em>) involves increasing the ability of a single server to handle increasing load, usually<a id="id230" class="indexterm"/> by increasing the number of CPUs, memory, storage<a id="id231" class="indexterm"/> space, and so on, on a single box. Horizontally scaling systems (<em>scaling out</em>) respond to a load by adding or subtracting servers or other network resources. Deploying a scalable Node solution can be done by utilizing both of these techniques either individually or in tandem.</p><div><div><div><div><h1 class="title"><a id="ch03lvl1sec17"/>Scaling vertically across multiple cores</h1></div></div></div><p>As we discussed in <a class="link" href="ch01.html" title="Chapter 1. Appreciating Node">Chapter 1</a>, <em>Appreciating Node</em>, <code class="literal">libuv</code> is used within the Node environment to manage multiple I/O threads. The OS itself also schedules threads, distributing<a id="id232" class="indexterm"/> the work required by various processes. Node provides a way for a developer to take advantage of this OS-level scheduling by spawning and forking many processes. In this section, we will learn how to distribute your program's tasks across independent processes generally and how to distribute a Node server's load across multiple cooperating server processes.</p><p>Modern software development is no longer the realm of monolithic programs. Modern applications are distributed and decoupled. We now build applications that connect users with resources distributed across the Internet. Many users are accessing shared resources simultaneously. A complex system is easier to understand if the whole is understood as a collection of interfaces to programs that solve one or a few clearly defined, related problems. In such a system, it is expected (and desirable) that processes should not sit idle.</p><p>While a single Node process runs on a single core, any number of Node processes can be "spun up" through the use of the <code class="literal">child_process</code> module. Basic usage of this module is straightforward: we fetch a <code class="literal">ChildProcess</code> object and listen for data events. This example will call the Unix command <code class="literal">ls</code>, listing the current directory:</p><div><pre class="programlisting">var spawn = require('child_process').spawn;
var ls  = spawn('ls', ['-lh', '.']);
ls.stdout.on('readable', function() {
 var d = this.read();
 d &amp;&amp; console.log(d.toString());
});
ls.on('close', function(code) {
 console.log('child process exited with code ' + code);
});</pre></div><p>Here, we use <code class="literal">spawn</code> on the <code class="literal">ls</code> process (list directory) and read from the resulting readable stream, receiving something like this:</p><div><pre class="programlisting">
<strong>-rw-r--r-- 1 root root  43 Jul 9 19:44 index.html</strong>
<strong>-rw-rw-r-- 1 root root 278 Jul 15 16:36 child_example.js</strong>
<strong>-rw-r--r-- 1 root root 1.2K Jul 14 19:08 server.js</strong>

<strong>child process exited with code 0</strong>
</pre></div><p>Any <a id="id233" class="indexterm"/>number of child processes can be spawned in this way. It is important to note here that when a child process is spawned or otherwise created, the OS itself assigns the responsibility for that process to a given CPU. Node is not responsible for how an OS allocates resources. The upshot is that on a machine with eight cores, it is likely that spawning eight processes will result in each being allocated to independent processors. In other words, child processes are automatically spread by the OS across CPUs, putting the lie to claims that Node cannot take full advantage of multicore environments.</p><div><div><h3 class="title"><a id="note13"/>Note</h3><p>Each new Node process (child) is allocated 10 MB of memory and represents a new V8 instance that will take at least 30 milliseconds to start up. While it is unlikely that you will spawn many thousands of these processes, understanding how to query and set OS limits on user-created processes is beneficial. You can use <code class="literal">htop</code> or <code class="literal">top</code> to report the number of processes currently running, or you can use <code class="literal">ps aux | wc –l</code> from the command line. The Unix command <code class="literal">ulimit</code> (<a class="ulink" href="http://ss64.com/bash/ulimit.html">http://ss64.com/bash/ulimit.html</a>) provides important information on user limits on an OS. Passing <code class="literal">ulimit</code> the <code class="literal">–u</code> argument will show the maximum number of user processes that can be spawned. Changing the limit is accomplished by passing it as an argument—<code class="literal">ulimit –u 8192</code>.</p></div></div><p>The <code class="literal">child_process </code>module represents a class exposing four main methods: <code class="literal">spawn</code>, <code class="literal">fork</code>, <code class="literal">exec</code>, and <code class="literal">execFile</code>. These methods return a <code class="literal">ChildProcess</code> object that extends <code class="literal">EventEmitter</code>, exposing an interface to child events, and a few functions helpful to manage <a id="id234" class="indexterm"/>child processes. We'll take a look at its main methods and follow up with a discussion of the common <code class="literal">ChildProcess</code> interface.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec32"/>spawn(command, [arguments], [options])</h2></div></div></div><p>This <a id="id235" class="indexterm"/>powerful command allows a Node program to start <a id="id236" class="indexterm"/>and interact with processes spawned via system commands. In the preceding example, we used <code class="literal">spawn</code> to call a native OS process, <code class="literal">ls</code>, passing that command the arguments <code class="literal">'-lh'</code> and <code class="literal">'.'</code>. In this way, any process can be started just as one might start it via a command line. The method takes three arguments:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>command</strong>: This is <a id="id237" class="indexterm"/>a command to be executed by the OS shell</li><li class="listitem" style="list-style-type: disc"><strong>arguments</strong>: These<a id="id238" class="indexterm"/> are optional command-line arguments sent as an array</li><li class="listitem" style="list-style-type: disc"><strong>options</strong>: This is<a id="id239" class="indexterm"/> an optional map of settings for <code class="literal">spawn</code></li></ul></div><p>The options for <code class="literal">spawn</code> allow its behavior to be carefully customized:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>cwd</strong> (string): By<a id="id240" class="indexterm"/> default, this command will understand its current working directory to be the same as that of the Node process calling <code class="literal">spawn</code>. Change that setting using this directive.</li><li class="listitem" style="list-style-type: disc"><strong>env</strong> (object): This<a id="id241" class="indexterm"/> is used to pass environment variables to a child process, for instance, we spawn a child process with an environment object, such as:<div><pre class="programlisting">{
  name : "Sandro",
  role : "admin"
}</pre></div><p>The child process environment will have access to the values specified in the preceding code.</p></li><li class="listitem" style="list-style-type: disc"><strong>detached</strong> (Boolean): When a parent process spawns a child process, both processes<a id="id242" class="indexterm"/> form a group, and the parent process is normally the leader of that group. To make a child process the group leader, use <code class="literal">detached</code>. This allows the child process to continue running even after the parent process exits. Because the parent process <a id="id243" class="indexterm"/>waits for the child process to exit by default, you can call <code class="literal">child.unref()</code> to tell the parent process's event loop that it should not count the child reference and exit if no other work exists.</li><li class="listitem" style="list-style-type: disc"><strong>uid</strong> (number): Set <a id="id244" class="indexterm"/>the uid (user identity) for the child process in terms of standard system permissions, such as a uid that has privileges to execute on the child process.</li><li class="listitem" style="list-style-type: disc"><strong>gid</strong> (number): Set<a id="id245" class="indexterm"/> the gid (group identity) for the child process in terms of standard system permissions, such as a gid that has execute privileges on the child process.</li><li class="listitem" style="list-style-type: disc"><strong>stdio</strong> (string or array): Child <a id="id246" class="indexterm"/>processes have file descriptors, the first three being <code class="literal">process.stdin</code>, <code class="literal">process.stdout</code>, and <code class="literal">process.stderr</code> standard I/O descriptors in that order (fds = 0,1,2). This directive allows those descriptors to be redefined, inherited, and so on.</li></ul></div><p>Normally, to read the output of the following child process program, a parent process would<a id="id247" class="indexterm"/> listen on <code class="literal">child.stdout</code>:</p><div><pre class="programlisting">process.stdout.write(new Buffer("Hello!"));</pre></div><p>If, instead, we <a id="id248" class="indexterm"/>wanted a child to inherit its parent's <code class="literal">stdio</code> such that when the child writes to <code class="literal">process.stdout</code>, what is emitted is piped through to the parent process's <code class="literal">process.stdout</code> stream, we would pass the relevant parent file descriptors to the child, overriding its own:</p><div><pre class="programlisting">spawn("node", ['./reader.js', './afile.txt'], {
  stdio: [process.stdin, process.stdout, process.stderr]
});</pre></div><p>In this case, the child's output will pipe straight through to the parent process's standard output channel. Also, see <code class="literal">fork</code>, in the upcoming paragraphs, for more information on this kind of pattern.</p><p>Each of the three (or more) file descriptors can take one of six values:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>'pipe'</strong>: This<a id="id249" class="indexterm"/> creates a pipe between the child process and the parent process. As the first three child file descriptors are already exposed to the parent process (<code class="literal">child.stdin</code>, <code class="literal">child.stdout</code>, <code class="literal">child.stderr</code>), this is only necessary in more complex child implementations.</li><li class="listitem" style="list-style-type: disc"><strong>'ipc'</strong>: Create an<a id="id250" class="indexterm"/> IPC channel to pass messages between a child process and a parent process. A child process can have a maximum of one IPC file descriptor. Once this connection is established, the parent process can communicate with the child process via <code class="literal">child.send</code>. If the child sends JSON messages through this file descriptor, those emissions can be caught using <code class="literal">child.on("message")</code>. If you are running a Node program as a child, it is likely a better choice to use <code class="literal">ChildProcess.fork</code>, which has this messaging channel built in.</li><li class="listitem" style="list-style-type: disc"><strong>'ignore'</strong>: The <a id="id251" class="indexterm"/>file descriptors 0–2 will have <code class="literal">/dev/null</code> attached to them. For others, the referenced file descriptor will not be set on the child.</li><li class="listitem" style="list-style-type: disc"><strong>A stream object</strong>: This allows the parent to share a stream with the child. For demonstration <a id="id252" class="indexterm"/>purposes, given a child that will write the same content to any provided <code class="literal">Writable </code>stream, we could do something like this:<div><pre class="programlisting">var writer = fs.createWriteStream("./a.out");
writer.on('open', function() {
  var cp = spawn("node", ['./reader.js'], {
    stdio: [null, writer, null]
  });
});</pre></div><p>The child will now fetch its content and pipe it to whichever output stream it has been sent to:</p><div><pre class="programlisting">fs.createReadStream('cached.data').pipe(process.stdout);</pre></div></li><li class="listitem" style="list-style-type: disc"><strong>An integer</strong>: This is <a id="id253" class="indexterm"/>a file descriptor ID.</li><li class="listitem" style="list-style-type: disc"><strong>null, undefined</strong>: These <a id="id254" class="indexterm"/>are the default values. For file descriptors 0–2 (<code class="literal">stdin</code>, <code class="literal">stdout</code>, <code class="literal">stderr</code>), a pipe is created. Others default to <em>ignore</em>.</li></ul></div><p>In <a id="id255" class="indexterm"/>addition to passing <code class="literal">stdio</code> settings as an array, certain <a id="id256" class="indexterm"/>common groupings can be implemented by passing a shortcut string value:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">'ignore' = ['ignore', 'ignore', 'ignore']</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">'pipe' = ['pipe', 'pipe', 'pipe']</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">'inherit' = [process.stdin, process.stdout, process.stderr] or [0,1,2]</code></li></ul></div><p>It should be noted that the ability to spawn any system process means that one can use Node to run other application environments installed on the OS. If we had the popular PHP language installed, the following would be possible:</p><div><pre class="programlisting">var spawn = require('child_process').spawn;

var php = spawn("php", ['-r', 'print "Hello from PHP!";']);

php.stdout.on('readable', function() {
  var d;
  while(d = this.read()) {
    console.log(d.toString());
  }
});

// Hello from PHP!</pre></div><p>Running a more interesting, larger program would be just as easy.</p><p>Apart from the ease with which one can run Java, Ruby, or other programs through Node using <a id="id257" class="indexterm"/>this technique, asynchronously, we also have here a <a id="id258" class="indexterm"/>good answer to a persistent criticism of Node: JavaScript is not as fast as other languages for crunching numbers or doing other CPU-heavy tasks. This is true in the sense that Node is primarily optimized for I/O efficiency and helping with the management of high-concurrency applications, and JavaScript is an interpreted language without a strong focus on heavy computation.</p><p>However, using <code class="literal">spawn</code>, one can very easily pass massive computations and long-running routines on analytic engines or calculation engines to separate processes in other environments. Node's simple event loop will notify the main application when those operations are done, seamlessly integrating the resultant data. Meantime, the main application is free to keep serving clients.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec33"/>fork(modulePath, [arguments], [options])</h2></div></div></div><p>Just like <code class="literal">spawn</code>, <code class="literal">fork</code> starts a child process but is designed to run Node programs with the<a id="id259" class="indexterm"/> added benefit of having a communication built in. Rather<a id="id260" class="indexterm"/> than passing a system command to <code class="literal">fork</code> as its first argument, we pass the path to a Node program. As with <code class="literal">spawn</code>, command-line options can be sent as a second argument, accessible via <code class="literal">process.argv</code> in the forked child process.</p><p>An optional object can be passed as its third argument, with the following parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>cwd</strong> (string): By default, this command will understand its current working directory to <a id="id261" class="indexterm"/>be the same as that of the Node process calling <code class="literal">fork</code>. Change that setting using this directive.</li><li class="listitem" style="list-style-type: disc"><strong>env</strong> (object): This<a id="id262" class="indexterm"/> is used to pass environment variables to a child process. See <code class="literal">spawn</code>.</li><li class="listitem" style="list-style-type: disc"><strong>encoding</strong> (string): This <a id="id263" class="indexterm"/>sets the encoding of the communication channel.</li><li class="listitem" style="list-style-type: disc"><strong>execPath</strong> (string): This<a id="id264" class="indexterm"/> is the executable used to create the child process.</li><li class="listitem" style="list-style-type: disc"><strong>silent</strong> (Boolean): By <a id="id265" class="indexterm"/>default, a child process for which <code class="literal">fork</code> has been used will have <code class="literal">stdio</code> associated with that of the parent process (<code class="literal">child.stdout</code> is identical to <code class="literal">parent.stdout</code>, for example). Setting this option to 'true' disables this behavior.</li></ul></div><p>An important difference between <code class="literal">fork</code> and <code class="literal">spawn</code> is that the former's child process <em>does not automatically exit</em> when it is finished. Such a child process must explicitly exit when it is done, which is easily accomplished via <code class="literal">process.exit()</code>.</p><p>In <a id="id266" class="indexterm"/>the following example, we will create a child process that emits <a id="id267" class="indexterm"/>an incrementing number every tenth of a second, which its parent process then dumps to the system console. First, let's look at the child program:</p><div><pre class="programlisting">var cnt = 0;

setInterval(function() {
  process.stdout.write(" -&gt; " + cnt++);
}, 100);</pre></div><p>Again, this will simply write a steadily increasing number. When forked a child process, a child process will inherit the <code class="literal">stdio</code> stream of its parent, so we only need to create the child process in order to get the output in a terminal running the parent process:</p><div><pre class="programlisting">var fork = require('child_process').fork;
fork('./emitter.js');

// -&gt; 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 -&gt; 7 -&gt; 8 -&gt; 9 -&gt; 10 ...</pre></div><div><div><h3 class="title"><a id="note14"/>Note</h3><p>The silent option can be demonstrated here. The following code turns off any output to the terminal:</p><div><pre class="programlisting">fork('./emitter.js', [], { silent: true });</pre></div></div></div><p>Creating multiple, parallel processes is easy. Let's multiply the number of children created:</p><div><pre class="programlisting">fork('./emitter.js');
fork('./emitter.js');
fork('./emitter.js');

-&gt; 0 -&gt; 0 -&gt; 0 -&gt; 1 -&gt; 1 -&gt; 1 -&gt; 2 -&gt; 2 -&gt; 2 -&gt; 3 -&gt; 3 -&gt; 3 -&gt; 4 ...</pre></div><p>It should be clear at this point that using <code class="literal">fork</code>, we are creating many parallel execution contexts spread across all machine cores.</p><p>This<a id="id268" class="indexterm"/> is straightforward enough, but the built-in communication <a id="id269" class="indexterm"/>channel that <code class="literal">fork</code> provides makes communicating with child processes for which <code class="literal">fork</code> has been used even easier and cleaner. Consider the following two code snippets:</p><p>
<strong>Parent</strong>:</p><div><pre class="programlisting">var fork = require('child_process').fork;
var cp = fork('./child.js');
cp.on('message', function(msgobj) {
  console.log('Parent got message:', msgobj.text);
});

cp.send({
  text: "I love you"
});</pre></div><p>
<strong>Child</strong>:</p><div><pre class="programlisting">process.on('message', function(msgobj) {
  console.log('Child got message:', msgobj.text);
  process.send({
    text: msgobj.text + ' too'
  });
});</pre></div><p>By executing the parent script, we will see the following in our console:</p><div><pre class="programlisting">
<strong>Child got message: I love you</strong>
<strong>Parent got message: I love you too</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec34"/>exec(command, [options], callback)</h2></div></div></div><p>In <a id="id270" class="indexterm"/>cases where the complete buffered output of a child process is sufficient, with no need to manage data through events, <code class="literal">child_process</code> offers the <code class="literal">exec</code> method. The <a id="id271" class="indexterm"/>method takes three arguments:</p><p>
<strong>command</strong>: This is<a id="id272" class="indexterm"/> a command-line string. Unlike <code class="literal">spawn</code> and <code class="literal">fork</code>, which pass arguments to a command via an array, this first argument accepts a full command string, such as <code class="literal">ps aux | grep node</code>.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>options</strong>: This is<a id="id273" class="indexterm"/> optional.</li><li class="listitem" style="list-style-type: disc"><strong>cwd</strong>: This is <a id="id274" class="indexterm"/>a string. Set the working directory for the command process.</li><li class="listitem" style="list-style-type: disc"><strong>env</strong>: This is <a id="id275" class="indexterm"/>an object. It's a map of key-value pairs that will be exposed to the child process.</li><li class="listitem" style="list-style-type: disc"><strong>encoding</strong>: This is <a id="id276" class="indexterm"/>a string. It is the encoding of the child process's data stream. The default value is <code class="literal">'utf8'</code>.</li><li class="listitem" style="list-style-type: disc"><strong>timeout</strong>: This is<a id="id277" class="indexterm"/> a number. It is the number of milliseconds that we need to wait for the process to complete, at which point the child process will be sent the <strong>killSignal</strong> signal.</li><li class="listitem" style="list-style-type: disc"><strong>maxBuffer</strong>: This is<a id="id278" class="indexterm"/> a number. It is the maximum number of bytes allowed on <code class="literal">stdout</code> or <code class="literal">stderr</code>. When this number is exceeded, the process is killed. The default value is 200 KB.</li><li class="listitem" style="list-style-type: disc"><strong>killSignal</strong>: This is<a id="id279" class="indexterm"/> a string. The child process receives this signal after a <strong>timeout</strong>. The default value is SIGTERM.</li><li class="listitem" style="list-style-type: disc"><strong>callback</strong>: This<a id="id280" class="indexterm"/> receives three arguments: an <code class="literal">Error</code> object, if any; <code class="literal">stdout</code> (a <code class="literal">Buffer</code> containing the result); and <code class="literal">stderr</code> (a <code class="literal">Buffer</code> containing <a id="id281" class="indexterm"/>error data, if any). If the process was killed, <code class="literal">Error.signal</code> will contain the kill signal.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec35"/>execFile</h2></div></div></div><p>Use this method <a id="id282" class="indexterm"/>when you want the functionality <a id="id283" class="indexterm"/>of <code class="literal">exec</code> but are targeting a Node file. Importantly, <code class="literal">execFile</code> does not spawn a new subshell, which makes it slightly less expensive to run.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec36"/>Communicating with your child process</h2></div></div></div><p>All instances<a id="id284" class="indexterm"/> of the <code class="literal">ChildProcess</code> object extend <code class="literal">EventEmitter</code>, exposing <a id="id285" class="indexterm"/>events which are useful to manage child data connections. Additionally, <code class="literal">ChildProcess</code> objects expose useful methods of interacting with child processes directly. Let's go through these now, beginning with attributes and methods.</p><div><div><div><div><h3 class="title"><a id="ch03lvl3sec01"/>child.connected</h3></div></div></div><p>When a<a id="id286" class="indexterm"/> child process is disconnected from its parent process via <code class="literal">child.disconnect()</code>, this flag will be set to false.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec02"/>child.stdin</h3></div></div></div><p>This is<a id="id287" class="indexterm"/> a <code class="literal">Writable</code> stream corresponding to the child process's standard in.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec03"/>child.stdout</h3></div></div></div><p>This <a id="id288" class="indexterm"/>is a <code class="literal">Readable</code> stream corresponding to the child process's standard out.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec04"/>child.stderr</h3></div></div></div><p>This<a id="id289" class="indexterm"/> is a <code class="literal">Readable</code> stream corresponding to the child process's standard error.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec05"/>child.pid</h3></div></div></div><p>This is<a id="id290" class="indexterm"/> an<a id="id291" class="indexterm"/> integer representing the <strong>process ID</strong> (<strong>PID</strong>) assigned to the child process.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec06"/>child.kill([signal])</h3></div></div></div><p>Try to terminate a child process, sending it an optional signal. If no signal is specified, the<a id="id292" class="indexterm"/> default is SIGTERM (for more about signals, see <a class="ulink" href="http://unixhelp.ed.ac.uk/CGI/man-cgi?signal+7">http://unixhelp.ed.ac.uk/CGI/man-cgi?signal+7</a>). While the method name sounds terminal, it is not guaranteed to kill a process—it only sends a signal to a process. Dangerously, if <code class="literal">kill</code> is attempted on a process that has already exited, it is possible that another process, which has been newly assigned the PID of the dead process, will receive the signal, with indeterminable consequences. You should fire a <code class="literal">close</code> event, which will receive the signal used to close the process.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec07"/>child.disconnect()</h3></div></div></div><p>When <code class="literal">child.disconnect()</code> is triggered on a child process belonging to a process group<a id="id293" class="indexterm"/> that it does not lead, the IPC connection between the child and its parent will be severed, resulting in the child dying gracefully as it has no IPC channel to keep it alive. You can also call <code class="literal">process.disconnect()</code> from within the child process itself. Once a child process has disconnected, the <code class="literal">connected</code> flag on that child reference will be set to false.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec08"/>child.send(message, [sendHandle])</h3></div></div></div><p>As we <a id="id294" class="indexterm"/>saw in our discussion of <code class="literal">fork</code>, and when using the <code class="literal">ipc</code> option on <code class="literal">spawn</code>, child processes can be sent messages via this method. A TCP server or socket object can be passed along with the message as a second argument. In this way, a TCP server can spread requests across multiple child processes. For example, the following server distributes socket handling across a number of child processes equaling the total number of CPUs available. Each forked child is given a unique ID, which it reports when started. Whenever the TCP server receives a socket, that socket is passed as a handle to a random child process. That child process then sends a unique response, demonstrating that socket handling is being distributed. The following code snippets show this:</p><p>
<strong>Parent</strong>:</p><div><pre class="programlisting">var fork = require('child_process').fork;
var net = require('net');

var children = [];

require('os').cpus().forEach(function(f, idx) {
  children.push(fork("./child.js", [idx]));
});

net.createServer(function(socket) {
  var rand = Math.floor(Math.random() * children.length);
  children[rand].send(null, socket);
}).listen(8080);</pre></div><p>
<strong>Child</strong>:</p><div><pre class="programlisting">var id = process.argv[2];
process.on('message', function(n, socket) {
  socket.write('child ' + id + ' was your server today.\r\n');
  socket.end();
});</pre></div><p>Start the<a id="id295" class="indexterm"/> parent server in a terminal window. In another window, run <code class="literal">telnet 127.0.0.1 8080</code>. You should see something similar to the following, with a random child ID being displayed on each connection (assuming there exist multiple cores):</p><div><pre class="programlisting">
<strong>Trying 127.0.0.1...</strong>
<strong>...</strong>
<strong>child 3 was your server today.</strong>
<strong>Connection closed by foreign host.</strong>
</pre></div></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec37"/>The cluster module</h2></div></div></div><p>We saw how<a id="id296" class="indexterm"/> spreading work across multiple cores by spawning <a id="id297" class="indexterm"/>independent processes helps to vertically scale Node applications. The Node API has been further augmented with a <code class="literal">cluster</code> module that formalizes this pattern and extends it. Continuing with Node's core purpose of helping to make scalable network software easier to build, the particular goal of the <code class="literal">cluster</code> module is to facilitate the sharing of network sockets among many child workers.</p><p>For example, the following code creates a cluster of worker processes, all sharing the same HTTP connection:</p><div><pre class="programlisting">var cluster = require('cluster');
var http = require('http');
var numCPUs = require('os').cpus().length;

if(cluster.isMaster) {
  for(var i = 0; i &lt; numCPUs; i++) {
    cluster.fork();
  }
}

if(cluster.isWorker) {
  http.createServer(function(req, res) {
    res.writeHead(200);
    res.end("Hello from " + cluster.worker.id);
  }).listen(8080);
}</pre></div><p>We'll dig into the details shortly. The important thing to note is how this program does different things depending on whether it is running as a master process or as a child process. On its first execution, it is the master, indicated by <code class="literal">cluster.isMaster</code>. When a master process calls <code class="literal">cluster.fork</code>, this same program is forked as a child process, in this case one child for each CPU. When this program is re-executed, in a forking context, <code class="literal">cluster.isWorker</code> will be <code class="literal">true</code>, and a new HTTP server <em>running on a shared port</em> is started. Multiple processes are sharing the load for a single server.</p><p>Connect to this server with a browser. You will see something like <strong>Hello from 8</strong>, the integer corresponding to the unique <code class="literal">cluster.worker.id</code> ID of the worker that is assigned the responsibility of handling your request. Balancing across all workers is handled automatically such that refreshing your browser a few times will result in different <a id="id298" class="indexterm"/>worker IDs being displayed.</p><p>The <code class="literal">cluster</code> API breaks <a id="id299" class="indexterm"/>down into two sections: the methods, attributes, and events available to the cluster master and those available to the child process. As workers in this context are defined using <code class="literal">fork</code>, the documentation for that method of <code class="literal">child_process</code> can be applied here as well.</p><div><div><div><div><h3 class="title"><a id="ch03lvl3sec09"/>cluster.isMaster</h3></div></div></div><p>This is a <a id="id300" class="indexterm"/>Boolean value indicating whether the process is a master.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec10"/>cluster.isWorker</h3></div></div></div><p>This is a <a id="id301" class="indexterm"/>Boolean value indicating whether the process was forked from a master.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec11"/>cluster.worker</h3></div></div></div><p>This is <a id="id302" class="indexterm"/>a reference to the current worker object and is only available to a child process.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec12"/>cluster.workers</h3></div></div></div><p>This is a <a id="id303" class="indexterm"/>hash containing references to all active worker objects, keyed by the worker ID. Use this to loop through all worker objects. This only exists within the master process.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec13"/>cluster.setupMaster([settings])</h3></div></div></div><p>This is a <a id="id304" class="indexterm"/>convenient way of passing a map of default arguments when a child is forked. If all child processes are going to use <code class="literal">fork</code> on the same file (as is often the case), you will save time by setting it here. The available defaults are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>exec</strong>: This is a string. The file path to the process file defaults to <code class="literal">__filename</code>.</li><li class="listitem" style="list-style-type: disc"><strong>args</strong>: This is an array. Strings are sent as arguments to the child process.</li><li class="listitem" style="list-style-type: disc"><strong>silent</strong>: This is a Boolean value that determines whether or not to send output to the master's <code class="literal">stdio</code>.</li></ul></div></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec14"/>cluster.fork([env])</h3></div></div></div><p>This <a id="id305" class="indexterm"/>creates a new worker process. Only the master process can call this method. To expose a map of key-value pairs to the child's process environment, send an object to <code class="literal">env</code>.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec15"/>cluster.disconnect([callback])</h3></div></div></div><p>This is <a id="id306" class="indexterm"/>used to terminate all workers in a cluster. Once all the workers have died gracefully, the cluster process will itself terminate if it has no further events to wait on. To be notified when all child processes have expired, pass <code class="literal">callback</code>.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec16"/>cluster events</h3></div></div></div><p>This <a id="id307" class="indexterm"/>cluster object emits several events:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>fork</strong>: This is <a id="id308" class="indexterm"/>fired when the master tries to use <code class="literal">fork</code> on a new child. This is not the same as <code class="literal">online</code>. This receives a worker object.</li><li class="listitem" style="list-style-type: disc"><strong>online</strong>: This<a id="id309" class="indexterm"/> is fired when the master receives notification that a child is fully bound. This differs from the <code class="literal">fork</code> event. This receives a worker object.</li><li class="listitem" style="list-style-type: disc"><strong>listening</strong>: When<a id="id310" class="indexterm"/> the worker performs an action that requires a <code class="literal">listen()</code> call (such as starting an HTTP server), this event will be fired in the master. The event emits two arguments: a worker object and the address object containing the <code class="literal">address</code>, <code class="literal">port</code>, and <code class="literal">addressType</code> of the connection.</li><li class="listitem" style="list-style-type: disc"><strong>disconnect</strong>: This is<a id="id311" class="indexterm"/> called whenever a child disconnects, which can happen either through process exit events or after calling <code class="literal">child.kill()</code>. This will fire prior to the <code class="literal">exit</code> event—they are not the same. This receives a worker object.</li><li class="listitem" style="list-style-type: disc"><strong>exit</strong>: Whenever<a id="id312" class="indexterm"/> a child dies, this event is emitted. It receives three arguments: a worker object, the exit code number, and the signal string, such as SIGHUP, that caused the process to be killed.</li><li class="listitem" style="list-style-type: disc"><strong>setup</strong>: This is<a id="id313" class="indexterm"/> called after <code class="literal">cluster.setupMaster</code> has executed.</li></ul></div></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec17"/>worker.id</h3></div></div></div><p>This is the <a id="id314" class="indexterm"/>unique ID assigned to a worker, which also represents the worker's key in the <code class="literal">cluster.workers</code> index.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec18"/>worker.process</h3></div></div></div><p>This is<a id="id315" class="indexterm"/> a <code class="literal">ChildProcess</code> object referencing a worker.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec19"/>worker.suicide</h3></div></div></div><p>These<a id="id316" class="indexterm"/> workers, that have recently had <code class="literal">kill </code>or <code class="literal">disconnect</code> called on them, will have their <code class="literal">suicide </code>attribute set to true.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec20"/>worker.send(message, [sendHandle])</h3></div></div></div><p>See <a id="id317" class="indexterm"/>
<code class="literal">child_process.fork()</code>in the <em>Scaling vertically across multiple cores</em> section where I describe the <code class="literal">#fork</code> method.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec21"/>worker.kill([signal])</h3></div></div></div><p>This <a id="id318" class="indexterm"/>kills a worker. The master can check this worker's <code class="literal">suicide</code> property in order to determine whether the death was intentional or accidental. The default signal sent is SIGTERM.</p></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec22"/>worker.disconnect()</h3></div></div></div><p>This instructs a<a id="id319" class="indexterm"/> worker to disconnect. Importantly, existing connections to the worker are not immediately terminated (as with <code class="literal">kill</code>) but are allowed to exit normally prior to the worker fully disconnecting. Because existing connections can stay in existence for a very long time, it is a good habit to regularly check whether the worker has actually disconnected, perhaps using timeouts.</p><p>Workers also emit events:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>message</strong>: See <a id="id320" class="indexterm"/><code class="literal">child_process.fork</code> in the <em>Scaling vertically across multiple cores</em> section where I describe the <code class="literal">#fork</code> method</li><li class="listitem" style="list-style-type: disc"><strong>online</strong>: This <a id="id321" class="indexterm"/>is identical to <code class="literal">cluster.online</code> except that the check is against only the specified worker</li><li class="listitem" style="list-style-type: disc"><strong>listening</strong>: This<a id="id322" class="indexterm"/> is identical to <code class="literal">cluster.listening</code> except that the check is against only the specified worker</li><li class="listitem" style="list-style-type: disc"><strong>disconnect</strong>: This <a id="id323" class="indexterm"/>is identical to <code class="literal">cluster.disconnect</code> except that the check is against only the specified worker</li><li class="listitem" style="list-style-type: disc"><strong>exit</strong>: See<a id="id324" class="indexterm"/> the <code class="literal">exit</code> event for <code class="literal">child_process</code></li><li class="listitem" style="list-style-type: disc"><strong>setup</strong>: This is<a id="id325" class="indexterm"/> called after <code class="literal">cluster.setupMaster</code> has executed</li></ul></div><p>Now that we have a good understanding of how to accomplish vertical <a id="id326" class="indexterm"/>scaling with Node, let's take a look at some ways to handle horizontal scaling</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Scaling horizontally across different machines</h1></div></div></div><p>Because Node is so efficient, most websites or applications can accommodate all of their scaling<a id="id327" class="indexterm"/> needs in the vertical dimension. As we learned from Eran Hammer's experiences at Walmart, Node can handle enormous levels of traffic using only a few CPUs and an unexceptional volume of memory.</p><p>Nevertheless, horizontal scaling can still be the right choice, even if only for architectural reasons. Having one point of failure, no matter how robust, still entails some risk. The <em>parking lot problem</em> is another consideration that Walmart likely faces—during shopping holidays, you will need many thousands of parking spots, but during the rest of the year this investment in empty space is hard to justify. In terms of servers, the ability to dynamically scale both up and down argues against building fixed vertical silos. Adding hardware to a running server is also a more complicated process than spinning up and seamlessly<a id="id328" class="indexterm"/> linking another virtual machine to your application.</p><p>In this section, we'll look at a few techniques for horizontal scaling, considering load balancing using native Node techniques, third-party solutions, and some ideas for cross-server communication.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec38"/>Using Nginx</h2></div></div></div><p>
<strong>Nginx</strong> (pronounced <strong>Engine X</strong>) remains<a id="id329" class="indexterm"/> a popular choice for those whose architecture benefits<a id="id330" class="indexterm"/> from hiding Node servers behind a <a id="id331" class="indexterm"/>proxy. Nginx is a very popular high-performance web server that is often used as a proxy server. Given its design, Nginx is a popular choice with <a id="id332" class="indexterm"/>Node developers. According to <a class="ulink" href="http://www.linuxjournal.com/magazine/nginx-high-performance-web-server-and-reverse-proxy">http://www.linuxjournal.com/magazine/nginx-high-performance-web-server-and-reverse-proxy</a>:</p><div><blockquote class="blockquote"><p><em>"Nginx is able to serve more requests per second with less resources because of its architecture. It consists of a master process, which delegates work to one or more worker processes. Each worker handles multiple requests in an event-driven or asynchronous manner using special functionality from the Linux kernel (epoll/select/poll). This allows Nginx to handle a large number of concurrent requests quickly with very little overhead."</em></p></blockquote></div><p>Its similarity in design to Node is striking: event delegation across processes and an evented, asynchronous environment coordinated by the OS delivering high concurrency.</p><p>A <strong>proxy</strong> is<a id="id333" class="indexterm"/> someone or something acting on behalf of another.</p><p>A <em>forward</em> proxy<a id="id334" class="indexterm"/> normally works on behalf of clients in a private network, brokering requests to an outside network, such as retrieving data from the Internet. Early <em>web providers</em>, such as AOL, functioned in this way:</p><div><img src="img/1403OS_03_01.jpg" alt="Using Nginx"/></div><p>Network administrators often use forward proxies when restrictions on access to the outside world (that is, the Internet) are needed. If malware is downloaded from a bad website via an e-mail attachment, the administrator might block access to that location. Restrictions on access to social networking sites might be imposed on an office network. Some countries even restrict access to the general Internet in this way.</p><p>A <em>reverse</em> proxy, not<a id="id335" class="indexterm"/> surprisingly, works in the opposite manner, accepting requests from a public <a id="id336" class="indexterm"/>network and servicing those requests within a private network that the client might not have much <a id="id337" class="indexterm"/>visibility into. Direct access to servers by clients is first delegated to a reverse proxy. This can be shown with the help of the following diagram:</p><div><img src="img/1403OS_03_02.jpg" alt="Using Nginx"/></div><p>This is the type<a id="id338" class="indexterm"/> of proxy we can use to balance requests from clients <a id="id339" class="indexterm"/>across many Node servers. Client X does not communicate with any given server directly. A broker Y is the first point of contact that is able to direct X to a server under less load, is located closer to X, or is, in some other way, the best server for X to access at the time.</p><p>Let's take a look at how Nginx can be used as a proxy, in particular, as a load balancer, by deploying such a system on the <a id="id340" class="indexterm"/>cloud hosting service <strong>Digital Cloud</strong>.</p><div><div><div><div><h3 class="title"><a id="ch03lvl3sec23"/>Deploying an Nginx load balancer on DigitalOcean</h3></div></div></div><p>DigitalOcean is a <a id="id341" class="indexterm"/>cloud hosting provider that is inexpensive and easy to<a id="id342" class="indexterm"/> set up. We will build an Nginx<a id="id343" class="indexterm"/> load balancer on this service.</p><p>To sign<a id="id344" class="indexterm"/> up, visit <a class="ulink" href="http://www.digitalocean.com">http://www.digitalocean.com</a>. The basic package (at the time of writing this) incurs a $5 fee, but promotion codes are regularly made available—a simple web search should result in a usable code. Create and verify an account to get started.</p><p>DigitalOcean packages are described as droplets with certain characteristics—the amount of storage space, transfer limits, and so on. A basic package is sufficient for our needs. Also, you will indicate a hosting region and the OS to install in your droplet (in this example, we'll use the latest version of Ubuntu). Create a droplet and check your e-mail for login instructions. You're done!</p><p>You will receive full login information for your instance. You can now open a terminal and SSH into your box using those login credentials.</p><div><div><h3 class="title"><a id="note15"/>Note</h3><p>On your initial login, you might want to update your packages. For Ubuntu, you would run <code class="literal">apt-get update</code> and <code class="literal">apt-get upgrade</code>. Other package managers have similar commands (such as <code class="literal">yum update</code> for RHEL/CentOS).</p></div></div><p>Before we begin to install, let's change our root password and create a nonroot user (it is unsafe to expose the root to external logins and software installs). To change your root password, type <code class="literal">passwd</code> and follow the instructions in your terminal. To create a new user, enter <code class="literal">adduser &lt;new user name&gt;</code> (for example, <code class="literal">adduser john</code>). Follow the instructions mentioned in the upcoming paragraphs.</p><p>One more step: we want to give some administrative privileges to our new user as we'll install software as that user. In Unix parlance, you want to give <code class="literal">sudo</code> access to this new user. Instructions on how to do this are easy to find for whichever OS you've chosen. Essentially, you will want to change the <code class="literal">/etc/sudoers</code> file. Remember to do this using a command such as <code class="literal">visudo</code>—do not edit the <code class="literal">sudoers</code> file by hand! You may also want to restrict root logins and do other SSH access management at this point.</p><div><div><h3 class="title"><a id="tip07"/>Tip</h3><p>After successfully executing <code class="literal">sudo -i</code> in your terminal, you will be able to enter commands without prefixing each one with <code class="literal">sudo</code>. The following examples assume that you've done this.</p></div></div><p>We'll now create an Nginx load balancer frontend for two Node servers. This means that we will <a id="id345" class="indexterm"/>create three droplets—one for the<a id="id346" class="indexterm"/> balancer and two added droplets as Node servers. In the end, we will end up with an architecture that looks something like this:</p><div><img src="img/1403OS_03_05.jpg" alt="Deploying an Nginx load balancer on DigitalOcean"/></div></div><div><div><div><div><h3 class="title"><a id="ch03lvl3sec24"/>Installing and configuring Nginx</h3></div></div></div><p>Let's install<a id="id347" class="indexterm"/> Nginx and Node/npm. If you're still logged in as root, log out <a id="id348" class="indexterm"/>and reauthenticate as the new user you've just created. To install Nginx (on Ubuntu), simply type:</p><div><pre class="programlisting">
<strong>apt-get install nginx</strong>
</pre></div><p>Most other Unix package managers will have Nginx installers. To start Nginx, use:</p><div><pre class="programlisting">
<strong>service nginx start</strong>
</pre></div><div><div><h3 class="title"><a id="note16"/>Note</h3><p>Full <a id="id349" class="indexterm"/>documentation for Nginx can be found at <a class="ulink" href="http://wiki.nginx.org/Configuration">http://wiki.nginx.org/Configuration</a>.</p></div></div><p>You should now be able to point your browser to the IP you were assigned (check your inbox if you've forgotten) and see something like this:</p><div><img src="img/1403OS_03_04.jpg" alt="Installing and configuring Nginx"/></div><p>Now, let's set up the two servers that Nginx will balance.</p><p>Create an additional two droplets in DigitalOcean. You must <em>not</em> install Nginx on these servers. Configure permissions on these servers as we did earlier. Now, install Node in both droplets. An easy way to manage your Node installation is using Tim Caswell's <strong>Node Version Manager</strong> (<strong>NVM</strong>). NVM is essentially a bash script that provides a set of command-line<a id="id350" class="indexterm"/> tools facilitating Node version management and allowing you to easily switch between versions. To install it, use the following command:</p><div><pre class="programlisting">
<strong>curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.25.4/install.sh | bash</strong>
</pre></div><p>Now, install your preferred Node version (here we ask for the latest release of the 0.12 version):</p><div><pre class="programlisting">
<strong>nvm install 0.12</strong>
</pre></div><p>You might want to add a command to your <code class="literal">.bashrc</code> or <code class="literal">.profile</code> file to ensure that a certain node version is used each time you start a shell:</p><div><pre class="programlisting">
<strong># start with node 0.12</strong>
<strong>nvm use 0.12</strong>
</pre></div><p>To test<a id="id351" class="indexterm"/> our system, we need to set up Node servers on both of these <a id="id352" class="indexterm"/>machines. Create the following program file on each server, changing '**' to something unique on each (such as <em>one</em> and <em>two</em>):</p><div><pre class="programlisting">var http = require('http');

http.createServer(function(req, res) {
   res.writeHead(200, {
    "Content-Type" : "text/html"
  });
  res.write('HOST **');
  res.end();
}).listen(8080)</pre></div><p>Start this file <a id="id353" class="indexterm"/>on each server (<code class="literal">node serverfile.js</code>). Each server will now<a id="id354" class="indexterm"/> answer on port <code class="literal">8080</code>.</p><p>You should now be able to reach this server by pointing a browser to each droplet's IP:8080. Once you have two servers responding with distinct messages, we can set up the Nginx load balancer.</p><p>Load balancing across servers is straightforward with Nginx. You need to simply indicate in the Nginx configuration <a id="id355" class="indexterm"/>script which <strong>upstream</strong> servers should be balanced. The two Node servers we've just created are the upstream servers. The following diagram describes how Nginx evenly distributes requests across upstream servers:</p><div><img src="img/1403OS_03_03.jpg" alt="Installing and configuring Nginx"/></div><p>Each <a id="id356" class="indexterm"/>request will be handled first by Nginx, which will check its <em>upstream</em> configuration and, based on how it is configured, will (reverse) proxy requests to <a id="id357" class="indexterm"/>upstream servers that will actually handle the request.</p><p>You will find the default Nginx server configuration file on your balancer droplet at <code class="literal">/etc/nginx/sites-available/default</code>. In production, you'll most likely want to create a custom directory and configuration file, but for our purposes, we'll simply modify the default configuration file (you might want to make a backup before you start modifying it).</p><p>At the top of the Nginx configuration file, we want to define <em>upstream</em> servers that will be candidates for redirection. This is simply a map with the arbitrary key <code class="literal">lb-servers</code> to be referenced in the server definition that follows:</p><div><pre class="programlisting">upstream lb_servers {
  server first.node.server.ip;
  server second.node.server.ip;
}</pre></div><p>Now that we've established the candidate map, we need to configure Nginx such that it forwards requests in a balanced way to each of the members of <code class="literal">lb-servers</code>:</p><div><pre class="programlisting">server {
  listen 80 default_server;
  listen [::]:80 default_server ipv6only=on;

  #root /usr/share/nginx/html;
  #index index.html index.htm;

  # Make site accessible from http://localhost/
  server_name localhost;

  location / {
    proxy_pass http://lb-servers; # Load balance mapped servers
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection 'upgrade';
    proxy_set_header Host $host;
    proxy_cache_bypass $http_upgrade;
  }

  ... more configuration options not specifically relevant to our purposes
}</pre></div><p>The key <a id="id358" class="indexterm"/>line is this one:</p><div><pre class="programlisting">proxy_pass http://lb-servers</pre></div><p>Note how<a id="id359" class="indexterm"/> the name <code class="literal">lb-servers</code> matches the name of our upstream definition. This should make what is happening clear: an Nginx server listening on port <code class="literal">80</code> will pass the request on to a server definition contained in <code class="literal">lb-servers</code>. If the upstream definition has only one server in it, that server gets all the traffic. If several servers are defined, Nginx attempts to distribute traffic evenly among them.</p><div><div><h3 class="title"><a id="note17"/>Note</h3><p>It is also possible to balance load across several <em>local servers</em> using the same technique. One would simply run different Node servers on different ports, such as <code class="literal">server 127.0.0.1:8001; server 127.0.0.1:8002; ...</code>.</p></div></div><p>Go ahead and change the Nginx configuration (consult the <code class="literal">nginx.config</code> file in the code bundle for this book if you get stuck). Once you've changed it, restart Nginx with the following command:</p><div><pre class="programlisting">
<strong>service nginx restart</strong>
</pre></div><p>Or, you can use this command:</p><div><pre class="programlisting">
<strong>service nginx stop</strong>
<strong>service nginx start</strong>
</pre></div><p>Assuming that the other two droplets running Node servers are active, you should now be able to point your browser to your Nginx-enabled droplet and see messages from those servers!</p><p>Because we will likely want more precise control over how traffic is distributed across our upstream servers, there are further directives that can be applied to upstream server definitions.</p><p>Nginx balances load using a weighted round-robin algorithm. In order to control the relative weighting of traffic <a id="id360" class="indexterm"/>distribution, we use the <strong>weight</strong> directive:</p><div><pre class="programlisting">upstream lb-servers {
  server first.node.server.ip weight=10;
  server second.node.server.ip weight=20;
}</pre></div><p>This definition tells Nginx to distribute twice as much load to the second server as to the first. Servers with more memory or CPUs might be favored, for example. Another way to use this system is<a id="id361" class="indexterm"/> to create an A/B testing scenario, where one server containing a <a id="id362" class="indexterm"/>proposed new design receives a small fraction of the total traffic such that metrics on the testing server (sales, downloads, engagement length, and so on) can be compared against the wider average.</p><p>Three other useful directives are available, which work together to manage connection failures:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>max_fails</strong>: This<a id="id363" class="indexterm"/> is the number of times communication with a server fails prior to marking that server as inoperative. The period of time within which these failures must occur is defined by <strong>fail_timeout</strong>.</li><li class="listitem" style="list-style-type: disc"><strong>fail_timeout</strong>: This is <a id="id364" class="indexterm"/>the time slice during which <strong>max_fails</strong> must occur, indicating that a server is inoperative. This number also indicates the amount of time after a server is marked inoperative that Nginx will again attempt to reach the flagged server. Here's an example:<div><pre class="programlisting">upstream lb-servers {
  server first.node.server.ip weight=10 max_fails=2 fail_timeout=20s;
  server second.node.server.ip weight=20 max_fails=10 fail_timeout=5m;
}</pre></div></li><li class="listitem" style="list-style-type: disc"><strong>backup</strong>: A <a id="id365" class="indexterm"/>server marked with this directive will only be called when and if <em>all</em> of the other listed servers are unavailable.</li></ul></div><p>Additionally, there <a id="id366" class="indexterm"/>are some directives for the upstream definition that <a id="id367" class="indexterm"/>add some control over how clients are directed to upstream servers:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>least_conn</strong>: This<a id="id368" class="indexterm"/> passes a request to the server with the least connections. This provides a slightly smarter balancing, taking into consideration server load as well as weighting.</li><li class="listitem" style="list-style-type: disc"><strong>ip_hash</strong>: The<a id="id369" class="indexterm"/> idea here is to create a hash of each connecting IP and to ensure that requests from a given client are always passed to the same server.</li></ul></div><div><div><h3 class="title"><a id="note18"/>Note</h3><p>Another commonly used tool for balancing Node servers is the dedicated load balancer <a id="id370" class="indexterm"/>
<strong>HAProxy</strong>, which <a id="id371" class="indexterm"/>is available at <a class="ulink" href="http://haproxy.1wt.eu/">http://haproxy.1wt.eu/</a>.</p></div></div></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec39"/>Load balancing with Node</h2></div></div></div><p>For many <a id="id372" class="indexterm"/>years, it was recommended that a web server (such as Nginx) be placed in front of Node servers. The claim was that mature web servers handle<a id="id373" class="indexterm"/> static file transfers more efficiently. While this may have been true for earlier Node versions (which did suffer from the bugs that new technologies face), it is no longer necessarily true in terms of pure speed. Some recent benchmarks bear this out: <a class="ulink" href="http://centminmod.com/siegebenchmarks/2013/020313/index.html">http://centminmod.com/siegebenchmarks/2013/020313/index.html</a>.</p><p>File serving speeds are, of course, not the only reason you might use a proxy such as Nginx. It is often true that network topology characteristics make a reverse proxy the better choice, especially when the centralization of common services, such as compression, makes sense. The point is simply that Node should not be excluded solely due to outdated biases about its ability to efficiently serve files. Let's look at one example of a purely Node-based proxying and balancing solution, <code class="literal">node-http-proxy</code>.</p><div><div><div><div><h3 class="title"><a id="ch03lvl3sec25"/>Using node-http-proxy</h3></div></div></div><p>Node is <a id="id374" class="indexterm"/>designed to facilitate the creation of network<a id="id375" class="indexterm"/> software, so it comes as no surprise that several proxying modules have been developed. The team at NodeJitsu has released the proxy they use in production—<code class="literal">http-proxy</code>. Let's take a look at how we would use it to route requests to different Node servers.</p><p>Unlike with Nginx, the entirety of our routing stack will exist in Node. Listening on port <code class="literal">80</code>, one Node server will run our proxy. Three scenarios will be covered: using a single box to run multiple Node servers on separate ports on the same machine; using one box as a pure router proxying to external URLs; and creating a basic round-robin load balancer.</p><p>As an initial example, let's look at how to use this module to redirect requests:</p><div><pre class="programlisting">var httpProxy = require('http-proxy');

var proxy = httpProxy.createServer({
  target: {
    host: 'www.example.com',
    port: 80
  }
}).listen(80);</pre></div><p>By starting this server on port <code class="literal">80</code> of our local machine, we are able to redirect the user to another URL.</p><p>To run several distinct Node servers, each responding to a different URL, on a single machine, you simply have to define a router:</p><div><pre class="programlisting">var httpProxy = httpProxy.createServer({
  router: {
    'www.mywebsite.com'    : '127.0.0.1:8001',
    'www.myothersite.com'  : '127.0.0.1:8002',
  }
});
httpProxy.listen(80);</pre></div><p>For each of your distinct websites, you can now point your DNS name servers (via ANAME or CNAME) to the same endpoint (wherever this Node program is running), and they will resolve to different Node servers. This is handy when you want to run several websites but don't want to create a new physical server for each one. Another strategy is to handle different paths within the same website on different Node servers:</p><div><pre class="programlisting">var httpProxy = httpProxy.createServer({
  router: {
    'www.mywebsite.com/friends'  : '127.0.0.1:8001',
    'www.mywebsite.com/foes'  : '127.0.0.1:8002',
  }
});
httpProxy.listen(80);</pre></div><p>This allows specialized functionality in your application to be handled by uniquely configured servers.</p><p>Setting<a id="id376" class="indexterm"/> up a load balancer is also straightforward. As <a id="id377" class="indexterm"/>with Nginx's <code class="literal">upstream</code> directive, we simply list the servers to be balanced and cycle through them:</p><div><pre class="programlisting">var httpProxy = require('http-proxy');
var addresses = [
  {
    host: 'one.example.com',
    port: 80
  },
  {
    host: 'two.example.com',
    port: 80
  }
];

httpProxy.createServer(function(req, res, proxy) {
  var target = addresses.shift();
  proxy.proxyRequest(req, res, target);
  addresses.push(target);
}).listen(80);</pre></div><p>Unlike <a id="id378" class="indexterm"/>with Nginx, we are responsible for doing the <a id="id379" class="indexterm"/>actual balancing. In this example, we treat servers equally, cycling through them in order. After the selected server is proxied, it is returned to the <em>rear</em> of the list.</p><p>It should be clear that this example could be easily extended to accommodate other directives, such as Nginx's <code class="literal">weight</code>.</p><div><div><h3 class="title"><a id="note19"/>Note</h3><p>Another good option for proxying Node is James Halliday's <code class="literal">bouncy</code> module available<a id="id380" class="indexterm"/> at <a class="ulink" href="https://github.com/substack/bouncy">https://github.com/substack/bouncy</a>.</p></div></div></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec40"/>Using message queues</h2></div></div></div><p>One<a id="id381" class="indexterm"/> of the best ways to ensure that distributed <a id="id382" class="indexterm"/>servers maintain a dependable communication channel is to bundle the complexity of remote procedure calls into a messaging queue. When one server wishes to send a message to another server, the message can simply be placed on this queue—like a "to-do" list for your application—with the queue service doing the work of ensuring that messages get delivered as well as delivering any important replies back to the original sender.</p><p>There are a few enterprise-grade message queues available, many of which deploy the <strong>Advanced Message Queuing Protocol</strong> (<strong>AMQP</strong>). We will focus on a very stable and well-known <a id="id383" class="indexterm"/>implementation: RabbitMQ.</p><div><div><h3 class="title"><a id="note20"/>Note</h3><p>To install <a id="id384" class="indexterm"/>RabbitMQ in your environment, follow the instructions found at <a class="ulink" href="http://www.rabbitmq.com/download.html">http://www.rabbitmq.com/download.html</a>. Note that you will<a id="id385" class="indexterm"/> also need to install Erlang (the instructions for which can be found at the same link).</p></div></div><p>After installing it, you can start the RabbitMQ server with this command:</p><div><pre class="programlisting">
<strong>service rabbitmq-server start</strong>
</pre></div><p>To interact with RabbitMQ using Node, we will use Theo Schlossnagle's <code class="literal">node-amqp</code> module:</p><div><pre class="programlisting">
<strong>npm install amqp</strong>
</pre></div><p>To use a message queue, one must first create a consumer bound to RabbitMQ that will listen for messages published to the queue. The most basic consumer will listen for all messages:</p><div><pre class="programlisting">var amqp = require('amqp');

var consumer = amqp.createConnection({ host: 'localhost', port: 5672 });
var exchange;

consumer.on('ready', function() {
  exchange = consumer.exchange('node-topic-exchange', {type: "topic"});
  consumer.queue('node-topic-queue', function(q) {

    q.bind(exchange, '#');

    q.subscribe(function(message) {
      // Messages are buffers
      //
      console.log(message.data.toString('utf8'));
    });
  });
});</pre></div><p>We are <a id="id386" class="indexterm"/>now listening for messages from the <a id="id387" class="indexterm"/>RabbitMQ server bound to port <code class="literal">5672</code>. It should be obvious that the <em>localhost</em> can be replaced with a proper server address and bound to any number of distributed servers.</p><p>Once this consumer establishes a connection, it will establish the name of the queue it will listen to and should <code class="literal">bind</code> to an <strong>exchange</strong>. In this example, we create a topic <code class="literal">exchange</code> (the default), giving it<a id="id388" class="indexterm"/> a unique name. We also indicate that we would like to listen for <em>all</em> messages via <code class="literal">#</code>. All that is left to do is subscribe to the queue, receiving a message object. We will learn more about the message object as we progress. For now, note the important <code class="literal">data</code> property containing the sent messages.</p><p>Now that we have established a consumer, let's publish a message to the exchange. If all goes well, we will see the sent message appear in our console:</p><div><pre class="programlisting">consumer.on('ready', function() {

  ...

  exchange.publish("some-topic", "Hello!");
});

// Hello!</pre></div><p>We have already learned enough to implement useful scaling tools. If we have a number of distributed Node processes, even on different physical servers, each can reliably send messages to the others via RabbitMQ. Each process needs to simply implement an exchange queue subscriber to receive messages and an exchange publisher when messages need to be sent.</p><p>Three<a id="id389" class="indexterm"/> types <a id="id390" class="indexterm"/>of exchanges exist: <strong>direct</strong>, <strong>fanout</strong>, and <strong>topic</strong>. The<a id="id391" class="indexterm"/> differences appear in the way each type of exchange <a id="id392" class="indexterm"/>processes <strong>routing keys</strong>—the first argument sent to <code class="literal">exchange.publish</code>.</p><p>A direct <a id="id393" class="indexterm"/>exchange matches routing keys directly. Here's an example of <a id="id394" class="indexterm"/>a queue binding:</p><div><pre class="programlisting">queue.bind(exchange, 'room-1');</pre></div><p>The preceding queue binding will match <em>only</em> messages sent to <code class="literal">room-1</code>. Because no parsing is necessary, direct exchanges are able to process more messages than topic exchanges in a set period of time.</p><p>A fanout <a id="id395" class="indexterm"/>exchange is indiscriminate: it routes messages<a id="id396" class="indexterm"/> to all of the queues bound to it, ignoring routing keys. This type of exchange is used for wide broadcasts.</p><p>A topic exchange<a id="id397" class="indexterm"/> matches routing keys based on the wildcards <code class="literal">#</code> and <code class="literal">*</code>. Unlike other types, routing keys for topic exchanges <em>must</em> be composed of words separated by dots—<em>animals.dogs.poodle</em>, for example. A <code class="literal">#</code> matches <em>zero or more</em> words—it will match every message (as we saw in the previous example) just like a fanout exchange. The other wildcard is <code class="literal">*</code>, and this matches <em>exactly one</em> word.</p><p>Direct and <a id="id398" class="indexterm"/>fanout exchanges can be implemented<a id="id399" class="indexterm"/> using nearly the same code as the given topic exchange example, requiring only that the exchange type be changed, and that bind operations be aware of how they will be associated with routing keys (fanout subscribers receive all messages, regardless of the key; for a direct exchange, the routing key must match directly).</p><p>This last example should drive home how topic exchanges work. We will create three queues with different matching rules, filtering the messages each queue receives from the exchange:</p><div><pre class="programlisting">consumer.on('ready', function() {

  // When all 3 queues are ready, publish.
  //
  var cnt = 3;
  var queueReady = function() {
    if(--cnt &gt; 0) {
      return;
    }
    exchange.publish('animals.dogs.poodles', 'Poodle!');
    exchange.publish('animals.dogs.dachshund', 'Dachshund!');
    exchange.publish('animals.cats.shorthaired', 'Shorthaired Cat!');
    exchange.publish('animals.dogs.shorthaired', 'Shorthaired Dog!');
    exchange.publish('animals.misc', 'Misc!');
  }

  var exchange = consumer.exchange('topical', {type: "topic"});

  consumer.queue('queue-1', function(q) {

    q.bind(exchange, 'animals.*.shorthaired');
    q.subscribe(function(message) {
      console.log('animals.*.shorthaired -&gt; ' + message.data.toString('utf8'));
    });

    queueReady();
  });

  consumer.queue('queue-2', function(q) {
    q.bind(exchange, '#');
    q.subscribe(function(message) {
      console.log('# -&gt; ' + message.data.toString('utf8'));
    });

    queueReady();
  });

  consumer.queue('queue-3', function(q) {
    q.bind(exchange, '*.cats.*');
    q.subscribe(function(message) {
      console.log('*.cats.* -&gt; ' + message.data.toString('utf8'));
    });

    queueReady();
  });
});

//  # -&gt; Poodle!
//  animals.*.shorthaired -&gt; Shorthaired Cat!
//  *.cats.* -&gt; Shorthaired Cat!
//  # -&gt; Dachshund!
//  # -&gt; Shorthaired Cat!
//  animals.*.shorthaired -&gt; Shorthaired Dog!
//  # -&gt; Shorthaired Dog!
//  # -&gt; Misc!</pre></div><p>The <code class="literal">node-amqp</code> module <a id="id400" class="indexterm"/>contains further<a id="id401" class="indexterm"/> methods to control connections, queues, and exchanges, in particular methods of removing queues from exchanges and subscribers from queues. Generally, changing the makeup of a running queue on the fly can lead to unexpected errors, so use these with caution.</p><div><div><h3 class="title"><a id="note21"/>Note</h3><p>To learn more <a id="id402" class="indexterm"/>about the AMQP (and the options available when setting up with <code class="literal">node-amqp</code>), visit <a class="ulink" href="http://www.rabbitmq.com/tutorials/amqp-concepts.html">http://www.rabbitmq.com/tutorials/amqp-concepts.html</a>.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec41"/>Using Node's UDP Module</h2></div></div></div><p>
<strong>User Datagram Protocol</strong> (<strong>UDP</strong>) is a lightweight core Internet messaging protocol, enabling <a id="id403" class="indexterm"/>servers to pass around concise <em>datagrams</em>. UDP<a id="id404" class="indexterm"/> was designed with a minimum of protocol overhead, forgoing delivery, ordering, and duplication prevention mechanisms in favor of ensuring high performance. UDP is a good choice when perfect reliability is not required and high-speed transmission is, as found in networked video games and videoconferencing applications. Logging is another popular use for <a id="id405" class="indexterm"/>UDP.</p><p>This is not to say that UDP is <em>normally</em> unreliable. In most applications, it delivers messages with high probability. It is simply not suitable when <em>perfect</em> reliability is needed, such as in a banking application. It is an excellent candidate for monitoring and logging applications and for noncritical messaging services.</p><p>Creating a UDP server with Node is straightforward:</p><div><pre class="programlisting">var dgram = require('dgram');
var socket = dgram.createSocket('udp4');

socket.on('message', function(msg, info) {
  console.log('socket got: ' + msg + ' from ' +
  info.address + ':' + info.port);
});

socket.bind(41234);

socket.on('listening', function() {
  console.log('Listening for datagrams.');
});</pre></div><p>The bind <a id="id406" class="indexterm"/>command takes three arguments:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>port</strong>: This is<a id="id407" class="indexterm"/> the integer port number.</li><li class="listitem" style="list-style-type: disc"><strong>address</strong>: This is<a id="id408" class="indexterm"/> an optional address. If this is not specified, the OS will try to listen on all addresses (which is often what you want). You might also try using <code class="literal">0.0.0.0</code> explicitly.</li><li class="listitem" style="list-style-type: disc"><strong>callback</strong>: This is<a id="id409" class="indexterm"/> an optional callback, which receives no arguments.</li></ul></div><p>This socket will now emit a <strong>message</strong> event whenever it receives a datagram via port <code class="literal">41234</code>. The event callback receives the message itself as the first parameter and a map of packet information as the second:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>address</strong>: This is the originating IP</li><li class="listitem" style="list-style-type: disc"><strong>family</strong>: This is one of IPv4 or IPv6</li><li class="listitem" style="list-style-type: disc"><strong>port</strong>: This is the originating port</li><li class="listitem" style="list-style-type: disc"><strong>size</strong>: This is the size of the message in bytes</li></ul></div><p>This map is similar to the map returned when calling <code class="literal">socket.address()</code>.</p><p>In addition to the message and listening events, a UDP socket also emits a <code class="literal">close</code> event and an <code class="literal">error</code> event, with the latter receiving an <code class="literal">Error</code> object whenever an error occurs. To close a UDP socket (and trigger the <code class="literal">close</code> event), use <code class="literal">server.close()</code>.</p><p>Sending a message is even easier:</p><div><pre class="programlisting">var client = dgram.createSocket('udp4');
var message = new Buffer('UDP says Hello!');
client.send(message, 0, message.length, 41234, 'localhost', function(err, bytes) {
  client.close();
});</pre></div><p>The send method takes the form <code class="literal">client.send(buffer, offset, length, port, host, callback)</code>:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>buffer</strong>: This is a buffer containing the datagram to be sent</li><li class="listitem" style="list-style-type: disc"><strong>offset</strong>: This is an integer indicating the position in the <strong>buffer </strong>where the datagram begins</li><li class="listitem" style="list-style-type: disc"><strong>length</strong>: This is the number of bytes in a datagram. In combination with <strong>offset</strong>, this value identifies the full datagram within the <strong>buffer</strong></li><li class="listitem" style="list-style-type: disc"><strong>port</strong>: This is an integer identifying the destination port</li><li class="listitem" style="list-style-type: disc"><strong>address</strong>: This is a string indicating the destination IP for the datagram</li><li class="listitem" style="list-style-type: disc"><strong>callback</strong>: This is an optional callback function called after the send has taken place.</li></ul></div><div><div><h3 class="title"><a id="note22"/>Note</h3><p>The size of a datagram cannot exceed 65,507 bytes, which is equal to <em>2^16-1</em> (65,535) bytes minus the 8 bytes used by the UDP header minus the 20 bytes used by the IP header.</p></div></div><p>We now<a id="id410" class="indexterm"/> have another candidate for interprocess messaging. It would be rather easy to set up a monitoring server for our Node application that listens on a <a id="id411" class="indexterm"/>UDP socket for program updates and statistics sent from other processes. The protocol speed is fast enough for real-time systems, and any packet loss or other UDP hiccups would be insignificant taken as a percentage of total volume over time.</p><p>Taking the idea of broadcasting further, we can also use the <code class="literal">dgram</code> module to create a multicast server. A "multicast" is simply a one-to-many server broadcast. We can broadcast to a range of IPs that have been permanently reserved as multicast addresses. The website <a class="ulink" href="http://www.iana.org/assignments/multicast-addresses/multicast-addresses.xhtml">http://www.iana.org/assignments/multicast-addresses/multicast-addresses.xhtml</a> has this to say:</p><div><blockquote class="blockquote"><p><em>"Host Extensions for IP Multicasting [RFC1112] specifies the extensions required of a host implementation of the Internet Protocol (IP) to support multicasting. The multicast addresses are in the range 224.0.0.0 through 239.255.255.255."</em></p></blockquote></div><p>Additionally, the range between 224.0.0.0 and 224.0.0.255 is further reserved for special routing protocols.</p><p>Also, certain <a id="id412" class="indexterm"/>port numbers are allocated for use by UDP (and TCP), a list of which can be found at <a class="ulink" href="https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers">https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers</a>.</p><p>The upshot of all this fascinating information is the knowledge that there is a block of IPs and ports reserved for UDP and/or multicasting, and we are now going to use some of them to implement multicasting over UDP with Node.</p><p>The only difference between setting up a multicasting UDP server and a "standard" one is the binding of the multicasting server to a special UDP port to indicate that we'd like to listen to <em>all</em> available network adapters. Our multicasting server initialization looks like this:</p><div><pre class="programlisting">var socket = dgram.createSocket('udp4');

var multicastAddress   = '230.1.2.3';
var multicastPort   = 5554;

socket.bind(multicastPort);

socket.on('listening', function() {
  this.setMulticastTTL(64);
  this.addMembership(multicastAddress);
});</pre></div><p>After<a id="id413" class="indexterm"/> requesting a multicast port binding<a id="id414" class="indexterm"/>, we wait for the socket listen event, at which point we can configure our server.</p><p>The most important command is <code class="literal">socket.addMembership</code>, which tells the kernel to join the multicast group at <code class="literal">multicastAddress</code>. Other UDP sockets can now subscribe to the multicast group at this address.</p><p>Datagrams hop through networks just like any network packet. The <code class="literal">setMulticastTTL</code> method is used to set the maximum number of hops ("time to live") a datagram is allowed to make before it is abandoned and not delivered. The acceptable range is 0–255, with the default being one (1) on most systems. This is not usually a setting one must worry about, but it is available if deep visibility into network topology lends relevance to this aspect of packet delivery.</p><div><div><h3 class="title"><a id="note23"/>Note</h3><p>If you'd also like to allow listening on the <em>local</em> interface, use <code class="literal">socket.setBroadcast(true)</code> and <code class="literal">socket.setMulticastLoopback(true)</code>. This is normally not necessary.</p></div></div><p>We are eventually going to use this server to broadcast messages to all UDP listeners on <code class="literal">multicastAddress</code>. For now, let's create two clients that will listen for multicasts:</p><div><pre class="programlisting">dgram.createSocket('udp4')
.on('message', function(message, remote) {
  console.log('Client1 received message ' + message + ' from ' + remote.address + ':' + remote.port);
})
.bind(multicastPort, multicastAddress);

dgram.createSocket('udp4')
.on('message', function(message, remote) {
  console.log('Client2 received message ' + message + ' from ' + remote.address + ':' + remote.port);
})
.bind(multicastPort, multicastAddress);</pre></div><p>We <a id="id415" class="indexterm"/>now have two clients listening to the same multicast <a id="id416" class="indexterm"/>port. All that is left to do is the multicasting. In this example, we will use <code class="literal">setTimeout</code> to send a counter value every second:</p><div><pre class="programlisting">var cnt = 1;
var sender;

(sender = function() {
  var msg = new Buffer("This is message #" + cnt);
  socket.send(
    msg,
    0,
    msg.length,
    multicastPort,
    multicastAddress
  );

  ++cnt;

  setTimeout(sender, 1000);

})();</pre></div><p>The preceding code will produce something like the following:</p><div><pre class="programlisting">
<strong>Client2 received message This is message #1 from 67.40.141.16:5554</strong>
<strong>Client1 received message This is message #1 from 67.40.141.16:5554</strong>
<strong>Client2 received message This is message #2 from 67.40.141.16:5554</strong>
<strong>Client1 received message This is message #2 from 67.40.141.16:5554</strong>
<strong>Client2 received message This is message #3 from 67.40.141.16:5554</strong>
<strong>...</strong>
</pre></div><p>We have two clients listening to broadcasts from a specific group. Let's add another client, listening on a different group—let's say at the multicast address <code class="literal">230.3.2.1</code>:</p><div><pre class="programlisting">dgram.createSocket('udp4')
.on('message', function(message, remote) {
  console.log('Client3 received message ' + message + ' from ' + remote.address + ':' + remote.port);
})
.bind(multicastPort, '230.3.2.1');</pre></div><p>Because <a id="id417" class="indexterm"/>our server currently broadcasts messages to a different <a id="id418" class="indexterm"/>address, we will need to change our server configuration and add this new address with another <code class="literal">addMembership</code> call:</p><div><pre class="programlisting">socket.on("listening", function() {
  this.addMembership(multicastAddress);
  this.addMembership('230.3.2.1');
});</pre></div><p>We can now send messages to <em>both</em> addresses:</p><div><pre class="programlisting">(sender = function() {
  socket.send(
    ...
    multicastAddress
  );

  socket.send(
    ...
    '230.3.2.1'
  );

  ...
})();</pre></div><p>Of course, nothing stops the client from broadcasting to others in its group or even members of <em>another</em> group:</p><div><pre class="programlisting">dgram.createSocket('udp4')
.on('message', function(message, remote) {
  var msg = new Buffer("Calling original group!");
  socket.send(
    msg,
    0,
    msg.length,
    multicastPort,
    '230.1.2.3' // multicastAddress
  );
})
.bind(multicastPort, '230.3.2.1');</pre></div><p>Any Node <a id="id419" class="indexterm"/>process that has an address on our network interface<a id="id420" class="indexterm"/> can now listen on a UDP multicast address for messages, providing a fast and elegant interprocess communication system.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Summary</h1></div></div></div><p>In this chapter, we looked at ways in which Node applications can be scaled both vertically and horizontally. We learned how to use <code class="literal">spawn</code> on OS processes and to use <code class="literal">fork</code> on new Node processes. The overview of the <code class="literal">cluster</code> module demonstrated how easy it is to scale across cores using Node and efficiently and easily distribute client connections across workers with built-in messaging channels to the central (master) hub. We also looked at how horizontally distributed processes and servers can communicate using message queues and UDP servers and how these servers can be load balanced and proxied using Nginx or using Node modules designed for that purpose.</p><p>Scaling is not only about servers and load balancing. In the next chapter, we'll look at how to scale and manage resources, learn about memory management techniques, synchronize data across distributed services, synchronize data-caching strategies, and look at how to deal with massive numbers of simultaneous connections.</p></div></body></html>