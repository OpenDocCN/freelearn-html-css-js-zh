- en: Applying the Event Sourcing and CQRS Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the event-first variant of the Event Sourcing pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a micro event store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the database-first variant of the Event Sourcing pattern with DynamoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying the database-first variant of the Event Sourcing pattern with Cognito
    datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a materialized view in DynamoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a materialized view in S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a materialized view in Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a materialized view in a Cognito dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replaying events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indexing the data lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing bi-directional synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud-native is autonomous. It empowers self-sufficient, full-stack teams to
    rapidly perform lean experiments and continuously deliver innovation with confidence.
    The operative word here is *confidence*. We leverage fully managed cloud services,
    such as function-as-a-service, cloud-native databases, and event streaming to
    decrease the risk of running these advanced technologies. However, at this rapid
    pace of change, we cannot completely eliminate the potential for human error.
    To remain stable despite the pace of change, cloud-native systems are composed
    of bounded, isolated, and autonomous services that are separated by bulkheads
    to minimize the blast radius when any given service experiences a failure. Each
    service is completely self-sufficient and stands on its own, even when related
    services are unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: Following reactive principles, these autonomous services leverage event streaming
    for all inter-service communication. Event streaming turns the database inside
    out by replicating data across services in the form of materialized views stored
    in cloud-native databases. This cloud-native data forms a bulkhead between services
    and effectively turns the cloud into the database to maximize responsiveness,
    resilience, and elasticity. The **Event Sourcing** and **Command Query Responsibility
    Segregation** (**CQRS**) patterns are fundamental to creating autonomous services.
    This chapter contains recipes that demonstrate how to use fully managed, serverless
    cloud services to apply these patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data lake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the major benefits of the Event Sourcing pattern is that it results in
    an audit trail of all the state changes within a system. This store of events
    can also be leveraged to replay events to repair broken services and seed new
    components. A cloud-native event stream, such as **AWS Kinesis**, only stores
    events for a short period of time, ranging from 24 hours to 7 days. An **event
    stream** can be thought of as a temporary or temporal event store that is used
    for normal, near real-time operation. In the *Creating a micro event store* recipe,
    we will discuss how to create specialized event stores that are dedicated to a
    single service. In this recipe, we will create a data lake in S3\. A **data lake**
    is a perpetual event store that collects and stores all events in their raw format
    in perpetuity with complete fidelity and high durability to support auditing and
    replay.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need an **AWS Kinesis Stream**, such as
    the one created in the *Creating an event stream* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-data-lake-s3` directory with `cd cncb-data-lake-s3`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, data lake bucket, and Firehose delivery stream in the AWS
    Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Allow the Firehose buffer time to process and then review the data lake contents
    created in the S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the data lake stack after you have worked through all the other recipes.
    This will allow you to watch the data lake accumulating all the other events.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important characteristic of a data lake is that it stores data in perpetuity.
    The only way to really meet this requirement is to use object storage, such as
    AWS S3\. S3 provides 11 nines of durability. Said another way, S3 provides 99.999999999%
    durability of objects over a given year. It is also fully managed and provides
    life cycle management features to age objects into cold storage. Note that the
    bucket is defined with the `DeletionPolicy` set to `Retain`. This highlights that
    even if the stack is deleted, we still want to ensure that we are not inappropriately
    deleting this valuable data.
  prefs: []
  type: TYPE_NORMAL
- en: We are using Kinesis Firehose because it performs the heavy lifting of writing
    the events to the bucket. It provides a buffer based on the time and size, compression,
    encryption, and error handling. To simplify this recipe, I did not use compression
    or encryption, but it is recommended that you use these features.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe defines one delivery stream, because in this cookbook, our stream
    topology consists of only one stream with `${cf:cncb-event-stream-${opt:stage}.streamArn}`.
    In practice, your topology will consist of multiple streams, and you will define
    one Firehose delivery stream per Kinesis stream to ensure that the data lake is
    capturing all events. We set `prefix` to `${cf:cncb-event-stream-${opt:stage}.streamName}/`
    so that we can easily distinguish the events in the data lake by their stream.
  prefs: []
  type: TYPE_NORMAL
- en: Another important characteristic of a data lake is that the data is stored in
    its raw format, without modification. To this end, the `transformer` function
    adorns all available metadata about the specific Kinesis stream and Firehose delivery
    stream, to ensure that all available information is collected. In the *Replaying
    events* recipe, we will see how this metadata can be leveraged. Also, note that
    `transformer` adds the end-of-line character (`\n`) to facilitate future processing
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the event-first variant of the Event Sourcing pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Event sourcing is a key pattern for designing eventually consistent cloud-native
    systems. Upstream services produce events as their state changes, and downstream
    services consume these events and produce their own events as needed. This results
    in a chain of events whereby services collaborate to produce a business process
    that results in an eventual consistency solution. Each step in this chain must
    be implemented as an atomic unit of work. Cloud-native systems do not support
    distributed transactions, because they do not scale horizontally in a cost-effective
    manner. Therefore, each step must update one, and only one, system. If multiple
    systems must be updated, then each is updated in a series of steps. In this recipe,
    we leverage the event-first variant of the Event Sourcing pattern where the atomic
    unit of work is writing to the event stream. The ultimate persistence of the data
    is delegated to downstream components.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-event-first` directory with `cd cncb-event-first`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and function in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the `submit` function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we implement a command function called `submit` that would be
    part of a Backend For Frontend service. Following the Event Sourcing pattern,
    we make this command atomic by only writing to a single resource. In some scenarios,
    such as initiating a long-lived business process or tracking user clicks, we only
    need to **fire-and-forget**. In these cases, the event-first variant is most appropriate.
    The command just needs to execute quickly and leave as little to chance as possible.
    We write the event to the highly available, fully-managed cloud-native event stream
    and trust that the downstream services will eventually consume the event.
  prefs: []
  type: TYPE_NORMAL
- en: The logic wraps the domain object in the standard event format, as discussed
    in the *Creating an event stream and publishing an event* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml), *Getting
    Started with Cloud-Native*. The event `type` is specified, the domain object ID
    is used as the `partitionKey`, and useful `tags` are adorned. Finally, the event
    is written to the stream specified by the `STREAM_NAME` environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a micro event store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Creating a data lake* recipe, we will discuss how the Event Sourcing
    pattern provides the system with an audit trail of all the state-change events
    in the system. An event stream essentially provides a temporal event store that
    feeds downstream event processors in near real-time. The data lake provides a
    high durability, perpetual event store that is the official source of record.
    However, we have a need for a middle ground. Individual stream processors need
    the ability to source specific events that support their processing requirement.
    In this recipe, we will implement a micro event store in **AWS DynamoDB** that
    is owned by and tailored to the needs of a specific service.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-micro-event-store` directory with `cd cncb-micro-event-store`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, functions, and table in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs for the `listener` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs for the `trigger` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When implementing a stream processor function, we often need more information
    than is available in the current event object. It is a best practice when publishing
    events to include all the relevant data that is available in the publishing context
    so that each event represents a micro snapshot of the system at the time of publishing.
    When this data is not enough, we need to retrieve more data; however, in cloud-native
    systems, we strive to eliminate all synchronous inter-service communication because
    it reduces the *autonomy* of the services. Instead, we create a micro event store
    that is tailored to the needs of the specific service.
  prefs: []
  type: TYPE_NORMAL
- en: First, we implement a `listener` function and `filter` for the desired events
    from the stream. Each event is stored in a DynamoDB table. You can store the entire
    event or just the information that is needed. When storing these events, we need
    to collate related events by carefully defining the `HASH` and `RANGE` keys. For
    example, we might want to collate all events for a specific domain object ID or
    all events from a specific user ID. In this example, we use `event.partitionKey`
    as the hash key, but you can calculate the hash key from any of the available
    data. For the range key, we need a value that is unique within the hash key. The
    `event.id` is a good choice if it is implemented with a V1 UUID because they are
    time-based. The Kinesis sequence number is another good choice. The `event.timestamp`
    is another alternative, but there could be a potential that events are created
    at the exact same time within a hash key.
  prefs: []
  type: TYPE_NORMAL
- en: The `trigger` function, which is attached to the DynamoDB stream, takes over
    after the `listener` has saved an event. The trigger calls `getMicroEventStore`
    to retrieve the micro event store based on the hash key calculated for the current
    event. At this point, the stream processor has all the relevant data available
    in memory. The events in the micro event store are in historical order, based
    on the value used for the range key. The stream processor can use this data however
    it sees fit to implement its business logic.
  prefs: []
  type: TYPE_NORMAL
- en: Use the DynamoDB TTL feature to keep the micro event store from growing unbounded.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the database-first variant of the Event Sourcing pattern with DynamoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, *Applying the event-first variant of the Event Sourcing
    pattern*, we discussed how the Event Sourcing pattern allows us to design eventually
    consistent systems that are composed of a chain of atomic steps. Distributed transactions
    are not supported in cloud-native systems, because they do not scale effectively.
    Therefore, each step must update one, and only one, system. In this recipe, we
    will leverage the **database-first** variant of the Event Sourcing pattern, where
    the atomic unit of work is writing to a single cloud-native database. A cloud-native
    database provides a **change data capture** mechanism that allows further logic
    to be atomically triggered that publishes an appropriate **domain event** to the
    event stream for further downstream processing. In this recipe, we will demonstrate
    implementing this pattern with **AWS DynamoDB** and **DynamoDB Streams**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-db-first-dynamodb` directory with `cd cncb-db-first-dynamodb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, functions, and table in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `command` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the trigger function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Review the events collected in the data lake bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we implement a `command` function that would be part of a Backend
    For Frontend service. Following the Event Sourcing pattern, we make this command
    atomic by only writing to a single resource. In many scenarios, such as the authoring
    of data, we need to write data and make sure it's immediately available for reading.
    In these cases, the *database-first* variant is most appropriate. The command
    just needs to execute quickly and leave as little to chance as possible. We write
    the **domain object** to the highly available, fully-managed cloud-native database
    and trust that the database's **change data capture** mechanism will handle the
    next step.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, the database is DynamoDB and the change data capture mechanism
    is DynamoDB Streams. The `trigger` function is a stream processor that is consuming
    events from the specified DynamoDB stream. We enable the stream by adding the
    `StreamSpecification` to the definition of the table.
  prefs: []
  type: TYPE_NORMAL
- en: The stream processor logic wraps the domain object in the standard event format,
    as discussed in the *Creating an event stream and publishing an event* recipe
    in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml), *Getting Started with
    Cloud-Native*. The `record.eventID` generated by DynamoDB is reused as the domain
    event ID, the database trigger's `record.eventName` is translated into the domain
    event type, the domain object ID is used as `partitionKey`, and useful `tags`
    are adorned. The `old` and `new` values of the domain object are included in the
    event so that downstream services can calculate a delta however they see fit.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the event is written to the stream specified by the `STREAM_NAME` environment
    variable. Note that the trigger function is similar to the *event-first* variant.
    It just needs to execute quickly and leave as little to chance as possible. We
    write the event to a single resource, the highly available, fully-managed cloud-native
    event stream, and trust that the downstream services will eventually consume the
    event.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the database-first variant of the Event Sourcing pattern with Cognito
    datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Applying the event-first variant of the Event Sourcing pattern* recipe,
    we discussed how the Event Sourcing pattern allows us to design eventually consistent
    systems that are composed of a chain of atomic steps. Distributed transactions
    are not supported in cloud-native systems, because they do not scale effectively.
    Therefore, each step must update one, and only one, system. In this recipe, we
    leverage the *database-first* variant of the Event Sourcing pattern, where the
    atomic unit of work is writing to a single cloud-native database. A cloud-native
    database provides a change data capture mechanism that allows further logic to
    be atomically triggered that publishes an appropriate domain event to the event
    stream for further downstream processing. In the recipe, we demonstrate an **offline-first**
    implementation of this pattern with **AWS Cognito datasets**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-db-first-cognito` directory with `cd cncb-db-first-cognito`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, function, and identity pool in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the file named `./index.html` with the  `identityPoolId` from the previous
    output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the file named `./index.html` in a browser, enter a `name` and `description`,
    and press `Save` and then `Synchronize`, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9b194c69-5f43-45c1-95cc-69277df5207e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we are implementing an offline-first solution where the ReactJS
    client application stores all changes in local storage and then synchronizes the
    data to a Cognito dataset in the cloud when connectivity is available. This scenario
    is very common in mobile applications where the mobile application may not always
    be connected. An AWS Cognito dataset is associated with a specific user in an
    **AWS Identity Pool**. In this recipe, the identity pool supports unauthenticated
    users. Anonymous access is another common characteristic of mobile applications.
  prefs: []
  type: TYPE_NORMAL
- en: The bare bones ReactJS application is implemented in the `./index.html` file.
    It contains a form for the user to enter data. The `Save` button saves the form's
    data to local storage via the Cognito SDK. The `Synchronize` button uses the SDK
    to send the local data to the cloud. In a typical application, this synchronization
    would be triggered behind the scenes by events in the normal flow of the application,
    such as on save, on load, and before exit. In the *Creating a materialized view
    in a Cognito Dataset* recipe, we show how synchronizing will also retrieve data
    from the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Cognito's change data capture feature is implemented via **AWS Kinesis**. Therefore,
    we create a Kinesis stream called `CognitoStream` that is dedicated to our Cognito
    datasets. The `trigger` function is a stream processor that is consuming sync
    records from this stream. The stream processor's `recordToSync` step extracts
    the domain object from each sync record, where it is stored as a JSON string.
    The `toEvent` step wraps the domain object in the standard event format, as discussed
    in the *Creating an event stream and publishing an event* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. Finally, the event is written to the stream
    specified by the `STREAM_NAME` environment variable. Note that the trigger function
    is similar to the event-first variant. It just needs to execute quickly and leave
    as little to chance as possible. We write the event to a single resource, the
    highly available, fully managed cloud-native event stream, and trust that the
    downstream services will eventually consume the event.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a materialized view in DynamoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Command Query Responsibility Segregation** (**CQRS**) pattern is critical
    for designing cloud-native systems that are composed of bounded, isolated, and
    autonomous services with appropriate bulkheads to limit the blast radius when
    a service experiences an outage. These bulkheads are implemented by creating materialized
    views in downstream services.
  prefs: []
  type: TYPE_NORMAL
- en: Upstream services are responsible for the commands that write data using the
    Event Sourcing pattern. Downstream services take responsibility for their own
    queries by creating materialized views that are specifically tailored to their
    needs. This **replication** of data increases scalability, reduces latency, and
    allows services to be completely **autonomous** and function even when upstream
    source services are unavailable. In this recipe, we will implement a materialized
    view in **AWS DynamoDB**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-materialized-view-dynamodb` directory with `cd cncb-materialized-view-dynamodb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, functions, and table in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `listener` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the `query` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we implemented a `listener` function that consumes upstream
    events and populates a materialized view that is used by a **Backend For Frontend** (**BFF**)
    service. This function is a *stream processor,* such as the one we discussed in
    the *Creating a stream processor* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. The function performs a `filter` for the
    desired events and then transforms the data in a `map` step to the desired materialized
    view. The materialized view is optimized to support the requirements of the query
    needed by the BFF. Only the minimum necessary data is stored and the optimal database
    type is used. In this recipe, the database type is DynamoDB. DynamoDB is a good
    choice for a materialized view when the data changes frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `asOf` timestamp is included in the record. In an eventually consistent
    system, it is important to provide the user with the `asOf` value so that he or
    she can access the latency of the data. Finally, the data is stored in the highly
    available, fully managed, cloud-native database.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a materialized view in S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Creating a materialized view in* *DynamoDB* recipe, we discussed how
    the CQRS pattern allows us to design services that are bounded, isolated, and
    autonomous. This allows services to operate, even when their upstream dependencies
    are unavailable, because we have eliminated all synchronous inter-service communication
    in favor of replicating and caching the required data locally in dedicated materialized
    views. In this recipe, we will implement a materialized view in AWS S3.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-materialized-view-s3` directory with `cd cncb-materialized-view-s3`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, function, and bucket from the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the following command, after updating the `bucket-suffix`, to get the
    data from S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Use the console to delete the objects from the bucket before removing the stack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we implement a `listener` function that consumes upstream events
    and populates a materialized view that is used by a Backend For Frontend service.
    This function is a stream processor, such as the one we discussed in the *Creating
    a stream processor* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. The function performs a `filter` for the
    desired events and then transforms the data in a `map` step to the desired materialized
    view. The materialized view is optimized to support the requirements of the query
    needed by the BFF. Only the minimum necessary data is stored, and the optimal
    database type is used.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, the database type is S3\. S3 is a good choice for a materialized
    view when the data changes infrequently, and it can be cached in the CDN. Note
    that the `asOf` timestamp is included in the record so that the user can access
    the latency of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a materialized view in Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Creating a materialized view in DynamoDB* recipe, we discussed how the
    CQRS pattern allows us to design services that are bounded, isolated, and autonomous.
    This allows services to operate, even when their upstream dependencies are unavailable,
    because we have eliminated all synchronous inter-service communication in favor
    of replicating and caching the required data locally in dedicated materialized
    views. In this recipe, we will implement a materialized view in AWS Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-materialized-view-es` directory with `cd cncb-materialized-view-es`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying an Elasticsearch domain can take upwards of 1-20 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, function, and Elasticsearch domain in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `listener` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the `search` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we implement a `listener` function that consumes upstream events
    and populates a materialized view that is used by a Backend For Frontend service.
    This function is a stream processor, such as the one we discussed in the *Creating
    a stream processor* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. The function performs a `filter` for the
    desired events and then transforms the data in a `map` step to the desired materialized
    view. The materialized view is optimized to support the requirements of the query
    needed by the BFF. Only the minimum necessary data is stored, and the optimal
    database type is used. In this recipe, the database type is Elasticsearch. Elasticsearch
    is a good choice for a materialized view when the data must be searched and filtered.
    Note that the `asOf` timestamp is included in the record so that the user can
    access the latency of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a materialized view in a Cognito dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Creating a materialized view in DynamoDB* recipe, we discussed how the
    CQRS pattern allows us to design services that are bounded, isolated, and autonomous.
    This allows services to operate, even when their upstream dependencies are unavailable,
    because we have eliminated all synchronous inter-service communication in favor
    of replicating and caching the required data locally in materialized views. In
    this recipe, we will implement an offline-first materialized view in an AWS Cognito
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-materialized-view-cognito` directory with `cd cncb-materialized-view-cognito`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, function, and identity pool in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the file named `index.html` file with the `identityPoolId` from previous
    output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the file named `index.html` in a browser and copy the identity ID for use
    in the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the file named `index.html` in a browser and press the Synchronize button
    to retrieve the data from the materialized view:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/16110567-c727-48a5-83cc-3657233b5f86.png)'
  prefs: []
  type: TYPE_IMG
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we implement a `listener` function that consumes upstream events
    and populates a materialized view that is used by a Backend For Frontend service.
    This function is a stream processor, such as the one we discussed in the *Creating
    a stream processor* recipe in [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml),
    *Getting Started with Cloud-Native*. The function performs a `filter` for the
    desired events and then transforms the data in a `map` step to the desired materialized
    view. The materialized view is optimized to support the requirements of the query
    needed by the BFF. Only the minimum necessary data is stored, and the optimal
    database type is used.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, the database type is a Cognito dataset. A Cognito dataset is
    a good choice for a materialized view when network availability is intermittent,
    and thus an offline-first approach is needed to synchronize data to a user's devices.
    The data must also be specific to a user so that it can be targeted to the user
    based on the user's `identityId`.  Due to the intermittent nature of connectivity,
    the `asOf` timestamp is included in the record so that the user can access the
    latency of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Replaying events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the advantages of the Event Sourcing and data lake patterns is that they
    allow us to replay events when necessary to repair broken services and seed new
    services, and even new versions of a service. In this recipe, we will implement
    a utility that reads selected events from the data lake and applies them to a
    specified Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need the data lake that was created in
    the *Creating a data lake* recipe in this chapter. The data lake should contain
    events that were generated by working through the other recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-replaying-events` directory with `cd cncb-replaying-events`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `./lib/replay.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Replay events with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we implement a **Command-Line Interface** (**CLI**) program
    that reads events from the data lake S3 bucket and sends them to a specific AWS
    Lambda function. When replaying events, we do not re-publish the events because
    this would broadcast the events to all subscribers. Instead, we want to replay
    events to a specific function to either repair the specific service or seed a
    new service.
  prefs: []
  type: TYPE_NORMAL
- en: When executing the program, we provide the name of the data lake `bucket` and
    the specific path `prefix` as arguments. The `prefix` allows us to replay only
    a portion of the events, such as a specific month, day, or hour. The program uses
    functional reactive programming with the `Highland.js` library. We use a `generator`
    function to page through the objects in the bucket and `push` each object down the
    stream. **Backpressure** is a major advantage of this programming approach, as
    we will discuss in [Chapter 8](5c400ff6-91da-4782-9369-549622d4a0d1.xhtml), *Designing
    for Failure*. If we retrieved all the data from the bucket in a loop, as we would
    in the imperative programming style, then we would likely run out of memory and/or
    overwhelm the Lambda function and receive throttling errors.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we pull data through the stream. When downstream steps are ready for
    more work they pull the `next` piece of data. This triggers the generator function
    to paginate data from S3 when the program is ready for more data.
  prefs: []
  type: TYPE_NORMAL
- en: When storing events in the data lake bucket, Kinesis Firehose buffers the events
    until a maximum amount of time is reached or a maximum file size is reached. This
    buffering maximizes the write performance when saving the events. When transforming
    the data for these files, we delimited the events with an EOL character. Therefore,
    when we `get` a specific file, we leverage the Highland.js `split` function to
    stream each row in the file one at a time. The split function also supports **backpressure**.
  prefs: []
  type: TYPE_NORMAL
- en: For each event, we `invoke` the `function` specified in the command-line arguments.
    These functions are designed to listen for events from a Kinesis stream. Therefore,
    we must wrap each event in the Kinesis input format that these functions are expecting.
    This is one reason why we included the Kinesis metadata when saving the events
    to the data lake in the *Creating a data lake* recipe. To maximize throughput,
    we invoke the Lambda *asynchronously* with the `Event` InvocationType, provided
    that the payload size is within the limits. Otherwise, we invoke the Lambda *synchronously*
    with the `RequestReponse` InvocationType. We also leverage the Lambda `DryRun`
    feature so that we can see what events might be replayed before actually effecting
    the change.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing the data lake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data lake is a crucial design pattern for providing cloud-native systems with
    an audit trail of all the events in a system and for supporting the ability to
    replay events. In the *Creating a data lake* recipe, we implemented the S3 component
    of the data lake that provides high durability. However, a data lake is only useful
    if we can find the relevant data. In this recipe, we will index all the events
    in Elasticsearch so that we can search events for troubleshooting and business
    analytics.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-data-lake-es` directory with `cd cncb-data-lake-es`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying an Elasticsearch domain can take upwards of 20 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, function, and Elasticsearch domain in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Allow the Firehose buffer time to process, as the buffer interval is 60 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the `transformer` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Open Kibana using the preceding `KibanaEndpoint` output with protocol `https`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `Management` menu, set the index pattern to `events-*`, and press
    the `Next step` button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `timestamp` as the `Time Filter field name`, and press `Create Index
    pattern`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `Discover` menu to view the current events in the index.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data lake is a valuable source of information. Elasticsearch is uniquely
    suited for indexing this coarse-grained time series information. **Kibana** is
    the data visualization plugin for Elasticsearch. Kibana is a great tool for creating
    dashboards containing statistics about the events in the data lake and to perform
    ad hoc searches to troubleshoot system problems based on the contents of the events.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are using Kinesis Firehose because it performs the heavy
    lifting of writing the events to Elasticsearch. It provides buffering based on
    time and size, hides the complexity of the Elasticsearch bulk index API, provides
    error handling, and supports index rotation. In the custom `elasticsearch` Serverless
    plugin, we create the index template that defines the `index_patterns` and the
    `timestamp` field used to affect the index rotation.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe defines one delivery stream, because in this cookbook, our stream
    topology consists of only one stream with `${cf:cncb-event-stream-${opt:stage}.streamArn}`.
    In practice, your topology will consist of multiple streams and you will define
    one Firehose delivery stream per Kinesis stream to ensure that all events are
    indexed.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing bi-directional synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud-native systems are architectured to support the continuous evolution of
    the system. Upstream and downstream services are designed to be pluggable. New
    service implementations can be added without impacting related services. Furthermore,
    continuous deployment and delivery necessitate the ability to run multiple versions
    of a service side by side and synchronize data between the different versions.
    The old version is simply removed when the new version is complete and the feature
    is flipped on. In this recipe, we will enhance the *database-first* variant of
    the Event Sourcing pattern with the *latching* pattern to facilitate bi-directional
    synchronization without causing an infinite loop of events.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create two projects from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `serverless.yml` with the following content in each project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-1-bi-directional-sync` directory with`cd cncb-1-bi-directional-sync`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack, functions, and table in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the `cncb-2-bi-directional-sync` directory with `cd cncb-2-bi-directional-sync`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 5-9 for the `cncb-2-bi-directional-sync` project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate back to the `cncb-1-bi-directional-sync` directory with `cd cncb-1-bi-directional-sync`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the `command` function to save data to the first service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs for the `command` and `trigger` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-2-bi-directional-sync` directory with `cd cncb-2-bi-directional-sync`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a look at the logs for the `listener` and `trigger` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the `query` function to retrieve the synchronized data to the second
    service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Remove both stacks once you have finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud-native systems are architected to evolve. Over time, the functional requirements
    will change and the technology options will improve. However, some changes are
    not incremental and/or do not support an immediate switch from one implementation
    to another. In these cases, it is necessary to have multiple versions of the same
    functionality running simultaneously. If these services produce data, then it
    is necessary to synchronize data changes between the services. This bi-directional
    synchronization will produce an infinite messaging loop if an appropriate **latching**
    mechanism is not employed.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe builds on the database-first variant of the Event Sourcing pattern.
    A user of service one invokes the command function. The `command` function opens
    the `latch` by setting the latch on the domain object to `open`. The `trigger`
    function's `forLatchOpen` filter will only allow publishing an event when the
    latch is `open`, because the open latch indicates that the change originated in
    service one. The `listener` function's `forSourceNotSelf` filter in service one
    ignores the event because the `source` tag indicates that the event originates
    from service one. The `listener` function in service two closes the `latch` before
    saving the data by setting the latch on the domain object to `closed`. The `trigger`
    function in service two does not publish an event, because the `closed` latch
    indicates that the change did not originate in service two.
  prefs: []
  type: TYPE_NORMAL
- en: This same flow unfolds when the command originates in service two. You can add
    a third and fourth service and more, and all the services will remain in sync.
  prefs: []
  type: TYPE_NORMAL
