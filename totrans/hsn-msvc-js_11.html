<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-173"><a id="_idTextAnchor174"/>11</h1>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/>Microservices Architecture</h1>
			<p>The world of software development is constantly evolving. As applications grow in complexity, traditional monolithic architectures struggle to keep pace. This chapter dives into some key design patterns that empower developers to build scalable and resilient systems – an API gateway, <strong class="bold">Command Query Responsibility Segregation</strong> (<strong class="bold">CQRS</strong>), event <a id="_idIndexMarker777"/>sourcing, and Service Registry and discovery.</p>
			<p>These patterns, particularly when used together within a microservices architecture, offer numerous benefits. They promote loose coupling between services, making them easier to develop, maintain, and deploy independently. They also enhance scalability by allowing individual services to be scaled based on specific needs. Additionally, these patterns contribute to improved fault tolerance and resilience, ensuring that your application remains robust even if individual services encounter issues.</p>
			<p>This chapter will provide a comprehensive introduction to each of these patterns, outlining their core concepts, benefits, and use cases. We will explore how to apply some of them to create a robust foundation for building modern, scalable applications. By understanding these patterns, you’ll be equipped to design and develop applications that can thrive in the ever-changing landscape of software development.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Getting started with an API gateway</li>
				<li>CQRS and event sourcing</li>
				<li>Service Registry and discovery in microservices</li>
			</ul>
			<p>Let’s get into the chapter!</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor176"/>Technical requirements</h1>
			<p>To follow us along the chapter, we need an IDE (we prefer Visual Studio Code), Postman, Docker, and a browser of your choice.</p>
			<p>It is preferable to download the repository from <a href="https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript">https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript</a> and open the <code>Ch11</code> folder to easily follow our code snippets.</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor177"/>Getting started with an API gateway</h1>
			<p>An API Gateway<a id="_idIndexMarker778"/> integrates with a microservice architecture by acting as a central hub, managing communication between client applications and the distributed microservices. When we build our microservices, we want them independently developed, deployed, and scaled without affecting client applications. Clients only interact with the API gateway, which shields them from the complexities of the underlying microservice network.</p>
			<div><div><img alt="Figure 11.1: A simple API Gateway" src="img/B09148_11_001.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1: A simple API Gateway</p>
			<p>The API Gateway receives requests from clients and intelligently routes them to the appropriate microservice(s), based on the request content or URL. It can handle simple routing or complex scenarios, involving multiple microservices working together to fulfill a request. Let’s explore the importance of integrating an API Gateway into our microservice architecture:</p>
			<ul>
				<li><strong class="bold">Simplified client interaction</strong>: Clients have a centralized entry point/single point of contact (the API gateway) to interact with an application, regardless of how many microservices are involved. This reduces development complexity on the client side.</li>
				<li><strong class="bold">Improved scalability</strong>: An API Gateway can be independently scaled to handle increasing traffic volumes without impacting the individual microservices. Microservices can also be scaled independently based on their specific workloads, highlighting the importance of API gateways.</li>
				<li><strong class="bold">Enhanced security</strong>: Centralized security management of an API Gateway strengthens overall application security. The API Gateway can implement authentication, authorization, and other security policies to protect microservices from unauthorized access.</li>
				<li><strong class="bold">Reduced development complexity</strong>: Developers don’t need to implement functionalities such as routing, security, and monitoring logic within each microservice. An API Gateway handles these cross-cutting concerns centrally.</li>
			</ul>
			<p>Let’s now see how <a id="_idIndexMarker779"/>an API Gateway might work.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor178"/>How an API Gateway works</h2>
			<p>In a microservice <a id="_idIndexMarker780"/>architecture, an API Gateway acts as the central entry point for all client requests. It plays a crucial role in managing and optimizing the flow of communication between clients and backend services. By handling authentication, routing, load balancing, and other vital functions, the API Gateway ensures that the microservices remain loosely coupled and scalable.</p>
			<p>Here’s a step-by-step breakdown of how an API Gateway typically processes client requests:</p>
			<ol>
				<li><strong class="bold">Client request</strong>: A client (e.g., a web or mobile app) sends a request to an API Gateway. The request includes details such as the HTTP method, URL path, headers, and possibly a body.</li>
				<li><strong class="bold">Request handling</strong>: The API Gateway receives the request and examines its contents. Based on the URL path or other routing rules, the Gateway determines which backend service(s) should handle the request.</li>
				<li><strong class="bold">Authentication and authorization</strong>: The API Gateway checks the request for authentication tokens (e.g., JWT or OAuth tokens). It verifies the token’s validity and checks whether the client has the necessary permissions to access the requested resource.</li>
				<li><strong class="bold">Request transformation</strong>:  The API Gateway may modify the request to fit the requirements of the backend service. This might include changing the protocol, altering headers, or modifying the request body.</li>
				<li><strong class="bold">Routing and aggregation</strong>: The Gateway routes the request to the appropriate backend service(s). If the request involves multiple services, the Gateway will handle communication with each service and aggregate their responses into a single response for the client.</li>
				<li><strong class="bold">Caching and load balancing</strong>: The Gateway checks whether the response is cached to serve it quickly without hitting the backend service. It also distributes the request load among multiple instances of the backend service to balance traffic and improve <a id="_idIndexMarker781"/>performance.</li>
				<li><strong class="bold">Rate limiting and throttling</strong>: The API Gateway enforces rate limits to control the number of requests a client can make within a specified period. It may throttle requests if a client exceeds the allowed request rate.</li>
				<li><strong class="bold">Response handling</strong>: Once the backend service(s) respond, the Gateway may modify the response before sending it back to the client. This could include adding or removing headers, transforming data formats, or aggregating multiple responses.</li>
				<li><strong class="bold">Logging and monitoring</strong>: The API Gateway logs details of the request and response for monitoring and analysis. Metrics such as request counts, response times, and error rates are tracked to monitor the health and performance of the services.</li>
			</ol>
			<p>Now that we know <a id="_idIndexMarker782"/>how an API Gateway works, let’s see what the better choice in a given situation is – single or multiple API gateways.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor179"/>Single versus multiple API gateways</h2>
			<p>You can implement <a id="_idIndexMarker783"/>multiple API gateways in a<a id="_idIndexMarker784"/> microservice architecture, but it’s not always the most straightforward or recommended approach. There are situations where it might be beneficial, but generally, a single API Gateway is preferred for simplicity and maintainability.</p>
			<p>A single API Gateway is ideal when you want centralized management, a consistent client experience, and simplified scalability – all of which streamline API operations and reduce complexity.</p>
			<p>While a single Gateway is often preferred, there are some situations where multiple gateways might be considered:</p>
			<ul>
				<li><strong class="bold">Heterogeneous client types</strong>: If you have clients using vastly different protocols or communication styles (e.g., mobile apps, web applications, and legacy systems), separate API gateways could be used to cater to these specific needs with custom protocols or functionalities. This approach can be complex to maintain in the long run.</li>
				<li><strong class="bold">Physical separation</strong>: If your microservices are geographically distributed across different data centers or cloud regions, you might consider placing an API Gateway in each location for performance reasons. However, this introduces additional management overhead to maintain consistency across gateways.</li>
				<li><strong class="bold">Security segmentation</strong>: In very specific security-sensitive scenarios, you might implement separate API gateways for different security zones within your application. This allows for stricter control over access to certain microservices. However, this requires careful design and expertise to avoid creating unnecessary complexity.</li>
			</ul>
			<p>Generally, the benefits of a single API Gateway outweigh the potential advantages of using multiple gateways, as the former promotes simplicity, maintainability, and a consistent client experience.</p>
			<p>If you want to reap the benefits of multiple API gateways without the complexity, here are some alternatives:</p>
			<ul>
				<li><strong class="bold">An API Gateway with routing by client type</strong>: Consider using a single API Gateway with routing logic that can differentiate between different client types and tailor responses accordingly.</li>
				<li><strong class="bold">Microservice facades</strong>: Implement a <strong class="bold">facade</strong> pattern (more on this shortly) within some <a id="_idIndexMarker785"/>microservices to handle specific client interactions, potentially reducing the need for multiple gateways.</li>
			</ul>
			<p>You should carefully consider your specific needs before implementing multiple API gateways. In most cases, a well-designed single API Gateway will provide the optimal solution for your microservice architecture.</p>
			<p class="callout-heading">The facade pattern</p>
			<p class="callout">A facade in this context refers to implementing a layer within some microservices that specifically handles interactions with clients. Instead of introducing multiple API gateways, which can add complexity, a microservice facade acts as a simplified interface or <em class="italic">front</em> that abstracts the internal workings of the microservice for the client.</p>
			<p>It is time to<a id="_idIndexMarker786"/> implement<a id="_idIndexMarker787"/> and see the power of an API Gateway in practice. The next section will dive into the details of the practical implementation of an API gateway.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor180"/>Implementing microservices using an API gateway</h2>
			<p>It is possible to<a id="_idIndexMarker788"/> implement the API Gateway <a id="_idIndexMarker789"/>pattern using different forms with different libraries. In this context, <code>Ch11</code>/<code>ApiGateway</code> folder in our repo.</p>
			<p>To demonstrate the real value of the API Gateway pattern, we need to have at least two microservices. The reason for needing <em class="italic">at least two</em> microservices to show the true value of the API Gateway pattern is that the pattern is designed to handle multiple services and consolidate their<a id="_idIndexMarker791"/> functionality for<a id="_idIndexMarker792"/> clients. We will use the following two microservices in this chapter:</p>
			<ul>
				<li>The post-microservice</li>
				<li>The user microservice</li>
			</ul>
			<h3>Implementing post microservice</h3>
			<p>Our first<a id="_idIndexMarker793"/> microservice, the <em class="italic">post-microservice</em>, acts as a<a id="_idIndexMarker794"/> wrapper/abstraction over the <code>jsonplaceholder</code> service. <code>jsonplaceholder</code> is a free online service that provides a REST API with fake data. It’s often used by developers to easily access and utilize realistic-looking sample data (users, posts, comments, etc.) without having to set up their databases. This allows them to quickly test API endpoints, frontend functionality, and user interactions.</p>
			<ol>
				<li>Create a new folder (a <code>post-microservice</code> folder in our case).</li>
				<li>Run <code>npm install express axios</code> to install the required packages.</li>
			</ol>
			<p>Here is what your <code>package.json</code> should look like:</p>
			<pre class="console">
{
  "dependencies": {
    "axios": "^1.7.2",
    "express": "^4.19.2"
  }
}</pre>			<p>For all chapters, you don’t need to install the exact package versions listed. While our focus is on using the packages themselves rather than specific versions, if there are major changes or breaking differences in newer versions, refer to the official documentation for updates.</p>
			<p>Now, let’s create a new file called <code>server.js</code> in the folder we created (i.e., <code>post-microservices</code>) with<a id="_idIndexMarker795"/> the following<a id="_idIndexMarker796"/> code block:</p>
			<pre class="source-code">
const express = require('express');
const axios = require('axios'); // Requires the axios library for making HTTP requests
const app = express();
const port = 3001; // Port on which the server will listen
app.get('/posts/:id', async (req, res) =&gt; {
  const postId = req.params.id; // Extract the ID from the URL parameter
  try {
    const response = await axios.get(
     `https://jsonplaceholder.typicode.com/posts/${postId}`);
    const post = response.data;
    if (post) {
      res.json(post); // Send the retrieved post data as JSON response
    } else {
      res.status(404).send('Post not found'); // Respond with 404 if post not found
    }
  } catch (error) {
    console.error(error);
    res.status(500).send('Internal Server Error'); // Handle errors with 500 status
  }
});
app.listen(port, () =&gt; {
  console.log(`Server listening on port ${port}`);
});</pre>			<p>This code snippet uses the Express framework to create a simple web server that listens on port <code>3001</code>. It imports the <code>axios</code> library to make HTTP requests. The server has a single route, <code>/posts/:id</code>, which responds to <code>GET</code> requests. When a request is made to this route, it extracts the <code>id</code> parameter from the URL. The server then makes an asynchronous request to <code>https://jsonplaceholder.typicode.com/posts/${postId}</code> to fetch a specific post. If the post is found, it sends the post data as a JSON response. If the post is not found, it responds with a <code>404</code> status code. If there are any errors during the request, it logs<a id="_idIndexMarker797"/> them and responds with a <code>500</code>-status <a id="_idIndexMarker798"/>code, indicating an internal server error.</p>
			<p>Let’s run our microservice using the <code>node server.js</code> command and test whether everything is working. Open your favorite browser and navigate to <code>localhost:3001/posts/1</code> (<em class="italic">Figure 11</em><em class="italic">.2</em>).</p>
			<div><div><img alt="Figure 11.2: A post-microservice response" src="img/B09148_11_002.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2: A post-microservice response</p>
			<h3>Implementing user microsevice</h3>
			<p>Our second<a id="_idIndexMarker799"/> microservice <a id="_idIndexMarker800"/>is called a <em class="italic">user microservice</em>. It has approximately the same implementation as our post microservice, with a different port (<code>3002</code>) and different service abstraction (a GitHub service abstraction):</p>
			<pre class="source-code">
const express = require('express');
const axios = require('axios'); // Requires the axios library for making HTTP requests
const app = express();
const port = 3002; // Port on which the server will listen
app.get('/users/:id', async (req, res) =&gt; {
    const userId = req.params.id; // Extract the ID from the URL parameter
    try {
        const response = await
          axios.get(`https://api.github.com/users/${userId}`);
        const user = response.data;
        if (user) {
            res.json(user); // Send the retrieved employee data as JSON response
        } else {
            res.status(404).send('User not found'); // Respond with 404 if employee not found
        }
    } catch (error) {
        console.error(error);
        res.status(500).send('Internal Server Error'); // Handle errors with 500 status
    }
});
app.listen(port, () =&gt; {
    console.log(`Server listening on port ${port}`);
});</pre>			<p>Let’s run our microservice using the <code>node server.js</code> command and test whether everything is working. Open your favorite browser and navigate to <code>localhost:3002/users/1</code> (<em class="italic">Figure 11</em><em class="italic">.3</em>).</p>
			<p class="IMG---Figure"> </p>
			<div><div><img alt="Figure 11.3: A user microservice response" src="img/B09148_11_003.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3: A user microservice response</p>
			<p>Let’s build our<a id="_idIndexMarker801"/> API <a id="_idIndexMarker802"/>Gateway as a third microservice and combine the post and user microservices.</p>
			<h3>Developing an API gateway</h3>
			<p>After implementing<a id="_idIndexMarker803"/> two microservices, we’re ready to show the value and power of an API gateway. We plan to implement rate limit, cache, and response aggregation functionalities for the API gateway. You can add more features such as logging, appropriate exception handling, monitoring, and other interesting behaviors after understanding the essentials.</p>
			<p>First things first – you need to understand that an API Gateway by itself acts as a separate microservice. So, create a new folder for it (it is called <code>api-</code><code>g</code><code>ateway</code> in our GitHub repo). We have <code>package.json</code> with the following content:</p>
			<pre class="console">
{
  "dependencies": {
    "apicache": "^1.6.3",
    "axios": "1.7.2",
    "express": "4.19.2",
    "express-rate-limit": "7.3.1"
  }
}</pre>			<p>We will use an <code>express-rate-limit</code> package to implement rate-limit functionality in our microservice. In a microservice architecture, where applications are broken down into smaller, independent services, <strong class="bold">rate limiting</strong> is a<a id="_idIndexMarker804"/> technique used to control the number of requests<a id="_idIndexMarker805"/> that a service can receive within a specific timeframe. It acts like a traffic controller, preventing a service from being overloaded by a surge of requests.</p>
			<p>Conversely, <code>apicache</code> is used to implement cache behavior for an API gateway. <strong class="bold">Caching</strong> refers to <a id="_idIndexMarker806"/>a functionality that allows you to store responses from your backend services for a specific time. This cached data can then be served to subsequent requests, improving performance and reducing load on your backend.</p>
			<p>Let’s create a <code>server.js</code> file to implement an API gateway. Our imported packages look like this:</p>
			<pre class="source-code">
const express = require('express');
const apicache = require('apicache');
const axios = require('axios');
const rateLimit = require('express-rate-limit');</pre>			<p>First, let’s configure our rate limit:</p>
			<pre class="source-code">
const limiter = rateLimit({
    windowMs: 60000, // 1 minute window
    max: 100, // 100 requests per minute
    message: 'Too many requests, please slow down!'
});</pre>			<p>We use <code>express-rate-limit</code> to control how many times users can access your API Gateway in a minute. It acts like a gatekeeper. If a user makes fewer than a hundred requests within a minute, they get through. If they go over a hundred, they’ll be blocked with a <code>Too many requests, please slow down</code> message. This protects our API from overload and ensures a good user experience for everyone. We will use this <code>limiter</code> object later when we specify routing for our endpoint. Let’s move on and implement data <a id="_idIndexMarker807"/>aggregation:</p>
			<pre class="source-code">
async function getAggregatedData(id) {
    const postResponse = await axios.get(
        `http://postmicroservice:3001/posts/${id}`);
    const userResponse = await axios.get(
        `http://usermicroservice:3002/users/${id}`);
    const aggregatedData = {
        data: {
            id: userResponse.data.login,
            followers_url: userResponse.data.followers_url,
            following_url: userResponse.data.following_url,
            subscriptions_url:
              userResponse.data.subscriptions_url,
            repos_url: userResponse.data.repos_url,
            post: postResponse.data
        },
        location: userResponse.data.location
    };
    return aggregatedData;
}</pre>			<p>This function, <code>getAggregatedData</code>, retrieves data from two different microservices to build a combined response. It takes an ID as input:</p>
			<ol>
				<li>First, it makes two separate asynchronous calls using <code>axios.get</code>. One fetches post data from the post microservice at port <code>3001</code>, and the other fetches user data from the user microservice at port <code>3002</code>.</li>
				<li>Then, it combines the data into a single object, named <code>aggregatedData</code>. User data such as location, the followers’ URLs, and the person followed by the URL are included. Additionally, the post data retrieved from the first call is added under the key post.</li>
				<li>Finally, the function returns the <code>aggregatedData</code> object, containing all the relevant information about the user and their posts.</li>
			</ol>
			<p>By aggregating data in an API gateway, we present a simplified API to client applications. They only need to<a id="_idIndexMarker808"/> call a single endpoint (within the gateway) to receive the combined user and post data, instead of making separate calls to each microservice.</p>
			<p>For example, when requesting <code>localhost:3000/users/1</code>, we should get user information from both the post and user microservices. Here is how we get aggregated data from more than one microservice:</p>
			<pre class="source-code">
app.get('/users/:id', limiter, async (req, res) =&gt; {
    const id = req.params.id;
    try {
        const aggregatedData = await getAggregatedData(id);
        res.json(aggregatedData);
    }
    catch {
        res.status(400).json({ success: false, message:
          'Bad request' });
    }
});</pre>			<p>This code defines a route handler for the API Gateway using Express.js. It handles <code>GET </code>requests to the <code>/users/:id</code> URL path, where <code>:id</code> is a dynamic parameter representing the user ID. The <code>limiter</code> middleware is applied before the route handler function, which ensures that only allowed requests (typically, a hundred per minute based on the previous code) can proceed. Inside the function, the API extracts the ID from the request parameters. It then calls the <code>getAggregatedData</code> function to asynchronously retrieve and combine user and post data. If successful, the function sends a JSON response with the retrieved aggregated data. If there are errors during data fetching, it sends a response with a status code of <code>400</code> (bad request) and a generic error message.</p>
			<p>The last functionality in our API Gateway is caching. We need to add the following code snippet to the <code>server.js</code> file:</p>
			<pre class="source-code">
let cache = apicache.middleware;
app.use(cache('5 minutes'));</pre>			<p>Using this code, we apply caching  for five minutes for all types of endpoints.</p>
			<p>We’re done with our <a id="_idIndexMarker809"/>infrastructure (the post microservice, API Gateway, and user microservice); it is time to test all of them together.</p>
			<h3>Testing an API Gateway in Docker</h3>
			<p>To test an API<a id="_idIndexMarker810"/> Gateway, you <a id="_idIndexMarker811"/>can run every microservice separately, but as you know, we have different names for microservices in our <code>getAggregatedData</code> function – <code>http://post-microservice:3001</code> and <code>http://user-microservice:3002</code>. To make these microservices work properly and not run every microservice every time, we will containerize them.</p>
			<p>For every microservice, we have  <code>Dockerfile</code>, as shown in the following figure:</p>
			<div><div><img alt="Figure 11.4: An API Gateway project structure" src="img/B09148_11_004.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4: An API Gateway project structure</p>
			<p>A <code>Dockerfile</code> is a text file that contains instructions to build a Docker image. It acts like a recipe that tells Docker what steps to take to create a self-contained environment for your application.</p>
			<p>All three<a id="_idIndexMarker812"/> Docker <a id="_idIndexMarker813"/>files are completely the same, with the following content:</p>
			<pre class="console">
FROM node:alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
CMD [ "node", "server.js" ]</pre>			<p>This <code>Dockerfile</code> creates an image for a Node.js application. It starts with a lightweight Node.js base image, installs dependencies, copies your entire project, and then runs your server code upon startup.</p>
			<p>We have a <code>docker-compose.yml</code> file in our root folder that will combine all these three <code>Dockerfile</code> files and compose them:</p>
			<pre class="source-code">
services:
  post-microservice:
    build:
      context: ./post-microservice
      dockerfile: Dockerfile
    ports:
      - 3001:3001
  user-microservice:
    build:
      context: ./user-microservice # Correct the path if necessary
      dockerfile: Dockerfile
    ports:
      - 3002:3002
  api-Gateway:
    build:
      context: ./api-Gateway
      dockerfile: Dockerfile
    ports:
      - 3000:3000
    depends_on:
      - post-microservice
      - user-microservice</pre>			<p>This <code>docker-compose.yml</code> file defines a<a id="_idIndexMarker814"/> multi-container<a id="_idIndexMarker815"/> application. It creates three services – <code>post-microservice</code>, <code>user-microservice</code>, and <code>api-</code><code>g</code><code>ateway</code>. Each builds its own image from a separate directory (for example, <code>./post-microservice</code>) using a common <code>Dockerfile</code>.</p>
			<p>Each service gets exposed on a specific port (<code>3001</code> for posts, <code>3002</code> for users, and <code>3000</code> for the Gateway).</p>
			<p>The <code>api-Gateway</code> relies on both <code>post-microservice</code> and <code>user-microservice</code> to be active before starting itself, ensuring that the dependencies are available. To compose these microservices’ Docker files, navigate to the folder where we have the <code>docker-compose.yml</code> file and run the <code>docker-compose up -d</code> command. It should build and run composed services together. Here is what running all required services together <a id="_idIndexMarker816"/>via<a id="_idIndexMarker817"/> Docker looks like:</p>
			<div><div><img alt="Figure 11.5: An API Gateway in Docker" src="img/B09148_11_005.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5: An API Gateway in Docker</p>
			<p>Navigate to <code>localhost:3000/users/1</code> from your browser, and you should get the following aggregated data:</p>
			<div><div><img alt="Figure 11.6: An API Gateway in action" src="img/B09148_11_006.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6: An API Gateway in action</p>
			<p>So far, we have explored the role of an API Gateway in a microservices architecture, emphasizing how it simplifies client interactions by acting as a central entry point for routing, security, and load balancing. We learned how the API Gateway aggregates data from multiple microservices, applies caching and rate limiting, and enhances scalability. By integrating it into our architecture, we improve both performance and security while maintaining the flexibility and independence of individual microservices. Finally, we containerized the microservices and API Gateway using Docker for efficient testing and deployment.</p>
			<p>In our next section, we’re<a id="_idIndexMarker818"/> going to explore other interesting <a id="_idIndexMarker819"/>patterns such as CQRS and event sourcing. First, we will learn what are they and why we use them.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor181"/>CQRS and event sourcing</h1>
			<p>CQRS is a<a id="_idIndexMarker820"/> software design pattern used in <a id="_idIndexMarker821"/>distributed systems (often microservices) to separate read and write operations. This separation offers several advantages, particularly when dealing with applications with high read/write disparities or complex data models.</p>
			<p>When you apply for jobs that use distributed architecture in their applications, you often hear about CQRS and, most probably, will be asked about its usage. First things first – we need to understand that CQRS is not an architecture style; it is neither an architecture nor architectural principle. It is just a design pattern that has no wide usage. So, what is CQRS? Before answering this question, let’s understand the problem that CQRS seeks to resolve.</p>
			<p>Traditional monolithic applications typically use a single database to both read and write data. This approach can lead to the<a id="_idIndexMarker822"/> following <a id="_idIndexMarker823"/>challenges as an application grows:</p>
			<ul>
				<li><strong class="bold">Scaling bottlenecks</strong>: When read traffic spikes, it can impact write performance (and vice versa).</li>
				<li><strong class="bold">Data model mismatch</strong>: Optimal read and write models may differ. Reads might benefit from denormalized data for faster retrieval, while writes might require a normalized structure for data integrity. This mismatch creates inefficiencies or duplication.</li>
				<li><strong class="bold">Transaction conflicts</strong>: Updates and reads can compete for resources, potentially blocking each other or causing<a id="_idIndexMarker824"/> inconsistencies (violations of <strong class="bold">ACID</strong> (<strong class="bold">Atomicity, Consistency, Isolation, </strong><strong class="bold">Durability</strong>) principles).</li>
				<li><strong class="bold">Optimization challenges</strong>: Optimizing for reads might hinder write performance, and vice versa.</li>
			</ul>
			<p>When we work with monolithic applications, we often use one single data store. This means we have multiple read and write instructions in the same database. We use the same data store model, and everything is simple when it comes to working with only one single storage in terms of development. But is that all? Well, not everything is okay when we have only<a id="_idIndexMarker825"/> one data store. Depending on our<a id="_idIndexMarker826"/> requirements, we may need to separate our database into read and write databases.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor182"/>Understanding CQRS</h2>
			<p>CQRS helps us to separate data stores into read and write data stores. Why? One reason is that we need to optimize our read and write operations. Using CQRS, we can optimize our read data store to read data effectively. We can also configure our schema to optimize reading operations. The same is applicable for writing data stores.</p>
			<p>When we have separate data storages, depending on loading, we can scale them independently. When we have separate data stores for reading and writing, we can scale them independently, based on the specific load requirements of each. This is particularly useful in applications that experience high demand for read operations. By decoupling the read and write operations, we can scale the read data store to handle the load without affecting the performance of the write data store, or vice versa. This approach allows more efficient resource allocation, ensuring that each data store is optimized for its specific role.</p>
			<p>With CQRS, read and write are separated storages, and we have two different data models. We can now focus on optimizing and building them to support only one operation – either<a id="_idIndexMarker827"/> read or write.</p>
			<p>In summary, here are the benefits of CQRS:</p>
			<ul>
				<li><strong class="bold">Improved performance</strong>: Optimized <a id="_idIndexMarker828"/>read and write models can significantly enhance performance for both read and write operations.</li>
				<li><strong class="bold">Enhanced scalability</strong>: You can scale read and write models independently based on their access patterns. This allows you to handle fluctuating read/write loads more effectively.</li>
				<li><strong class="bold">Flexibility in data modeling</strong>: Each model can be designed for its specific purpose, improving overall data management and reducing complexity.</li>
			</ul>
			<p>Is CQRS a silver bullet? Of course not. You should consider the following when you integrate CQRS into your projects:</p>
			<ul>
				<li><strong class="bold">Added complexity</strong>: Implementing CQRS introduces additional complexity compared to a single store. Careful design and trade-off analysis are necessary for successful implementation.</li>
				<li><strong class="bold">Data consistency</strong>: Maintaining consistency across read and write models requires careful consideration. Strategies such as eventual consistency or materialized views can be employed.</li>
			</ul>
			<p>CQRS is a valuable pattern for applications with <em class="italic">high read/write disparities</em> (e.g., e-commerce with frequent product views and infrequent purchases), <em class="italic">complex data models</em> with different requirements for reads and writes, and scenarios that require <em class="italic">independent scaling</em> of read and write operations.</p>
			<p>Before adopting CQRS, carefully analyze your application’s needs. While it offers significant benefits in specific scenarios, the added complexity might not be necessary for simpler applications.</p>
			<p>When discussing CQRS, it is also important to discuss event sourcing. They are complementary <a id="_idIndexMarker829"/>patterns that work well together, but they address different aspects of an application’s architecture.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor183"/>Event sourcing</h2>
			<p><strong class="bold">Event sourcing</strong> is a design<a id="_idIndexMarker830"/> pattern to persist data as a sequence of events. Instead of storing the current state of an entity (such as a user account), you record each action that modifies that entity. This creates an immutable history of changes, allowing you to do the following:</p>
			<ul>
				<li>Replay events to rebuild state at any point in time.</li>
				<li>Gain deep insights into an application’s history for auditing and debugging purposes.</li>
				<li>Simplify data evolution, as new events can be added without modifying existing ones.</li>
			</ul>
			<p>These events represent what happened, not the current state of the data. By replaying an event stream, you can reconstruct the state at any point in time. Traditional databases in CQRS can be used to write models (i.e., store commands). Event sourcing shines on the read model side of CQRS. The event stream from event sourcing serves as the source of truth for read models. Read models are materialized projections built by replaying relevant events.</p>
			<p>However, it is very important to note that CQRS can be implemented without event sourcing. Event sourcing often benefits from CQRS when managing read models, as the two patterns work well together in many scenarios:</p>
			<ul>
				<li>CQRS handles the high volume of reads efficiently by using optimized read models.</li>
				<li>Event sourcing provides a complete history to build these read models.</li>
				<li>Updates to an event stream automatically trigger updates in the read models, ensuring consistency (although eventual consistency might apply).</li>
			</ul>
			<p class="callout-heading">Event sourcing versus event streaming</p>
			<p class="callout">Event streaming is <a id="_idIndexMarker831"/>not the same as event sourcing, although <a id="_idIndexMarker832"/>they are closely related and often used together. The key difference is that event streaming is a mechanism for transmitting a sequence of events between different parts of a system, or even between different systems. Event streaming focuses on the delivery of events, ensuring that they are received by interested parties. It can be used for various purposes, such as real-time notifications, data pipelines, or triggering actions in other microservices.</p>
			<p class="callout">Conversely, event sourcing is a data persistence pattern where the entire history of changes to an entity is stored as a sequence of events. It focuses on the storage and utilization of events as a system’s source of truth. These events are used to replay the history and rebuild the current state of data if needed.</p>
			<p class="callout">Here’s an analogy for better understanding. Imagine event streaming as a live stream – it continuously delivers updates (events) to anyone subscribed. Event sourcing is like a detailed log – it keeps a permanent record of all past updates (events) for future reference.</p>
			<p class="callout">But how are these two connected? Event sourcing often leverages event streaming to efficiently store and transmit the sequence of events. The event stream from event sourcing can be used by other systems or services subscribed to it. Some event stores (to be discussed further shortly), which are specialized databases for event sourcing, might have built-in functionalities for event streaming. In essence, event streaming is a broader concept for data in motion. Event sourcing utilizes event streaming to preserve its event history.</p>
			<p>Let’s take a quick <a id="_idIndexMarker833"/>look at an event store next.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor184"/>Event store</h2>
			<p>The other element we <a id="_idIndexMarker834"/>need to consider in our CQRS, and event-sourcing ecosystem<a id="_idIndexMarker835"/> is an <strong class="bold">event store</strong>. This is a specialized type of database designed specifically to store sequences of events. Unlike traditional relational databases that focus on the current state of data, event stores record every change made to an entity as a unique event. This creates an immutable history of all actions that have occurred, allowing several benefits:</p>
			<ul>
				<li><strong class="bold">Auditability and debugging</strong>: You <a id="_idIndexMarker836"/>can easily track changes and identify issues by reviewing the sequence of events. This provides a detailed log of what happened, when, and why.</li>
				<li><strong class="bold">Data evolution</strong>: As your application evolves, new events can be added to a store without modifying existing logic. This makes it easier to adapt to changing requirements without breaking existing functionality.</li>
				<li><strong class="bold">Replayability</strong>:  By replaying an event stream in a specific order, you can reconstruct the state of an entity at any point in time. This is useful for various purposes, such as rebuilding materialized views or disaster recovery.</li>
				<li><strong class="bold">Scalability</strong>: Event stores are often optimized to handle high volumes of writes, making them well-suited for event-driven architectures with frequent data changes.</li>
			</ul>
			<p>In essence, an event store not only captures a complete and immutable history of changes but also enhances flexibility and scalability. By preserving every event that modifies the state of an entity, the event store provides a foundation for reliable audit trails, effortless adaptation to new business requirements, and the ability to reconstruct the state as needed. These features make it a vital component in modern architectures, especially where high data throughput and accountability are important.</p>
			<p>Here’s how event stores typically work:</p>
			<ul>
				<li><strong class="bold">Events</strong>: Each action or change to an entity is represented as an event. These events contain relevant data about the change, such as timestamps, user IDs, and the specific modifications made.</li>
				<li><strong class="bold">Append-only</strong>: Events are stored in an append-only fashion, meaning they cannot be modified or deleted after being added. This ensures the immutability of the event history.</li>
				<li><strong class="bold">Event stream</strong>: Each entity typically has its event stream, which is a sequence of all events related to that entity.</li>
			</ul>
			<p>Event stores typically work by representing each action or change to an entity as an event. These events capture relevant information about the change, such as the time it occurred, the user responsible, and the specific details of the modification. Once an event is recorded, it is stored in an append-only fashion, meaning that it cannot be altered or deleted after being added. This ensures that the event history remains immutable, providing a reliable audit trail. Additionally, each entity is associated with its own event stream, which is a chronological sequence of all the events related to that specific entity. This stream allows you to trace the life cycle of the entity from its initial state to its current form, based entirely on the sequence of events recorded in a store.</p>
			<p>Event stores offer<a id="_idIndexMarker837"/> the following <a id="_idIndexMarker838"/>significant benefits that make them highly suitable for modern architectures, especially those driven by events:</p>
			<ul>
				<li>One of the key advantages is the<a id="_idIndexMarker839"/> creation of an <strong class="bold">immutable history</strong>. Every change to a system is stored as an event, ensuring that past actions cannot be tampered with or altered. This creates a reliable, tamper-proof audit trail that allows you to track the complete life cycle of an entity, making it particularly useful for debugging, compliance, and historical analysis.</li>
				<li>In terms of <strong class="bold">scalability</strong>, event <a id="_idIndexMarker840"/>stores are designed to handle high volumes of writes efficiently. Since events are appended to a store rather than modifying existing records, they can support applications with frequent data changes and ensure that performance remains consistent, even as the volume of data grows. This makes them an excellent choice for systems that need to process large amounts of data or handle real-time event streams.</li>
				<li>Another important<a id="_idIndexMarker841"/> benefit is <strong class="bold">data evolution</strong>. As applications evolve and new business requirements emerge, event stores allow you to adapt without disrupting existing functionality. New events can be added to reflect changes in a system, while the old event data remains intact, preserving the full history. This flexibility simplifies the process of evolving your application over time while maintaining backward compatibility with previous versions of the data.</li>
				<li><strong class="bold">Replayability</strong> is another important feature of event stores. By replaying the event stream, you can reconstruct the state of an entity at any point in time. This capability is invaluable for disaster recovery, rebuilding materialized views, or even simulating past system states for analysis or testing. It gives you the power to revisit the past and see exactly how an entity reached its current state, something that’s not possible with traditional databases that only store the latest state of data.</li>
			</ul>
			<p>These benefits make event stores a powerful tool for building scalable, flexible, and resilient systems, particularly in event-driven architectures where maintaining a detailed history of <a id="_idIndexMarker842"/>changes is <a id="_idIndexMarker843"/>critical.</p>
			<p>Here are the challenges of using event stores:</p>
			<ul>
				<li><strong class="bold">Querying</strong>: Traditional <a id="_idIndexMarker844"/>relational database querying techniques might not be directly applicable. Designing efficient queries on event streams can require different approaches.</li>
				<li><strong class="bold">Increased complexity</strong>: Event stores require a different data management mindset compared to traditional databases.</li>
			</ul>
			<p>Finally, let’s look at some popular event store options, including the following:</p>
			<ul>
				<li><strong class="bold">EventStoreDB</strong>: A leading<a id="_idIndexMarker845"/> dedicated event store solution.</li>
				<li><strong class="bold">Apache Kafka</strong>: A distributed streaming platform that can be used for event storage.</li>
				<li><strong class="bold">Traditional databases (with modifications)</strong>: Relational databases such as PostgreSQL can be configured for append-only functionality to act as basic event stores.</li>
			</ul>
			<p>In conclusion, event stores are a valuable tool for building event-driven architectures and applications that<a id="_idIndexMarker846"/> require a detailed history of changes, data evolution<a id="_idIndexMarker847"/> capabilities, and resilience.</p>
			<p>That’s enough theory; it’s time to put CQRS and event sourcing into practice.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor185"/>Implementing CQRS and event sourcing</h2>
			<p>Let’s create a simple <a id="_idIndexMarker848"/>application that uses CQRS and event-sourcing. Our application will allow us to attach multiple payment mechanisms to our account. It will be possible to register payment mechanisms to accounts, disable them, and enable them. We will use NestJS, but you can use any other framework. In <code>Ch11</code>, in the <code>CQRS_EventSourcing</code> folder, we have the <code>cqrs_app</code> folder in our Git repository. You can download it to properly follow throughout the chapter, but the other option is to implement everything from scratch, as we plan to do here:</p>
			<ol>
				<li>Create any folder and open your favorite IDE. Load the empty folder to your IDE, and from the command line, type <code>npx @nestjs/cli new cqrs_app</code> or <code>npm i -g @nestjs/cli</code> with <code>nest </code><code>new cqrs_app</code>.</li>
				<li>This should install the NestJS template in the folder. Now, let’s install the required packages:<pre class="source-code">
<strong class="bold">npm i @nestjs/cqrs</strong>
<strong class="bold">npm i @eventstore/db-client</strong>
<strong class="bold">npm i uuid</strong>
<code>EventStoreDB</code> for Docker. You can easily run it using a simple Dockerfile; however, to operate it with all the necessary infrastructure components, you’ll need to compose them together in the future. Create a <code>docker-compose.yml</code> file with the following content:<pre class="source-code">
<strong class="bold">services:</strong>
<strong class="bold">  eventstore.db:</strong>
<strong class="bold">    image: eventstore/eventstore:24.2.0-jammy</strong>
<strong class="bold">    environment:</strong>
<strong class="bold">      - EVENTSTORE_CLUSTER_SIZE=1</strong>
<strong class="bold">      - EVENTSTORE_RUN_PROJECTIONS=All</strong>
<strong class="bold">      - EVENTSTORE_START_STANDARD_PROJECTIONS=true</strong>
<strong class="bold">      - EVENTSTORE_HTTP_PORT=2113</strong>
<strong class="bold">      - EVENTSTORE_INSECURE=true</strong>
<strong class="bold">      - EVENTSTORE_ENABLE_ATOM_PUB_OVER_HTTP=true</strong>
<strong class="bold">    ports:</strong>
<strong class="bold">      - '2113:2113'</strong>
<strong class="bold">    volumes:</strong>
<strong class="bold">      - type: volume</strong>
<strong class="bold">        source: eventstore-volume-data</strong>
<strong class="bold">        target: /var/lib/eventstore</strong>
<strong class="bold">      - type: volume</strong>
<strong class="bold">        source: eventstore-volume-logs</strong>
<strong class="bold">        target: /var/log/eventstore</strong>
<strong class="bold">volumes:</strong>
<strong class="bold">  eventstore-volume-data:</strong>
<code>eventstore.db</code>. It uses the <code>eventstore/eventstore:24.2.0-jammy</code> image, which is a specific version of <code>EventStoreDB</code>. You can use any other versions with a bit different configuration. The service runs with several environment variables to configure <code>EventStore</code>, including starting all projections and enabling insecure connections (which is not recommended for production). The service maps port <code>2113</code> on the host machine to port <code>2113</code> within the container, allowing access to the <code>EventStoreDB</code> instance. Finally, it defines persistent volumes for data and logs to ensure that information is preserved even if the container restarts.</li>
				<li>Run <code>docker-compose up -d</code> command to run it. After a successful run, you can navigate to <code>localhost:2213</code> for the <code>EventStoreDB</code> dashboard.</li>
			</ol>
			<div><div><img alt="Figure 11.7: The event store dashboard" src="img/B09148_11_007.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7: The event store dashboard</p>
			<ol>
				<li value="6">Now, in<a id="_idIndexMarker850"/> our <code>src</code> folder, create an <code>eventstore.ts</code> file with the following content:<pre class="source-code">
import {EventStoreDBClient, FORWARDS, START} from
  '@eventstore/db-client'
const client = EventStoreDBClient.connectionString(
  'esdb://localhost:2113?tls=false',
)
const connect = () =&gt; {
  try {
    client.readAll({
      direction: FORWARDS,
      fromPosition: START,
      maxCount: 1,
    })
  } catch (error) {
    console.error('Failed to connect to
      EventStoreDB:', error) }
}
export {client, connect}
@eventstore/db-client</strong> library to interact with <code>EventStoreDB</code>. It establishes a connection (stored in the client), using a connection string that points to a local <code>EventStoreDB</code> instance (<code>localhost:2113</code>) with <code>EventStoreDB</code> for production is that it provides encryption for data transmitted over a network. Without TLS, data transmitted between the client and <code>EventStoreDB</code>, such as commands, events, and sensitive information, is sent in plain text. This means anyone with access to the <a id="_idIndexMarker852"/>network could potentially intercept and read the data, leading to security vulnerabilities, including data theft or man-in-the-middle attacks.</pre><p class="list-inset">The provided <code>connect</code> function attempts to read a single event (<code>maxCount: 1</code>) from the beginning (<code>direction: FORWARDS, fromPosition: START</code>) of the event stream. Any errors encountered during this read operation are caught and logged to the console. Finally, both the client connection and the connect function are exported for potential use in other parts of the code.</p></li>				<li>We will store account-based elements such as events, commands, and aggregates together. Storing account-based elements such as events, commands, and aggregates together helps maintain consistency and clarity within the domain model. These elements are tightly interconnected commands that initiate actions that change the state of an aggregate, and these changes are captured as events. Keeping them together simplifies the logical flow of operations, ensuring that all related components are easily accessible and organized. That is why we need to create a folder called <code>account</code> under <code>src</code>. After creating a folder, create a new file called <code>account.commands.ts</code> under <code>src</code> / <code>account</code> with <a id="_idIndexMarker853"/>the following content:<pre class="source-code">
import {ICommand} from '@nestjs/cqrs'
export class RegisterAccountUnitCommand implements
  ICommand {
  constructor(
    public readonly aggregateId: string,
    public readonly paymentmechanismCount: string,
  ) {}
}
export class DisableAccountUnitCommand implements
  ICommand {
  constructor(public readonly aggregateId: string) {}
}
export class EnableAccountUnitCommand implements
  ICommand {
  constructor(public readonly aggregateId: string) {}
}</pre><p class="list-inset">This code defines three commands for an account unit system in a NestJS application, using CQRS:</p><ul><li><code>RegisterAccountUnitCommand</code>: This command takes an <code>aggregateId</code> (a unique identifier for the account unit) and a <code>paymentmechanismCount</code> (the number of payment methods associated). It’s used to create a new account unit.</li><li><code>DisableAccountUnitCommand</code>: This command simply takes <code>aggregateId</code> and presumably disables the account unit.</li><li><code>EnableAccountUnitCommand</code>: Similar to the disabling command, this takes <code>aggregateId</code> and typically reenables a previously disabled account unit.</li></ul><p class="list-inset">These commands represent different actions that users might take on account units, and they follow the CQRS pattern by focusing on modifying the system state (i.e., creating, disabling, or enabling).</p></li>				<li>Instead of calling the required functionalities directly, we will encapsulate them using commands. Our commands work based on a command design pattern. Using a command pattern, it <a id="_idIndexMarker854"/>is possible to encapsulate every action/request as an object. This encapsulation brings a lot of additional features, depending on the context; you can implement late execution, redo, undo, transactional operations, and so on. The <code>ICommand</code> interface helps us to achieve this.<p class="list-inset">The other contracts we need to implement to cover CQRS with event sourcing are events. In the <code>src/account</code> folder, create a new file called <code>account.events.ts</code> with the following content:</p><pre class="source-code">
import {UUID} from 'uuid'
import {IEvent} from "@nestjs/cqrs";
export class AccountEvent implements IEvent {
  constructor(
      public readonly aggregateId: UUID,
      public readonly paymentmechanismCount: string
  ) {}
}
export class AccountRegisteredEvent extends
  AccountEvent {}
export class AccountDisabledEvent extends AccountEvent {}
export class AccountEnabledEvent extends AccountEvent {}</pre></li>			</ol>
			<p>In CQRS, events are used to communicate changes that occur in a system. By inheriting from <code>IEvent</code> (provided by the <code>@nestjs/cqrs</code> package), we ensure that <code>AccountEvent</code> and its subclasses conform to the expected event structure within the CQRS framework. This allows the framework to handle these events appropriately, such as publishing them to an event bus or persisting them for eventual consistency:</p>
			<ul>
				<li><code>AccountEvent</code> (the base class): Acts as a base for all account events. It inherits from <code>IEvent</code> (from <code>@nestjs/cqrs</code>) and holds common properties such as <code>aggregateId</code> and <code>paymentmechanismCount</code>.</li>
				<li><code>AccountRegisteredEvent</code> inherit from <code>AccountEvent</code>, customizing it for specific actions (i.e., registration, disabling, and enabling) with potentially additional properties if needed.</li>
			</ul>
			<p class="list-inset">This approach promotes code reuse and keeps event data consistent across different account unit events.</p>
			<p>We have specified our commands and events, but we haven’t used them. The purpose of the <code>account.aggregate.ts</code> file under <code>src</code> | <code>account</code> is exactly for that. We need first to specify our <a id="_idIndexMarker855"/>command handler. If you have a command, there should be a handler to handle it.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor186"/>Commands and handlers</h2>
			<p>Commands represent<a id="_idIndexMarker856"/> the actions that users or external systems want to<a id="_idIndexMarker857"/> perform on the domain model. They encapsulate the data needed to execute the action. In our example, <code>RegisterAccountUnitCommand</code>, <code>DisableAccountUnitCommand</code>, and <code>EnableAccountUnitCommand</code> are all commands that represent actions on account units.</p>
			<p>Commands are typically defined as interfaces or classes. They often include properties that specify an action and any necessary data (e.g., <code>aggregateId</code> in our commands). Conversely, <strong class="bold">command handlers</strong> (also referred to as handlers in this chapter) are responsible for receiving commands, executing the necessary logic to modify the system state, and potentially producing events that reflect the changes. They act as the bridge between commands and the domain model.</p>
			<p>Each command typically has a corresponding command handler. The handler receives the command, interacts with the domain logic (i.e., aggregate root, entities and services), and updates the system state accordingly. It might also trigger the creation of events to communicate the changes.</p>
			<p>Our <code>account.aggregate.ts</code> contains <code>AggregateRoot</code>, <code>CommandHandler</code>, and <code>EventHandler</code> implementations. First, we will look at the command handler:</p>
			<pre class="source-code">
@CommandHandler(RegisterAccountUnitCommand)
export class RegisterAccountUnitHandler
  implements ICommandHandler&lt;RegisterAccountUnitCommand&gt;
{
  constructor(private readonly publisher: EventPublisher) {}
  async execute(command: RegisterAccountUnitCommand): Promise&lt;void&gt; {
    const aggregate = this.publisher.mergeObjectContext
      (new AccountAggregate())
    aggregate.registerAccount(command.aggregateId,
      command.paymentmechanismCount)
    aggregate.commit()
  }
}</pre>			<p>This NestJS code defines a command handler to register account units using CQRS. The <code>@CommandHandler</code> decorator associates it with the <code>RegisterAccountUnitCommand</code>. It injects <code>EventPublisher</code> (for event sourcing). In the <code>execute</code> method, it creates an <code>AccountAggregate</code> instance, calls its <code>registerAccount</code> method with command data, and potentially commits the changes. This demonstrates processing a command by interacting with the domain model and potentially publishing events. We will discuss <code>AggregateRoot</code> a bit later. For now, we will just focus on the base idea behind the commands.</p>
			<p>We have two<a id="_idIndexMarker858"/> more <a id="_idIndexMarker859"/>commands that have approximately the same implementation, with different method calls:</p>
			<pre class="source-code">
@CommandHandler(DisableAccountUnitCommand)
export class DisableAccountUnitHandler implements
  ICommandHandler&lt;DisableAccountUnitCommand&gt; {
  constructor(private readonly publisher: EventPublisher){}
  async execute(command: DisableAccountUnitCommand):
    Promise&lt;void&gt; {
    const aggregate = this.publisher.mergeObjectContext(
        await AccountAggregate.loadAggregate
          (command.aggregateId)
    );
    if (!aggregate.disabled) {
      aggregate.disableAccount();
      aggregate.commit();
    }
  }
}</pre>			<p><code>DisableAccountUnitHandler</code> retrieves the <code>AccountAggregate</code> instance associated with the <code>command.aggregateId</code>, using <code>AccountAggregate.loadAggregate</code>.</p>
			<p>It verifies whether the account is already disabled, using <code>!aggregate.disabled</code>. If not disabled, it calls <code>aggregate.disableAccount</code> to perform the disabling logic and then <code>aggregate.commit</code> to potentially persist the change as an event.</p>
			<p>This handler ensures that an account unit is only disabled once and triggers event publication (if applicable) upon successful disabling. The last handler is <code>EnableAccountHandler</code>, which<a id="_idIndexMarker860"/> is <a id="_idIndexMarker861"/>a counterpart of <code>DisableAccountUnitHandler</code>:</p>
			<pre class="source-code">
@CommandHandler(EnableAccountUnitCommand)
export class EnableAccountUnitHandler implements
  ICommandHandler&lt;EnableAccountUnitCommand&gt; {
  constructor(private readonly publisher: EventPublisher){}
  async execute(command: EnableAccountUnitCommand):
    Promise&lt;void&gt; {
    const aggregate = this.publisher.mergeObjectContext(
        await AccountAggregate.loadAggregate
          (command.aggregateId)
    );
    if (aggregate.disabled) {
      aggregate.enableAccount();
      aggregate.commit();
    }
  }
}</pre>			<p>We’re done with handlers. It is time to explore <code>IEventHandler&lt;T&gt;</code> interface from the <code>@nestjs/cqrs</code> package. These handlers respond to specific domain events that are emitted by the aggregate.</p>
			<p>An event handler in the context of CQRS is responsible for handling the domain events that occur within a system. The events represent significant state changes within your aggregates, and the event handlers respond to these changes by performing side effects or additional logic outside the aggregate itself.</p>
			<p>In the same <a id="_idIndexMarker863"/>file (<code>account.aggregate.ts</code>), we have three event<a id="_idIndexMarker864"/> handlers (<code>AccountRegisteredEventHandler</code>, <code>AccountDisabledEventHandler</code>, and <code>AccountEnabledEventHandler</code>):</p>
			<pre class="source-code">
interface AccountEvent {
  aggregateId: string;
  paymentmechanismCount: string;
}
async function handleAccountEvent(eventType: string, event:
  AccountEvent): Promise&lt;void&gt; {
  const eventData = jsonEvent({
    type: eventType,
    data: {
      id: event.aggregateId,
      paymentmechanismCount: event.paymentmechanismCount,
    },
  });
  await eventStore.appendToStream(
    'Account-unit-stream-' + event.aggregateId,
    [eventData],
  );
}</pre>			<p>All event handlers have the same contract, and that is why we use the <code>AccountEvent</code> interface. It then implements a function, <code>handleAccountEvent</code>, that takes an event type and an event object as arguments. The function prepares data in a JSON-compatible format and uses an event store service to persist the event information, under a stream specific to the involved account aggregate.</p>
			<p>Now, let’s take a<a id="_idIndexMarker865"/> look<a id="_idIndexMarker866"/> at concrete event handler implementations:</p>
			<pre class="source-code">
@EventsHandler(AccountRegisteredEvent)
export class AccountRegisteredEventHandler
  implements IEventHandler&lt;AccountRegisteredEvent&gt; {
  async handle(event: AccountRegisteredEvent):
    Promise&lt;void&gt; {
    await handleAccountEvent('AccountUnitCreated', event);
  }
}
@EventsHandler(AccountDisabledEvent)
export class AccountDisabledEventHandler implements
  IEventHandler&lt;AccountDisabledEvent&gt; {
  async handle(event: AccountDisabledEvent): Promise&lt;void&gt; {
    await handleAccountEvent('AccountUnitDisabled', event);
  }
}
@EventsHandler(AccountEnabledEvent)
export class AccountEnabledEventHandler implements
  IEventHandler&lt;AccountEnabledEvent&gt; {
  async handle(event: AccountEnabledEvent): Promise&lt;void&gt; {
    await handleAccountEvent('AccountUnitEnabled', event);
  }
}</pre>			<p>In this code, we define event handlers for account registration, disabling, and enabling. When an account is registered, the <code>AccountRegisteredEventHandler</code> triggers logic related to account creation. Similarly, <code>AccountDisabledEventHandler</code> and <code>AccountEnabledEventHandler</code> handle account disabling and enabling events, respectively. These handlers leverage the <code>handleAccountEvent</code> function for centralized event processing.</p>
			<p>That is great, but how do these commands interact with events? To demonstrate this, we need to discuss<a id="_idIndexMarker867"/> one <a id="_idIndexMarker868"/>more concept, called an aggregate root, a popular <a id="_idIndexMarker869"/>pattern in <strong class="bold"> Domain-Driven </strong><strong class="bold">Design</strong> (<strong class="bold">DDD</strong>).</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor187"/>Implementing an aggregate root</h2>
			<p>In DDD, an <strong class="bold">aggregate root</strong> is a fundamental <a id="_idIndexMarker870"/>concept for modeling complex domains. It acts as the central entity within a cluster of related objects, also known <a id="_idIndexMarker871"/>as an <strong class="bold">aggregate</strong>.</p>
			<p>An aggregate root encapsulates the core data and logic associated with a particular domain concept. In our example, <code>AccountAggregate</code> will hold all the essential information about an account (i.e., ID, payment mechanism count, and disabled status). This centralizes the account’s state and promotes data integrity.</p>
			<p>An aggregate root plays a crucial role in event sourcing, a technique for persisting domain object changes as a sequence of events. In our code, <code>AccountAggregate</code> methods such as <code>registerAccount</code> apply events to the aggregate, reflecting state changes. By reconstructing the state from the event stream, the aggregate root becomes the central source of truth for the account’s history.</p>
			<p>An aggregate root defines the transactional boundaries within our domain. Within an aggregate, changes to the state of all related entities (including the root itself) must happen atomically. This ensures data consistency within the aggregate.</p>
			<p>An aggregate root also serves as the sole entry point for external interactions with the aggregate. This means other parts of your application (or other aggregates) should interact with the domain through the aggregate root’s methods. This promotes loose coupling and simplifies reasoning about domain logic.</p>
			<p>Aggregate roots promote data consistency and integrity by centralizing state management and defining transactional boundaries. They simplify domain logic by providing a clear entry point for interactions. They also improve code maintainability by encapsulating related entities and their behavior.</p>
			<p>By effectively utilizing aggregate roots in DDD, we can build robust and maintainable domain models that accurately reflect your business processes. Now, let’s see how it is possible to rebuild the <a id="_idIndexMarker872"/>state of <code>AccountAggregate</code> by reading its event stream from the event store:</p>
			<pre class="source-code">
export class AccountAggregate extends AggregateRoot {
..........
static async loadAggregate(aggregateId: string):
  Promise&lt;AccountAggregate&gt; {
    const events = eventStore.readStream(
      'Account-unit-stream-' + aggregateId);
    let count = 0;
    const aggregate = new AccountAggregate();
    for await (const event of events) {
      const eventData: any = event.event.data;
      try {
        switch (event.event.type) {
          case 'AccountUnitCreated':
            aggregate.applyAccountRegisteredEventToAggregate({
              aggregateId: eventData.id,
              paymentmechanismCount:
                eventData.paymentmechanismCount,
            });
            break;
          case 'AccountUnitDisabled':
            aggregate.accountDisabled();
            break;
          case 'AccountUnitEnabled':
            aggregate.accountEnabled();
            break;
          default:
            break
        }
      } catch(e) {
        console.error("Could not process event")
      }
      count++;
    }
    return aggregate;
}}</pre>			<p>This NestJS code defines <a id="_idIndexMarker873"/>an asynchronous function named <code>loadAggregate</code> that takes an aggregate ID as input. It retrieves a <code>stream</code> of events related to that ID from the event store. The function then iterates through each event and applies the changes it describes to an <code>AccountAggregate</code> object. There are cases for handling different event types, such as <code>AccountUnitCreated</code>, <code>AccountUnitDisabled</code>, and <code>AccountUnitEnabled</code>. If an event type isn’t recognized, it’s skipped. If there are errors processing an event, it logs an error message but keeps iterating. Finally, the function returns the populated <code>AccountAggregate</code> object.</p>
			<p>Download our Git repository for a more complete example of implementing an aggregate root. Here is a snippet from an aggregate root that handles the operations:</p>
			<pre class="source-code">
export class AccountAggregate extends AggregateRoot {
……
 registerAccount(aggregateId: string,
    paymentmechanismCount: string) {
    this.apply(new AccountRegisteredEvent(aggregateId,
      paymentmechanismCount));
  }
  enableAccount(): void {
    if(this.disabled) {
      this.apply(new AccountEnabledEvent(this.id,
        this.paymentmechanismCount))
    }
  }
  disableAccount() {
    if (!this.disabled) {
      this.apply(new AccountDisabledEvent(this.id,
        this.paymentmechanismCount));
    }
  }
…
}</pre>			<p>As you might<a id="_idIndexMarker874"/> guess, commands interact with events using an aggregate root, and the latter encapsulates the logic that triggers events.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor188"/>Implementing projection</h2>
			<p>In CQRS and event sourcing <a id="_idIndexMarker875"/>architectures, <code>Account Created</code> or <code>Account Disabled</code>).</p>
			<p>Projections are like the projection booth in a movie theater. They take the event stream (the film reel) and <em class="italic">project</em> it into a specific format, suitable for reading. This format, called a <strong class="bold">read model</strong>, is optimized to query data efficiently.</p>
			<p>With that, let’s understand why projections are important:</p>
			<ul>
				<li><strong class="bold">Read efficiency</strong>: Projections help rebuild the entire system state from the event stream, as doing so for every read query would be slow. Projections pre-process the event stream, creating a separate, optimized data structure for frequently accessed information.</li>
				<li><strong class="bold">Flexibility</strong>: We can create multiple projections tailored to different reading needs with projections. One projection might focus on account details, while another might analyze purchase history.</li>
			</ul>
			<p>Next, let’s see how projections work:</p>
			<ol>
				<li><strong class="bold">Event listeners</strong>: Projections act as event listeners, subscribing to the event stream.</li>
				<li><strong class="bold">Processing events</strong>: As new events arrive, a projection processes them one by one, updating its internal read model accordingly.</li>
				<li><strong class="bold">Read model access</strong>: When a read query arrives, a system retrieves the relevant data from a projection’s read model instead of the entire event stream.</li>
			</ol>
			<p>Projections are not a replacement for an event store. The event store remains the single source of truth for all historical events. Projections simply offer a way to efficiently access specific data from that history. Having said that, let’s look at some of the benefits of projections:</p>
			<ul>
				<li><strong class="bold">Faster reads</strong>: Queries run against read models are significantly faster than replaying an entire event stream.</li>
				<li><strong class="bold">Scalability</strong>: Projections can be scaled independently to handle increasing read traffic.</li>
				<li><strong class="bold">Flexibility</strong>: Different projections cater to diverse read needs without impacting write performance.</li>
			</ul>
			<p>We plan to implement a simple projection that demonstrates the usage of projection in CQRS and event sourcing architectures.</p>
			<p>Under the <code>src</code> /<code>paymentmechanism</code> folder, create a <code>paymentmechanism-total.projection.ts</code> file<a id="_idIndexMarker876"/> with the following functionalities:</p>
			<pre class="source-code">
@EventsHandler(AccountRegisteredEvent,
  AccountDisabledEvent, AccountEnabledEvent)
export class PaymentMechanismProjection implements
  IEventHandler&lt;AccountRegisteredEvent |
  AccountDisabledEvent | AccountEnabledEvent&gt; {
  private currentPaymentMechanismTotal: number = 0;
  constructor() {
    console.log('Account info Projection instance created:', this);
  }
  handle(event: AccountRegisteredEvent |
    AccountDisabledEvent | AccountEnabledEvent): void {
    if (event instanceof AccountRegisteredEvent) {
      this.handleAccountRegistered(event);
    } else if (event instanceof AccountDisabledEvent) {
      this.handleAccountDisabled(event);
    } else if (event instanceof AccountEnabledEvent) {
      this.handleAccountEnabled(event);
    }
  }
 ........
 .......</pre>			<p>This code defines an event handler class named <code>PaymentMechanismProjection</code> in a CQRS architecture with event sourcing. It listens for three specific events related to account management:</p>
			<ul>
				<li><code>AccountRegisteredEvent</code>: Triggers when a new account is created.</li>
				<li><code>AccountDisabledEvent</code>: Triggers when an account is deactivated.</li>
				<li><code>AccountEnabledEvent</code>: Triggers when a deactivated account is reactivated.</li>
			</ul>
			<p>The class keeps track of the total number of payment mechanisms (<code>currentPayment</code><strong class="source-inline">
MechanismTotal</strong>), but its initial value is zero.</p>
			<p>The <code>handle</code> method is<a id="_idIndexMarker877"/> the core functionality. It checks the type of the incoming event and calls a specific handler function, based on the event type:</p>
			<ul>
				<li><code>handleAccountRegistered</code>: Handles <code>AccountRegisteredEvent</code> by incrementing <code>currentPaymentMechanismTotal</code>, based on information in the event data.</li>
				<li><code>handleAccountDisabled</code>: Handles <code>AccountDisabledEvent</code> and decrements <code>currentPaymentMechanismTotal</code>.</li>
				<li><code>handleAccountEnabled</code>: Handles the <code>AccountEnabledEvent</code> and applies the opposite operation of <code>handleAccountDisabled</code>.</li>
			</ul>
			<p>This is a simplified example, but it demonstrates how an event handler projection can listen for specific events and update its internal state accordingly, maintaining a view of the data optimized for a particular purpose (e.g., tracking total payment mechanisms). Here are our detailed handler methods in this class:</p>
			<pre class="source-code">
handleAccountRegistered(event: AccountRegisteredEvent) {
    const pmCount = parseInt(event.paymentmechanismCount,
      10);
    this.currentPaymentMechanismTotal += pmCount;
    console.log("currentPaymentMechanismTotal",
      this.currentPaymentMechanismTotal)
  }
  handleAccountDisabled(event: AccountDisabledEvent) {
    const pmCount = parseInt(event.paymentmechanismCount,
      10);
    this.currentPaymentMechanismTotal -= pmCount;
    console.log("currentPaymentMechanismTotal",
      this.currentPaymentMechanismTotal)
  }
  handleAccountEnabled(event: AccountEnabledEvent) {
    const pmCount = parseInt(event.paymentmechanismCount,
      10);
    this.currentPaymentMechanismTotal += pmCount;
    console.log("currentPaymentMechanismTotal",
      this.currentPaymentMechanismTotal)
  }</pre>			<p>Our handlers<a id="_idIndexMarker878"/> simply interact with <code>currentPaymentMechanismTotal</code> and build logic around it. The idea is simple, but you can implement more complex logic based on this knowledge.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor189"/>Implementing API functionalities</h2>
			<p>We use controllers<a id="_idIndexMarker879"/> as an entry point to our request flow. In a classical flow, controllers accept requests and forward them to the related services. When we apply CQRS and event sourcing, we usually use the same controllers, but instead of specifying direct services, we apply a command pattern to provide commands and their handlers.  Controllers serve as the intermediary between a client and the backend logic, determining how an application should respond to various requests. Controllers map specific routes to corresponding methods that contain business logic. By organizing request handling within controllers, the application maintains a clear separation of concerns, making it more structured, scalable, and easier to manage.</p>
			<p>Create a new folder called <code>api</code> under the <code>src</code> folder. Then, create a new file called <code>account.controller.ts</code> under <code>src</code> / <code>api</code>, with the following content:</p>
			<pre class="source-code">
@Controller('Account')
export class AccountUnitController {
  constructor(private readonly commandBus: CommandBus) {}
  @Post('/register')
  async registerAccount(@Query('paymentmechanismCount')
    paymentmechanismCount: string): Promise&lt;any&gt; {
    const aggregateId = uuid()
    await this.commandBus.execute(new
      RegisterAccountUnitCommand(aggregateId,
        paymentmechanismCount))
    return { message: 'Request received as a command',
      aggregateId };
  }
  @Post('/:id/disable')
  async disableAccount(@Param('id') id: string):
    Promise&lt;any&gt; {
    await this.commandBus.execute(new
      DisableAccountUnitCommand(id))
    return { message: 'Request received as a command' };
  }
  @Post('/:id/enable')
  async enableAccount(@Param('id') id: string):
    Promise&lt;any&gt; {
    await this.commandBus.execute(new
      EnableAccountUnitCommand(id))
    return { message: 'Request received as a command' };
  } }</pre>			<p>This NestJS controller handles account management. It’s named <code>AccountUnitController</code> and is mapped to the <code>/Account</code> route. The controller uses a command bus to send commands. There are three functionalities exposed through <code>POST</code> requests:</p>
			<ul>
				<li><code>registerAccount</code> allows you to create a new account with a payment mechanism count, by sending <code>RegisterAccountUnitCommand</code>.</li>
				<li><code>disableAccount</code> deactivates an account by ID using <code>DisableAccountUnitCommand</code>.</li>
				<li><code>enableAccount</code> reactivates an account using an <code>EnableAccountUnitCommand</code>, based on its ID.</li>
			</ul>
			<p>All successful <a id="_idIndexMarker880"/>requests return a message indicating the command was received and the aggregate ID (for registration).</p>
			<p>In order to enable a controller’s functionality, we need to import several essential elements. <code>Controller</code>, <code>Param</code>, <code>Post</code>, and <code>Query</code> from <code>@nestjs/common</code> are necessary to define the controller, handle route parameters, and process HTTP <code>POST</code> requests with query parameters. <code>CommandBus</code> from <code>@nestjs/cqrs</code> allows us to dispatch commands, following the CQRS pattern. We import the specific commands (<code>DisableAccountUnitCommand</code>, <code>EnableAccountUnitCommand</code>, and <code>RegisterAccountUnitCommand</code>) from the <code>account.commands</code> file to perform specific operations on the account unit. Finally, we import the <code>uuid</code> package to generate unique IDs for these operations:</p>
			<pre class="source-code">
import {Controller, Param, Post, Query} from
  '@nestjs/common'
import {CommandBus} from '@nestjs/cqrs'
import {
  DisableAccountUnitCommand,
  EnableAccountUnitCommand,
  RegisterAccountUnitCommand
} from '../account/account.commands'
import {v4 as uuid} from 'uuid'</pre>			<p>Our controller <a id="_idIndexMarker881"/>doesn’t know about events. It only interacts with commands. The request will flow to command handlers, and they will trigger our events.</p>
			<p>Besides the controller, we have the <code>account.module.ts</code> file, which contains <code>AccountModule</code>:</p>
			<pre class="source-code">
export class AccountModule implements OnModuleInit {
  async onModuleInit() {
    this.startSubscription();
  }
  private startSubscription() {
    (async (): Promise&lt;void&gt; =&gt; {
      await this.subscribeToAll();
    })();
  }
  private async subscribeToAll() {
    const subscriptionList = eventStore.subscribeToAll({
      filter: streamNameFilter({ prefixes: ["Account-unit-stream-"] 
        }),
    });
    for await (const subscriptionItem of subscriptionList){
      console.log(
          `Handled event ${subscriptionItem.event?.revision}@${subscriptionItem.event?.streamId}`
      );
      const subscriptionData: any =
        subscriptionItem.event.data;
      console.log("subscription data:", subscriptionData);
    }
  }
}</pre>			<p>For a complete<a id="_idIndexMarker882"/> example with imported functionalities, check out our repository.</p>
			<p>This code defines  <code>AccountModule</code> used in a CQRS architecture with event sourcing. It implements the <code>OnModuleInit</code> life cycle hook, which gets called after the module is initialized.</p>
			<p>Here’s a breakdown of the functionality:</p>
			<ul>
				<li><code>onModuleInit</code>: This method is called when the module is ready.</li>
				<li><code>startSubscription (private)</code>: This private method initiates a subscription to an event stream. It uses an <strong class="bold">Immediately Invoked Function Expression</strong> (<strong class="bold">IIFE</strong>) to encapsulate the asynchronous logic.</li>
			</ul>
			<p>Finally, we will take a look at <code>subscribeToAll(private, async)</code>; this private asynchronous method does the actual subscription work. It uses <code>eventStore.subscribeToAll</code> to subscribe to all event streams that start with the <code>Account-unit-stream-</code> prefix. This method typically captures all events related to account management. It iterates through the subscription using  <code>for await...</code> of the loop. For each event received, it logs the event revision number and stream ID, extracts the event data, and logs it as well. The <code>AccountModule</code> subscribes to a specific category of events in the event store (events related to accounts). Whenever a new <a id="_idIndexMarker883"/>event related to accounts arrives, it logs details about the event and its data for potential processing or monitoring.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor190"/>Testing an application</h2>
			<p>Before running our <a id="_idIndexMarker884"/>application, you should run the provided <code>docker-compose</code> file via the <code>docker-compose up -d</code> command. It ensures that we already have <code>EventStoreDB</code> as a data store. To make sure if data store is running, just navigate to <code>localhost:2113</code>, and you should see the <code>EventStoreDB</code>’s dashboard.</p>
			<p>To run our application, execute the <code>nest start</code> command from the command line. Open your Postman application, and create a new tab. Select the <code>paymentmechanismcount</code> set to <code>67</code>. Then, click the <strong class="bold">Send</strong> button.</p>
			<div><div><img alt="Figure 11.8: Account registration" src="img/B09148_11_008.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8: Account registration</p>
			<p>After successful operation, you should get the following message to your VS Code console.</p>
			<div><div><img alt="Figure 11.9: Account registration logs" src="img/B09148_11_009.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9: Account registration logs</p>
			<p>The ID will be different in <a id="_idIndexMarker885"/>your case because it is automatically generated by the system. After running the same command with a different payment mechanism count (it is twenty-three in our case), you should get the following message with <code>currentPaymentMechanismCount=90</code>. The ID is different again, but if you use the same payment mechanism count, the values should be totaled based on the  <code>currentPaymentMechanismTotal = currentPaymentMechanismTotal + </code><code>paymentMechanismCount</code> formula:</p>
			<div><div><img alt="Figure 11.10: Account registration calculation" src="img/B09148_11_010.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10: Account registration calculation</p>
			<p>Now, we have two different IDs (aggregate IDs), and we can use any of them to enable and disable requests.</p>
			<p>Open a new tab on Postman and send a POST request to <code>http://localhost:8080/account/YOUR_AGGREGATE_ID/disable</code>. The last aggregate ID stores the value of <code>paymentmechanismCount</code>, which is twenty-three. So, disabling the endpoint should end up making a value of <code>currentPaymentMechanismTotal = 67</code>. The logic is ninety minus twenty-three equals to sixty-seven.</p>
			<p>Let’s run the <code>http://localhost:8080/account/90f80d89-4620-4526-ae3e-02a8156df9a1/disable</code> and click <strong class="bold">Send</strong>:</p>
			<div><div><img alt="Figure 11.11: The disabled account response" src="img/B09148_11_011.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11: The disabled account response</p>
			<p>To enable the account, just replace <code>disable</code> with <code>enable</code> and run the command again. It should restore <code>currentPaymentMechanismTotal</code> to <code>90</code>.</p>
			<p>Besides CQRS and<a id="_idIndexMarker886"/> event sourcing, we have a Service Registry and discovery for microservices. The next section will help us to understand them in more detail.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor191"/>Service Registry and discovery in microservices</h1>
			<p>Microservices<a id="_idIndexMarker887"/> development by itself consists of huge amounts of patterns and best practices. It is indeed not possible to cover all of them in one book. In this section, we will provide popular patterns and techniques used in microservices development.</p>
			<p>In a microservice architecture, applications are built as a collection of small, independent services. These services need to communicate with each other to cover user requests. Service Registry and discovery is a mechanism that simplifies this communication by <a id="_idIndexMarker888"/>enabling services to find each other dynamically.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor192"/>Understanding Service Registry and discovery</h2>
			<p>Imagine a central <a id="_idIndexMarker889"/>database. This database, called the <strong class="bold">Service Registry</strong>, acts as a <a id="_idIndexMarker890"/>directory of all the microservices in your system. Each service instance (i.e., an individual running a copy of a microservice) registers itself with the registry. During registration, the service provides details such as the following:</p>
			<ul>
				<li><strong class="bold">Network location</strong>: The address (IP address and port) where the service can be found.</li>
				<li><strong class="bold">Capabilities</strong>: What the service can do (e.g., processes payments or provides user data).</li>
				<li><strong class="bold">Health Information</strong>: Status details such as whether the service is currently healthy and available to handle requests.</li>
			</ul>
			<p>You can use tools such as Consul, ZooKeeper, and Eureka Server (as used by Netflix) for real-world service registries.</p>
			<p>Service Registry often integrates with API gateways, which are a single-entry point for external clients to access microservices. An API Gateway might leverage the Service Registry to discover the latest locations of the microservices it needs to route requests to.</p>
			<p>Conversely, <strong class="bold">Service </strong><strong class="bold">d</strong><strong class="bold">iscovery</strong> is the<a id="_idIndexMarker891"/> process where microservices find the location of other services they need to interact with. There are two main approaches:</p>
			<ul>
				<li><strong class="bold">Client-side discovery</strong>: The service that needs another service (the client) directly queries the registry to find the address of the target service.</li>
				<li><strong class="bold">Server-side discovery</strong>: A separate component, such as a load balancer, sits in front of the services. This component retrieves service locations from the registry and routes requests to the appropriate service instance.</li>
			</ul>
			<p>Let’s look at some benefits of Service Registry and discovery:</p>
			<ul>
				<li><strong class="bold">Dynamic service location</strong>: Services<a id="_idIndexMarker892"/> don’t need to be hardcoded with the addresses of other services. They can discover them on-demand from the registry, making the system more adaptable to changes.</li>
				<li><strong class="bold">Scalability and elasticity</strong>: As you add or remove service instances, the registry automatically reflects the changes. This ensures that clients always interact with available services.</li>
				<li><strong class="bold">Loose coupling</strong>: Services become loosely coupled, as they rely on the registry for communication. This promotes independent development and deployment of microservices.</li>
			</ul>
			<p>By using a central registry and enabling dynamic discovery, Service Registry and discovery simplify <a id="_idIndexMarker893"/>communication and promote flexibility in a microservice architecture.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor193"/>Approaches for implementing Service Registry and Discovery</h2>
			<p>There are two <a id="_idIndexMarker894"/>main approaches to implementing Service Registry and discovery in Node.js microservices:</p>
			<ul>
				<li>The first option is using a dedicated Service Registry tool. This approach leverages a separate service specifically designed for Service Registry and discovery functionalities. We can use popular options such as Consul, ZooKeeper, and Eureka Server (Netflix). These tools offer robust features for registration, discovery, health checks, and so on.</li>
				<li>The second option is Node.js client libraries. Each registry tool typically provides a Node.js client library that simplifies interaction with the registry. The library allows your microservices to register themselves, discover other services, and monitor their health.</li>
			</ul>
			<p>Finally, let us look at how we can implement a Service Registry before wrapping up this chapter.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor194"/>Implementing Service Registry</h2>
			<p>Now, let’s take a brief <a id="_idIndexMarker895"/>look at implementing a Service Registry:</p>
			<ol>
				<li>Choose a Service Registry tool, and install its Node.js client library:<ul><li>During startup, each microservice registers itself with a registry using the library. It provides its network location, capabilities, and health information.</li><li>In client-side discovery, the service needing another service uses the library to query the registry for the target service’s address.</li><li>In server-side discovery, a separate component, such as a load balancer, retrieves service locations from the registry and routes requests accordingly.</li></ul></li>
				<li>Now, let’s move on to building a simple registry with Node.js: For smaller deployments or learning purposes, you can implement a basic Service Registry using Node.js itself. Here’s a simplified example:<ul><li><strong class="bold">For data storage</strong>: Use a lightweight in-memory data store, such as Redis, or a simple Node.js object to store service information</li><li><strong class="bold">Registration</strong>: During startup, each microservice registers itself with the registry by sending a message containing its details</li><li><strong class="bold">Discovery</strong>: Services can query the registry to retrieve a list of available services and their addresses</li></ul></li>
			</ol>
			<p>Before we end this section, let’s look at some important considerations of Service Registry and discovery, the first being <strong class="bold">security</strong>. When implementing your own registry, ensure proper authentication and authorization mechanisms to control access to registration and discovery functionalities. Next is <strong class="bold">scalability</strong>. A homegrown registry might not scale well for large deployments. Consider a dedicated tool for production environments. Finally, <strong class="bold">health checks</strong> are very important. Regularly check the health of registered services to ensure that they are available.</p>
			<p>We’ve covered everything <a id="_idIndexMarker896"/>about microservice architecture in this chapter. It’s now time to wrap up.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor195"/>Summary</h1>
			<p>This chapter dived into the building blocks of a strong microservice architecture. It covered API gateways, explaining their purpose, use cases, and how to implement them for optimal performance, with caching, rate limiting, and response aggregation. The chapter then explored CQRS and event sourcing patterns, along with event streaming, a technology that makes them work. Finally, it discussed Service Registry and discovery, essential for microservices to communicate with each other. This chapter provided the knowledge and practical examples to build a well-designed and scalable microservice infrastructure.</p>
			<p>In the next chapter, we will explore testing strategies in depth and cover how to write effective unit and integration tests for your microservices.</p>
		</div>
	</body></html>