<html><head></head><body>
		<div id="_idContainer112">
			<h1 class="chapter-number" id="_idParaDest-173"><a id="_idTextAnchor174"/>11</h1>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/>Microservices Architecture</h1>
			<p>The world of software development is constantly evolving. As applications grow in complexity, traditional monolithic architectures struggle to keep pace. This chapter dives into some key design patterns that empower developers to build scalable and resilient systems – an API gateway, <strong class="bold">Command Query Responsibility Segregation</strong> (<strong class="bold">CQRS</strong>), event <a id="_idIndexMarker777"/>sourcing, and Service Registry <span class="No-Break">and discovery.</span></p>
			<p>These patterns, particularly when used together within a microservices architecture, offer numerous benefits. They promote loose coupling between services, making them easier to develop, maintain, and deploy independently. They also enhance scalability by allowing individual services to be scaled based on specific needs. Additionally, these patterns contribute to improved fault tolerance and resilience, ensuring that your application remains robust even if individual services <span class="No-Break">encounter issues.</span></p>
			<p>This chapter will provide a comprehensive introduction to each of these patterns, outlining their core concepts, benefits, and use cases. We will explore how to apply some of them to create a robust foundation for building modern, scalable applications. By understanding these patterns, you’ll be equipped to design and develop applications that can thrive in the ever-changing landscape of <span class="No-Break">software development.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Getting started with an <span class="No-Break">API gateway</span></li>
				<li>CQRS and <span class="No-Break">event sourcing</span></li>
				<li>Service Registry and discovery <span class="No-Break">in microservices</span></li>
			</ul>
			<p>Let’s get into <span class="No-Break">the chapter!</span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor176"/>Technical requirements</h1>
			<p>To follow us along the chapter, we need an IDE (we prefer Visual Studio Code), Postman, Docker, and a browser of <span class="No-Break">your choice.</span></p>
			<p>It is preferable to download the repository from <a href="https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript">https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript</a> and open the <strong class="source-inline">Ch11</strong> folder to easily follow our <span class="No-Break">code snippets.</span></p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor177"/>Getting started with an API gateway</h1>
			<p>An API Gateway<a id="_idIndexMarker778"/> integrates with a microservice architecture by acting as a central hub, managing communication between client applications and the distributed microservices. When we build our microservices, we want them independently developed, deployed, and scaled without affecting client applications. Clients only interact with the API gateway, which shields them from the complexities of the underlying <span class="No-Break">microservice network.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer101">
					<img alt="Figure 11.1: A simple API Gateway" src="image/B09148_11_001.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1: A simple API Gateway</p>
			<p>The API Gateway receives requests from clients and intelligently routes them to the appropriate microservice(s), based on the request content or URL. It can handle simple routing or complex scenarios, involving multiple microservices working together to fulfill a request. Let’s explore the importance of integrating an API Gateway into our <span class="No-Break">microservice architecture:</span></p>
			<ul>
				<li><strong class="bold">Simplified client interaction</strong>: Clients have a centralized entry point/single point of contact (the API gateway) to interact with an application, regardless of how many microservices are involved. This reduces development complexity on the <span class="No-Break">client side.</span></li>
				<li><strong class="bold">Improved scalability</strong>: An API Gateway can be independently scaled to handle increasing traffic volumes without impacting the individual microservices. Microservices can also be scaled independently based on their specific workloads, highlighting the importance of <span class="No-Break">API gateways.</span></li>
				<li><strong class="bold">Enhanced security</strong>: Centralized security management of an API Gateway strengthens overall application security. The API Gateway can implement authentication, authorization, and other security policies to protect microservices from <span class="No-Break">unauthorized access.</span></li>
				<li><strong class="bold">Reduced development complexity</strong>: Developers don’t need to implement functionalities such as routing, security, and monitoring logic within each microservice. An API Gateway handles these cross-cutting <span class="No-Break">concerns centrally.</span></li>
			</ul>
			<p>Let’s now see how <a id="_idIndexMarker779"/>an API Gateway <span class="No-Break">might work.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor178"/>How an API Gateway works</h2>
			<p>In a microservice <a id="_idIndexMarker780"/>architecture, an API Gateway acts as the central entry point for all client requests. It plays a crucial role in managing and optimizing the flow of communication between clients and backend services. By handling authentication, routing, load balancing, and other vital functions, the API Gateway ensures that the microservices remain loosely coupled <span class="No-Break">and scalable.</span></p>
			<p>Here’s a step-by-step breakdown of how an API Gateway typically processes <span class="No-Break">client requests:</span></p>
			<ol>
				<li><strong class="bold">Client request</strong>: A client (e.g., a web or mobile app) sends a request to an API Gateway. The request includes details such as the HTTP method, URL path, headers, and possibly <span class="No-Break">a body.</span></li>
				<li><strong class="bold">Request handling</strong>: The API Gateway receives the request and examines its contents. Based on the URL path or other routing rules, the Gateway determines which backend service(s) should handle <span class="No-Break">the request.</span></li>
				<li><strong class="bold">Authentication and authorization</strong>: The API Gateway checks the request for authentication tokens (e.g., JWT or OAuth tokens). It verifies the token’s validity and checks whether the client has the necessary permissions to access the <span class="No-Break">requested resource.</span></li>
				<li><strong class="bold">Request transformation</strong>:  The API Gateway may modify the request to fit the requirements of the backend service. This might include changing the protocol, altering headers, or modifying the <span class="No-Break">request body.</span></li>
				<li><strong class="bold">Routing and aggregation</strong>: The Gateway routes the request to the appropriate backend service(s). If the request involves multiple services, the Gateway will handle communication with each service and aggregate their responses into a single response for <span class="No-Break">the client.</span></li>
				<li><strong class="bold">Caching and load balancing</strong>: The Gateway checks whether the response is cached to serve it quickly without hitting the backend service. It also distributes the request load among multiple instances of the backend service to balance traffic and <span class="No-Break">improve </span><span class="No-Break"><a id="_idIndexMarker781"/></span><span class="No-Break">performance.</span></li>
				<li><strong class="bold">Rate limiting and throttling</strong>: The API Gateway enforces rate limits to control the number of requests a client can make within a specified period. It may throttle requests if a client exceeds the allowed <span class="No-Break">request rate.</span></li>
				<li><strong class="bold">Response handling</strong>: Once the backend service(s) respond, the Gateway may modify the response before sending it back to the client. This could include adding or removing headers, transforming data formats, or aggregating <span class="No-Break">multiple responses.</span></li>
				<li><strong class="bold">Logging and monitoring</strong>: The API Gateway logs details of the request and response for monitoring and analysis. Metrics such as request counts, response times, and error rates are tracked to monitor the health and performance of <span class="No-Break">the services.</span></li>
			</ol>
			<p>Now that we know <a id="_idIndexMarker782"/>how an API Gateway works, let’s see what the better choice in a given situation is – single or multiple <span class="No-Break">API gateways.</span></p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor179"/>Single versus multiple API gateways</h2>
			<p>You can implement <a id="_idIndexMarker783"/>multiple API gateways in a<a id="_idIndexMarker784"/> microservice architecture, but it’s not always the most straightforward or recommended approach. There are situations where it might be beneficial, but generally, a single API Gateway is preferred for simplicity <span class="No-Break">and maintainability.</span></p>
			<p>A single API Gateway is ideal when you want centralized management, a consistent client experience, and simplified scalability – all of which streamline API operations and <span class="No-Break">reduce complexity.</span></p>
			<p>While a single Gateway is often preferred, there are some situations where multiple gateways might <span class="No-Break">be considered:</span></p>
			<ul>
				<li><strong class="bold">Heterogeneous client types</strong>: If you have clients using vastly different protocols or communication styles (e.g., mobile apps, web applications, and legacy systems), separate API gateways could be used to cater to these specific needs with custom protocols or functionalities. This approach can be complex to maintain in the <span class="No-Break">long run.</span></li>
				<li><strong class="bold">Physical separation</strong>: If your microservices are geographically distributed across different data centers or cloud regions, you might consider placing an API Gateway in each location for performance reasons. However, this introduces additional management overhead to maintain consistency <span class="No-Break">across gateways.</span></li>
				<li><strong class="bold">Security segmentation</strong>: In very specific security-sensitive scenarios, you might implement separate API gateways for different security zones within your application. This allows for stricter control over access to certain microservices. However, this requires careful design and expertise to avoid creating <span class="No-Break">unnecessary complexity.</span></li>
			</ul>
			<p>Generally, the benefits of a single API Gateway outweigh the potential advantages of using multiple gateways, as the former promotes simplicity, maintainability, and a consistent <span class="No-Break">client experience.</span></p>
			<p>If you want to reap the benefits of multiple API gateways without the complexity, here are <span class="No-Break">some alternatives:</span></p>
			<ul>
				<li><strong class="bold">An API Gateway with routing by client type</strong>: Consider using a single API Gateway with routing logic that can differentiate between different client types and tailor <span class="No-Break">responses accordingly.</span></li>
				<li><strong class="bold">Microservice facades</strong>: Implement a <strong class="bold">facade</strong> pattern (more on this shortly) within some <a id="_idIndexMarker785"/>microservices to handle specific client interactions, potentially reducing the need for <span class="No-Break">multiple gateways.</span></li>
			</ul>
			<p>You should carefully consider your specific needs before implementing multiple API gateways. In most cases, a well-designed single API Gateway will provide the optimal solution for your <span class="No-Break">microservice architecture.</span></p>
			<p class="callout-heading">The facade pattern</p>
			<p class="callout">A facade in this context refers to implementing a layer within some microservices that specifically handles interactions with clients. Instead of introducing multiple API gateways, which can add complexity, a microservice facade acts as a simplified interface or <em class="italic">front</em> that abstracts the internal workings of the microservice for <span class="No-Break">the client.</span></p>
			<p>It is time to<a id="_idIndexMarker786"/> implement<a id="_idIndexMarker787"/> and see the power of an API Gateway in practice. The next section will dive into the details of the practical implementation of an <span class="No-Break">API gateway.</span></p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor180"/>Implementing microservices using an API gateway</h2>
			<p>It is possible to<a id="_idIndexMarker788"/> implement the API Gateway <a id="_idIndexMarker789"/>pattern using different forms with different libraries. In this context, <strong class="bold">forms</strong> refers to the different ways or approaches available to <a id="_idIndexMarker790"/>implement the API Gateway pattern, depending on the specific technologies or libraries chosen. The API Gateway can be set up using various frameworks or methods, each offering unique features or capabilities. To make things clear and understandable, we will implement our API Gateway as simply as possible. Ultimately, you will understand how to implement an API Gateway with caching, rate limiting, and response aggregating. For more detailed implementation, check out the <strong class="source-inline">Ch11</strong>/<strong class="source-inline">ApiGateway</strong> folder in <span class="No-Break">our repo.</span></p>
			<p>To demonstrate the real value of the API Gateway pattern, we need to have at least two microservices. The reason for needing <em class="italic">at least two</em> microservices to show the true value of the API Gateway pattern is that the pattern is designed to handle multiple services and consolidate their<a id="_idIndexMarker791"/> functionality for<a id="_idIndexMarker792"/> clients. We will use the following two microservices in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><span class="No-Break">The post-microservice</span></li>
				<li>The <span class="No-Break">user microservice</span></li>
			</ul>
			<h3>Implementing post microservice</h3>
			<p>Our first<a id="_idIndexMarker793"/> microservice, the <em class="italic">post-microservice</em>, acts as a<a id="_idIndexMarker794"/> wrapper/abstraction over the <strong class="source-inline">jsonplaceholder</strong> service. <strong class="source-inline">jsonplaceholder</strong> is a free online service that provides a REST API with fake data. It’s often used by developers to easily access and utilize realistic-looking sample data (users, posts, comments, etc.) without having to set up their databases. This allows them to quickly test API endpoints, frontend functionality, and <span class="No-Break">user interactions.</span></p>
			<ol>
				<li>Create a new folder (a <strong class="source-inline">post-microservice</strong> folder in <span class="No-Break">our case).</span></li>
				<li>Run <strong class="source-inline">npm install express axios</strong> to install the <span class="No-Break">required packages.</span></li>
			</ol>
			<p>Here is what your <strong class="source-inline">package.json</strong> should <span class="No-Break">look like:</span></p>
			<pre class="console">
{
  "dependencies": {
    "axios": "^1.7.2",
    "express": "^4.19.2"
  }
}</pre>			<p>For all chapters, you don’t need to install the exact package versions listed. While our focus is on using the packages themselves rather than specific versions, if there are major changes or breaking differences in newer versions, refer to the official documentation <span class="No-Break">for updates.</span></p>
			<p>Now, let’s create a new file called <strong class="source-inline">server.js</strong> in the folder we created (i.e., <strong class="source-inline">post-microservices</strong>) with<a id="_idIndexMarker795"/> the following<a id="_idIndexMarker796"/> <span class="No-Break">code block:</span></p>
			<pre class="source-code">
const express = require('express');
const axios = require('axios'); // Requires the axios library for making HTTP requests
const app = express();
const port = 3001; // Port on which the server will listen
app.get('/posts/:id', async (req, res) =&gt; {
  const postId = req.params.id; // Extract the ID from the URL parameter
  try {
    const response = await axios.get(
     `https://jsonplaceholder.typicode.com/posts/${postId}`);
    const post = response.data;
    if (post) {
      res.json(post); // Send the retrieved post data as JSON response
    } else {
      res.status(404).send('Post not found'); // Respond with 404 if post not found
    }
  } catch (error) {
    console.error(error);
    res.status(500).send('Internal Server Error'); // Handle errors with 500 status
  }
});
app.listen(port, () =&gt; {
  console.log(`Server listening on port ${port}`);
});</pre>			<p>This code snippet uses the Express framework to create a simple web server that listens on port <strong class="source-inline">3001</strong>. It imports the <strong class="source-inline">axios</strong> library to make HTTP requests. The server has a single route, <strong class="source-inline">/posts/:id</strong>, which responds to <strong class="source-inline">GET</strong> requests. When a request is made to this route, it extracts the <strong class="source-inline">id</strong> parameter from the URL. The server then makes an asynchronous request to <strong class="source-inline">https://jsonplaceholder.typicode.com/posts/${postId}</strong> to fetch a specific post. If the post is found, it sends the post data as a JSON response. If the post is not found, it responds with a <strong class="source-inline">404</strong> status code. If there are any errors during the request, it logs<a id="_idIndexMarker797"/> them and responds with a <strong class="source-inline">500</strong>-status <a id="_idIndexMarker798"/>code, indicating an internal <span class="No-Break">server error.</span></p>
			<p>Let’s run our microservice using the <strong class="source-inline">node server.js</strong> command and test whether everything is working. Open your favorite browser and navigate to <strong class="source-inline">localhost:3001/posts/1</strong> (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer102">
					<img alt="Figure 11.2: A post-microservice response" src="image/B09148_11_002.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2: A post-microservice response</p>
			<h3>Implementing user microsevice</h3>
			<p>Our second<a id="_idIndexMarker799"/> microservice <a id="_idIndexMarker800"/>is called a <em class="italic">user microservice</em>. It has approximately the same implementation as our post microservice, with a different port (<strong class="source-inline">3002</strong>) and different service abstraction (a GitHub <span class="No-Break">service abstraction):</span></p>
			<pre class="source-code">
const express = require('express');
const axios = require('axios'); // Requires the axios library for making HTTP requests
const app = express();
const port = 3002; // Port on which the server will listen
app.get('/users/:id', async (req, res) =&gt; {
    const userId = req.params.id; // Extract the ID from the URL parameter
    try {
        const response = await
          axios.get(`https://api.github.com/users/${userId}`);
        const user = response.data;
        if (user) {
            res.json(user); // Send the retrieved employee data as JSON response
        } else {
            res.status(404).send('User not found'); // Respond with 404 if employee not found
        }
    } catch (error) {
        console.error(error);
        res.status(500).send('Internal Server Error'); // Handle errors with 500 status
    }
});
app.listen(port, () =&gt; {
    console.log(`Server listening on port ${port}`);
});</pre>			<p>Let’s run our microservice using the <strong class="source-inline">node server.js</strong> command and test whether everything is working. Open your favorite browser and navigate to <strong class="source-inline">localhost:3002/users/1</strong> (<span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">).</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div class="IMG---Figure" id="_idContainer103">
					<img alt="Figure 11.3: A user microservice response" src="image/B09148_11_003.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3: A user microservice response</p>
			<p>Let’s build our<a id="_idIndexMarker801"/> API <a id="_idIndexMarker802"/>Gateway as a third microservice and combine the post and <span class="No-Break">user microservices.</span></p>
			<h3>Developing an API gateway</h3>
			<p>After implementing<a id="_idIndexMarker803"/> two microservices, we’re ready to show the value and power of an API gateway. We plan to implement rate limit, cache, and response aggregation functionalities for the API gateway. You can add more features such as logging, appropriate exception handling, monitoring, and other interesting behaviors after understanding <span class="No-Break">the essentials.</span></p>
			<p>First things first – you need to understand that an API Gateway by itself acts as a separate microservice. So, create a new folder for it (it is called <strong class="source-inline">api-</strong><strong class="source-inline">g</strong><strong class="source-inline">ateway</strong> in our GitHub repo). We have <strong class="source-inline">package.json</strong> with the <span class="No-Break">following content:</span></p>
			<pre class="console">
{
  "dependencies": {
    "apicache": "^1.6.3",
    "axios": "1.7.2",
    "express": "4.19.2",
    "express-rate-limit": "7.3.1"
  }
}</pre>			<p>We will use an <strong class="source-inline">express-rate-limit</strong> package to implement rate-limit functionality in our microservice. In a microservice architecture, where applications are broken down into smaller, independent services, <strong class="bold">rate limiting</strong> is a<a id="_idIndexMarker804"/> technique used to control the number of requests<a id="_idIndexMarker805"/> that a service can receive within a specific timeframe. It acts like a traffic controller, preventing a service from being overloaded by a surge <span class="No-Break">of requests.</span></p>
			<p>Conversely, <strong class="source-inline">apicache</strong> is used to implement cache behavior for an API gateway. <strong class="bold">Caching</strong> refers to <a id="_idIndexMarker806"/>a functionality that allows you to store responses from your backend services for a specific time. This cached data can then be served to subsequent requests, improving performance and reducing load on <span class="No-Break">your backend.</span></p>
			<p>Let’s create a <strong class="source-inline">server.js</strong> file to implement an API gateway. Our imported packages look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
const express = require('express');
const apicache = require('apicache');
const axios = require('axios');
const rateLimit = require('express-rate-limit');</pre>			<p>First, let’s configure our <span class="No-Break">rate limit:</span></p>
			<pre class="source-code">
const limiter = rateLimit({
    windowMs: 60000, // 1 minute window
    max: 100, // 100 requests per minute
    message: 'Too many requests, please slow down!'
});</pre>			<p>We use <strong class="source-inline">express-rate-limit</strong> to control how many times users can access your API Gateway in a minute. It acts like a gatekeeper. If a user makes fewer than a hundred requests within a minute, they get through. If they go over a hundred, they’ll be blocked with a <strong class="source-inline">Too many requests, please slow down</strong> message. This protects our API from overload and ensures a good user experience for everyone. We will use this <strong class="source-inline">limiter</strong> object later when we specify routing for our endpoint. Let’s move on and implement <span class="No-Break">data </span><span class="No-Break"><a id="_idIndexMarker807"/></span><span class="No-Break">aggregation:</span></p>
			<pre class="source-code">
async function getAggregatedData(id) {
    const postResponse = await axios.get(
        `http://postmicroservice:3001/posts/${id}`);
    const userResponse = await axios.get(
        `http://usermicroservice:3002/users/${id}`);
    const aggregatedData = {
        data: {
            id: userResponse.data.login,
            followers_url: userResponse.data.followers_url,
            following_url: userResponse.data.following_url,
            subscriptions_url:
              userResponse.data.subscriptions_url,
            repos_url: userResponse.data.repos_url,
            post: postResponse.data
        },
        location: userResponse.data.location
    };
    return aggregatedData;
}</pre>			<p>This function, <strong class="source-inline">getAggregatedData</strong>, retrieves data from two different microservices to build a combined response. It takes an ID <span class="No-Break">as input:</span></p>
			<ol>
				<li>First, it makes two separate asynchronous calls using <strong class="source-inline">axios.get</strong>. One fetches post data from the post microservice at port <strong class="source-inline">3001</strong>, and the other fetches user data from the user microservice at <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">3002</strong></span><span class="No-Break">.</span></li>
				<li>Then, it combines the data into a single object, named <strong class="source-inline">aggregatedData</strong>. User data such as location, the followers’ URLs, and the person followed by the URL are included. Additionally, the post data retrieved from the first call is added under the <span class="No-Break">key post.</span></li>
				<li>Finally, the function returns the <strong class="source-inline">aggregatedData</strong> object, containing all the relevant information about the user and <span class="No-Break">their posts.</span></li>
			</ol>
			<p>By aggregating data in an API gateway, we present a simplified API to client applications. They only need to<a id="_idIndexMarker808"/> call a single endpoint (within the gateway) to receive the combined user and post data, instead of making separate calls to <span class="No-Break">each microservice.</span></p>
			<p>For example, when requesting <strong class="source-inline">localhost:3000/users/1</strong>, we should get user information from both the post and user microservices. Here is how we get aggregated data from more than <span class="No-Break">one microservice:</span></p>
			<pre class="source-code">
app.get('/users/:id', limiter, async (req, res) =&gt; {
    const id = req.params.id;
    try {
        const aggregatedData = await getAggregatedData(id);
        res.json(aggregatedData);
    }
    catch {
        res.status(400).json({ success: false, message:
          'Bad request' });
    }
});</pre>			<p>This code defines a route handler for the API Gateway using Express.js. It handles <strong class="source-inline">GET </strong>requests to the <strong class="source-inline">/users/:id</strong> URL path, where <strong class="source-inline">:id</strong> is a dynamic parameter representing the user ID. The <strong class="source-inline">limiter</strong> middleware is applied before the route handler function, which ensures that only allowed requests (typically, a hundred per minute based on the previous code) can proceed. Inside the function, the API extracts the ID from the request parameters. It then calls the <strong class="source-inline">getAggregatedData</strong> function to asynchronously retrieve and combine user and post data. If successful, the function sends a JSON response with the retrieved aggregated data. If there are errors during data fetching, it sends a response with a status code of <strong class="source-inline">400</strong> (bad request) and a generic <span class="No-Break">error message.</span></p>
			<p>The last functionality in our API Gateway is caching. We need to add the following code snippet to the <span class="No-Break"><strong class="source-inline">server.js</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
let cache = apicache.middleware;
app.use(cache('5 minutes'));</pre>			<p>Using this code, we apply caching  for five minutes for all types <span class="No-Break">of endpoints.</span></p>
			<p>We’re done with our <a id="_idIndexMarker809"/>infrastructure (the post microservice, API Gateway, and user microservice); it is time to test all of <span class="No-Break">them together.</span></p>
			<h3>Testing an API Gateway in Docker</h3>
			<p>To test an API<a id="_idIndexMarker810"/> Gateway, you <a id="_idIndexMarker811"/>can run every microservice separately, but as you know, we have different names for microservices in our <strong class="source-inline">getAggregatedData</strong> function – <strong class="source-inline">http://post-microservice:3001</strong> and <strong class="source-inline">http://user-microservice:3002</strong>. To make these microservices work properly and not run every microservice every time, we will <span class="No-Break">containerize them.</span></p>
			<p>For every microservice, we have  <strong class="source-inline">Dockerfile</strong>, as shown in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer104">
					<img alt="Figure 11.4: An API Gateway project structure" src="image/B09148_11_004.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4: An API Gateway project structure</p>
			<p>A <strong class="source-inline">Dockerfile</strong> is a text file that contains instructions to build a Docker image. It acts like a recipe that tells Docker what steps to take to create a self-contained environment for <span class="No-Break">your application.</span></p>
			<p>All three<a id="_idIndexMarker812"/> Docker <a id="_idIndexMarker813"/>files are completely the same, with the <span class="No-Break">following content:</span></p>
			<pre class="console">
FROM node:alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
CMD [ "node", "server.js" ]</pre>			<p>This <strong class="source-inline">Dockerfile</strong> creates an image for a Node.js application. It starts with a lightweight Node.js base image, installs dependencies, copies your entire project, and then runs your server code <span class="No-Break">upon startup.</span></p>
			<p>We have a <strong class="source-inline">docker-compose.yml</strong> file in our root folder that will combine all these three <strong class="source-inline">Dockerfile</strong> files and <span class="No-Break">compose them:</span></p>
			<pre class="source-code">
services:
  post-microservice:
    build:
      context: ./post-microservice
      dockerfile: Dockerfile
    ports:
      - 3001:3001
  user-microservice:
    build:
      context: ./user-microservice # Correct the path if necessary
      dockerfile: Dockerfile
    ports:
      - 3002:3002
  api-Gateway:
    build:
      context: ./api-Gateway
      dockerfile: Dockerfile
    ports:
      - 3000:3000
    depends_on:
      - post-microservice
      - user-microservice</pre>			<p>This <strong class="source-inline">docker-compose.yml</strong> file defines a<a id="_idIndexMarker814"/> multi-container<a id="_idIndexMarker815"/> application. It creates three services – <strong class="source-inline">post-microservice</strong>, <strong class="source-inline">user-microservice</strong>, and <strong class="source-inline">api-</strong><strong class="source-inline">g</strong><strong class="source-inline">ateway</strong>. Each builds its own image from a separate directory (for example, <strong class="source-inline">./post-microservice</strong>) using a <span class="No-Break">common </span><span class="No-Break"><strong class="source-inline">Dockerfile</strong></span><span class="No-Break">.</span></p>
			<p>Each service gets exposed on a specific port (<strong class="source-inline">3001</strong> for posts, <strong class="source-inline">3002</strong> for users, and <strong class="source-inline">3000</strong> for <span class="No-Break">the Gateway).</span></p>
			<p>The <strong class="source-inline">api-Gateway</strong> relies on both <strong class="source-inline">post-microservice</strong> and <strong class="source-inline">user-microservice</strong> to be active before starting itself, ensuring that the dependencies are available. To compose these microservices’ Docker files, navigate to the folder where we have the <strong class="source-inline">docker-compose.yml</strong> file and run the <strong class="source-inline">docker-compose up -d</strong> command. It should build and run composed services together. Here is what running all required services together <a id="_idIndexMarker816"/>via<a id="_idIndexMarker817"/> Docker <span class="No-Break">looks like:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer105">
					<img alt="Figure 11.5: An API Gateway in Docker" src="image/B09148_11_005.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5: An API Gateway in Docker</p>
			<p>Navigate to <strong class="source-inline">localhost:3000/users/1</strong> from your browser, and you should get the following <span class="No-Break">aggregated data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer106">
					<img alt="Figure 11.6: An API Gateway in action" src="image/B09148_11_006.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6: An API Gateway in action</p>
			<p>So far, we have explored the role of an API Gateway in a microservices architecture, emphasizing how it simplifies client interactions by acting as a central entry point for routing, security, and load balancing. We learned how the API Gateway aggregates data from multiple microservices, applies caching and rate limiting, and enhances scalability. By integrating it into our architecture, we improve both performance and security while maintaining the flexibility and independence of individual microservices. Finally, we containerized the microservices and API Gateway using Docker for efficient testing <span class="No-Break">and deployment.</span></p>
			<p>In our next section, we’re<a id="_idIndexMarker818"/> going to explore other interesting <a id="_idIndexMarker819"/>patterns such as CQRS and event sourcing. First, we will learn what are they and why we <span class="No-Break">use them.</span></p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor181"/>CQRS and event sourcing</h1>
			<p>CQRS is a<a id="_idIndexMarker820"/> software design pattern used in <a id="_idIndexMarker821"/>distributed systems (often microservices) to separate read and write operations. This separation offers several advantages, particularly when dealing with applications with high read/write disparities or complex <span class="No-Break">data models.</span></p>
			<p>When you apply for jobs that use distributed architecture in their applications, you often hear about CQRS and, most probably, will be asked about its usage. First things first – we need to understand that CQRS is not an architecture style; it is neither an architecture nor architectural principle. It is just a design pattern that has no wide usage. So, what is CQRS? Before answering this question, let’s understand the problem that CQRS seeks <span class="No-Break">to resolve.</span></p>
			<p>Traditional monolithic applications typically use a single database to both read and write data. This approach can lead to the<a id="_idIndexMarker822"/> following <a id="_idIndexMarker823"/>challenges as an <span class="No-Break">application grows:</span></p>
			<ul>
				<li><strong class="bold">Scaling bottlenecks</strong>: When read traffic spikes, it can impact write performance (and <span class="No-Break">vice versa).</span></li>
				<li><strong class="bold">Data model mismatch</strong>: Optimal read and write models may differ. Reads might benefit from denormalized data for faster retrieval, while writes might require a normalized structure for data integrity. This mismatch creates inefficiencies <span class="No-Break">or duplication.</span></li>
				<li><strong class="bold">Transaction conflicts</strong>: Updates and reads can compete for resources, potentially blocking each other or causing<a id="_idIndexMarker824"/> inconsistencies (violations of <strong class="bold">ACID</strong> (<strong class="bold">Atomicity, Consistency, Isolation, </strong><span class="No-Break"><strong class="bold">Durability</strong></span><span class="No-Break">) principles).</span></li>
				<li><strong class="bold">Optimization challenges</strong>: Optimizing for reads might hinder write performance, and <span class="No-Break">vice versa.</span></li>
			</ul>
			<p>When we work with monolithic applications, we often use one single data store. This means we have multiple read and write instructions in the same database. We use the same data store model, and everything is simple when it comes to working with only one single storage in terms of development. But is that all? Well, not everything is okay when we have only<a id="_idIndexMarker825"/> one data store. Depending on our<a id="_idIndexMarker826"/> requirements, we may need to separate our database into read and <span class="No-Break">write databases.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor182"/>Understanding CQRS</h2>
			<p>CQRS helps us to separate data stores into read and write data stores. Why? One reason is that we need to optimize our read and write operations. Using CQRS, we can optimize our read data store to read data effectively. We can also configure our schema to optimize reading operations. The same is applicable for writing <span class="No-Break">data stores.</span></p>
			<p>When we have separate data storages, depending on loading, we can scale them independently. When we have separate data stores for reading and writing, we can scale them independently, based on the specific load requirements of each. This is particularly useful in applications that experience high demand for read operations. By decoupling the read and write operations, we can scale the read data store to handle the load without affecting the performance of the write data store, or vice versa. This approach allows more efficient resource allocation, ensuring that each data store is optimized for its <span class="No-Break">specific role.</span></p>
			<p>With CQRS, read and write are separated storages, and we have two different data models. We can now focus on optimizing and building them to support only one operation – either<a id="_idIndexMarker827"/> read <span class="No-Break">or write.</span></p>
			<p>In summary, here are the benefits <span class="No-Break">of CQRS:</span></p>
			<ul>
				<li><strong class="bold">Improved performance</strong>: Optimized <a id="_idIndexMarker828"/>read and write models can significantly enhance performance for both read and <span class="No-Break">write operations.</span></li>
				<li><strong class="bold">Enhanced scalability</strong>: You can scale read and write models independently based on their access patterns. This allows you to handle fluctuating read/write loads <span class="No-Break">more effectively.</span></li>
				<li><strong class="bold">Flexibility in data modeling</strong>: Each model can be designed for its specific purpose, improving overall data management and <span class="No-Break">reducing complexity.</span></li>
			</ul>
			<p>Is CQRS a silver bullet? Of course not. You should consider the following when you integrate CQRS into <span class="No-Break">your projects:</span></p>
			<ul>
				<li><strong class="bold">Added complexity</strong>: Implementing CQRS introduces additional complexity compared to a single store. Careful design and trade-off analysis are necessary for <span class="No-Break">successful implementation.</span></li>
				<li><strong class="bold">Data consistency</strong>: Maintaining consistency across read and write models requires careful consideration. Strategies such as eventual consistency or materialized views can <span class="No-Break">be employed.</span></li>
			</ul>
			<p>CQRS is a valuable pattern for applications with <em class="italic">high read/write disparities</em> (e.g., e-commerce with frequent product views and infrequent purchases), <em class="italic">complex data models</em> with different requirements for reads and writes, and scenarios that require <em class="italic">independent scaling</em> of read and <span class="No-Break">write operations.</span></p>
			<p>Before adopting CQRS, carefully analyze your application’s needs. While it offers significant benefits in specific scenarios, the added complexity might not be necessary for <span class="No-Break">simpler applications.</span></p>
			<p>When discussing CQRS, it is also important to discuss event sourcing. They are complementary <a id="_idIndexMarker829"/>patterns that work well together, but they address different aspects of an <span class="No-Break">application’s architecture.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor183"/>Event sourcing</h2>
			<p><strong class="bold">Event sourcing</strong> is a design<a id="_idIndexMarker830"/> pattern to persist data as a sequence of events. Instead of storing the current state of an entity (such as a user account), you record each action that modifies that entity. This creates an immutable history of changes, allowing you to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Replay events to rebuild state at any point <span class="No-Break">in time.</span></li>
				<li>Gain deep insights into an application’s history for auditing and <span class="No-Break">debugging purposes.</span></li>
				<li>Simplify data evolution, as new events can be added without modifying <span class="No-Break">existing ones.</span></li>
			</ul>
			<p>These events represent what happened, not the current state of the data. By replaying an event stream, you can reconstruct the state at any point in time. Traditional databases in CQRS can be used to write models (i.e., store commands). Event sourcing shines on the read model side of CQRS. The event stream from event sourcing serves as the source of truth for read models. Read models are materialized projections built by replaying <span class="No-Break">relevant events.</span></p>
			<p>However, it is very important to note that CQRS can be implemented without event sourcing. Event sourcing often benefits from CQRS when managing read models, as the two patterns work well together in <span class="No-Break">many scenarios:</span></p>
			<ul>
				<li>CQRS handles the high volume of reads efficiently by using optimized <span class="No-Break">read models.</span></li>
				<li>Event sourcing provides a complete history to build these <span class="No-Break">read models.</span></li>
				<li>Updates to an event stream automatically trigger updates in the read models, ensuring consistency (although eventual consistency <span class="No-Break">might apply).</span></li>
			</ul>
			<p class="callout-heading">Event sourcing versus event streaming</p>
			<p class="callout">Event streaming is <a id="_idIndexMarker831"/>not the same as event sourcing, although <a id="_idIndexMarker832"/>they are closely related and often used together. The key difference is that event streaming is a mechanism for transmitting a sequence of events between different parts of a system, or even between different systems. Event streaming focuses on the delivery of events, ensuring that they are received by interested parties. It can be used for various purposes, such as real-time notifications, data pipelines, or triggering actions in <span class="No-Break">other microservices.</span></p>
			<p class="callout">Conversely, event sourcing is a data persistence pattern where the entire history of changes to an entity is stored as a sequence of events. It focuses on the storage and utilization of events as a system’s source of truth. These events are used to replay the history and rebuild the current state of data <span class="No-Break">if needed.</span></p>
			<p class="callout">Here’s an analogy for better understanding. Imagine event streaming as a live stream – it continuously delivers updates (events) to anyone subscribed. Event sourcing is like a detailed log – it keeps a permanent record of all past updates (events) for <span class="No-Break">future reference.</span></p>
			<p class="callout">But how are these two connected? Event sourcing often leverages event streaming to efficiently store and transmit the sequence of events. The event stream from event sourcing can be used by other systems or services subscribed to it. Some event stores (to be discussed further shortly), which are specialized databases for event sourcing, might have built-in functionalities for event streaming. In essence, event streaming is a broader concept for data in motion. Event sourcing utilizes event streaming to preserve its <span class="No-Break">event history.</span></p>
			<p>Let’s take a quick <a id="_idIndexMarker833"/>look at an event <span class="No-Break">store next.</span></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor184"/>Event store</h2>
			<p>The other element we <a id="_idIndexMarker834"/>need to consider in our CQRS, and event-sourcing ecosystem<a id="_idIndexMarker835"/> is an <strong class="bold">event store</strong>. This is a specialized type of database designed specifically to store sequences of events. Unlike traditional relational databases that focus on the current state of data, event stores record every change made to an entity as a unique event. This creates an immutable history of all actions that have occurred, allowing <span class="No-Break">several benefits:</span></p>
			<ul>
				<li><strong class="bold">Auditability and debugging</strong>: You <a id="_idIndexMarker836"/>can easily track changes and identify issues by reviewing the sequence of events. This provides a detailed log of what happened, when, <span class="No-Break">and why.</span></li>
				<li><strong class="bold">Data evolution</strong>: As your application evolves, new events can be added to a store without modifying existing logic. This makes it easier to adapt to changing requirements without breaking <span class="No-Break">existing functionality.</span></li>
				<li><strong class="bold">Replayability</strong>:  By replaying an event stream in a specific order, you can reconstruct the state of an entity at any point in time. This is useful for various purposes, such as rebuilding materialized views or <span class="No-Break">disaster recovery.</span></li>
				<li><strong class="bold">Scalability</strong>: Event stores are often optimized to handle high volumes of writes, making them well-suited for event-driven architectures with frequent <span class="No-Break">data changes.</span></li>
			</ul>
			<p>In essence, an event store not only captures a complete and immutable history of changes but also enhances flexibility and scalability. By preserving every event that modifies the state of an entity, the event store provides a foundation for reliable audit trails, effortless adaptation to new business requirements, and the ability to reconstruct the state as needed. These features make it a vital component in modern architectures, especially where high data throughput and accountability <span class="No-Break">are important.</span></p>
			<p>Here’s how event stores <span class="No-Break">typically work:</span></p>
			<ul>
				<li><strong class="bold">Events</strong>: Each action or change to an entity is represented as an event. These events contain relevant data about the change, such as timestamps, user IDs, and the specific <span class="No-Break">modifications made.</span></li>
				<li><strong class="bold">Append-only</strong>: Events are stored in an append-only fashion, meaning they cannot be modified or deleted after being added. This ensures the immutability of the <span class="No-Break">event history.</span></li>
				<li><strong class="bold">Event stream</strong>: Each entity typically has its event stream, which is a sequence of all events related to <span class="No-Break">that entity.</span></li>
			</ul>
			<p>Event stores typically work by representing each action or change to an entity as an event. These events capture relevant information about the change, such as the time it occurred, the user responsible, and the specific details of the modification. Once an event is recorded, it is stored in an append-only fashion, meaning that it cannot be altered or deleted after being added. This ensures that the event history remains immutable, providing a reliable audit trail. Additionally, each entity is associated with its own event stream, which is a chronological sequence of all the events related to that specific entity. This stream allows you to trace the life cycle of the entity from its initial state to its current form, based entirely on the sequence of events recorded in <span class="No-Break">a store.</span></p>
			<p>Event stores offer<a id="_idIndexMarker837"/> the following <a id="_idIndexMarker838"/>significant benefits that make them highly suitable for modern architectures, especially those driven <span class="No-Break">by events:</span></p>
			<ul>
				<li>One of the key advantages is the<a id="_idIndexMarker839"/> creation of an <strong class="bold">immutable history</strong>. Every change to a system is stored as an event, ensuring that past actions cannot be tampered with or altered. This creates a reliable, tamper-proof audit trail that allows you to track the complete life cycle of an entity, making it particularly useful for debugging, compliance, and <span class="No-Break">historical analysis.</span></li>
				<li>In terms of <strong class="bold">scalability</strong>, event <a id="_idIndexMarker840"/>stores are designed to handle high volumes of writes efficiently. Since events are appended to a store rather than modifying existing records, they can support applications with frequent data changes and ensure that performance remains consistent, even as the volume of data grows. This makes them an excellent choice for systems that need to process large amounts of data or handle real-time <span class="No-Break">event streams.</span></li>
				<li>Another important<a id="_idIndexMarker841"/> benefit is <strong class="bold">data evolution</strong>. As applications evolve and new business requirements emerge, event stores allow you to adapt without disrupting existing functionality. New events can be added to reflect changes in a system, while the old event data remains intact, preserving the full history. This flexibility simplifies the process of evolving your application over time while maintaining backward compatibility with previous versions of <span class="No-Break">the data.</span></li>
				<li><strong class="bold">Replayability</strong> is another important feature of event stores. By replaying the event stream, you can reconstruct the state of an entity at any point in time. This capability is invaluable for disaster recovery, rebuilding materialized views, or even simulating past system states for analysis or testing. It gives you the power to revisit the past and see exactly how an entity reached its current state, something that’s not possible with traditional databases that only store the latest state <span class="No-Break">of data.</span></li>
			</ul>
			<p>These benefits make event stores a powerful tool for building scalable, flexible, and resilient systems, particularly in event-driven architectures where maintaining a detailed history of <a id="_idIndexMarker842"/>changes <span class="No-Break">is </span><span class="No-Break"><a id="_idIndexMarker843"/></span><span class="No-Break">critical.</span></p>
			<p>Here are the challenges of using <span class="No-Break">event stores:</span></p>
			<ul>
				<li><strong class="bold">Querying</strong>: Traditional <a id="_idIndexMarker844"/>relational database querying techniques might not be directly applicable. Designing efficient queries on event streams can require <span class="No-Break">different approaches.</span></li>
				<li><strong class="bold">Increased complexity</strong>: Event stores require a different data management mindset compared to <span class="No-Break">traditional databases.</span></li>
			</ul>
			<p>Finally, let’s look at some popular event store options, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">EventStoreDB</strong>: A leading<a id="_idIndexMarker845"/> dedicated event <span class="No-Break">store solution.</span></li>
				<li><strong class="bold">Apache Kafka</strong>: A distributed streaming platform that can be used for <span class="No-Break">event storage.</span></li>
				<li><strong class="bold">Traditional databases (with modifications)</strong>: Relational databases such as PostgreSQL can be configured for append-only functionality to act as basic <span class="No-Break">event stores.</span></li>
			</ul>
			<p>In conclusion, event stores are a valuable tool for building event-driven architectures and applications that<a id="_idIndexMarker846"/> require a detailed history of changes, data evolution<a id="_idIndexMarker847"/> capabilities, <span class="No-Break">and resilience.</span></p>
			<p>That’s enough theory; it’s time to put CQRS and event sourcing <span class="No-Break">into practice.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor185"/>Implementing CQRS and event sourcing</h2>
			<p>Let’s create a simple <a id="_idIndexMarker848"/>application that uses CQRS and event-sourcing. Our application will allow us to attach multiple payment mechanisms to our account. It will be possible to register payment mechanisms to accounts, disable them, and enable them. We will use NestJS, but you can use any other framework. In <strong class="source-inline">Ch11</strong>, in the <strong class="source-inline">CQRS_EventSourcing</strong> folder, we have the <strong class="source-inline">cqrs_app</strong> folder in our Git repository. You can download it to properly follow throughout the chapter, but the other option is to implement everything from scratch, as we plan to <span class="No-Break">do here:</span></p>
			<ol>
				<li>Create any folder and open your favorite IDE. Load the empty folder to your IDE, and from the command line, type <strong class="source-inline">npx @nestjs/cli new cqrs_app</strong> or <strong class="source-inline">npm i -g @nestjs/cli</strong> with <strong class="source-inline">nest </strong><span class="No-Break"><strong class="source-inline">new cqrs_app</strong></span><span class="No-Break">.</span></li>
				<li>This should install the NestJS template in the folder. Now, let’s install the <span class="No-Break">required packages:</span><pre class="source-code">
<strong class="bold">npm i @nestjs/cqrs</strong>
<strong class="bold">npm i @eventstore/db-client</strong>
<strong class="bold">npm i uuid</strong>
<strong class="bold">npm i prettier</strong></pre></li>				<li>Before switching to development, we need to configure our <strong class="source-inline">EventStoreDB</strong> for Docker. You can easily run it using a simple Dockerfile; however, to operate it with all the necessary infrastructure components, you’ll need to compose them together in the future. Create a <strong class="source-inline">docker-compose.yml</strong> file with the <span class="No-Break">following content:</span><pre class="source-code">
<strong class="bold">services:</strong>
<strong class="bold">  eventstore.db:</strong>
<strong class="bold">    image: eventstore/eventstore:24.2.0-jammy</strong>
<strong class="bold">    environment:</strong>
<strong class="bold">      - EVENTSTORE_CLUSTER_SIZE=1</strong>
<strong class="bold">      - EVENTSTORE_RUN_PROJECTIONS=All</strong>
<strong class="bold">      - EVENTSTORE_START_STANDARD_PROJECTIONS=true</strong>
<strong class="bold">      - EVENTSTORE_HTTP_PORT=2113</strong>
<strong class="bold">      - EVENTSTORE_INSECURE=true</strong>
<strong class="bold">      - EVENTSTORE_ENABLE_ATOM_PUB_OVER_HTTP=true</strong>
<strong class="bold">    ports:</strong>
<strong class="bold">      - '2113:2113'</strong>
<strong class="bold">    volumes:</strong>
<strong class="bold">      - type: volume</strong>
<strong class="bold">        source: eventstore-volume-data</strong>
<strong class="bold">        target: /var/lib/eventstore</strong>
<strong class="bold">      - type: volume</strong>
<strong class="bold">        source: eventstore-volume-logs</strong>
<strong class="bold">        target: /var/log/eventstore</strong>
<strong class="bold">volumes:</strong>
<strong class="bold">  eventstore-volume-data:</strong>
<strong class="bold">  eventstore-volume-logs:</strong></pre></li>				<li>This Docker<a id="_idIndexMarker849"/> Compose file defines a service named <strong class="source-inline">eventstore.db</strong>. It uses the <strong class="source-inline">eventstore/eventstore:24.2.0-jammy</strong> image, which is a specific version of <strong class="source-inline">EventStoreDB</strong>. You can use any other versions with a bit different configuration. The service runs with several environment variables to configure <strong class="source-inline">EventStore</strong>, including starting all projections and enabling insecure connections (which is not recommended for production). The service maps port <strong class="source-inline">2113</strong> on the host machine to port <strong class="source-inline">2113</strong> within the container, allowing access to the <strong class="source-inline">EventStoreDB</strong> instance. Finally, it defines persistent volumes for data and logs to ensure that information is preserved even if the <span class="No-Break">container restarts.</span></li>
				<li>Run <strong class="source-inline">docker-compose up -d</strong> command to run it. After a successful run, you can navigate to <strong class="source-inline">localhost:2213</strong> for the <span class="No-Break"><strong class="source-inline">EventStoreDB</strong></span><span class="No-Break"> dashboard.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer107">
					<img alt="Figure 11.7: The event store dashboard" src="image/B09148_11_007.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7: The event store dashboard</p>
			<ol>
				<li value="6">Now, in<a id="_idIndexMarker850"/> our <strong class="source-inline">src</strong> folder, create an <strong class="source-inline">eventstore.ts</strong> file with the <span class="No-Break">following content:</span><pre class="source-code">
import {EventStoreDBClient, FORWARDS, START} from
  '@eventstore/db-client'
const client = EventStoreDBClient.connectionString(
  'esdb://localhost:2113?tls=false',
)
const connect = () =&gt; {
  try {
    client.readAll({
      direction: FORWARDS,
      fromPosition: START,
      maxCount: 1,
    })
  } catch (error) {
    console.error('Failed to connect to
      EventStoreDB:', error) }
}
export {client, connect}</pre><p class="list-inset">This code snippet utilizes the <strong class="source-inline">@eventstore/db-client</strong> library to interact with <strong class="source-inline">EventStoreDB</strong>. It establishes a connection (stored in the client), using a connection string that points to a local <strong class="source-inline">EventStoreDB</strong> instance (<strong class="source-inline">localhost:2113</strong>) with <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) encryption disabled (which is <a id="_idIndexMarker851"/>not recommended for production). The reason that it’s not recommended to disable TLS in your connection to <strong class="source-inline">EventStoreDB</strong> for production is that it provides encryption for data transmitted over a network. Without TLS, data transmitted between the client and <strong class="source-inline">EventStoreDB</strong>, such as commands, events, and sensitive information, is sent in plain text. This means anyone with access to the <a id="_idIndexMarker852"/>network could potentially intercept and read the data, leading to security vulnerabilities, including data theft or <span class="No-Break">man-in-the-middle attacks.</span></p><p class="list-inset">The provided <strong class="source-inline">connect</strong> function attempts to read a single event (<strong class="source-inline">maxCount: 1</strong>) from the beginning (<strong class="source-inline">direction: FORWARDS, fromPosition: START</strong>) of the event stream. Any errors encountered during this read operation are caught and logged to the console. Finally, both the client connection and the connect function are exported for potential use in other parts of <span class="No-Break">the code.</span></p></li>				<li>We will store account-based elements such as events, commands, and aggregates together. Storing account-based elements such as events, commands, and aggregates together helps maintain consistency and clarity within the domain model. These elements are tightly interconnected commands that initiate actions that change the state of an aggregate, and these changes are captured as events. Keeping them together simplifies the logical flow of operations, ensuring that all related components are easily accessible and organized. That is why we need to create a folder called <strong class="source-inline">account</strong> under <strong class="source-inline">src</strong>. After creating a folder, create a new file called <strong class="source-inline">account.commands.ts</strong> under <strong class="source-inline">src</strong> / <strong class="source-inline">account</strong> with <a id="_idIndexMarker853"/>the <span class="No-Break">following content:</span><pre class="source-code">
import {ICommand} from '@nestjs/cqrs'
export class RegisterAccountUnitCommand implements
  ICommand {
  constructor(
    public readonly aggregateId: string,
    public readonly paymentmechanismCount: string,
  ) {}
}
export class DisableAccountUnitCommand implements
  ICommand {
  constructor(public readonly aggregateId: string) {}
}
export class EnableAccountUnitCommand implements
  ICommand {
  constructor(public readonly aggregateId: string) {}
}</pre><p class="list-inset">This code defines three commands for an account unit system in a NestJS application, <span class="No-Break">using CQRS:</span></p><ul><li><strong class="source-inline">RegisterAccountUnitCommand</strong>: This command takes an <strong class="source-inline">aggregateId</strong> (a unique identifier for the account unit) and a <strong class="source-inline">paymentmechanismCount</strong> (the number of payment methods associated). It’s used to create a new <span class="No-Break">account unit.</span></li><li><strong class="source-inline">DisableAccountUnitCommand</strong>: This command simply takes <strong class="source-inline">aggregateId</strong> and presumably disables the <span class="No-Break">account unit.</span></li><li><strong class="source-inline">EnableAccountUnitCommand</strong>: Similar to the disabling command, this takes <strong class="source-inline">aggregateId</strong> and typically reenables a previously disabled <span class="No-Break">account unit.</span></li></ul><p class="list-inset">These commands represent different actions that users might take on account units, and they follow the CQRS pattern by focusing on modifying the system state (i.e., creating, disabling, <span class="No-Break">or enabling).</span></p></li>				<li>Instead of calling the required functionalities directly, we will encapsulate them using commands. Our commands work based on a command design pattern. Using a command pattern, it <a id="_idIndexMarker854"/>is possible to encapsulate every action/request as an object. This encapsulation brings a lot of additional features, depending on the context; you can implement late execution, redo, undo, transactional operations, and so on. The <strong class="source-inline">ICommand</strong> interface helps us to <span class="No-Break">achieve this.</span><p class="list-inset">The other contracts we need to implement to cover CQRS with event sourcing are events. In the <strong class="source-inline">src/account</strong> folder, create a new file called <strong class="source-inline">account.events.ts</strong> with the <span class="No-Break">following content:</span></p><pre class="source-code">
import {UUID} from 'uuid'
import {IEvent} from "@nestjs/cqrs";
export class AccountEvent implements IEvent {
  constructor(
      public readonly aggregateId: UUID,
      public readonly paymentmechanismCount: string
  ) {}
}
export class AccountRegisteredEvent extends
  AccountEvent {}
export class AccountDisabledEvent extends AccountEvent {}
export class AccountEnabledEvent extends AccountEvent {}</pre></li>			</ol>
			<p>In CQRS, events are used to communicate changes that occur in a system. By inheriting from <strong class="source-inline">IEvent</strong> (provided by the <strong class="source-inline">@nestjs/cqrs</strong> package), we ensure that <strong class="source-inline">AccountEvent</strong> and its subclasses conform to the expected event structure within the CQRS framework. This allows the framework to handle these events appropriately, such as publishing them to an event bus or persisting them for <span class="No-Break">eventual consistency:</span></p>
			<ul>
				<li><strong class="source-inline">AccountEvent</strong> (the base class): Acts as a base for all account events. It inherits from <strong class="source-inline">IEvent</strong> (from <strong class="source-inline">@nestjs/cqrs</strong>) and holds common properties such as <strong class="source-inline">aggregateId</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">paymentmechanismCount</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Concrete events</strong>: Classes such as <strong class="source-inline">AccountRegisteredEvent</strong> inherit from <strong class="source-inline">AccountEvent</strong>, customizing it for specific actions (i.e., registration, disabling, and enabling) with potentially additional properties <span class="No-Break">if needed.</span></li>
			</ul>
			<p class="list-inset">This approach promotes code reuse and keeps event data consistent across different account <span class="No-Break">unit events.</span></p>
			<p>We have specified our commands and events, but we haven’t used them. The purpose of the <strong class="source-inline">account.aggregate.ts</strong> file under <strong class="source-inline">src</strong> | <strong class="source-inline">account</strong> is exactly for that. We need first to specify our <a id="_idIndexMarker855"/>command handler. If you have a command, there should be a handler to <span class="No-Break">handle it.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor186"/>Commands and handlers</h2>
			<p>Commands represent<a id="_idIndexMarker856"/> the actions that users or external systems want to<a id="_idIndexMarker857"/> perform on the domain model. They encapsulate the data needed to execute the action. In our example, <strong class="source-inline">RegisterAccountUnitCommand</strong>, <strong class="source-inline">DisableAccountUnitCommand</strong>, and <strong class="source-inline">EnableAccountUnitCommand</strong> are all commands that represent actions on <span class="No-Break">account units.</span></p>
			<p>Commands are typically defined as interfaces or classes. They often include properties that specify an action and any necessary data (e.g., <strong class="source-inline">aggregateId</strong> in our commands). Conversely, <strong class="bold">command handlers</strong> (also referred to as handlers in this chapter) are responsible for receiving commands, executing the necessary logic to modify the system state, and potentially producing events that reflect the changes. They act as the bridge between commands and the <span class="No-Break">domain model.</span></p>
			<p>Each command typically has a corresponding command handler. The handler receives the command, interacts with the domain logic (i.e., aggregate root, entities and services), and updates the system state accordingly. It might also trigger the creation of events to communicate <span class="No-Break">the changes.</span></p>
			<p>Our <strong class="source-inline">account.aggregate.ts</strong> contains <strong class="source-inline">AggregateRoot</strong>, <strong class="source-inline">CommandHandler</strong>, and <strong class="source-inline">EventHandler</strong> implementations. First, we will look at the <span class="No-Break">command handler:</span></p>
			<pre class="source-code">
@CommandHandler(RegisterAccountUnitCommand)
export class RegisterAccountUnitHandler
  implements ICommandHandler&lt;RegisterAccountUnitCommand&gt;
{
  constructor(private readonly publisher: EventPublisher) {}
  async execute(command: RegisterAccountUnitCommand): Promise&lt;void&gt; {
    const aggregate = this.publisher.mergeObjectContext
      (new AccountAggregate())
    aggregate.registerAccount(command.aggregateId,
      command.paymentmechanismCount)
    aggregate.commit()
  }
}</pre>			<p>This NestJS code defines a command handler to register account units using CQRS. The <strong class="source-inline">@CommandHandler</strong> decorator associates it with the <strong class="source-inline">RegisterAccountUnitCommand</strong>. It injects <strong class="source-inline">EventPublisher</strong> (for event sourcing). In the <strong class="source-inline">execute</strong> method, it creates an <strong class="source-inline">AccountAggregate</strong> instance, calls its <strong class="source-inline">registerAccount</strong> method with command data, and potentially commits the changes. This demonstrates processing a command by interacting with the domain model and potentially publishing events. We will discuss <strong class="source-inline">AggregateRoot</strong> a bit later. For now, we will just focus on the base idea behind <span class="No-Break">the commands.</span></p>
			<p>We have two<a id="_idIndexMarker858"/> more <a id="_idIndexMarker859"/>commands that have approximately the same implementation, with different <span class="No-Break">method calls:</span></p>
			<pre class="source-code">
@CommandHandler(DisableAccountUnitCommand)
export class DisableAccountUnitHandler implements
  ICommandHandler&lt;DisableAccountUnitCommand&gt; {
  constructor(private readonly publisher: EventPublisher){}
  async execute(command: DisableAccountUnitCommand):
    Promise&lt;void&gt; {
    const aggregate = this.publisher.mergeObjectContext(
        await AccountAggregate.loadAggregate
          (command.aggregateId)
    );
    if (!aggregate.disabled) {
      aggregate.disableAccount();
      aggregate.commit();
    }
  }
}</pre>			<p><strong class="source-inline">DisableAccountUnitHandler</strong> retrieves the <strong class="source-inline">AccountAggregate</strong> instance associated with the <strong class="source-inline">command.aggregateId</strong>, <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">AccountAggregate.loadAggregate</strong></span><span class="No-Break">.</span></p>
			<p>It verifies whether the account is already disabled, using <strong class="source-inline">!aggregate.disabled</strong>. If not disabled, it calls <strong class="source-inline">aggregate.disableAccount</strong> to perform the disabling logic and then <strong class="source-inline">aggregate.commit</strong> to potentially persist the change as <span class="No-Break">an event.</span></p>
			<p>This handler ensures that an account unit is only disabled once and triggers event publication (if applicable) upon successful disabling. The last handler is <strong class="source-inline">EnableAccountHandler</strong>, which<a id="_idIndexMarker860"/> is <a id="_idIndexMarker861"/>a counterpart <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">DisableAccountUnitHandler</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
@CommandHandler(EnableAccountUnitCommand)
export class EnableAccountUnitHandler implements
  ICommandHandler&lt;EnableAccountUnitCommand&gt; {
  constructor(private readonly publisher: EventPublisher){}
  async execute(command: EnableAccountUnitCommand):
    Promise&lt;void&gt; {
    const aggregate = this.publisher.mergeObjectContext(
        await AccountAggregate.loadAggregate
          (command.aggregateId)
    );
    if (aggregate.disabled) {
      aggregate.enableAccount();
      aggregate.commit();
    }
  }
}</pre>			<p>We’re done with handlers. It is time to explore <strong class="bold">event handlers</strong>. In this context, event handlers are classes that<a id="_idIndexMarker862"/> implement the <strong class="source-inline">IEventHandler&lt;T&gt;</strong> interface from the <strong class="source-inline">@nestjs/cqrs</strong> package. These handlers respond to specific domain events that are emitted by <span class="No-Break">the aggregate.</span></p>
			<p>An event handler in the context of CQRS is responsible for handling the domain events that occur within a system. The events represent significant state changes within your aggregates, and the event handlers respond to these changes by performing side effects or additional logic outside the <span class="No-Break">aggregate itself.</span></p>
			<p>In the same <a id="_idIndexMarker863"/>file (<strong class="source-inline">account.aggregate.ts</strong>), we have three event<a id="_idIndexMarker864"/> handlers (<strong class="source-inline">AccountRegisteredEventHandler</strong>, <strong class="source-inline">AccountDisabledEventHandler</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">AccountEnabledEventHandler</strong></span><span class="No-Break">):</span></p>
			<pre class="source-code">
interface AccountEvent {
  aggregateId: string;
  paymentmechanismCount: string;
}
async function handleAccountEvent(eventType: string, event:
  AccountEvent): Promise&lt;void&gt; {
  const eventData = jsonEvent({
    type: eventType,
    data: {
      id: event.aggregateId,
      paymentmechanismCount: event.paymentmechanismCount,
    },
  });
  await eventStore.appendToStream(
    'Account-unit-stream-' + event.aggregateId,
    [eventData],
  );
}</pre>			<p>All event handlers have the same contract, and that is why we use the <strong class="source-inline">AccountEvent</strong> interface. It then implements a function, <strong class="source-inline">handleAccountEvent</strong>, that takes an event type and an event object as arguments. The function prepares data in a JSON-compatible format and uses an event store service to persist the event information, under a stream specific to the involved <span class="No-Break">account aggregate.</span></p>
			<p>Now, let’s take a<a id="_idIndexMarker865"/> look<a id="_idIndexMarker866"/> at concrete event <span class="No-Break">handler implementations:</span></p>
			<pre class="source-code">
@EventsHandler(AccountRegisteredEvent)
export class AccountRegisteredEventHandler
  implements IEventHandler&lt;AccountRegisteredEvent&gt; {
  async handle(event: AccountRegisteredEvent):
    Promise&lt;void&gt; {
    await handleAccountEvent('AccountUnitCreated', event);
  }
}
@EventsHandler(AccountDisabledEvent)
export class AccountDisabledEventHandler implements
  IEventHandler&lt;AccountDisabledEvent&gt; {
  async handle(event: AccountDisabledEvent): Promise&lt;void&gt; {
    await handleAccountEvent('AccountUnitDisabled', event);
  }
}
@EventsHandler(AccountEnabledEvent)
export class AccountEnabledEventHandler implements
  IEventHandler&lt;AccountEnabledEvent&gt; {
  async handle(event: AccountEnabledEvent): Promise&lt;void&gt; {
    await handleAccountEvent('AccountUnitEnabled', event);
  }
}</pre>			<p>In this code, we define event handlers for account registration, disabling, and enabling. When an account is registered, the <strong class="source-inline">AccountRegisteredEventHandler</strong> triggers logic related to account creation. Similarly, <strong class="source-inline">AccountDisabledEventHandler</strong> and <strong class="source-inline">AccountEnabledEventHandler</strong> handle account disabling and enabling events, respectively. These handlers leverage the <strong class="source-inline">handleAccountEvent</strong> function for centralized <span class="No-Break">event processing.</span></p>
			<p>That is great, but how do these commands interact with events? To demonstrate this, we need to discuss<a id="_idIndexMarker867"/> one <a id="_idIndexMarker868"/>more concept, called an aggregate root, a popular <a id="_idIndexMarker869"/>pattern in <strong class="bold"> Domain-Driven </strong><span class="No-Break"><strong class="bold">Design</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DDD</strong></span><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor187"/>Implementing an aggregate root</h2>
			<p>In DDD, an <strong class="bold">aggregate root</strong> is a fundamental <a id="_idIndexMarker870"/>concept for modeling complex domains. It acts as the central entity within a cluster of related objects, also known <a id="_idIndexMarker871"/>as <span class="No-Break">an </span><span class="No-Break"><strong class="bold">aggregate</strong></span><span class="No-Break">.</span></p>
			<p>An aggregate root encapsulates the core data and logic associated with a particular domain concept. In our example, <strong class="source-inline">AccountAggregate</strong> will hold all the essential information about an account (i.e., ID, payment mechanism count, and disabled status). This centralizes the account’s state and promotes <span class="No-Break">data integrity.</span></p>
			<p>An aggregate root plays a crucial role in event sourcing, a technique for persisting domain object changes as a sequence of events. In our code, <strong class="source-inline">AccountAggregate</strong> methods such as <strong class="source-inline">registerAccount</strong> apply events to the aggregate, reflecting state changes. By reconstructing the state from the event stream, the aggregate root becomes the central source of truth for the <span class="No-Break">account’s history.</span></p>
			<p>An aggregate root defines the transactional boundaries within our domain. Within an aggregate, changes to the state of all related entities (including the root itself) must happen atomically. This ensures data consistency within <span class="No-Break">the aggregate.</span></p>
			<p>An aggregate root also serves as the sole entry point for external interactions with the aggregate. This means other parts of your application (or other aggregates) should interact with the domain through the aggregate root’s methods. This promotes loose coupling and simplifies reasoning about <span class="No-Break">domain logic.</span></p>
			<p>Aggregate roots promote data consistency and integrity by centralizing state management and defining transactional boundaries. They simplify domain logic by providing a clear entry point for interactions. They also improve code maintainability by encapsulating related entities and <span class="No-Break">their behavior.</span></p>
			<p>By effectively utilizing aggregate roots in DDD, we can build robust and maintainable domain models that accurately reflect your business processes. Now, let’s see how it is possible to rebuild the <a id="_idIndexMarker872"/>state of <strong class="source-inline">AccountAggregate</strong> by reading its event stream from the <span class="No-Break">event store:</span></p>
			<pre class="source-code">
export class AccountAggregate extends AggregateRoot {
..........
static async loadAggregate(aggregateId: string):
  Promise&lt;AccountAggregate&gt; {
    const events = eventStore.readStream(
      'Account-unit-stream-' + aggregateId);
    let count = 0;
    const aggregate = new AccountAggregate();
    for await (const event of events) {
      const eventData: any = event.event.data;
      try {
        switch (event.event.type) {
          case 'AccountUnitCreated':
            aggregate.applyAccountRegisteredEventToAggregate({
              aggregateId: eventData.id,
              paymentmechanismCount:
                eventData.paymentmechanismCount,
            });
            break;
          case 'AccountUnitDisabled':
            aggregate.accountDisabled();
            break;
          case 'AccountUnitEnabled':
            aggregate.accountEnabled();
            break;
          default:
            break
        }
      } catch(e) {
        console.error("Could not process event")
      }
      count++;
    }
    return aggregate;
}}</pre>			<p>This NestJS code defines <a id="_idIndexMarker873"/>an asynchronous function named <strong class="source-inline">loadAggregate</strong> that takes an aggregate ID as input. It retrieves a <strong class="source-inline">stream</strong> of events related to that ID from the event store. The function then iterates through each event and applies the changes it describes to an <strong class="source-inline">AccountAggregate</strong> object. There are cases for handling different event types, such as <strong class="source-inline">AccountUnitCreated</strong>, <strong class="source-inline">AccountUnitDisabled</strong>, and <strong class="source-inline">AccountUnitEnabled</strong>. If an event type isn’t recognized, it’s skipped. If there are errors processing an event, it logs an error message but keeps iterating. Finally, the function returns the populated <span class="No-Break"><strong class="source-inline">AccountAggregate</strong></span><span class="No-Break"> object.</span></p>
			<p>Download our Git repository for a more complete example of implementing an aggregate root. Here is a snippet from an aggregate root that handles <span class="No-Break">the operations:</span></p>
			<pre class="source-code">
export class AccountAggregate extends AggregateRoot {
……
 registerAccount(aggregateId: string,
    paymentmechanismCount: string) {
    this.apply(new AccountRegisteredEvent(aggregateId,
      paymentmechanismCount));
  }
  enableAccount(): void {
    if(this.disabled) {
      this.apply(new AccountEnabledEvent(this.id,
        this.paymentmechanismCount))
    }
  }
  disableAccount() {
    if (!this.disabled) {
      this.apply(new AccountDisabledEvent(this.id,
        this.paymentmechanismCount));
    }
  }
…
}</pre>			<p>As you might<a id="_idIndexMarker874"/> guess, commands interact with events using an aggregate root, and the latter encapsulates the logic that <span class="No-Break">triggers events.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor188"/>Implementing projection</h2>
			<p>In CQRS and event sourcing <a id="_idIndexMarker875"/>architectures, <strong class="bold">projections</strong> play a crucial role in efficiently retrieving data for reads. Event sourcing acts like a film reel, storing a sequence of events that represent all the changes that happened in a system. Each event captures a specific action (such as <strong class="source-inline">Account Created</strong> or <span class="No-Break"><strong class="source-inline">Account Disabled</strong></span><span class="No-Break">).</span></p>
			<p>Projections are like the projection booth in a movie theater. They take the event stream (the film reel) and <em class="italic">project</em> it into a specific format, suitable for reading. This format, called a <strong class="bold">read model</strong>, is optimized to query <span class="No-Break">data efficiently.</span></p>
			<p>With that, let’s understand why projections <span class="No-Break">are important:</span></p>
			<ul>
				<li><strong class="bold">Read efficiency</strong>: Projections help rebuild the entire system state from the event stream, as doing so for every read query would be slow. Projections pre-process the event stream, creating a separate, optimized data structure for frequently <span class="No-Break">accessed information.</span></li>
				<li><strong class="bold">Flexibility</strong>: We can create multiple projections tailored to different reading needs with projections. One projection might focus on account details, while another might analyze <span class="No-Break">purchase history.</span></li>
			</ul>
			<p>Next, let’s see how <span class="No-Break">projections work:</span></p>
			<ol>
				<li><strong class="bold">Event listeners</strong>: Projections act as event listeners, subscribing to the <span class="No-Break">event stream.</span></li>
				<li><strong class="bold">Processing events</strong>: As new events arrive, a projection processes them one by one, updating its internal read <span class="No-Break">model accordingly.</span></li>
				<li><strong class="bold">Read model access</strong>: When a read query arrives, a system retrieves the relevant data from a projection’s read model instead of the entire <span class="No-Break">event stream.</span></li>
			</ol>
			<p>Projections are not a replacement for an event store. The event store remains the single source of truth for all historical events. Projections simply offer a way to efficiently access specific data from that history. Having said that, let’s look at some of the benefits <span class="No-Break">of projections:</span></p>
			<ul>
				<li><strong class="bold">Faster reads</strong>: Queries run against read models are significantly faster than replaying an entire <span class="No-Break">event stream.</span></li>
				<li><strong class="bold">Scalability</strong>: Projections can be scaled independently to handle increasing <span class="No-Break">read traffic.</span></li>
				<li><strong class="bold">Flexibility</strong>: Different projections cater to diverse read needs without impacting <span class="No-Break">write performance.</span></li>
			</ul>
			<p>We plan to implement a simple projection that demonstrates the usage of projection in CQRS and event <span class="No-Break">sourcing architectures.</span></p>
			<p>Under the <strong class="source-inline">src</strong> /<strong class="source-inline">paymentmechanism</strong> folder, create a <strong class="source-inline">paymentmechanism-total.projection.ts</strong> file<a id="_idIndexMarker876"/> with the <span class="No-Break">following functionalities:</span></p>
			<pre class="source-code">
@EventsHandler(AccountRegisteredEvent,
  AccountDisabledEvent, AccountEnabledEvent)
export class PaymentMechanismProjection implements
  IEventHandler&lt;AccountRegisteredEvent |
  AccountDisabledEvent | AccountEnabledEvent&gt; {
  private currentPaymentMechanismTotal: number = 0;
  constructor() {
    console.log('Account info Projection instance created:', this);
  }
  handle(event: AccountRegisteredEvent |
    AccountDisabledEvent | AccountEnabledEvent): void {
    if (event instanceof AccountRegisteredEvent) {
      this.handleAccountRegistered(event);
    } else if (event instanceof AccountDisabledEvent) {
      this.handleAccountDisabled(event);
    } else if (event instanceof AccountEnabledEvent) {
      this.handleAccountEnabled(event);
    }
  }
 ........
 .......</pre>			<p>This code defines an event handler class named <strong class="source-inline">PaymentMechanismProjection</strong> in a CQRS architecture with event sourcing. It listens for three specific events related to <span class="No-Break">account management:</span></p>
			<ul>
				<li><strong class="source-inline">AccountRegisteredEvent</strong>: Triggers when a new account <span class="No-Break">is created.</span></li>
				<li><strong class="source-inline">AccountDisabledEvent</strong>: Triggers when an account <span class="No-Break">is deactivated.</span></li>
				<li><strong class="source-inline">AccountEnabledEvent</strong>: Triggers when a deactivated account <span class="No-Break">is reactivated.</span></li>
			</ul>
			<p>The class keeps track of the total number of payment <span class="No-Break">mechanisms (</span><span class="No-Break"><strong class="source-inline">currentPayment</strong></span><strong class="source-inline">
MechanismTotal</strong>), but its initial value <span class="No-Break">is </span><span class="No-Break">zero</span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">handle</strong> method is<a id="_idIndexMarker877"/> the core functionality. It checks the type of the incoming event and calls a specific handler function, based on the <span class="No-Break">event type:</span></p>
			<ul>
				<li><strong class="source-inline">handleAccountRegistered</strong>: Handles <strong class="source-inline">AccountRegisteredEvent</strong> by incrementing <strong class="source-inline">currentPaymentMechanismTotal</strong>, based on information in the <span class="No-Break">event data.</span></li>
				<li><strong class="source-inline">handleAccountDisabled</strong>: Handles <strong class="source-inline">AccountDisabledEvent</strong> and <span class="No-Break">decrements </span><span class="No-Break"><strong class="source-inline">currentPaymentMechanismTotal</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">handleAccountEnabled</strong>: Handles the <strong class="source-inline">AccountEnabledEvent</strong> and applies the opposite operation <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">handleAccountDisabled</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>This is a simplified example, but it demonstrates how an event handler projection can listen for specific events and update its internal state accordingly, maintaining a view of the data optimized for a particular purpose (e.g., tracking total payment mechanisms). Here are our detailed handler methods in <span class="No-Break">this class:</span></p>
			<pre class="source-code">
handleAccountRegistered(event: AccountRegisteredEvent) {
    const pmCount = parseInt(event.paymentmechanismCount,
      10);
    this.currentPaymentMechanismTotal += pmCount;
    console.log("currentPaymentMechanismTotal",
      this.currentPaymentMechanismTotal)
  }
  handleAccountDisabled(event: AccountDisabledEvent) {
    const pmCount = parseInt(event.paymentmechanismCount,
      10);
    this.currentPaymentMechanismTotal -= pmCount;
    console.log("currentPaymentMechanismTotal",
      this.currentPaymentMechanismTotal)
  }
  handleAccountEnabled(event: AccountEnabledEvent) {
    const pmCount = parseInt(event.paymentmechanismCount,
      10);
    this.currentPaymentMechanismTotal += pmCount;
    console.log("currentPaymentMechanismTotal",
      this.currentPaymentMechanismTotal)
  }</pre>			<p>Our handlers<a id="_idIndexMarker878"/> simply interact with <strong class="source-inline">currentPaymentMechanismTotal</strong> and build logic around it. The idea is simple, but you can implement more complex logic based on <span class="No-Break">this knowledge.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor189"/>Implementing API functionalities</h2>
			<p>We use controllers<a id="_idIndexMarker879"/> as an entry point to our request flow. In a classical flow, controllers accept requests and forward them to the related services. When we apply CQRS and event sourcing, we usually use the same controllers, but instead of specifying direct services, we apply a command pattern to provide commands and their handlers.  Controllers serve as the intermediary between a client and the backend logic, determining how an application should respond to various requests. Controllers map specific routes to corresponding methods that contain business logic. By organizing request handling within controllers, the application maintains a clear separation of concerns, making it more structured, scalable, and easier <span class="No-Break">to manage.</span></p>
			<p>Create a new folder called <strong class="source-inline">api</strong> under the <strong class="source-inline">src</strong> folder. Then, create a new file called <strong class="source-inline">account.controller.ts</strong> under <strong class="source-inline">src</strong> / <strong class="source-inline">api</strong>, with the <span class="No-Break">following content:</span></p>
			<pre class="source-code">
@Controller('Account')
export class AccountUnitController {
  constructor(private readonly commandBus: CommandBus) {}
  @Post('/register')
  async registerAccount(@Query('paymentmechanismCount')
    paymentmechanismCount: string): Promise&lt;any&gt; {
    const aggregateId = uuid()
    await this.commandBus.execute(new
      RegisterAccountUnitCommand(aggregateId,
        paymentmechanismCount))
    return { message: 'Request received as a command',
      aggregateId };
  }
  @Post('/:id/disable')
  async disableAccount(@Param('id') id: string):
    Promise&lt;any&gt; {
    await this.commandBus.execute(new
      DisableAccountUnitCommand(id))
    return { message: 'Request received as a command' };
  }
  @Post('/:id/enable')
  async enableAccount(@Param('id') id: string):
    Promise&lt;any&gt; {
    await this.commandBus.execute(new
      EnableAccountUnitCommand(id))
    return { message: 'Request received as a command' };
  } }</pre>			<p>This NestJS controller handles account management. It’s named <strong class="source-inline">AccountUnitController</strong> and is mapped to the <strong class="source-inline">/Account</strong> route. The controller uses a command bus to send commands. There are three functionalities exposed through <span class="No-Break"><strong class="source-inline">POST</strong></span><span class="No-Break"> requests:</span></p>
			<ul>
				<li><strong class="source-inline">registerAccount</strong> allows you to create a new account with a payment mechanism count, by <span class="No-Break">sending </span><span class="No-Break"><strong class="source-inline">RegisterAccountUnitCommand</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">disableAccount</strong> deactivates an account by ID <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">DisableAccountUnitCommand</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">enableAccount</strong> reactivates an account using an <strong class="source-inline">EnableAccountUnitCommand</strong>, based on <span class="No-Break">its ID.</span></li>
			</ul>
			<p>All successful <a id="_idIndexMarker880"/>requests return a message indicating the command was received and the aggregate ID (<span class="No-Break">for registration).</span></p>
			<p>In order to enable a controller’s functionality, we need to import several essential elements. <strong class="source-inline">Controller</strong>, <strong class="source-inline">Param</strong>, <strong class="source-inline">Post</strong>, and <strong class="source-inline">Query</strong> from <strong class="source-inline">@nestjs/common</strong> are necessary to define the controller, handle route parameters, and process HTTP <strong class="source-inline">POST</strong> requests with query parameters. <strong class="source-inline">CommandBus</strong> from <strong class="source-inline">@nestjs/cqrs</strong> allows us to dispatch commands, following the CQRS pattern. We import the specific commands (<strong class="source-inline">DisableAccountUnitCommand</strong>, <strong class="source-inline">EnableAccountUnitCommand</strong>, and <strong class="source-inline">RegisterAccountUnitCommand</strong>) from the <strong class="source-inline">account.commands</strong> file to perform specific operations on the account unit. Finally, we import the <strong class="source-inline">uuid</strong> package to generate unique IDs for <span class="No-Break">these operations:</span></p>
			<pre class="source-code">
import {Controller, Param, Post, Query} from
  '@nestjs/common'
import {CommandBus} from '@nestjs/cqrs'
import {
  DisableAccountUnitCommand,
  EnableAccountUnitCommand,
  RegisterAccountUnitCommand
} from '../account/account.commands'
import {v4 as uuid} from 'uuid'</pre>			<p>Our controller <a id="_idIndexMarker881"/>doesn’t know about events. It only interacts with commands. The request will flow to command handlers, and they will trigger <span class="No-Break">our events.</span></p>
			<p>Besides the controller, we have the <strong class="source-inline">account.module.ts</strong> file, which <span class="No-Break">contains </span><span class="No-Break"><strong class="source-inline">AccountModule</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
export class AccountModule implements OnModuleInit {
  async onModuleInit() {
    this.startSubscription();
  }
  private startSubscription() {
    (async (): Promise&lt;void&gt; =&gt; {
      await this.subscribeToAll();
    })();
  }
  private async subscribeToAll() {
    const subscriptionList = eventStore.subscribeToAll({
      filter: streamNameFilter({ prefixes: ["Account-unit-stream-"] 
        }),
    });
    for await (const subscriptionItem of subscriptionList){
      console.log(
          `Handled event ${subscriptionItem.event?.revision}@${subscriptionItem.event?.streamId}`
      );
      const subscriptionData: any =
        subscriptionItem.event.data;
      console.log("subscription data:", subscriptionData);
    }
  }
}</pre>			<p>For a complete<a id="_idIndexMarker882"/> example with imported functionalities, check out <span class="No-Break">our repository.</span></p>
			<p>This code defines  <strong class="source-inline">AccountModule</strong> used in a CQRS architecture with event sourcing. It implements the <strong class="source-inline">OnModuleInit</strong> life cycle hook, which gets called after the module <span class="No-Break">is initialized.</span></p>
			<p>Here’s a breakdown of <span class="No-Break">the functionality:</span></p>
			<ul>
				<li><strong class="source-inline">onModuleInit</strong>: This method is called when the module <span class="No-Break">is ready.</span></li>
				<li><strong class="source-inline">startSubscription (private)</strong>: This private method initiates a subscription to an event stream. It uses an <strong class="bold">Immediately Invoked Function Expression</strong> (<strong class="bold">IIFE</strong>) to encapsulate the <span class="No-Break">asynchronous logic.</span></li>
			</ul>
			<p>Finally, we will take a look at <strong class="source-inline">subscribeToAll(private, async)</strong>; this private asynchronous method does the actual subscription work. It uses <strong class="source-inline">eventStore.subscribeToAll</strong> to subscribe to all event streams that start with the <strong class="source-inline">Account-unit-stream-</strong> prefix. This method typically captures all events related to account management. It iterates through the subscription using  <strong class="source-inline">for await...</strong> of the loop. For each event received, it logs the event revision number and stream ID, extracts the event data, and logs it as well. The <strong class="source-inline">AccountModule</strong> subscribes to a specific category of events in the event store (events related to accounts). Whenever a new <a id="_idIndexMarker883"/>event related to accounts arrives, it logs details about the event and its data for potential processing <span class="No-Break">or monitoring.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor190"/>Testing an application</h2>
			<p>Before running our <a id="_idIndexMarker884"/>application, you should run the provided <strong class="source-inline">docker-compose</strong> file via the <strong class="source-inline">docker-compose up -d</strong> command. It ensures that we already have <strong class="source-inline">EventStoreDB</strong> as a data store. To make sure if data store is running, just navigate to <strong class="source-inline">localhost:2113</strong>, and you should see the <span class="No-Break"><strong class="source-inline">EventStoreDB</strong></span><span class="No-Break">’s dashboard.</span></p>
			<p>To run our application, execute the <strong class="source-inline">nest start</strong> command from the command line. Open your Postman application, and create a new tab. Select the <strong class="bold">POST</strong> command for the <a href="http://localhost:8080/account/register?paymentmechanismCount=67">http://localhost:8080/account/register?paymentmechanismCount=67</a> URL. Here, we register a new account, with the value of <strong class="source-inline">paymentmechanismcount</strong> set to <strong class="source-inline">67</strong>. Then, click the <span class="No-Break"><strong class="bold">Send</strong></span><span class="No-Break"> button.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer108">
					<img alt="Figure 11.8: Account registration" src="image/B09148_11_008.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8: Account registration</p>
			<p>After successful operation, you should get the following message to your VS <span class="No-Break">Code console.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer109">
					<img alt="Figure 11.9: Account registration logs" src="image/B09148_11_009.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9: Account registration logs</p>
			<p>The ID will be different in <a id="_idIndexMarker885"/>your case because it is automatically generated by the system. After running the same command with a different payment mechanism count (it is twenty-three in our case), you should get the following message with <strong class="source-inline">currentPaymentMechanismCount=90</strong>. The ID is different again, but if you use the same payment mechanism count, the values should be totaled based on the  <strong class="source-inline">currentPaymentMechanismTotal = currentPaymentMechanismTotal + </strong><span class="No-Break"><strong class="source-inline">paymentMechanismCount</strong></span><span class="No-Break"> formula:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer110">
					<img alt="Figure 11.10: Account registration calculation" src="image/B09148_11_010.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10: Account registration calculation</p>
			<p>Now, we have two different IDs (aggregate IDs), and we can use any of them to enable and <span class="No-Break">disable requests.</span></p>
			<p>Open a new tab on Postman and send a POST request to <strong class="source-inline">http://localhost:8080/account/YOUR_AGGREGATE_ID/disable</strong>. The last aggregate ID stores the value of <strong class="source-inline">paymentmechanismCount</strong>, which is twenty-three. So, disabling the endpoint should end up making a value of <strong class="source-inline">currentPaymentMechanismTotal = 67</strong>. The logic is ninety minus twenty-three equals <span class="No-Break">to sixty-seven.</span></p>
			<p>Let’s run the <strong class="bold">POST</strong> command for <strong class="source-inline">http://localhost:8080/account/90f80d89-4620-4526-ae3e-02a8156df9a1/disable</strong> and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Send</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 11.11: The disabled account response" src="image/B09148_11_011.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11: The disabled account response</p>
			<p>To enable the account, just replace <strong class="source-inline">disable</strong> with <strong class="source-inline">enable</strong> and run the command again. It should restore <strong class="source-inline">currentPaymentMechanismTotal</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">90</strong></span><span class="No-Break">.</span></p>
			<p>Besides CQRS and<a id="_idIndexMarker886"/> event sourcing, we have a Service Registry and discovery for microservices. The next section will help us to understand them in <span class="No-Break">more detail.</span></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor191"/>Service Registry and discovery in microservices</h1>
			<p>Microservices<a id="_idIndexMarker887"/> development by itself consists of huge amounts of patterns and best practices. It is indeed not possible to cover all of them in one book. In this section, we will provide popular patterns and techniques used in <span class="No-Break">microservices development.</span></p>
			<p>In a microservice architecture, applications are built as a collection of small, independent services. These services need to communicate with each other to cover user requests. Service Registry and discovery is a mechanism that simplifies this communication by <a id="_idIndexMarker888"/>enabling services to find each <span class="No-Break">other dynamically.</span></p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor192"/>Understanding Service Registry and discovery</h2>
			<p>Imagine a central <a id="_idIndexMarker889"/>database. This database, called the <strong class="bold">Service Registry</strong>, acts as a <a id="_idIndexMarker890"/>directory of all the microservices in your system. Each service instance (i.e., an individual running a copy of a microservice) registers itself with the registry. During registration, the service provides details such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Network location</strong>: The address (IP address and port) where the service can <span class="No-Break">be found.</span></li>
				<li><strong class="bold">Capabilities</strong>: What the service can do (e.g., processes payments or provides <span class="No-Break">user data).</span></li>
				<li><strong class="bold">Health Information</strong>: Status details such as whether the service is currently healthy and available to <span class="No-Break">handle requests.</span></li>
			</ul>
			<p>You can use tools such as Consul, ZooKeeper, and Eureka Server (as used by Netflix) for real-world <span class="No-Break">service registries.</span></p>
			<p>Service Registry often integrates with API gateways, which are a single-entry point for external clients to access microservices. An API Gateway might leverage the Service Registry to discover the latest locations of the microservices it needs to route <span class="No-Break">requests to.</span></p>
			<p>Conversely, <strong class="bold">Service </strong><strong class="bold">d</strong><strong class="bold">iscovery</strong> is the<a id="_idIndexMarker891"/> process where microservices find the location of other services they need to interact with. There are two <span class="No-Break">main approaches:</span></p>
			<ul>
				<li><strong class="bold">Client-side discovery</strong>: The service that needs another service (the client) directly queries the registry to find the address of the <span class="No-Break">target service.</span></li>
				<li><strong class="bold">Server-side discovery</strong>: A separate component, such as a load balancer, sits in front of the services. This component retrieves service locations from the registry and routes requests to the appropriate <span class="No-Break">service instance.</span></li>
			</ul>
			<p>Let’s look at some benefits of Service Registry <span class="No-Break">and discovery:</span></p>
			<ul>
				<li><strong class="bold">Dynamic service location</strong>: Services<a id="_idIndexMarker892"/> don’t need to be hardcoded with the addresses of other services. They can discover them on-demand from the registry, making the system more adaptable <span class="No-Break">to changes.</span></li>
				<li><strong class="bold">Scalability and elasticity</strong>: As you add or remove service instances, the registry automatically reflects the changes. This ensures that clients always interact with <span class="No-Break">available services.</span></li>
				<li><strong class="bold">Loose coupling</strong>: Services become loosely coupled, as they rely on the registry for communication. This promotes independent development and deployment <span class="No-Break">of microservices.</span></li>
			</ul>
			<p>By using a central registry and enabling dynamic discovery, Service Registry and discovery simplify <a id="_idIndexMarker893"/>communication and promote flexibility in a <span class="No-Break">microservice architecture.</span></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor193"/>Approaches for implementing Service Registry and Discovery</h2>
			<p>There are two <a id="_idIndexMarker894"/>main approaches to implementing Service Registry and discovery in <span class="No-Break">Node.js microservices:</span></p>
			<ul>
				<li>The first option is using a dedicated Service Registry tool. This approach leverages a separate service specifically designed for Service Registry and discovery functionalities. We can use popular options such as Consul, ZooKeeper, and Eureka Server (Netflix). These tools offer robust features for registration, discovery, health checks, and <span class="No-Break">so on.</span></li>
				<li>The second option is Node.js client libraries. Each registry tool typically provides a Node.js client library that simplifies interaction with the registry. The library allows your microservices to register themselves, discover other services, and monitor <span class="No-Break">their health.</span></li>
			</ul>
			<p>Finally, let us look at how we can implement a Service Registry before wrapping up <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor194"/>Implementing Service Registry</h2>
			<p>Now, let’s take a brief <a id="_idIndexMarker895"/>look at implementing a <span class="No-Break">Service Registry:</span></p>
			<ol>
				<li>Choose a Service Registry tool, and install its Node.js <span class="No-Break">client library:</span><ul><li>During startup, each microservice registers itself with a registry using the library. It provides its network location, capabilities, and <span class="No-Break">health information.</span></li><li>In client-side discovery, the service needing another service uses the library to query the registry for the target <span class="No-Break">service’s address.</span></li><li>In server-side discovery, a separate component, such as a load balancer, retrieves service locations from the registry and routes <span class="No-Break">requests accordingly.</span></li></ul></li>
				<li>Now, let’s move on to building a simple registry with Node.js: For smaller deployments or learning purposes, you can implement a basic Service Registry using Node.js itself. Here’s a <span class="No-Break">simplified example:</span><ul><li><strong class="bold">For data storage</strong>: Use a lightweight in-memory data store, such as Redis, or a simple Node.js object to store <span class="No-Break">service information</span></li><li><strong class="bold">Registration</strong>: During startup, each microservice registers itself with the registry by sending a message containing <span class="No-Break">its details</span></li><li><strong class="bold">Discovery</strong>: Services can query the registry to retrieve a list of available services and <span class="No-Break">their addresses</span></li></ul></li>
			</ol>
			<p>Before we end this section, let’s look at some important considerations of Service Registry and discovery, the first being <strong class="bold">security</strong>. When implementing your own registry, ensure proper authentication and authorization mechanisms to control access to registration and discovery functionalities. Next is <strong class="bold">scalability</strong>. A homegrown registry might not scale well for large deployments. Consider a dedicated tool for production environments. Finally, <strong class="bold">health checks</strong> are very important. Regularly check the health of registered services to ensure that they <span class="No-Break">are available.</span></p>
			<p>We’ve covered everything <a id="_idIndexMarker896"/>about microservice architecture in this chapter. It’s now time to <span class="No-Break">wrap up.</span></p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor195"/>Summary</h1>
			<p>This chapter dived into the building blocks of a strong microservice architecture. It covered API gateways, explaining their purpose, use cases, and how to implement them for optimal performance, with caching, rate limiting, and response aggregation. The chapter then explored CQRS and event sourcing patterns, along with event streaming, a technology that makes them work. Finally, it discussed Service Registry and discovery, essential for microservices to communicate with each other. This chapter provided the knowledge and practical examples to build a well-designed and scalable <span class="No-Break">microservice infrastructure.</span></p>
			<p>In the next chapter, we will explore testing strategies in depth and cover how to write effective unit and integration tests for <span class="No-Break">your microservices.</span></p>
		</div>
	</body></html>