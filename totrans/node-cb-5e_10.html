<html><head></head><body>
  <div><h1 class="chapter-number" id="_idParaDest-311">
    <a id="_idTextAnchor318">
    </a>
    
     10
    
   </h1>
   <h1 id="_idParaDest-312">
    <a id="_idTextAnchor319">
    </a>
    
     Optimizing Performance
    
   </h1>
   <p>
    
     Performance optimization is an endless activity.
    
    
     Further optimizations can always be made.
    
    
     The recipes in this chapter will demonstrate typical performance
    
    
     
      optimization workflows.
     
    
   </p>
   <p>
    
     The performance optimization workflow starts with establishing a baseline.
    
    
     Often, this involves benchmarking our application in some way.
    
    
     In the case of a web server, this could be measuring how many requests our server can handle per second.
    
    
     A baseline measure must be recorded for us to have evidence of any performance improvements that have
    
    
     
      been made.
     
    
   </p>
   <p>
    
     Once the baseline has been determined, the next step is to identify the bottleneck.
    
    
     The recipes in this chapter will cover using tools such as flame graphs and memory profilers to help us identify the specific bottlenecks in an application.
    
    
     Using these performance tools will ensure that our optimization efforts are invested in the
    
    
     
      correct place.
     
    
   </p>
   <p>
    
     Identifying a bottleneck is the first step to understanding where the optimization work should begin, and performance tools can help us determine the starting point.
    
    
     For instance, a flame graph can identify a specific function responsible for causing the bottleneck.
    
    
     After making the necessary optimizations, the changes must be verified by rerunning the initial baseline test.
    
    
     This allows us to have numerical evidence supporting whether the optimization has improved the
    
    
     
      application’s performance.
     
    
   </p>
   <p>
    
     This chapter will cover the
    
    
     
      following recipes:
     
    
   </p>
   <ul>
    <li>
     
      Benchmarking
     
     
      
       HTTP requests
      
     
    </li>
    <li>
     
      Interpreting
     
     
      
       flame graphs
      
     
    </li>
    <li>
     
      Detecting
     
     
      
       memory leaks
      
     
    </li>
    <li>
     
      Optimizing
     
     
      
       synchronous functions
      
     
    </li>
    <li>
     
      Optimizing
     
     
      
       asynchronous functions
      
     
    </li>
    <li>
     
      Working with
     
     
      
       worker threads
      
     
    </li>
   </ul>
   <h1 id="_idParaDest-313">
    <a id="_idTextAnchor320">
    </a>
    
     Technical requirements
    
   </h1>
   <p>
    
     You should have the latest version of Node.js 22 installed, as well as access to a terminal.
    
    
     You will also need access to an editor and browser of
    
    
     
      your choice.
     
    
   </p>
   <p>
    
     The
    
    <em class="italic">
     
      Optimizing synchronous functions
     
    </em>
    
     recipe will require the use of MongoDB.
    
    
     We’ll be using Docker to provision a containerized MongoDB instance.
    
    
     Please refer to
    
    <a href="B19212_07.xhtml#_idTextAnchor212">
     
      <em class="italic">
       
        Chapter 7
       
      </em>
     
    </a>
    
     , for detailed technical setup information regarding how to use MongoDB
    
    
     
      via Docker.
     
    
   </p>
   <p>
    
     The code samples that will be used in this chapter can be found in this book’s GitHub repository at
    
    <a href="https://github.com/PacktPublishing/Node.js-Cookbook-Fifth-Edition">
     
      https://github.com/PacktPublishing/Node.js-Cookbook-Fifth-Edition
     
    </a>
    
     , in the
    
    
     <strong class="source-inline">
      
       Chapter10
      
     </strong>
    
    
     
      directory.
     
    
   </p>
   <h1 id="_idParaDest-314">
    <a id="_idTextAnchor321">
    </a>
    
     Benchmarking HTTP requests
    
   </h1>
   <p>
    
     As we’ve seen
    
    <a id="_idIndexMarker781">
    </a>
    
     throughout this book, HTTP communications are the foundation of many Node.js applications and microservices.
    
    
     For these applications, the HTTP requests should be handled as efficiently as possible.
    
    
     To be able to optimize, we must first record a baseline measure of our application’s performance.
    
    
     Once we’ve recorded the baseline, we’ll be able to determine the impact of our
    
    
     
      optimization efforts.
     
    
   </p>
   <p>
    
     To create a baseline, it’s necessary to simulate the load on the application and record how it responds.
    
    
     For an HTTP-based application, we must simulate HTTP requests being sent to
    
    
     
      the server.
     
    
   </p>
   <p>
    
     In this recipe, we’ll capture a baseline performance measure for an HTTP web server using a tool named
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     (
    
    <a href="https://github.com/mcollina/autocannon">
     
      https://github.com/mcollina/autocannon
     
    </a>
    
     ), which will simulate
    
    
     
      HTTP requests.
     
    
   </p>
   <h2 id="_idParaDest-315">
    <a id="_idTextAnchor322">
    </a>
    
     Getting ready
    
   </h2>
   <p>
    
     In this recipe, we’ll be using the
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     tool to benchmark an Express.js web server.
    
    
     Instead of creating a web server from scratch, we’ll use the Express.js generator to create one.
    
    
     The web server will return an HTML page
    
    
     
      at
     
    
    
     <strong class="source-inline">
      
       http://localhost:3000
      
     </strong>
    
    
     
      :
     
    
   </p>
   <ol>
    <li>
     
      Enter the following commands to use the Express.js generator to generate a sample
     
     
      
       web server:
      
     
     <pre class="source-code">
<strong class="bold">$ npx express-generator --no-view benchmarking-http</strong>
<strong class="bold">$ cd benchmarking-http</strong>
<strong class="bold">$ npm install</strong></pre>
    </li>
    <li>
     
      The
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      tool is available on the
     
     <strong class="source-inline">
      
       npm
      
     </strong>
     
      registry.
     
     
      Globally install the
     
     
      <strong class="source-inline">
       
        autocannon
       
      </strong>
     
     
      
       module:
      
     
     <pre class="source-code">
<strong class="bold">$ npm install --global autocannon</strong></pre>
    </li>
   </ol>
   <p>
    
     Now that we’ve created a web server to test, we’re ready to start
    
    
     
      this recipe.
     
    
   </p>
   <h2 id="_idParaDest-316">
    <a id="_idTextAnchor323">
    </a>
    
     How to do it…
    
   </h2>
   <p>
    
     In this recipe, we’ll learn
    
    <a id="_idIndexMarker782">
    </a>
    
     how to use the
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     tool to benchmark
    
    
     
      HTTP requests:
     
    
   </p>
   <ol>
    <li>
     
      Start the Express.js web server with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ npm start</strong></pre>
    </li>
    <li>
     
      Navigate to
     
     <strong class="source-inline">
      
       http://localhost:3000
      
     </strong>
     
      in your browser.
     
     
      You should see the
     
     
      
       following output:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.1 – Browser window showing the “Welcome to Express” web page" src="img/Figure_10.1_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.1 – Browser window showing the “Welcome to Express” web page
    
   </p>
   <ol>
    <li value="3">
     
      We’ve confirmed
     
     <a id="_idIndexMarker783">
     </a>
     
      our server has started and is responding to requests at
     
     <strong class="source-inline">
      
       http://localhost:3000
      
     </strong>
     
      .
     
     
      Now, we can use the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      tool to benchmark our HTTP requests.
     
     
      Open a new terminal window and enter the following command to run a load test
     
     
      
       with
      
     
     
      <strong class="source-inline">
       
        autocannon
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 100 http://localhost:3000/</strong></pre>
    </li>
    <li>
     
      While the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      load test is running, switch to the terminal window where you started the web server.
     
     
      You should see a mass of
     
     
      
       incoming requests:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.2 – The Express.js server receiving many HTTP GET requests" src="img/Figure_10.2_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.2 – The Express.js server receiving many HTTP GET requests
    
   </p>
   <ol>
    <li value="5">
     
      Switch back to the
     
     <a id="_idIndexMarker784">
     </a>
     
      terminal window where you’re running the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      load test.
     
     
      Once the load test is complete, you should see an output similar to the following, detailing
     
     
      
       the results:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.3 – autocannon results summary" src="img/Figure_10.3_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.3 – autocannon results summary
    
   </p>
   <ol>
    <li value="6">
     
      Observe the table of results.
     
     
      The first table details the request latency.
     
     
      The average was recorded as
     
     <strong class="source-inline">
      
       12.74
      
     </strong>
     
      ms.
     
     
      The second table details the request volume.
     
     
      Here, it was recorded that our server handled an average of
     
     <strong class="source-inline">
      
       7,555.2
      
     </strong>
     
      requests per second, with an average throughput of
     
     <strong class="source-inline">
      
       3.71
      
     </strong>
     
      MB
     
     
      
       per second.
      
     
    </li>
   </ol>
   <p>
    
     With that, we’ve learned
    
    <a id="_idIndexMarker785">
    </a>
    
     how to use the
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     tool to benchmark
    
    
     
      HTTP requests.
     
    
   </p>
   <h2 id="_idParaDest-317">
    <a id="_idTextAnchor324">
    </a>
    
     How it works…
    
   </h2>
   <p>
    
     The
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     tool is a cross-platform HTTP benchmarking tool written in Node.js and published to the
    
    
     <strong class="source-inline">
      
       npm
      
     </strong>
    
    
     
      registry.
     
    
   </p>
   <p>
    
     In this recipe, we used
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     to load test our Express.js web server at the
    
    <strong class="source-inline">
     
      http://localhost:3000
     
    </strong>
    
     endpoint.
    
    
     We passed
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     the
    
    <strong class="source-inline">
     
      --connections 100
     
    </strong>
    
     flag.
    
    
     This flag instructs
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     to allocate a pool of
    
    <strong class="source-inline">
     
      100
     
    </strong>
    
     concurrent connections to our server.
    
    
     Had we omitted this flag,
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     would have defaulted to allocating
    
    <strong class="source-inline">
     
      10
     
    </strong>
    
     concurrent connections.
    
    
     The number of concurrent connections should be altered to best represent the anticipated load on your server so that you can simulate
    
    
     
      production workloads.
     
    
   </p>
   <p class="callout-heading">
    
     Important note
    
   </p>
   <p class="callout">
    
     This recipe used the full-form command-line flags for
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     for readability.
    
    
     However, as with many command-line flags, it’s possible to use an abbreviated form.
    
    
     The
    
    <strong class="source-inline">
     
      --connections
     
    </strong>
    
     flag can be abbreviated to
    
    <strong class="source-inline">
     
      -c
     
    </strong>
    
     and the
    
    <strong class="source-inline">
     
      --duration
     
    </strong>
    
     flag can be abbreviated
    
    
     
      to
     
    
    
     <strong class="source-inline">
      
       -d
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Note that
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     defaults to running the load test for
    
    <strong class="source-inline">
     
      10
     
    </strong>
    
     seconds, immediately sending a new request on each socket after the previous request has been completed.
    
    
     It’s possible to extend the length of the load test using the
    
    <strong class="source-inline">
     
      --duration
     
    </strong>
    
     flag.
    
    
     For example, you could use the following command to extend the load test shown in this recipe to
    
    
     <strong class="source-inline">
      
       20
      
     </strong>
    
    
     
      seconds:
     
    
   </p>
   <pre class="console">
$ autocannon --connections 100 --duration 20 http://localhost:3000/</pre>
   <p>
    
     By default,
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     outputs the data from the load test in two tables.
    
    
     The first table details the request latency, while the second table details the
    
    
     
      request volume.
     
    
   </p>
   <p>
    <strong class="bold">
     
      Request latency
     
    </strong>
    
     is the
    
    <a id="_idIndexMarker786">
    </a>
    
     amount of time that’s elapsed
    
    <a id="_idIndexMarker787">
    </a>
    
     between when a request is made, and a response is received.
    
    
     The request latency table is broken down into various percentiles.
    
    
     The
    
    <strong class="source-inline">
     
      2.5%
     
    </strong>
    
     percentile records the fastest
    
    <strong class="source-inline">
     
      2.5%
     
    </strong>
    
     of requests, whereas the
    
    <strong class="source-inline">
     
      99%
     
    </strong>
    
     percentile records the slowest
    
    <strong class="source-inline">
     
      1%
     
    </strong>
    
     of requests.
    
    
     When benchmarking requests, it can be useful to record and consider both the best and worst-case scenarios.
    
    
     The latency table also details the average, standard deviation, and maximum recorded latency.
    
    
     Generally, the lower the latency,
    
    
     
      the better.
     
    
   </p>
   <p>
    
     The request volume table details the number of requests per second (
    
    <strong class="source-inline">
     
      Req/Sec
     
    </strong>
    
     ) and the throughput, which is recorded as the number of bytes processed per second (
    
    <strong class="source-inline">
     
      Bytes/Sec
     
    </strong>
    
     ).
    
    
     Again, the results are broken down into percentiles so that the best and worst cases can be interpreted.
    
    
     For these two measures, the higher the number, the better, as it indicates more requests were processed by the server in the
    
    
     
      given timeframe.
     
    
   </p>
   <p class="callout-heading">
    
     Important note
    
   </p>
   <p class="callout">
    
     For more information about the available
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     command-line flags, please refer to the
    
    <em class="italic">
     
      Usage
     
    </em>
    
     documentation on
    
    
     
      GitHub:
     
    
    <a href="https://github.com/mcollina/autocannon#usage">
     
      
       https://github.com/mcollina/autocannon#usage
      
     
    </a>
    
     
      .
     
    
   </p>
   <h2 id="_idParaDest-318">
    <a id="_idTextAnchor325">
    </a>
    
     There’s more…
    
   </h2>
   <p>
    
     Next, we’ll cover how to use
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     to benchmark HTTP
    
    <strong class="source-inline">
     
      POST
     
    </strong>
    
     requests.
    
    
     We’ll also consider how we
    
    <a id="_idIndexMarker788">
    </a>
    
     can best replicate a production environment during our benchmarks and how this can change our latency
    
    
     
      and throughput.
     
    
   </p>
   <h3>
    
     Benchmarking HTTP POST requests
    
   </h3>
   <p>
    
     In this recipe, we
    
    <a id="_idIndexMarker789">
    </a>
    
     benchmarked an HTTP
    
    <strong class="source-inline">
     
      GET
     
    </strong>
    
     request.
    
    
     The
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     tool provides allows you to send requests using other HTTP methods, such as
    
    
     
      HTTP
     
    
    
     <strong class="source-inline">
      
       POST
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Let’s see how we can use
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     to send an HTTP
    
    <strong class="source-inline">
     
      POST
     
    </strong>
    
     request with a
    
    
     
      JSON payload:
     
    
   </p>
   <ol>
    <li>
     
      In the same directory (
     
     <strong class="source-inline">
      
       benchmarking-http
      
     </strong>
     
      ), create a file
     
     
      
       named
      
     
     
      <strong class="source-inline">
       
        post-server.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ touch post-server.js</strong></pre>
    </li>
    <li>
     
      Now, we need to define an endpoint on an Express.js server that will handle an HTTP
     
     <strong class="source-inline">
      
       POST
      
     </strong>
     
      request with a JSON payload.
     
     
      Add the following
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        post-server.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const express = require('express');
const app = express();
const bodyParser = require('body-parser');
app.use(bodyParser.json());
app.use(bodyParser.urlencoded({ extended: false }));
app.post('/', (req, res) =&gt; {
  res.send(req.body);
});
app.listen(3000, () =&gt; {
  console.log('Server listening on port 3000');
});</pre>
    </li>
    <li>
     
      Now, we need to
     
     
      
       start
      
     
     
      <strong class="source-inline">
       
        post-server.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ node post-server.js</strong></pre>
    </li>
    <li>
     
      In a separate terminal window, enter the following command to load test the HTTP
     
     <strong class="source-inline">
      
       POST
      
     </strong>
     
      request.
     
     
      Note that we pass
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      the
     
     <strong class="source-inline">
      
       --method
      
     </strong>
     
      ,
     
     <strong class="source-inline">
      
       --headers
      
     </strong>
     
      , and
     
     <strong class="source-inline">
      
       --
      
     </strong>
     
      <strong class="source-inline">
       
        body
       
      </strong>
     
     
      
       flags:
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 100 --method POST --headers 'content-type=application/json' --body '{ "hello": "world"}' http://localhost:3000/</strong></pre>
     <p class="list-inset">
      
       As in the main recipe,
      
      <strong class="source-inline">
       
        autocannon
       
      </strong>
      
       will run the load test and output a
      
      
       
        results summary.
       
      
     </p>
    </li>
   </ol>
   <p>
    
     This demonstrates
    
    <a id="_idIndexMarker790">
    </a>
    
     how we can use
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     to simulate other HTTP method requests, including requests with
    
    
     
      a payload.
     
    
   </p>
   <h3>
    
     Replicating a production environment
    
   </h3>
   <p>
    
     When measuring
    
    <a id="_idIndexMarker791">
    </a>
    
     performance, it’s important to replicate the production environment as closely as possible; otherwise, we may produce misleading results.
    
    
     The behavior of applications in development and production may differ, which can result in
    
    
     
      performance differences.
     
    
   </p>
   <p>
    
     We can use an Express.js-generated application to demonstrate how performance results may differ, depending on the environment we’re
    
    
     
      running in.
     
    
   </p>
   <p>
    
     Use
    
    <strong class="source-inline">
     
      express-generator
     
    </strong>
    
     to generate an Express.js application in a new directory named
    
    <strong class="source-inline">
     
      benchmarking-views
     
    </strong>
    
     .
    
    
     For more information on the Express.js generator, please refer to the
    
    <em class="italic">
     
      Creating an Express.js web application
     
    </em>
    
     recipe in
    
    <a href="B19212_06.xhtml#_idTextAnchor178">
     
      <em class="italic">
       
        Chapter 6
       
      </em>
     
    </a>
    
     .
    
    
     In this example, we’ll be using the
    
    <strong class="source-inline">
     
      pug
     
    </strong>
    
     view engine to generate a simple
    
    
     
      HTML page:
     
    
   </p>
   <ol>
    <li>
     
      Enter the following command in your terminal to generate
     
     
      
       the application:
      
     
     <pre class="source-code">
<strong class="bold">$ npx express-generator --views=pug benchmarking-views</strong>
<strong class="bold">$ cd benchmarking-views</strong>
<strong class="bold">$ npm install</strong></pre>
    </li>
    <li>
     
      Start the server with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ npm start</strong></pre>
    </li>
    <li>
     
      In a new terminal
     
     <a id="_idIndexMarker792">
     </a>
     
      window, use
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      to load
     
     
      
       test
      
     
     
      <strong class="source-inline">
       
        http://localhost:3000
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 100 http://localhost:3000/</strong></pre>
     <p class="list-inset">
      
       Once the load test has been completed,
      
      <strong class="source-inline">
       
        autocannon
       
      </strong>
      
       will output the load test
      
      
       
        results summary:
       
      
     </p>
    </li>
   </ol>
   <div><div><img alt="Figure 10.4 – autocannon result summary from the development mode run" src="img/Figure_10.4_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.4 – autocannon result summary from the development mode run
    
   </p>
   <p class="list-inset">
    
     In this load test, the average number of requests per second was around 1,584, and the average throughput was around 632 kB per second.
    
    
     This is considerably slower than the HTTP
    
    <strong class="source-inline">
     
      GET
     
    </strong>
    
     request that we benchmarked in the
    
    
     
      main recipe.
     
    
   </p>
   <p>
    
     The reason why the requests are slower is that when in development mode, the pug templating engine will reload the template for every request.
    
    
     This is useful in development mode because
    
    <a id="_idIndexMarker793">
    </a>
    
     changes to the template can be reflected without having to restart the server.
    
    
     When the mode is set to production, Express.js will no longer reload the template for every request.
    
    
     This will result in
    
    
     
      performance differences.
     
    
   </p>
   <ol>
    <li>
     
      Restart the Express.js server in production mode using the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ NODE_ENV=production npm start</strong></pre>
    </li>
    <li>
     
      Now, in your other terminal window, rerun the same benchmark test
     
     
      
       using
      
     
     
      <strong class="source-inline">
       
        autocannon
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 100 http://localhost:3000/</strong></pre>
    </li>
    <li>
     
      Compare the output between the
     
     
      
       two runs:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.5 – autocannon result summary from the production mode run" src="img/Figure_10.5_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.5 – autocannon result summary from the production mode run
    
   </p>
   <p>
    
     In the second load test, we can see that the average number of requests per second has increased to approximately
    
    <strong class="source-inline">
     
      8744
     
    </strong>
    
     (up from
    
    <strong class="source-inline">
     
      1584
     
    </strong>
    
     ), and the throughput has increased to
    
    <strong class="source-inline">
     
      3.49
     
    </strong>
    
     MB per second (up from
    
    <strong class="source-inline">
     
      632
     
    </strong>
    
     kB).
    
    
     This performance increase is due to the template being cached when in
    
    
     
      production mode.
     
    
   </p>
   <p>
    
     This highlights the need
    
    <a id="_idIndexMarker794">
    </a>
    
     to benchmark our application in an environment that best represents the expected
    
    
     
      production environment.
     
    
   </p>
   <h2 id="_idParaDest-319">
    <a id="_idTextAnchor326">
    </a>
    
     See also
    
   </h2>
   <ul>
    <li>
     
      The
     
     <em class="italic">
      
       Interpreting flame graphs
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Detecting memory leaks
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Optimizing synchronous functions
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Optimizing asynchronous functions
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
   </ul>
   <h1 id="_idParaDest-320">
    <a id="_idTextAnchor327">
    </a>
    
     Interpreting flame graphs
    
   </h1>
   <p>
    
     A flame graph is a
    
    <a id="_idIndexMarker795">
    </a>
    
     visual tool that allows us to identify “hot code paths” within our application.
    
    
     The term “hot code path” is used to describe execution paths in the program that consume a relatively large amount of time, which can indicate a bottleneck in
    
    
     
      an application.
     
    
   </p>
   <p>
    
     Flame graphs provide a visualization of an application’s call stack during execution.
    
    
     From this visualization, it’s possible to determine which functions are spending the most time on the CPU while the application
    
    
     
      is running.
     
    
   </p>
   <p>
    
     In this recipe, we’re going to use the
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     flame graph tool (
    
    <a href="https://github.com/davidmarkclements/0x">
     
      https://github.com/davidmarkclements/0x
     
    </a>
    
     ) to generate a flame graph for our
    
    
     
      Node.js application.
     
    
   </p>
   <h2 id="_idParaDest-321">
    <a id="_idTextAnchor328">
    </a>
    
     Getting ready
    
   </h2>
   <p>
    
     We need to create an application that we
    
    <a id="_idIndexMarker796">
    </a>
    
     can profile.
    
    <strong class="bold">
     
      Profiling
     
    </strong>
    
     is a type of program analysis that measures how frequently and for how long functions or methods in our program are being used.
    
    
     We’ll use the Express.js generator to create a base application.
    
    
     Our application will use the
    
    <strong class="source-inline">
     
      pug
     
    </strong>
    
     
      view engine:
     
    
   </p>
   <pre class="console">
$ npx express-generator --views=pug flamegraph-app
$ cd flamegraph-app
$ npm install</pre>
   <p>
    
     Now that we’ve generated an application, we’re ready to start generating a
    
    
     
      flame graph.
     
    
   </p>
   <h2 id="_idParaDest-322">
    <a id="_idTextAnchor329">
    </a>
    
     How to do it…
    
   </h2>
   <p>
    
     In this recipe, we’ll be using the
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     tool to profile our server and generate a flame graph.
    
    
     We’ll also need to use the
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     tool, which we covered in the
    
    <em class="italic">
     
      Benchmarking HTTP requests
     
    </em>
    
     recipe of this chapter, to generate a load on
    
    
     
      our application:
     
    
   </p>
   <ol>
    <li>
     
      First, we need to ensure that we have both the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      and
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      tools
     
     
      
       installed globally:
      
     
     <pre class="source-code">
<strong class="bold">$ npm install --global autocannon 0x</strong></pre>
    </li>
    <li>
     
      Now, instead of starting our server with the
     
     <strong class="source-inline">
      
       node
      
     </strong>
     
      binary, we need to start it with the
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      executable.
     
     
      If we
     
     <a id="_idIndexMarker797">
     </a>
     
      open the
     
     <strong class="source-inline">
      
       package.json
      
     </strong>
     
      file, we’ll see that the
     
     <strong class="source-inline">
      
       npm start
      
     </strong>
     
      script is
     
     <strong class="source-inline">
      
       node ./bin/www
      
     </strong>
     
      .
     
     
      We need to substitute the
     
     <strong class="source-inline">
      
       node
      
     </strong>
     
      binary in the terminal command
     
     
      
       with
      
     
     
      <strong class="source-inline">
       
        0x
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ 0x ./bin/www</strong>
Profiling</pre>
    </li>
    <li>
     
      Now, we need to generate some load on the server.
     
     
      In a new terminal window, use the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      benchmarking tool to generate a load by running the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 100 http://localhost:3000</strong></pre>
    </li>
    <li>
     
      Expect to see the following output when the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      load test has
     
     
      
       been completed:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.6 – autocannon result summary" src="img/Figure_10.6_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.6 – autocannon result summary
    
   </p>
   <p class="list-inset">
    
     Note that in this load test, our server was handling
    
    <strong class="source-inline">
     
      1512
     
    </strong>
    
     requests per second
    
    
     
      on average.
     
    
   </p>
   <ol>
    <li value="5">
     
      Return to the terminal window where the server was started and press
     
     <em class="italic">
      
       Ctrl
      
     </em>
     
      +
     
     <em class="italic">
      
       C
      
     </em>
     
      .
     
     
      This will stop the
     
     <a id="_idIndexMarker798">
     </a>
     
      server.
     
     
      At this point,
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      will convert the captured stacks into a
     
     
      
       flame graph.
      
     
    </li>
    <li>
     
      Expect to see the following output after pressing
     
     <em class="italic">
      
       Ctrl
      
     </em>
     
      +
     
     <em class="italic">
      
       C
      
     </em>
     
      .
     
     
      This output details the location where
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      has generated the flame graph.
     
     
      Observe that the
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      tool has created a directory named
     
     <strong class="source-inline">
      
       96552.0x
      
     </strong>
     
      , where
     
     <strong class="source-inline">
      
       96552
      
     </strong>
     
      is the
     
     <strong class="bold">
      
       process identifier
      
     </strong>
     
      (
     
     <strong class="bold">
      
       PID
      
     </strong>
     
      ) of
     
     <a id="_idIndexMarker799">
     </a>
     
      the
     
     
      
       server process:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.7 – The 0x tool generating a flame graph" src="img/Figure_10.7_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.7 – The 0x tool generating a flame graph
    
   </p>
   <ol>
    <li value="7">
     
      Open the
     
     <strong class="source-inline">
      
       flamegraph.html
      
     </strong>
     
      file that’s been generated in the
     
     <strong class="source-inline">
      
       flamegraph-app
      
     </strong>
     
      directory with Google Chrome.
     
     
      You can do this by copying the path to the flame graph and pasting it into the Google Chrome address bar.
     
     
      Expect to see the generated flame graph and
     
     
      
       some controls.
      
     
    </li>
    <li>
     
      Observe that the
     
     <a id="_idIndexMarker800">
     </a>
     
      bars in the flame graph are of different shades.
     
     
      A darker (redder) shade indicates a hot
     
     
      
       code path.
      
     
    </li>
   </ol>
   <p class="callout-heading">
    
     Important note
    
   </p>
   <p class="callout">
    
     Each generated flame graph may be slightly different, even when running the same load test.
    
    
     The flame graph that’s generated on your device is likely to look different from the output shown in this recipe.
    
    
     This is due to the non-deterministic nature of the profiling process, which may have subtle impacts on the flame graph’s output.
    
    
     However, generally, the flame graph’s overall results and bottlenecks are
    
    
     
      identified consistently.
     
    
   </p>
   <ol>
    <li value="9">
     
      Identify one of the darker frames.
     
     
      In the example flame graph, we can see that the
     
     <strong class="source-inline">
      
       readFileSync()
      
     </strong>
     
      frame method has a darker shade – indicating that that function has spent a relatively large amount of time on
     
     
      
       the CPU:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.8 – ﻿An overview of the 0x flame graph highlighting readFileSync() as a hot frame" src="img/Figure_10.8_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.8 – An overview of the 0x flame graph highlighting readFileSync() as a hot frame
    
   </p>
   <ol>
    <li value="10">
     
      Click on the
     
     <a id="_idIndexMarker801">
     </a>
     
      darker frame.
     
     
      If it’s difficult to identify the frame, you can enter
     
     <strong class="source-inline">
      
       readFileSync
      
     </strong>
     
      into the
     
     <strong class="bold">
      
       search
      
     </strong>
     
      bar (top right), after which the frame will be highlighted.
     
     
      Upon clicking on the frame,
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      will expand the parent and child stacks of the
     
     
      
       selected frame:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.9 – ﻿An overview of the 0x flame graph showing a drilled-down view of readFileSync()" src="img/Figure_10.9_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.9 – An overview of the 0x flame graph showing a drilled-down view of readFileSync()
    
   </p>
   <p class="list-inset">
    
     From the drilled-down view, we can see the hot code path.
    
    
     From the flame graph, we can make an educated guess about which functions it would be worthwhile to invest
    
    <a id="_idIndexMarker802">
    </a>
    
     time in optimizing.
    
    
     In this case, we can see references to
    
    <strong class="source-inline">
     
      handleTemplateCache()
     
    </strong>
    
     .
    
    
     In the previous recipe,
    
    <em class="italic">
     
      Benchmarking HTTP requests
     
    </em>
    
     , we learned how
    
    <strong class="source-inline">
     
      pug
     
    </strong>
    
     reloads a template for each request when in development mode.
    
    
     This is the cause of this bottleneck.
    
    
     Let’s change the application so that it runs in production mode and see what the impact is on the load test results and
    
    
     
      flame graph.
     
    
   </p>
   <ol>
    <li value="11">
     
      Restart the Express.js server in production mode with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ NODE_ENV=production 0x ./bin/www</strong></pre>
    </li>
    <li>
     
      Rerun the load test using the
     
     
      <strong class="source-inline">
       
        autocannon
       
      </strong>
     
     
      
       tool:
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 100 http://localhost:3000</strong></pre>
    </li>
    <li>
     
      From the results of the load test, we can see that our server is handling more requests per second.
     
     
      In this run, our load test reported that our server handled an average of around
     
     <strong class="source-inline">
      
       7688
      
     </strong>
     
      requests per second, up from around
     
     <strong class="source-inline">
      
       1512
      
     </strong>
     
      before we changed the Express.js server so that it runs in
     
     
      
       production mode:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.10 – autocannon result summary from the production mode run" src="img/Figure_10.10_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.10 – autocannon result summary from the production mode run
    
   </p>
   <ol>
    <li value="14">
     
      As before, once
     
     <a id="_idIndexMarker803">
     </a>
     
      the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      load test is complete, stop your server using
     
     <em class="italic">
      
       Ctrl
      
     </em>
     
      +
     
     <em class="italic">
      
       C
      
     </em>
     
      .
     
     
      A new flame graph will be generated.
     
     
      Open the new flame graph in your browser and observe that the new flame graph is a different shape from the first.
     
     
      Observe that the second flame graph highlights a different set of darker frames.
     
     
      This is because we’ve resolved our first bottleneck.
     
     
      Hot code paths are relative.
     
     
      Despite having increased the performance of our application, the flame graph will identify the next set of hot
     
     
      
       code paths:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.11 – ﻿An overview of the 0x flame graph from production mode" src="img/Figure_10.11_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.11 – An overview of the 0x flame graph from production mode
    
   </p>
   <p>
    
     With that, we’ve used
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     to generate a flame graph, which has enabled us to identify a bottleneck in
    
    
     
      our application.
     
    
   </p>
   <h2 id="_idParaDest-323">
    <a id="_idTextAnchor330">
    </a>
    
     How it works…
    
   </h2>
   <p>
    
     In this recipe, we
    
    <a id="_idIndexMarker804">
    </a>
    
     used the
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     tool to profile and generate a flame graph for our application.
    
    
     Our application was a small, generated Express.js web server.
    
    
     The
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     tool was used to add load to our web server so that we could produce a flame graph that’s representative of a
    
    
     
      production workload.
     
    
   </p>
   <p>
    
     To use the
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     tool, we had to start our server with
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     .
    
    
     When we start an application with
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     , two processes
    
    
     
      are started.
     
    
   </p>
   <p>
    
     The first process uses the Node.js binary,
    
    <strong class="source-inline">
     
      node
     
    </strong>
    
     , to start our program.
    
    
     When
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     starts the node process, it passes the
    
    <strong class="source-inline">
     
      --perf-basic-prof
     
    </strong>
    
     command-line flag to the process.
    
    
     This command-line flag allows C++ V8 function calls to be mapped to the corresponding JavaScript
    
    
     
      function calls.
     
    
   </p>
   <p>
    
     The second process starts the local system’s stack tracing tool.
    
    
     On Linux, the
    
    <strong class="source-inline">
     
      perf
     
    </strong>
    
     tool will be invoked, whereas on macOS and SmartOS, the
    
    <strong class="source-inline">
     
      dtrace
     
    </strong>
    
     tool will be invoked.
    
    
     These tools capture the underlying C-level
    
    
     
      function calls.
     
    
   </p>
   <p>
    
     The underlying system stack tracing tool will take samples.
    
    
     A
    
    <strong class="bold">
     
      sample
     
    </strong>
    
     is a snapshot of all the functions
    
    <a id="_idIndexMarker805">
    </a>
    
     being executed by the CPU at the time the sample was taken, which will also record the parent
    
    
     
      function calls.
     
    
   </p>
   <p>
    
     The sampled stacks are grouped based on the call hierarchy, grouping the parent and child function calls together.
    
    
     These groups are what’s known as a
    
    <strong class="bold">
     
      flame
     
    </strong>
    
     , hence the name
    
    <strong class="bold">
     
      flame graph
     
    </strong>
    
     .
    
    
     The
    
    <a id="_idIndexMarker806">
    </a>
    
     same function
    
    <a id="_idIndexMarker807">
    </a>
    
     may appear in
    
    
     
      multiple flames.
     
    
   </p>
   <p>
    
     Each line in a flame is known as a frame.
    
    
     A
    
    <strong class="bold">
     
      frame
     
    </strong>
    
     represents a function call.
    
    
     The width of the frame corresponds
    
    <a id="_idIndexMarker808">
    </a>
    
     to the amount of time that that function was observed by the profiler on the CPU.
    
    
     The time representation of each frame aggregates the time that all child functions take as well, hence the triangular or
    
    <em class="italic">
     
      flame
     
    </em>
    
     shape of
    
    
     
      the graph.
     
    
   </p>
   <p>
    
     Darker (redder) frames indicate that a function has spent more time at the top of the stack relative to the other functions.
    
    
     This means that this function is spending a lot of time on the CPU, which
    
    <a id="_idIndexMarker809">
    </a>
    
     indicates a
    
    
     
      potential bottleneck.
     
    
   </p>
   <p class="callout-heading">
    
     Important note
    
   </p>
   <p class="callout">
    
     Chrome DevTools can also be used to profile the CPU, which can help identify bottlenecks.
    
    
     Using the
    
    <strong class="source-inline">
     
      --inspect
     
    </strong>
    
     command-line flag, the Node.js process can be debugged and profiled using Chrome DevTools.
    
    
     Please refer to the
    
    <em class="italic">
     
      Debugging with Chrome DevTools
     
    </em>
    
     recipe in
    
    <a href="B19212_12.xhtml#_idTextAnchor388">
     
      <em class="italic">
       
        Chapter 12
       
      </em>
     
    </a>
    
     for more information on using Chrome DevTools to debug a
    
    
     
      Node.js program.
     
    
   </p>
   <h2 id="_idParaDest-324">
    <a id="_idTextAnchor331">
    </a>
    
     See also
    
   </h2>
   <ul>
    <li>
     
      The
     
     <em class="italic">
      
       Creating an Express.js web application
      
     </em>
     
      recipe in
     
     <a href="B19212_06.xhtml#_idTextAnchor178">
      
       <em class="italic">
        
         Chapter 6
        
       </em>
      
     </a>
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Benchmarking HTTP requests
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Detecting memory leaks
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Optimizing synchronous functions
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Optimizing asynchronous functions
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Debugging with Chrome DevTools
      
     </em>
     
      recipe in
     
     <a href="B19212_12.xhtml#_idTextAnchor388">
      
       <em class="italic">
        
         Chapter 12
        
       </em>
      
     </a>
    </li>
   </ul>
   <h1 id="_idParaDest-325">
    <a id="_idTextAnchor332">
    </a>
    
     Detecting memory leaks
    
   </h1>
   <p>
    
     Memory leaks can
    
    <a id="_idIndexMarker810">
    </a>
    
     drastically reduce your application’s performance and can lead to crashes.
    
    
     V8 manages objects and dynamic data in its heap, a binary tree-based structure designed to manage parent-child node relationships.
    
    
     The V8
    
    <strong class="bold">
     
      Garbage Collector
     
    </strong>
    
     (
    
    <strong class="bold">
     
      GC
     
    </strong>
    
     ) is responsible
    
    <a id="_idIndexMarker811">
    </a>
    
     for managing the heap.
    
    
     It reclaims any memory that is no longer in use – freeing the memory so that it can
    
    
     
      be reused.
     
    
   </p>
   <p>
    
     A memory leak occurs when a block of memory is never reclaimed by the GC and is therefore idle and inefficient.
    
    
     This results in pieces of unused memory remaining on the heap.
    
    
     The performance of your application can be impacted when many of these unused memory blocks accumulate in the heap.
    
    
     In the worst cases, the unused memory could consume all the available heap space, which, in turn, can cause your application
    
    
     
      to crash.
     
    
   </p>
   <p>
    
     In this recipe, we’ll learn how to use Chrome DevTools to profile memory, enabling us to detect and fix
    
    
     
      memory leaks.
     
    
   </p>
   <h2 id="_idParaDest-326">
    <a id="_idTextAnchor333">
    </a>
    
     Getting ready
    
   </h2>
   <p>
    
     This recipe will require you to have Chrome DevTools installed, which is integrated into the Google Chrome browser.
    
    
     Visit
    
    <a href="https://www.google.com/chrome/">
     
      https://www.google.com/chrome/
     
    </a>
    
     to download
    
    
     
      Google Chrome:
     
    
   </p>
   <ol>
    <li>
     
      We’ll be using the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      tool to direct load to our application.
     
     
      Install
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      from the
     
     <strong class="source-inline">
      
       npm
      
     </strong>
     
      registry with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ npm install --global autocannon</strong></pre>
    </li>
    <li>
     
      We also need to create a directory to
     
     
      
       work in:
      
     
     <pre class="source-code">
<strong class="bold">$ mkdir profiling-memory</strong>
<strong class="bold">$ cd profiling-memory</strong>
<strong class="bold">$ npm init --yes</strong></pre>
    </li>
    <li>
     
      Create a file named
     
     <strong class="source-inline">
      
       leaky-server.js
      
     </strong>
     
      .
     
     
      This HTTP server will intentionally contain a
     
     
      
       memory leak:
      
     
     <pre class="source-code">
<strong class="bold">$ touch leaky-server.js</strong></pre>
    </li>
    <li>
     
      Add the following
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        leaky-server.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const http = require('node:http');
const server = http.createServer((req, res) =&gt; {
  server.on('connection', () =&gt; { });
  res.end('Hello World!');
});
server.listen(3000, () =&gt; {
  console.log('Server listening on port 3000');
});</pre>
    </li>
   </ol>
   <p>
    
     Now that we’ve installed the
    
    <a id="_idIndexMarker812">
    </a>
    
     necessary tools and created a sample application containing a memory leak, we’re ready to move on to this
    
    
     
      recipe’s steps.
     
    
   </p>
   <h2 id="_idParaDest-327">
    <a id="_idTextAnchor334">
    </a>
    
     How to do it…
    
   </h2>
   <p>
    
     In this recipe, we’ll use Chrome DevTools to identify a
    
    
     
      memory leak:
     
    
   </p>
   <ol>
    <li>
     
      Memory leaks can get progressively worse the longer an application is running.
     
     
      Sometimes, it can take several days or weeks of an application running before the memory leak causes the application to crash.
     
     
      We can use the Node.js process
     
     <strong class="source-inline">
      
       --max-old-space-size
      
     </strong>
     
      command-line flag to increase or reduce the maximum V8 old memory size (in MB).
     
     
      To demonstrate the presence of the memory leak, we’ll set this to a very small value.
     
     
      Start
     
     <strong class="source-inline">
      
       leaky-server.js
      
     </strong>
     
      with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ node --max-old-space-size=10 leaky-server.js</strong>
Server listening on port 3000</pre>
    </li>
    <li>
     
      In a second terminal window, use the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      tool to direct load to
     
     
      
       the server:
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon http://localhost:3000</strong></pre>
    </li>
    <li>
     
      Back in the terminal window where you started the server, observe that the server crashed
     
     <a id="_idIndexMarker813">
     </a>
     
      with
     
     <strong class="source-inline">
      
       JavaScript heap out
      
     </strong>
     
      <strong class="source-inline">
       
        of memory
       
      </strong>
     
     
      
       :
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.12 – JavaScript heap out of memory error" src="img/Figure_10.12_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.12 – JavaScript heap out of memory error
    
   </p>
   <ol>
    <li value="4">
     
      Now, we’ll start using Chrome DevTools to profile our application.
     
     
      First, we must restart the server with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ node --inspect leaky-server.js</strong></pre>
    </li>
    <li>
     
      Navigate to
     
     <strong class="source-inline">
      
       chrome://inspect
      
     </strong>
     
      in Google Chrome and click
     
     <strong class="bold">
      
       inspect
      
     </strong>
     
      (underneath
     
     <strong class="source-inline">
      
       leaky-server.js
      
     </strong>
     
      ).
     
     
      This should open the Chrome
     
     
      
       DevTools interface.
      
     
    </li>
    <li>
     
      Ensure you’re on
     
     <a id="_idIndexMarker814">
     </a>
     
      the
     
     <strong class="bold">
      
       Memory
      
     </strong>
     
      tab and that
     
     <strong class="bold">
      
       Heap snapshot
      
     </strong>
     
      is selected.
     
     
      Click
     
     
      <strong class="bold">
       
        Take snapshot
       
      </strong>
     
     
      
       :
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.13 – The Chrome DevTools Memory interface" src="img/Figure_10.13_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.13 – The Chrome DevTools Memory interface
    
   </p>
   <p class="list-inset">
    
     You should see
    
    <strong class="bold">
     
      Snapshot 1
     
    </strong>
    
     appear on the left of
    
    
     
      the interface:
     
    
   </p>
   <div><div><img alt="Figure 10.14 – Chrome DevTools memory snapshot interface" src="img/Figure_10.14_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.14 – Chrome DevTools memory snapshot interface
    
   </p>
   <ol>
    <li value="7">
     
      Return to your
     
     <a id="_idIndexMarker815">
     </a>
     
      second terminal window and rerun the
     
     
      <strong class="source-inline">
       
        autocannon
       
      </strong>
     
     
      
       benchmark:
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon http://localhost:3000</strong></pre>
    </li>
    <li>
     
      Once the load test has been completed, return to your Chrome DevTools window.
     
     
      Return to the
     
     <strong class="bold">
      
       Profiles
      
     </strong>
     
      interface of the
     
     <strong class="bold">
      
       Memory
      
     </strong>
     
      tab and take
     
     
      
       another snapshot:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.15 – Chrome DevTools memory snapshot interface" src="img/Figure_10.15_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.15 – Chrome DevTools memory snapshot interface
    
   </p>
   <p class="list-inset">
    
     Note
    
    <strong class="source-inline">
     
      MaxListenersExceededWarning
     
    </strong>
    
     in the
    
    <strong class="bold">
     
      Console
     
    </strong>
    
     tab – this will be covered in
    
    <a id="_idIndexMarker816">
    </a>
    
     more detail in the
    
    <em class="italic">
     
      There’s
     
    </em>
    
     <em class="italic">
      
       more…
      
     </em>
    
    
     
      section.
     
    
   </p>
   <ol>
    <li value="9">
     
      Now that we have two snapshots, we can use Chrome DevTools to compare them.
     
     
      To do this, change the drop-down window from
     
     <strong class="bold">
      
       Summary
      
     </strong>
     
      
       to
      
     
     
      <strong class="bold">
       
        Comparison
       
      </strong>
     
     
      
       :
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.16 – Chrome DevTools memory snapshot comparison interface" src="img/Figure_10.16_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.16 – Chrome DevTools memory snapshot comparison interface
    
   </p>
   <ol>
    <li value="10">
     
      Observe that the
     
     <a id="_idIndexMarker817">
     </a>
     
      constructors are now sorted by delta – the difference between two snapshots.
     
     
      Expand the
     
     <strong class="source-inline">
      
       (array)
      
     </strong>
     
      constructor and the
     
     <strong class="source-inline">
      
       (object elements) [ ]
      
     </strong>
     
      object within it; you should see the
     
     
      
       following output:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.17 – Chrome DevTools memory snapshot comparison interface expanded" src="img/Figure_10.17_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.17 – Chrome DevTools memory snapshot comparison interface expanded
    
   </p>
   <ol>
    <li value="11">
     
      The expanded view
     
     <a id="_idIndexMarker818">
     </a>
     
      indicates that there are masses of
     
     <strong class="source-inline">
      
       connectionListener()
      
     </strong>
     
      events stemming from
     
     <em class="italic">
      
       line 4
      
     </em>
     
      of
     
     <strong class="source-inline">
      
       leaky-server.js
      
     </strong>
     
      .
     
     
      If we take a look at that line, we’ll see that it starts on the
     
     <strong class="source-inline">
      
       server.on('connection',...
      
     </strong>
     
      block.
     
     
      This is our memory leak.
     
     
      We’re registering a listener for the connected event upon every request, causing our server to eventually run out of memory.
     
     
      We need to move this event listener outside of our request handler function.
     
     
      Create a new file
     
     
      
       named
      
     
     
      <strong class="source-inline">
       
        server.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ touch server.js</strong></pre>
    </li>
    <li>
     
      Add the following
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        server.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const http = require('node:http');
const server = http.createServer((req, res) =&gt; {
  res.end('Hello World!');
});
server.on('connection', () =&gt; {});
server.listen(3000, () =&gt; {
  console.log('Server listening on port 3000');
});</pre>
    </li>
    <li>
     
      Close the Chrome
     
     <a id="_idIndexMarker819">
     </a>
     
      DevTools window and then rerun the same experiment.
     
     
      Start the server with
     
     <strong class="source-inline">
      
       $ node --inspect server.js
      
     </strong>
     
      and take a snapshot.
     
     
      In a second terminal window, direct load to the server with
     
     <strong class="source-inline">
      
       $ autocannon http://localhost:3000
      
     </strong>
     
      and take another snapshot.
     
     
      Now, when we compare the two, we’ll see that the
     
     <strong class="source-inline">
      
       # Delta
      
     </strong>
     
      value of the
     
     <strong class="source-inline">
      
       (array)
      
     </strong>
     
      constructors has
     
     
      
       significantly reduced:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.18 – Chrome DevTools memory snapshot comparison interface" src="img/Figure_10.18_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.18 – Chrome DevTools memory snapshot comparison interface
    
   </p>
   <p>
    
     Observe that the
    
    <strong class="source-inline">
     
      MaxListenersExceededWarning
     
    </strong>
    
     warning is no longer appearing, indicating
    
    <a id="_idIndexMarker820">
    </a>
    
     that we’ve fixed our
    
    
     
      memory leak.
     
    
   </p>
   <p>
    
     With that, we’ve learned how to take heap snapshots of our application, enabling us to diagnose a memory leak in
    
    
     
      our application.
     
    
   </p>
   <h2 id="_idParaDest-328">
    <a id="_idTextAnchor335">
    </a>
    
     How it works…
    
   </h2>
   <p>
    
     The V8 JavaScript engine is used by both Google Chrome and Node.js.
    
    
     The common underlying engine means that we can use Chrome DevTools to debug and profile Node.js applications.
    
    
     To enable the debugging client, we must pass the
    
    <strong class="source-inline">
     
      --inspect
     
    </strong>
    
     command-line flag to the
    
    <strong class="source-inline">
     
      node
     
    </strong>
    
     process.
    
    
     Passing this flag instructs the V8 inspector to open a port that accepts WebSocket connections.
    
    
     The WebSocket connection allows the client and V8 inspector
    
    
     
      to interact.
     
    
   </p>
   <p>
    
     The V8 JavaScript engine retains a heap of all the objects and primitives referenced in our JavaScript code.
    
    
     The JavaScript heap can be exposed via an internal V8 API (
    
    <strong class="source-inline">
     
      v8_inspector
     
    </strong>
    
     ).
    
    
     Chrome DevTools uses this internal API to provide tooling interfaces, including the
    
    <strong class="bold">
     
      Memory Profiler
     
    </strong>
    
     interface we used in
    
    
     
      this recipe.
     
    
   </p>
   <p>
    
     We used the
    
    <strong class="bold">
     
      Memory
     
    </strong>
    
     interface of Chrome DevTools to take an initial heap snapshot of the server.
    
    
     This snapshot is considered our baseline.
    
    
     Then, we generated load on the server using the
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     tool to simulate usage over time.
    
    
     For our server, the memory leak could be observed with the default
    
    <strong class="source-inline">
     
      autocannon
     
    </strong>
    
     load (
    
    <strong class="source-inline">
     
      10
     
    </strong>
    
     connections for
    
    <strong class="source-inline">
     
      10
     
    </strong>
    
     seconds).
    
    
     Some memory leaks may only be observable under considerable load; in these cases, we’d
    
    <a id="_idIndexMarker821">
    </a>
    
     need to simulate a more extreme load on the server, potentially for a
    
    
     
      longer period.
     
    
   </p>
   <p class="callout-heading">
    
     autocannon
    
   </p>
   <p class="callout">
    
     The
    
    <em class="italic">
     
      Benchmarking HTTP requests
     
    </em>
    
     recipe in this chapter goes into more detail about how we can simulate more extreme server loads with the
    
    
     <strong class="source-inline">
      
       autocannon
      
     </strong>
    
    
     
      tool.
     
    
   </p>
   <p>
    
     Once we directed the load to our server, we took a second heap snapshot.
    
    
     This showed how much impact the load had on the heap size.
    
    
     Our second snapshot was much larger than the first, which is an indication of a memory leak.
    
    
     The heap snapshot
    
    <strong class="bold">
     
      Comparison
     
    </strong>
    
     view can be utilized to identify which constructors have the
    
    
     
      largest deltas.
     
    
   </p>
   <p>
    
     From inspecting and expanding the
    
    <strong class="source-inline">
     
      (array)
     
    </strong>
    
     constructor, we found a long list
    
    
     
      of
     
    
    
     <strong class="source-inline">
      
       connection
      
     </strong>
    
    <strong class="source-inline">
     
      Listener()
     
    </strong>
    
     events stemming from
    
    <em class="italic">
     
      line 4
     
    </em>
    
     of our
    
    <strong class="source-inline">
     
      leaky-server.js
     
    </strong>
    
     file.
    
    
     This enabled us to identify the memory leak.
    
    
     Note that the
    
    <strong class="source-inline">
     
      (array)
     
    </strong>
    
     constructor refers to an internal structure used by V8.
    
    
     For a JavaScript array, the constructor would be
    
    
     
      named
     
    
    
     <strong class="source-inline">
      
       Array
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     Once the memory leak has been identified and fixed, it’s prudent to rerun the test and confirm that the new heap snapshot shows a reduction in deltas.
    
    
     The snapshot is still likely to be larger than the initial baseline snapshot because of the load.
    
    
     However, it shouldn’t be as drastically large as it was with our
    
    
     <strong class="source-inline">
      
       leaky-server.js
      
     </strong>
    
    
     
      file.
     
    
   </p>
   <h2 id="_idParaDest-329">
    <a id="_idTextAnchor336">
    </a>
    
     There’s more…
    
   </h2>
   <p>
    
     In this recipe, when under
    
    <a id="_idIndexMarker822">
    </a>
    
     load,
    
    <strong class="source-inline">
     
      leaky-server.js
     
    </strong>
    
     emitted
    
    <strong class="source-inline">
     
      MaxListenersExceededWarning
     
    </strong>
    
     
      before crashing:
     
    
   </p>
   <pre class="console">
$ node --max-old-space-size=10 leaky-server.js
Server listening on port 3000
(node:16402) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 connection listeners added to [Server]. Use emitter.setMaxListeners() to increase limit</pre>
   <p>
    
     By default, Node.js allows a maximum of
    
    <strong class="source-inline">
     
      10
     
    </strong>
    
     listeners to be registered for a single event.
    
    
     In
    
    <strong class="source-inline">
     
      leaky-server.js
     
    </strong>
    
     , we were registering a new listener for each request.
    
    
     Once our application registered
    
    <a id="_idIndexMarker823">
    </a>
    
     the 11th request, it emitted
    
    <strong class="source-inline">
     
      MaxListenersExceededWarning
     
    </strong>
    
     .
    
    
     This is an early warning sign of a memory leak.
    
    
     It’s possible to change the maximum number of listeners.
    
    
     To change the threshold for an individual
    
    <strong class="source-inline">
     
      EventEmitter
     
    </strong>
    
     instance, we can use the
    
    <strong class="source-inline">
     
      emitter.setMaxListeners()
     
    </strong>
    
     method.
    
    
     For example, to lower the maximum number of listeners on our server to
    
    <strong class="source-inline">
     
      1
     
    </strong>
    
     , we could change
    
    <strong class="source-inline">
     
      leaky-server.js
     
    </strong>
    
     to
    
    
     
      the following:
     
    
   </p>
   <pre class="source-code">
const http = require('node:http');
const server = http.createServer((req, res) =&gt; {
  <strong class="bold">server.setMaxListeners(1);</strong>
  server.on('connection', () =&gt; { });
  res.end('Hello World!');
});
server.listen(3000, () =&gt; {
  console.log('Server listening on port 3000');
});</pre>
   <p>
    
     Then, if we were to run the same experiment, we’d see the following error after just two event listeners
    
    
     
      were registered:
     
    
   </p>
   <pre class="console">
(node:16629) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 2 connection listeners added to [Server]. Use emitter.setMaxListeners() to increase limit</pre>
   <p>
    
     It’s also possible to use the
    
    <strong class="source-inline">
     
      EventEmitter.defaultMaxListeners
     
    </strong>
    
     property to change the default maximum listeners for all
    
    <strong class="source-inline">
     
      EventEmitter
     
    </strong>
    
     instances.
    
    
     This should be done with caution as it will impact all
    
    <strong class="source-inline">
     
      EventEmitter
     
    </strong>
    
     instances.
    
    
     You could use the following to set the
    
    
     <strong class="source-inline">
      
       EventEmitter.defaultMaxListeners
      
     </strong>
    
    
     
      value:
     
    
   </p>
   <pre class="source-code">
require('events').EventEmitter.defaultMaxListeners = 15;</pre>
   <p>
    
     Note that
    
    <strong class="source-inline">
     
      emitter.setMaxListeners()
     
    </strong>
    
     will always take precedence over the global default set via
    
    <strong class="source-inline">
     
      EventEmitter.defaultMaxListeners
     
    </strong>
    
     .
    
    
     Before raising the maximum threshold
    
    <a id="_idIndexMarker824">
    </a>
    
     of listeners, it’s worth considering whether you’re inadvertently masking a memory leak in
    
    
     
      your application.
     
    
   </p>
   <h2 id="_idParaDest-330">
    <a id="_idTextAnchor337">
    </a>
    
     See also
    
   </h2>
   <ul>
    <li>
     
      The
     
     <em class="italic">
      
       Interpreting flame graphs
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Optimizing synchronous functions
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Optimizing asynchronous functions
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Debugging with Chrome DevTools
      
     </em>
     
      recipe in
     
     <a href="B19212_12.xhtml#_idTextAnchor388">
      
       <em class="italic">
        
         Chapter 12
        
       </em>
      
     </a>
    </li>
   </ul>
   <h1 id="_idParaDest-331">
    <a id="_idTextAnchor338">
    </a>
    
     Optimizing synchronous functions
    
   </h1>
   <p>
    
     The previous recipes
    
    <a id="_idIndexMarker825">
    </a>
    
     of this chapter covered how to detect hot code paths in our applications.
    
    
     Once a hot code path is identified, we can focus our optimization efforts on it to reduce
    
    
     
      the bottleneck.
     
    
   </p>
   <p>
    
     It’s important to optimize any hot code paths as any function that takes a long time to process can prevent I/O and other functions from executing, impacting the overall performance of
    
    
     
      your application.
     
    
   </p>
   <p>
    
     This recipe will cover how to micro-benchmark and optimize a synchronous function.
    
    
     A
    
    <strong class="bold">
     
      micro-benchmark
     
    </strong>
    
     is a type
    
    <a id="_idIndexMarker826">
    </a>
    
     of performance test that focuses on a small, specific piece of code or functionality within a larger system.
    
    
     We’ll use Benchmark.js (
    
    <a href="https://benchmarkjs.com/">
     
      https://benchmarkjs.com/
     
    </a>
    
     ) to create
    
    
     
      a micro-benchmark.
     
    
   </p>
   <h2 id="_idParaDest-332">
    <a id="_idTextAnchor339">
    </a>
    
     Getting ready
    
   </h2>
   <p>
    
     In real applications, we’d use tooling such as flame graphs or profilers to identify slow functions in our applications.
    
    
     For this recipe, we’ll create a single slow function that we can learn how to micro-benchmark
    
    
     
      and optimize:
     
    
   </p>
   <ol>
    <li>
     
      First, create a directory for this recipe’s code and initialize
     
     
      
       the project:
      
     
     <pre class="source-code">
<strong class="bold">$ mkdir optimize-sync</strong>
<strong class="bold">$ cd optimize-sync</strong>
<strong class="bold">$ npm init --yes</strong></pre>
    </li>
    <li>
     
      We also need to
     
     
      
       install Benchmark.js:
      
     
     <pre class="source-code">
<strong class="bold">$ npm install benchmark</strong></pre>
    </li>
   </ol>
   <p>
    
     Now that we’ve initialized our directory, we can start
    
    
     
      this recipe.
     
    
   </p>
   <h2 id="_idParaDest-333">
    <a id="_idTextAnchor340">
    </a>
    
     How to do it…
    
   </h2>
   <p>
    
     Let’s assume
    
    <a id="_idIndexMarker827">
    </a>
    
     that we’ve identified a bottleneck in our code base and it happens to be a function called
    
    <strong class="source-inline">
     
      sumOfSquares()
     
    </strong>
    
     .
    
    
     Our task is to make this
    
    
     
      function faster:
     
    
   </p>
   <ol>
    <li>
     
      First, let’s create a file named
     
     <strong class="source-inline">
      
       slow.js
      
     </strong>
     
      , which will hold our
     
     
      
       unoptimized function:
      
     
     <pre class="source-code">
<strong class="bold">$ touch slow.js</strong></pre>
    </li>
    <li>
     
      Add the following to
     
     <strong class="source-inline">
      
       slow.js
      
     </strong>
     
      to create the slow
     
     <strong class="source-inline">
      
       sumOfSquares()
      
     </strong>
     
      implementation.
     
     
      This uses the
     
     <strong class="source-inline">
      
       Array.from()
      
     </strong>
     
      method to generate an array of integers.
     
     
      The
     
     <strong class="source-inline">
      
       map
      
     </strong>
     
      function is used to square each number in the array, while the
     
     <strong class="source-inline">
      
       reduce
      
     </strong>
     
      function is used to sum the elements of
     
     
      
       the array:
      
     
     <pre class="source-code">
function sumOfSquares(maxNumber) {
  const array = Array.from(Array(maxNumber + 1).keys());
  return array
    .map((number) =&gt; {
      return number ** 2;
    })
    .reduce((accumulator, item) =&gt; {
      return accumulator + item;
    });
}</pre>
    </li>
    <li>
     
      Now that we have a slow version of our function, let’s turn it into a module so that we can benchmark it with ease.
     
     
      If our function formed part of a larger script or application, it would be worthwhile trying to extract it into a standalone script or module to enable it to be benchmarked in isolation.
     
     
      Add the following line to the bottom
     
     
      
       of
      
     
     
      <strong class="source-inline">
       
        slow.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
module.exports = sumOfSquares;</pre>
    </li>
    <li>
     
      Now, we can write a micro-benchmark for our
     
     <strong class="source-inline">
      
       sumOfSquares()
      
     </strong>
     
      function using Benchmark.js.
     
     
      Create a file
     
     
      
       named
      
     
     
      <strong class="source-inline">
       
        benchmark.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ touch benchmark.js</strong></pre>
    </li>
    <li>
     
      Add the following code to
     
     <strong class="source-inline">
      
       benchmark.js
      
     </strong>
     
      to create a benchmark for our
     
     
      <strong class="source-inline">
       
        sumOfSquares()
       
      </strong>
     
     
      
       function:
      
     
     <pre class="source-code">
const benchmark = require('benchmark');
const slow = require('./slow');
const suite = new benchmark.Suite();
const maxNumber = 100;
suite.add('slow', function () {
  slow(maxNumber);
});
suite.on('complete', printResults);
suite.run();
function printResults () {
  this.forEach((benchmark) =&gt; {
    console.log(benchmark.toString());
  });
  console.log('Fastest implementation is', this.filter('fastest')[0].name);
}</pre>
     <p class="list-inset">
      
       This file contains
      
      <a id="_idIndexMarker828">
      </a>
      
       the configuration of Benchmark.js, a single benchmark that calls our
      
      <strong class="source-inline">
       
        slow.js
       
      </strong>
      
       module, and a
      
      <strong class="source-inline">
       
        printResults()
       
      </strong>
      
       function, which outputs the benchmark
      
      
       
        run information.
       
      
     </p>
    </li>
    <li>
     
      Now, we can run the benchmark with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ node benchmark.js</strong>
slow x 231,893 ops/sec ±0.90% (90 runs sampled)
Fastest implementation is slow</pre>
    </li>
    <li>
     
      Let’s generate a flame graph using the
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      tool.
     
     
      A flame graph can help us identify which of the lines of our code are spending the most time on the CPU.
     
     
      Generate a flame graph with
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      by using the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ npx 0x benchmark.js</strong></pre>
    </li>
    <li>
     
      Open the flame graph in your browser.
     
     
      In the following example, there’s one pink frame, indicating a hot code path.
     
     
      Hover over the hotter frames to identify which line of the application they’re
     
     
      
       referring to:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.19 – ﻿An overview of the 0x flame graph showing a hot frame on line 9 of slow.js" src="img/Figure_10.19_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.19 – An overview of the 0x flame graph showing a hot frame on line 9 of slow.js
    
   </p>
   <ol>
    <li value="9">
     
      In the flame graph, we can see that the hottest function is an anonymous function
     
     <a id="_idIndexMarker829">
     </a>
     
      on
     
     <em class="italic">
      
       line 9
      
     </em>
     
      of
     
     <strong class="source-inline">
      
       slow.js
      
     </strong>
     
      .
     
     
      If we look at our code, we’ll see that this points to our use of
     
     <strong class="source-inline">
      
       Array.reduce()
      
     </strong>
     
      .
     
     
      Note that the line number may be different should you have formatted this recipe’s
     
     
      
       code differently.
      
     
    </li>
    <li>
     
      As we suspect that it’s the use of
     
     <strong class="source-inline">
      
       Array.reduce()
      
     </strong>
     
      that’s slowing our operations down, we should try rewriting the function in a procedural form (using a
     
     <strong class="source-inline">
      
       for
      
     </strong>
     
      loop) to see whether it improves the performance.
     
     
      Create a file
     
     
      
       named
      
     
     
      <strong class="source-inline">
       
        loop.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ touch loop.js</strong></pre>
    </li>
    <li>
     
      Add the following to
     
     <strong class="source-inline">
      
       loop.js
      
     </strong>
     
      to create a procedural implementation of the
     
     
      <strong class="source-inline">
       
        sumOfSquares()
       
      </strong>
     
     
      
       function:
      
     
     <pre class="source-code">
function sumOfSquares(maxNumber) {
    let i = 0;
    let sum = 0;
    for (i; i &lt;= maxNumber; i++) {
        sum += i ** 2;
    }
    return sum;
}
module.exports = sumOfSquares;</pre>
    </li>
    <li>
     
      Now, let’s add a
     
     <a id="_idIndexMarker830">
     </a>
     
      benchmark for the implementation of the
     
     <strong class="source-inline">
      
       sumOfSquares()
      
     </strong>
     
      function in
     
     <strong class="source-inline">
      
       loop.js
      
     </strong>
     
      .
     
     
      First, import the
     
     <strong class="source-inline">
      
       loop.js
      
     </strong>
     
      module by adding the following line below the
     
     <strong class="source-inline">
      
       slow.js
      
     </strong>
     
      import
     
     
      
       in
      
     
     
      <strong class="source-inline">
       
        benchmark.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const loop = require('./loop');</pre>
    </li>
    <li>
     
      Then, add a new benchmark to the suite, below the
     
     
      
       slow run:
      
     
     <pre class="source-code">
suite.add('loop', function () {
  loop(maxNumber);
});</pre>
    </li>
    <li>
     
      Rerun the benchmark.
     
     
      This time, it will run both of our implementations and determine which one
     
     
      
       is fastest:
      
     
     <pre class="source-code">
<strong class="bold">$ node benchmark.js</strong>
slow x 247,958 ops/sec ±1.17% (90 runs sampled)
loop x 7,337,014 ops/sec ±0.86% (94 runs sampled)
Fastest implementation is loop</pre>
    </li>
   </ol>
   <p>
    
     With that, we’ve confirmed that our procedural/loop implementation of the
    
    <strong class="source-inline">
     
      sumOfSquares()
     
    </strong>
    
     function is much faster than the
    
    
     
      original implementation.
     
    
   </p>
   <h2 id="_idParaDest-334">
    <a id="_idTextAnchor341">
    </a>
    
     How it works…
    
   </h2>
   <p>
    
     This recipe stepped through the process of optimizing a synchronous function call, starting with the slow implementation of a
    
    
     <strong class="source-inline">
      
       sumOfSquares()
      
     </strong>
    
    
     
      function.
     
    
   </p>
   <p>
    
     We created a micro-benchmark using Benchmark.js to create a baseline measure of our initial
    
    <strong class="source-inline">
     
      sumOfSquares()
     
    </strong>
    
     implementation in
    
    <strong class="source-inline">
     
      slow.js
     
    </strong>
    
     .
    
    
     This baseline measure is called a micro-benchmark.
    
    <strong class="bold">
     
      Micro-benchmarks
     
    </strong>
    
     are used to
    
    <a id="_idIndexMarker831">
    </a>
    
     benchmark a small facet of an application.
    
    
     In our case, it was for the single
    
    
     <strong class="source-inline">
      
       sumOfSquares()
      
     </strong>
    
    
     
      function.
     
    
   </p>
   <p>
    
     Once our micro-benchmark was created, we ran the benchmark via
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     to generate a flame graph.
    
    
     This flame graph enabled us to identify which frames were spending the most time on the CPU, which provided us with an indication of which specific line of code within our
    
    <strong class="source-inline">
     
      sumOfSquares()
     
    </strong>
    
     function was
    
    
     
      the bottleneck.
     
    
   </p>
   <p>
    
     From the flame graph, we determined that the use of the
    
    <strong class="source-inline">
     
      map
     
    </strong>
    
     and
    
    <strong class="source-inline">
     
      reduce
     
    </strong>
    
     functions of
    
    <strong class="source-inline">
     
      sumOfSquares()
     
    </strong>
    
     was slowing the operation down.
    
    
     Therefore, we created a second implementation of
    
    <strong class="source-inline">
     
      sumOfSquares()
     
    </strong>
    
     .
    
    
     The second implementation used traditional procedural code (a
    
    <strong class="source-inline">
     
      for
     
    </strong>
    
     loop).
    
    
     Once we had the second implementation of the function, in
    
    <strong class="source-inline">
     
      loop.js
     
    </strong>
    
     , we added it to our benchmarks.
    
    
     This allowed us to compare the two implementations to see which
    
    
     
      was faster.
     
    
   </p>
   <p>
    
     Based on the number of operations that could be handled per second,
    
    <strong class="source-inline">
     
      loop.js
     
    </strong>
    
     was found to be
    
    <a id="_idIndexMarker832">
    </a>
    
     significantly faster than the initial
    
    <strong class="source-inline">
     
      slow.js
     
    </strong>
    
     implementation.
    
    
     The benefit of writing a micro-benchmark is that you have evidence and confirmation of
    
    
     
      your optimizations.
     
    
   </p>
   <h2 id="_idParaDest-335">
    <a id="_idTextAnchor342">
    </a>
    
     See also
    
   </h2>
   <ul>
    <li>
     
      The
     
     <em class="italic">
      
       Benchmarking HTTP requests
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Interpreting flame graphs
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Detecting memory leaks
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Optimizing asynchronous functions
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Working with worker threads
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
   </ul>
   <h1 id="_idParaDest-336">
    <a id="_idTextAnchor343">
    </a>
    
     Optimizing asynchronous functions
    
   </h1>
   <p>
    
     The Node.js runtime
    
    <a id="_idIndexMarker833">
    </a>
    
     was built with I/O in mind, hence its asynchronous programming model.
    
    
     In the previous recipes of this chapter, we explored how to diagnose performance issues within synchronous
    
    
     
      JavaScript functions.
     
    
   </p>
   <p>
    
     However, a performance bottleneck may occur as part of an asynchronous workflow.
    
    
     In this recipe, we’ll cover profiling and optimizing an asynchronous
    
    
     
      performance problem.
     
    
   </p>
   <h2 id="_idParaDest-337">
    <a id="_idTextAnchor344">
    </a>
    
     Getting ready
    
   </h2>
   <p>
    
     In this recipe, we’ll diagnose a bottleneck in an Express.js web server that communicates with a MongoDB database.
    
    
     For more information on MongoDB, please refer to the
    
    <em class="italic">
     
      Storing and retrieving data with MongoDB
     
    </em>
    
     recipe in
    
    <a href="B19212_05.xhtml#_idTextAnchor139">
     
      <em class="italic">
       
        Chapter 5
       
      </em>
     
    </a>
    
     
      :
     
    
   </p>
   <ol>
    <li>
     
      To start MongoDB, we’ll use Docker (as we did in
     
     <a href="B19212_05.xhtml#_idTextAnchor139">
      
       <em class="italic">
        
         Chapter 5
        
       </em>
      
     </a>
     
      ).
     
     
      Ensuring that you have Docker running, enter the following command in your terminal to initialize a
     
     
      
       MongoDB database:
      
     
     <pre class="source-code">
<strong class="bold">$ docker run --publish 27017:27017 --name node-mongo --detach mongo:7</strong></pre>
    </li>
    <li>
     
      Now, we need to create a directory to work in.
     
     
      We’ll also install the
     
     <strong class="source-inline">
      
       express
      
     </strong>
     
      and
     
     <strong class="source-inline">
      
       mongodb
      
     </strong>
     
      modules
     
     
      
       from
      
     
     
      <strong class="source-inline">
       
        npm
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ mkdir optimize-async</strong>
<strong class="bold">$ cd optimize-async</strong>
<strong class="bold">$ npm init --yes</strong>
<strong class="bold">$ npm install express mongodb</strong></pre>
    </li>
    <li>
     
      To simulate a real application, some data needs to be present in MongoDB.
     
     
      Create a file
     
     
      
       named
      
     
     
      <strong class="source-inline">
       
        values.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ touch values.js</strong></pre>
    </li>
    <li>
     
      Add the following to
     
     <strong class="source-inline">
      
       values.js
      
     </strong>
     
      .
     
     
      This creates a load script that will enter a series of numbers into
     
     <a id="_idIndexMarker834">
     </a>
     
      our
     
     
      
       MongoDB database:
      
     
     <pre class="source-code">
const { MongoClient } = require('mongodb');
const URL = 'mongodb://localhost:27017/';
const numberOfValues = 1000;
const values = [];
for (let count = 0; count &lt; numberOfValues; count++) {
  values.push({ value: Math.round(Math.random() * 100000) });
}
async function main () {
  const client = new MongoClient(URL);
  try {
    await client.connect();
    const db = client.db('data');
    await db.collection('values').insertMany(values);
    console.log(`Added ${numberOfValues} random values.`);
  } catch (err) {
    console.error(err);
  } finally {
    await client.close();
  }
}
main().catch(console.error);</pre>
    </li>
    <li>
     
      Run the
     
     <strong class="source-inline">
      
       values.js
      
     </strong>
     
      script to populate the database for
     
     
      
       this recipe:
      
     
     <pre class="source-code">
<strong class="bold">$ node values.js</strong></pre>
    </li>
    <li>
     
      Make sure the
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      and
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      performance tools are
     
     
      
       installed globally:
      
     
     <pre class="source-code">
<strong class="bold">$ npm install --global 0x autocannon</strong></pre>
    </li>
   </ol>
   <p>
    
     Now that our directory has been initialized and a MongoDB database is available with some sample data, let’s start
    
    
     
      this recipe.
     
    
   </p>
   <h2 id="_idParaDest-338">
    <a id="_idTextAnchor345">
    </a>
    
     How to do it…
    
   </h2>
   <p>
    
     In this recipe, we’re going to diagnose a bottleneck in a web application that communicates with a MongoDB database.
    
    
     We’ll build a sample application that calculates the average
    
    <a id="_idIndexMarker835">
    </a>
    
     of all the values stored in
    
    
     
      the database:
     
    
   </p>
   <ol>
    <li>
     
      Create a file named
     
     <strong class="source-inline">
      
       server.js
      
     </strong>
     
      .
     
     
      This will store our server that calculates the average of the values in
     
     
      
       the database:
      
     
     <pre class="source-code">
<strong class="bold">$ touch server.js</strong></pre>
    </li>
    <li>
     
      A
     
     <a id="_idTextAnchor346">
     </a>
     
      dd the following code
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        server.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const { MongoClient } = require('mongodb');
const express = require('express');
const URL = 'mongodb://localhost:27017/';
const app = express();
(async () =&gt; {
  try {
    const client = new MongoClient(URL);
    await client.connect();
    const db = client.db('data');
    const values = db.collection('values');
    app.get('/', async (req, res) =&gt; {
      try {
        const data = await values.find({}).toArray();
        const average =
          data.reduce((accumulator, value) =&gt; accumulator + value.value, 0) /
          data.length;
        res.send(`Average of all values is ${average}.`);
      } catch (err) {
        res.send(err);
      }
    });
    app.listen(3000, () =&gt; {
      console.log('Server is running on port 3000');
    });
  } catch (err) {
    console.error(err);
  }
})();</pre>
    </li>
    <li>
     
      Start the server by entering the following command in
     
     
      
       your terminal:
      
     
     <pre class="source-code">
<strong class="bold">$ node server.js</strong>
Server is running on port 3000</pre>
    </li>
    <li>
     
      Navigate to
     
     <strong class="source-inline">
      
       http://localhost:3000
      
     </strong>
     
      in your browser to check that the server is running.
     
     
      Expect to see a message printing the average of the random values we persisted to the database in the
     
     <em class="italic">
      
       Getting
      
     </em>
     
      <em class="italic">
       
        ready
       
      </em>
     
     
      
       section.
      
     
    </li>
    <li>
     
      In a second terminal, we’ll use the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      benchmarking tool to simulate a load on
     
     
      
       the server:
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 500 http://localhost:3000</strong></pre>
     <p class="list-inset">
      
       Expect to see the
      
      <a id="_idIndexMarker836">
      </a>
      
       following
      
      <strong class="source-inline">
       
        autocannon
       
      </strong>
      
       result summary once the load test has
      
      
       
        been completed:
       
      
     </p>
    </li>
   </ol>
   <div><div><img alt="Figure 10.20 – autocannon result summary for server.js" src="img/Figure_10.20_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.20 – autocannon result summary for server.js
    
   </p>
   <p class="list-inset">
    
     This load test shows an average of around
    
    <strong class="source-inline">
     
      317
     
    </strong>
    
     requests
    
    
     
      per second.
     
    
   </p>
   <ol>
    <li value="6">
     
      Now, let’s see where the bottlenecks are in our application.
     
     
      We’ll use the
     
     <strong class="source-inline">
      
       0x
      
     </strong>
     
      tool to generate a flame graph.
     
     
      Restart the server with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ 0x server.js</strong></pre>
    </li>
    <li>
     
      In the second terminal, let’s simulate a load on the server again using the
     
     
      <strong class="source-inline">
       
        autocannon
       
      </strong>
     
     
      
       tool:
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 500 http://localhost:3000</strong></pre>
    </li>
    <li>
     
      Stop the server and open the generated flame graph in your browser.
     
     
      Expect a flame graph similar
     
     <a id="_idIndexMarker837">
     </a>
     
      to
     
     
      
       the following:
      
     
    </li>
   </ol>
   <div><div><img alt="Figure 10.21 – ﻿An overview of the 0x flame graph showing deserializeObject() hot frames" src="img/Figure_10.21_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.21 – An overview of the 0x flame graph showing deserializeObject() hot frames
    
   </p>
   <ol>
    <li value="9">
     
      As we learned in the
     
     <em class="italic">
      
       Interpreting flame graphs
      
     </em>
     
      recipe of this chapter, the darker/more red frames can indicate bottlenecks in our application.
     
     
      In our example, the
     
     <strong class="source-inline">
      
       deserializeObject()
      
     </strong>
     
      function appears to be the hottest, meaning it was spending the most amount of time on the CPU.
     
     
      This is a commonly observed bottleneck in MongoDB-based applications.
     
     
      The bottleneck in
     
     <strong class="source-inline">
      
       deserializeObject()
      
     </strong>
     
      is related to the large amount of data we’re querying and receiving from our
     
     
      
       MongoDB instance.
      
     
    </li>
    <li>
     
      Let’s try and solve this bottleneck by precomputing and storing the average in the database.
     
     
      This should help by reducing the amount of data we request from MongoDB and removing the need to calculate the average.
     
     
      We’ll create a script called
     
     <strong class="source-inline">
      
       calculate-average.js
      
     </strong>
     
      that calculates the average and stores it in MongoDB.
     
     
      Create the
     
     
      <strong class="source-inline">
       
        calculate-average.js
       
      </strong>
     
     
      
       file:
      
     
     <pre class="source-code">
<strong class="bold">$ touch calculate-average.js</strong></pre>
    </li>
    <li>
     
      Add the following
     
     <a id="_idIndexMarker838">
     </a>
     
      code
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        calculate-average.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const { MongoClient } = require('mongodb');
const URL = 'mongodb://localhost:27017/';
async function main () {
  const client = new MongoClient(URL);
  try {
    await client.connect();
    const db = client.db('data');
    const values = db.collection('values');
    const averages = db.collection('averages');
    const data = await values.find({}).toArray();
    // Calculate average
    const average =
      data.reduce((accumulator, value) =&gt; accumulator + value.value, 0) /
      data.length;
    await averages.insertOne({ value: average });
    console.log('Stored average in database.');
  } catch (err) {
    console.error(err);
  } finally {
    await client.close();
  }
}
main().catch(console.error);</pre>
    </li>
    <li>
     
      Run the
     
     <strong class="source-inline">
      
       calculate-averages.js
      
     </strong>
     
      script to calculate and store the average in
     
     
      
       the
      
     
     
      <a id="_idIndexMarker839">
      </a>
     
     
      
       database:
      
     
     <pre class="source-code">
<strong class="bold">$ node calculate-average.js</strong>
Stored average in database.</pre>
    </li>
    <li>
     
      Now, we can rewrite the server so that it returns the stored average, rather than calculating it upon each request.
     
     
      Create a new file
     
     
      
       named
      
     
     
      <strong class="source-inline">
       
        server-no-processing.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ touch server-no-processing.js</strong></pre>
    </li>
    <li>
     
      Add the following
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        server-no-processing.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const { MongoClient } = require('mongodb');
const express = require('express');
const URL = 'mongodb://localhost:27017/';
const app = express();
async function main () {
  const client = new MongoClient(URL);
  try {
    await client.connect();
    const db = client.db('data');
    const average = db.collection('averages');
    app.get('/', async (req, res) =&gt; {
      try {
        const data = await average.findOne({});
        res.send(`Average of all values is ${data.value}.`);
      } catch (err) {
        console.error(err);
        res.status(500).send('Error fetching average');
      }
    });
    app.listen(3000, () =&gt; {
      console.log('Server is listening on port 3000');
    });
  } catch (err) {
    console.error(err);
  }
}
main().catch(console.error);</pre>
    </li>
    <li>
     
      Let’s rerun the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      benchmark.
     
     
      Start the server with
     
     <strong class="source-inline">
      
       $ node server-no-process.js
      
     </strong>
     
      .
     
     
      Then, in a second terminal window, rerun the
     
     <strong class="source-inline">
      
       autocannon
      
     </strong>
     
      
       load test:
      
     
     <pre class="source-code">
<strong class="bold">$ autocannon --connections 500 http://localhost:3000</strong></pre>
     <p class="list-inset">
      
       Expect to see
      
      <a id="_idIndexMarker840">
      </a>
      
       the
      
      <strong class="source-inline">
       
        autocannon
       
      </strong>
      
       result summary once the load test has
      
      
       
        been completed:
       
      
     </p>
    </li>
   </ol>
   <div><div><img alt="Figure 10.22 – autocannon result summary for server-no-processing.js" src="img/Figure_10.22_B19212.jpg"/>
     
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    
     Figure 10.22 – autocannon result summary for server-no-processing.js
    
   </p>
   <p class="list-inset">
    
     Here, we can see that the average number of requests per second has increased from around
    
    <strong class="source-inline">
     
      317
     
    </strong>
    
     in
    
    <strong class="source-inline">
     
      server.js
     
    </strong>
    
     to
    
    <strong class="source-inline">
     
      6430
     
    </strong>
    
     using the precomputed average
    
    
     
      in
     
    
    
     <strong class="source-inline">
      
       server-no-processing.js
      
     </strong>
    
    
     
      .
     
    
   </p>
   <p>
    
     In this recipe, we learned how obtaining and processing large amounts of data from MongoDB can introduce bottlenecks in our application.
    
    
     We solved the bottleneck showcased in this recipe by precomputing and storing
    
    
     
      the average.
     
    
   </p>
   <h2 id="_idParaDest-339">
    <a id="_idTextAnchor347">
    </a>
    
     How it works…
    
   </h2>
   <p>
    
     This recipe demonstrated
    
    <a id="_idIndexMarker841">
    </a>
    
     a bottleneck in an application that communicated with a
    
    
     
      MongoDB database.
     
    
   </p>
   <p>
    
     The slowness was caused by both the large amount of data being requested and the calculation of the average upon each request.
    
    
     By using the
    
    <strong class="source-inline">
     
      0x
     
    </strong>
    
     tool to generate a flame graph, it was possible to diagnose the specific function that was causing
    
    
     
      the bottleneck.
     
    
   </p>
   <p>
    
     In this case, the bottleneck was solved by precomputing the average and storing it in the database.
    
    
     This meant that instead of having to query the database for all values and computing the average on each request, it was possible to just query and obtain the average directly.
    
    
     This showed a significant increase
    
    
     
      in performance.
     
    
   </p>
   <p>
    
     It was worthwhile amending the data model to store the precomputed average so that it didn’t need to be calculated on each request.
    
    
     However, in a real application, it may not always be possible to edit the data model to store computed values.
    
    
     When building a new application, it’s worth considering what data should be stored in the data model to minimize computation on the
    
    
     
      live server.
     
    
   </p>
   <p>
    
     Micro-optimizations, such as precomputing an average, can enhance performance by reducing runtime computation.
    
    
     These small improvements can boost efficiency, especially under heavy load.
    
    
     However, premature optimizations can complicate code, making maintenance harder.
    
    
     As such, it’s usually recommended to prioritize optimizations that offer substantial performance gains for your application and
    
    
     
      end users.
     
    
   </p>
   <h2 id="_idParaDest-340">
    <a id="_idTextAnchor348">
    </a>
    
     See also
    
   </h2>
   <ul>
    <li>
     
      The
     
     <em class="italic">
      
       Creating an Express.js web application
      
     </em>
     
      recipe in
     
     <a href="B19212_06.xhtml#_idTextAnchor178">
      
       <em class="italic">
        
         Chapter 6
        
       </em>
      
     </a>
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Storing and retrieving data with MongoDB
      
     </em>
     
      recipe in
     
     <a href="B19212_07.xhtml#_idTextAnchor212">
      
       <em class="italic">
        
         Chapter 7
        
       </em>
      
     </a>
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Benchmarking HTTP requests
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Detecting memory leaks
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Optimizing synchronous functions
      
     </em>
     
      recipe in
     
     
      
       this chapter
      
     
    </li>
    <li>
     
      The
     
     <em class="italic">
      
       Working with worker threads
      
     </em>
     
      recipe in
     
     <a id="_idIndexMarker842">
     </a>
     
      
       this chapter
      
     
    </li>
   </ul>
   <h1 id="_idParaDest-341">
    <a id="_idTextAnchor349">
    </a>
    
     Working with worker threads
    
   </h1>
   <p>
    
     JavaScript is a single-threaded
    
    <a id="_idIndexMarker843">
    </a>
    
     programming language, meaning that it executes one task at a time within a process.
    
    
     Node.js also runs on a single thread, but it uses an event loop to handle asynchronous operations, enabling non-blocking I/O calls.
    
    
     Despite this, the event loop processes one task at a time.
    
    
     As a result, CPU-intensive tasks can block the event loop and degrade the overall performance of
    
    
     
      your application.
     
    
   </p>
   <p>
    
     To handle CPU-intensive tasks in Node.js efficiently, you should consider using worker threads.
    
    
     Worker threads were declared stable in Node.js version 12 and later and are accessible through the core
    
    <strong class="source-inline">
     
      worker_threads
     
    </strong>
    
     core module.
    
    
     The worker threads API allows you to run JavaScript code in parallel across multiple threads, making it well-suited for
    
    
     
      CPU-intensive operations.
     
    
   </p>
   <p>
    
     This tutorial will introduce the
    
    <strong class="source-inline">
     
      worker_threads
     
    </strong>
    
     module and demonstrate how to use it to manage
    
    
     
      CPU-intensive tasks.
     
    
   </p>
   <h2 id="_idParaDest-342">
    <a id="_idTextAnchor350">
    </a>
    
     Getting ready
    
   </h2>
   <p>
    
     First, ensure you’re using Node.js 22.
    
    
     Then, create a project directory to work in
    
    
     
      named
     
    
    
     <strong class="source-inline">
      
       worker-app
      
     </strong>
    
    
     
      :
     
    
   </p>
   <pre class="source-code">
<strong class="bold">$ mkdir worker-app</strong>
<strong class="bold">$ cd worker-app</strong></pre>
   <p>
    
     Now that we’ve created a directory to work in, we can start
    
    
     
      this recipe.
     
    
   </p>
   <h2 id="_idParaDest-343">
    <a id="_idTextAnchor351">
    </a>
    
     How to do it…
    
   </h2>
   <p>
    
     In this recipe, we’ll learn how to leverage worker threads to handle a
    
    
     
      CPU-intensive task:
     
    
   </p>
   <ol>
    <li>
     
      We’ll start by creating a simplified worker that returns the
     
     <strong class="source-inline">
      
       Hello &lt;name&gt;!
      
     </strong>
     
      string.
     
     
      Create a file
     
     
      
       named
      
     
     
      <strong class="source-inline">
       
        hello-worker.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
<strong class="bold">$ touch hello-worker.js</strong></pre>
    </li>
    <li>
     
      In
     
     <strong class="source-inline">
      
       hello-worker.js
      
     </strong>
     
      , we need to import the necessary class
     
     
      
       and methods:
      
     
     <pre class="source-code">
const {
  Worker,
  isMainThread,
  parentPort,
  workerData
} = require('node:worker_threads');</pre>
    </li>
    <li>
     
      Now, we need to create an
     
     <strong class="source-inline">
      
       if
      
     </strong>
     
      statement using the
     
     <strong class="source-inline">
      
       isMainThread()
      
     </strong>
     
      method from the
     
     <strong class="source-inline">
      
       worker_threads
      
     </strong>
     
      module.
     
     
      Anything within the
     
     <strong class="source-inline">
      
       if
      
     </strong>
     
      block will be executed on the
     
     <a id="_idIndexMarker844">
     </a>
     
      main thread.
     
     
      Code within the
     
     <strong class="source-inline">
      
       else
      
     </strong>
     
      block will be executed in the worker.
     
     
      Add the following
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        hello-worker.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
if (isMainThread) {
  // Main thread code
} else {
  // Worker code
}</pre>
    </li>
    <li>
     
      Now, let’s populate the main thread code.
     
     
      First, create a new worker and pass the
     
     <strong class="source-inline">
      
       Worker
      
     </strong>
     
      constructor two arguments.
     
     
      The first argument will be the filename of the worker’s main script or module.
     
     
      In this case, we’ll use
     
     <strong class="source-inline">
      
       __filename
      
     </strong>
     
      to reference our current file.
     
     
      The second parameter will be an
     
     <strong class="source-inline">
      
       options
      
     </strong>
     
      object, which will specify a
     
     <strong class="source-inline">
      
       workerData
      
     </strong>
     
      property that holds the name we want to pass through to the worker thread.
     
     
      The
     
     <strong class="source-inline">
      
       workerData
      
     </strong>
     
      property is used to share values with the worker thread.
     
     
      Add the following line under the
     
     <strong class="source-inline">
      
       // Main thread
      
     </strong>
     
      <strong class="source-inline">
       
        code
       
      </strong>
     
     
      
       comment:
      
     
     <pre class="source-code">
  const worker = new Worker(__filename, {
    workerData: 'Beth'
  });</pre>
    </li>
    <li>
     
      Now, expect the worker thread to pass a value back to the main thread.
     
     
      To capture this, we can create a worker message event listener.
     
     
      Add the following line below the
     
     
      
       worker initialization:
      
     
     <pre class="source-code">
  worker.on('message', (msg) =&gt; {
    console.log(msg);
  });</pre>
    </li>
    <li>
     
      Now, we can write the worker code that will construct the greeting.
     
     
      Using the
     
     <strong class="source-inline">
      
       parentPort.postMessage()
      
     </strong>
     
      method will return the value to our main thread.
     
     
      Add the
     
     <a id="_idIndexMarker845">
     </a>
     
      following code below the
     
     <strong class="source-inline">
      
       // Worker
      
     </strong>
     
      <strong class="source-inline">
       
        code
       
      </strong>
     
     
      
       comment:
      
     
     <pre class="source-code">
  const greeting = `Hello ${workerData}!`;
  parentPort.postMessage(greeting);</pre>
    </li>
    <li>
     
      Now, run the program with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ node hello-worker.js</strong>
Hello Beth!</pre>
    </li>
    <li>
     
      Now, let’s try something CPU-intensive and compare the behaviors when using and not using worker threads.
     
     
      First, create a file named
     
     <strong class="source-inline">
      
       fibonacci.js
      
     </strong>
     
      .
     
     
      This will contain a Fibonacci calculator program that returns the Fibonacci number at a given index.
     
     
      Create the
     
     
      <strong class="source-inline">
       
        fibonacci.js
       
      </strong>
     
     
      
       file:
      
     
     <pre class="source-code">
<strong class="bold">$ touch fibonacci.js</strong></pre>
    </li>
    <li>
     
      Add the following
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        fibonacci.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const n = 10;
// Fibonacci calculator
const fibonacci = (n) =&gt; {
  let a = 0;
  let b = 1;
  let next = 1;
  let i = 2;
  for (i; i &lt;= n; i++) {
    next = a + b;
    a = b;
    b = next;
  }
  console.log(`The Fibonacci number at position ${n} is ${next}`);
};
fibonacci(n);
console.log('...');</pre>
    </li>
    <li>
     
      Run the script
     
     <a id="_idIndexMarker846">
     </a>
     
      with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ node fibonacci.js</strong>
The Fibonacci number at position 10 is 55
...</pre>
     <p class="list-inset">
      
       In this case, the
      
      <strong class="source-inline">
       
        fibonacci()
       
      </strong>
      
       function blocks the execution of
      
      <strong class="source-inline">
       
        console.log("...");
       
      </strong>
      
       until the
      
      <strong class="source-inline">
       
        fibonacci()
       
      </strong>
      
       function has
      
      
       
        finished running.
       
      
     </p>
    </li>
    <li>
     
      Now, let’s try writing it using worker threads to see how we can avoid blocking the main thread.
     
     
      Create a file
     
     
      
       named
      
     
     
      <strong class="source-inline">
       
        fibonacci-worker.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
$ touch fibonacci-worker.js</pre>
    </li>
    <li>
     
      Start by adding the following imports
     
     
      
       to
      
     
     
      <strong class="source-inline">
       
        fibonacci-worker.js
       
      </strong>
     
     
      
       :
      
     
     <pre class="source-code">
const {
  Worker,
  isMainThread,
  parentPort,
  workerData
} = require('node:worker_threads');</pre>
    </li>
    <li>
     
      Next, as we
     
     <a id="_idIndexMarker847">
     </a>
     
      did in
     
     <strong class="source-inline">
      
       fibonacci.js
      
     </strong>
     
      in
     
     <em class="italic">
      
       Step 8
      
     </em>
     
      , add the
     
     <strong class="source-inline">
      
       Fibonacci
      
     </strong>
     
      <strong class="source-inline">
       
        calculator
       
      </strong>
     
     
      
       function:
      
     
     <pre class="source-code">
const n = 10;
// Fibonacci calculator
const fibonacci = (n) =&gt; {
  let a = 0;
  let b = 1;
  let next = 1;
  let i = 2;
  for (i; i &lt;= n; i++) {
    next = a + b;
    a = b;
    b = next;
  }
  return next;
};</pre>
    </li>
    <li>
     
      Finally, we can implement the structure that enables us to use the
     
     <strong class="source-inline">
      
       worker
      
     </strong>
     
      thread.
     
     
      Add the
     
     
      
       following code:
      
     
     <pre class="source-code">
if (isMainThread) {
  // Main thread code
  const worker = new Worker(__filename, {
    workerData: n
  });
  worker.on('message', (msg) =&gt; {
    console.log(`The Fibonacci number at position ${n} is ${msg}`);
  });
  console.log('...');
} else {
  // Worker code
  parentPort.postMessage(fibonacci(workerData));
}</pre>
    </li>
    <li>
     
      Now, run this
     
     <a id="_idIndexMarker848">
     </a>
     
      script with the
     
     
      
       following command:
      
     
     <pre class="source-code">
<strong class="bold">$ node fibonacci-worker.js</strong>
...
The Fibonacci number at position 10 is 55</pre>
     <p class="list-inset">
      
       Observe that
      
      <strong class="source-inline">
       
        console.log("...");
       
      </strong>
      
       is being printed before the result of the
      
      <strong class="source-inline">
       
        fibonacci()
       
      </strong>
      
       function returns.
      
      
       The
      
      <strong class="source-inline">
       
        fibonacci()
       
      </strong>
      
       function has been offloaded to the worker thread, meaning work on the main thread
      
      
       
        can continue.
       
      
     </p>
    </li>
   </ol>
   <p>
    
     With that, we’ve learned how to offload tasks to a worker thread using the Node.js core
    
    
     <strong class="source-inline">
      
       worker_threads
      
     </strong>
    
    
     
      module.
     
    
   </p>
   <h2 id="_idParaDest-344">
    <a id="_idTextAnchor352">
    </a>
    
     How it works…
    
   </h2>
   <p>
    
     This recipe served as an introduction to worker threads.
    
    
     As we’ve seen, worker threads can be used to handle CPU-intensive computations.
    
    
     Offloading CPU-intensive computations to a worker thread can help avoid blocking the Node.js event loop.
    
    
     This means the application can continue to handle other work – for example, I/O operations – while CPU-intensive
    
    <a id="_idIndexMarker849">
    </a>
    
     tasks are
    
    
     
      being processed.
     
    
   </p>
   <p>
    
     Worker threads are exposed via the core Node.js
    
    <strong class="source-inline">
     
      worker_threads
     
    </strong>
    
     module.
    
    
     To use a worker thread in this recipe, we imported the following four assets from the
    
    <strong class="source-inline">
     
      worker_threads
     
    </strong>
    
     
      core module:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       Worker
      
     </strong>
     
      : The worker thread class, which represents an independent
     
     
      
       JavaScript thread.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       isMainThread
      
     </strong>
     
      : A property that returns
     
     <strong class="source-inline">
      
       true
      
     </strong>
     
      if the code isn’t running in a
     
     
      
       worker thread.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       parentPort
      
     </strong>
     
      : This is a message port that allows communication from the worker to the
     
     
      
       parent thread.
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       workerData
      
     </strong>
     
      : This property clones the data that’s passed in the worker thread constructor.
     
     
      This is how the initial data from the main thread is passed to the
     
     
      
       worker thread.
      
     
    </li>
   </ul>
   <p>
    
     In this recipe, we initialized a worker thread with the
    
    
     
      following code:
     
    
   </p>
   <pre class="source-code">
  const worker = new Worker(__filename, {
    workerData: n,
  });</pre>
   <p>
    
     The
    
    <strong class="source-inline">
     
      Worker
     
    </strong>
    
     constructor requires a mandatory first argument – that is, a filename.
    
    
     This filename is the path to the worker thread’s main script
    
    
     
      or module.
     
    
   </p>
   <p>
    
     The second argument is an
    
    <strong class="source-inline">
     
      options
     
    </strong>
    
     object, which can accept many different configuration options.
    
    
     In
    
    <strong class="source-inline">
     
      fibonacci-worker.js
     
    </strong>
    
     , we provided just one configuration option,
    
    <strong class="source-inline">
     
      workerData
     
    </strong>
    
     , to pass the value of
    
    <strong class="source-inline">
     
      n
     
    </strong>
    
     to the worker thread.
    
    
     The full list of options that can be passed via the worker thread’s
    
    <strong class="source-inline">
     
      options
     
    </strong>
    
     object is listed in the Node.js
    
    <strong class="source-inline">
     
      worker_threads
     
    </strong>
    
     API
    
    
     
      documentation (
     
    
    <a href="https://nodejs.org/api/worker_threads.html#worker_threads_new_worker_filename_options">
     
      
       https://nodejs.org/api/worker_threads.html#worker_threads_new_worker_filename_options
      
     
    </a>
    
     
      ).
     
    
   </p>
   <p>
    
     Once the worker has been initialized, we can register event listeners on it.
    
    
     In this recipe, we registered a message event listener function that executes every time a message is received from the worker.
    
    
     The
    
    <a id="_idIndexMarker850">
    </a>
    
     following events can be listened for on
    
    
     
      a worker:
     
    
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      
       error
      
     </strong>
     
      : Emitted when the worker thread throws an
     
     
      
       uncaught exception
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       exit
      
     </strong>
     
      : Emitted once the worker thread has stopped
     
     
      
       executing code
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       message
      
     </strong>
     
      : Emitted when the worker thread emits a message
     
     
      
       using
      
     
     
      <strong class="source-inline">
       
        parentPort.postMessage()
       
      </strong>
     
    </li>
    <li>
     <strong class="source-inline">
      
       messagerror
      
     </strong>
     
      : Emitted when deserializing the
     
     
      
       message fails
      
     
    </li>
    <li>
     <strong class="source-inline">
      
       online
      
     </strong>
     
      : Emitted when the worker thread starts executing
     
     
      
       JavaScript code
      
     
    </li>
   </ul>
   <p>
    
     We use
    
    <strong class="source-inline">
     
      parentPort.postMessage()
     
    </strong>
    
     to send the value of
    
    <strong class="source-inline">
     
      fibonacci(n)
     
    </strong>
    
     back to the parent thread.
    
    
     In the parent thread, we register a message event listener to detect incoming
    
    <a id="_idIndexMarker851">
    </a>
    
     messages from the
    
    
     
      worker thread.
     
    
   </p>
   <p>
    
     With that, we’ve introduced worker threads and showcased how they can be used to handle
    
    
     
      CPU-intensive tasks.
     
    
   </p>
  </div>
 </body></html>