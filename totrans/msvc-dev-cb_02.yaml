- en: Edge Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Controlling access to your service with an edge proxy server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending your services with sidecars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using API Gateway to route requests to services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate limiting with an edge proxy server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopping cascading failure with Hystrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a service mesh to factor out shared concerns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you've had some experience breaking a monolith into microservices,
    you've seen that many of the challenges exist outside the monolith or service
    code bases themselves. Exposing your service to the internet, controlling routing,
    and building in resiliency are all concerns that can be addressed by what are commonly
    called **edge services**. These are services that exist at the edge of our architecture,
    generally handling requests from the public internet. Luckily, because many of
    these challenges are so common, open source projects exist to handle most of them
    for us. We'll use a lot of great open source software in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: With the recipes in this chapter, you'll learn how to use open source software
    to expose your services to the public internet, control routing, extend your service's
    functionality, and handle a number of common challenges when deploying and scaling
    microservices. You'll also learn about techniques for making client development
    against services easier and how to standardize the monitoring and observability
    of your microservice architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling access to your service with an edge proxy server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](d5b36f18-79eb-4c0a-bbde-3e94733ef97c.xhtml), *Breaking the Monolith*,
    we modified a monolith code base to provide easy routing to our microservices.
    This approach works and requires little effort, making it an ideal intermediary
    step. Eventually, your monolith will become a bottleneck in the development and
    resiliency of your architecture. As you try to scale your service and build more
    microservices, your monolith will need to be updated and deployed every time you
    make an API change to your service. Additionally, your monolith will have to handle
    connections to your services and is probably not well-configured to handle edge
    concerns such as load shedding or circuit breaking. In the *Routing requests to
    services* recipe of [Chapter 1](d5b36f18-79eb-4c0a-bbde-3e94733ef97c.xhtml), *Breaking
    the Monolith*, we introduced the concept of edge proxies. Using an edge proxy
    server to expose your service to the public internet allows you to factor out
    most of the shared concerns a publicly exposed service must address. Requirements
    such as request routing, load shedding, back pressure, and authentication can
    all be handled in a single edge proxy layer instead of being duplicated by every
    service you need to have exposed to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: An edge proxy is a proxy server that sits on the edge of your infrastructure,
    providing access to internal services. You can think of an edge proxy as the “front
    door” to your internal service architecture—it allows clients on the internet
    to make requests to internal services you deploy. There are multiple open source
    edge proxies that have a robust feature set and community, so we don't have to
    write and maintain our own edge proxy server. One of the most popular open source
    edge proxy servers is called **Zuul** and is built by Netflix. Zuul is an edge
    service that provides dynamic routing, monitoring, resiliency, security, and more.
    Zuul is packaged as a Java library. Services written in the Java framework Spring
    Boot can use an embedded Zuul service to provide edge-proxy functionality. In
    this recipe, we'll walk through building a small Zuul edge proxy and configuring
    it to route requests to our services.
  prefs: []
  type: TYPE_NORMAL
- en: Operational notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuing with our example application from the previous chapter, imagine that
    our photo-messaging application (we'll call it `pichat` from now on) was originally
    implemented as a Ruby on Rails monolithic code base. When the product first launched,
    we deployed the application to Amazon Web Services behind a single **Elastic Load
    Balancer** (**ELB**). We created a single **Auto Scale Group** (**ASG**) for the
    monolith, called `pichat-asg`.
  prefs: []
  type: TYPE_NORMAL
- en: Each EC2 instance in our ASG is running NGINX, which handles requests for static
    files (images, JavaScript, CSS) and proxies requests to unicorns running on the
    same host that is serving our Rails application. SSL is terminated at the ELB,
    and HTTP requests are forwarded to NGINX. The ELB is accessed through the DNS `monolith.pichat-int.me` name
    from within the **Virtual Private Cloud** (**VPC**).
  prefs: []
  type: TYPE_NORMAL
- en: We've now created a single `attachment-service`, which handles videos and images
    attached to messages being sent through the platform. The `attachment-service`
    is written in Java, using the Spring Boot platform and is deployed in its own
    ASG, called `attachment-service-asg`, that has its own ELB. We've created a private
    DNS record, called `attachment-service.pichat-int.me`, that points to this ELB.
  prefs: []
  type: TYPE_NORMAL
- en: With this architecture and topology in mind, we now want to route requests from
    the public internet to our Rails application or our newly created attachment service,
    depending on the path.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate using Zuul to route requests to services, we''ll first create
    a basic Java application that will serve as our edge proxy service. The Java project
    Spring Cloud provides an embedded Zuul service, making it pretty simple to create
    a service that uses the `zuul` library. We''ll start by creating a basic Java
    application. Create the `build.gradle` file with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a single class called `EdgeProxyApplication`. This will serve as the
    entry point to our application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `application.yml` in the `src/main/resources` directory
    of your application. This file will specify your route configurations. In this
    example, we''ll imagine that our monolith application can be accessed on the `monolith.pichat-int.me` internal
    host and we want to expose the `/signup` and `/auth/login` paths to the public
    internet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Start the project with `./gradlew bootRun` and you should be able to access
    the `/signup` and `/auth/login` URLs, which will be proxied to our monolith application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We want to expose the `attachment-service` URLs to the internet. The attachment
    service exposes the following endpoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll need to decide which paths we want to use in our public API. Modify
    `application.properties` to add the following entries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now all requests to `/attachments/*` will be forwarded to the attachment service
    and signup, and `auth/login` will continue to be served by our monolith application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can test this by running our service locally and sending requests to `localhost:8080/signup,
    localhost:8080/auth/login`, and `localhost:8080/attachments/foo`. You should be
    able to see that requests are routed to the respected services. Of course, the
    service will respond with an error because `attachment-service.pichat-int.me`
    cannot be resolved, but this shows that the routing is working as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Extending your services with sidecars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you start developing microservices, it's common to embed a certain amount
    of boilerplate into each service. Logging, metrics, and configuration are all
    functionalities that are commonly copied from service to service, resulting in
    a large amount of boilerplate and copied and pasted code. As your architecture
    grows and you develop more services, this kind of setup becomes harder and harder
    to maintain. The usual result is that you end up with a bunch of different ways
    of doing logging, metrics, service discovery, and so on, which results in systems
    that are hard to debug and maintain. Changing something as simple as a metrics
    namespace or adding a feature to your service discovery clients can require the
    coordination of multiple teams and code bases. More realistically, your microservices
    architecture will continue to grow with inconsistent logging, metrics, and service
    discovery conventions, making it harder for developers to operate, contributing
    to overall operational pain.
  prefs: []
  type: TYPE_NORMAL
- en: The sidecar pattern describes a pattern whereby you extend the functionality
    of a service with a separate process or container running on the same machine.
    Common functionalities, such as metrics, logging, service discovery, configuration,
    or even network RPC, can be factored out of your application and handled by a
    sidecar service running alongside it. This pattern makes it easy to standardize
    shared concerns within your architecture by implementing them in a separate process
    that can be used by all of your services.
  prefs: []
  type: TYPE_NORMAL
- en: A common method for implementing a sidecar is to build a small, separate process
    that exposes some functionality over a commonly used protocol, such as HTTP. Imagine,
    for instance, that you want all of your services to use a centralized service-discovery
    service instead of relying on DNS hosts and ports to be set in each application's
    configuration. To accomplish this, you'd need to have up-to-date client libraries
    for your service-discovery service available in all of the languages that your
    services and monolith are written in. A better way would be to run a sidecar parallel
    to each service that runs a service-discovery client. Your services could then
    proxy requests to the sidecar and have it determine where to send them. As an
    added benefit, you could configure the sidecar to emit consistent metrics around
    network RPC requests made between services.
  prefs: []
  type: TYPE_NORMAL
- en: This is such a common pattern that there are multiple open source solutions
    available for it. In this recipe, we'll use `spring-cloud-netflix-sidecar`, a
    project that includes a simple HTTP API that allows non-JVM applications to use
    JVM client libraries. The Netflix sidecar assumes you are using Eureka, a service
    registry designed to support the service-discovery needs of clients. We'll discuss
    service discovery in more detail in later chapters. The sidecar also assumes your
    non-JVM application is serving a health-check endpoint and will use this to advertise
    its health to Eureka. Our Rails application exposes such an endpoint, /health,
    which, when running normally, will return a small JSON payload with a key status
    and the `UP` value.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start by creating a basic Spring Boot service. Include the Spring Boot Gradle
    plugin and add dependencies for Spring Boot and the Spring Cloud Netflix sidecar
    project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re ready to create a simple Spring Boot application. We''ll use the `@EnableSidecar`
    annotation, which also includes the `@EnableZuulProxy`, `@EnableCircuitBreaker`,
    and `@EnableDiscoveryClient` annotations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The Netflix sidecar application requires a few configuration settings to be
    present. Create a new file called `application.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The sidecar will now expose an API that allows non-JVM applications to locate
    services registered with Eureka. If our `attachment-service` is registered with
    Eureka, the sidecar will proxy requests to `http://localhost:5678/attachment/1234`
    to `http://attachment-service.pichat-int.me/1234`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using API Gateways for routing requests to services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've seen in other recipes, microservices should provide a specific business
    capability and should be designed around one or more domain concepts, surrounded
    by a bounded context. This approach to designing service boundaries works well
    to guide you toward simple, independently-scalable services that can be managed
    and deployed by a single team dedicated to a certain area of your application
    or business.
  prefs: []
  type: TYPE_NORMAL
- en: When designing user interfaces, clients often aggregate related but distinct
    entities from various backend microservices. In our fictional messaging application,
    for instance, the screen that shows an actual message might have information from
    a message service, a media service, a likes service, a comments service, and so
    on. All of this information can be cumbersome to collect and can result in a large
    number of round-trip requests to the backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Porting a web application from a monolith with server-side-rendered HTML to
    a single-page JavaScript application, for example, can easily result in hundreds
    of `XMLHttpRequests` for a single page load:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb729cb-f3d8-473f-affc-652421565438.png)'
  prefs: []
  type: TYPE_IMG
- en: To reduce the amount of round-trip requests to the backend services, consider
    creating one or more API Gateways that provide an API that is catered to the client's
    needs. API Gateways can be used to present a single view of backend entities in
    a way that makes it easier for clients who use the API. In the preceding example,
    a request to a single message endpoint could return information about the message
    itself, media included in the message, likes and comments, and other information.
  prefs: []
  type: TYPE_NORMAL
- en: 'These entities can be concurrently collected from various backend services
    using a fan-out request pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58af6302-bb02-46c3-860f-7385c987080a.png)'
  prefs: []
  type: TYPE_IMG
- en: Design considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the benefits of using an API Gateway to provide access to microservices
    is that you can create a single, cohesive API for a specific client. In most cases,
    you'll want to create a specific API for mobile clients, perhaps even one API
    for iOS and one for Android. This implementation of API Gateways is commonly referred
    to as the **Backend for Frontend** (**BFF**) because it provides a single logical
    backend for each frontend application. A web application has very different needs
    than a mobile device.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our situation, we''ll focus on creating one endpoint that provides all the
    data needed by the message-view screen. This includes the message itself as well
    as the attachment(s), the user details of the sender, and any additional recipients
    of the message. If the message is public, it can also have likes and comments,
    which we''ll imagine are served by a separate service. Our endpoint could look
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The endpoint will return a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This response should have everything a client needs to show our message-view
    screen. The data itself comes from a variety of services, but, as we'll see, our
    API Gateway does the hard work of making those requests and aggregating the responses.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An API Gateway is responsible for exposing an API, making multiple service
    calls, aggregating the results, and returning them to the client. The **Finagle Scala**
    framework makes this natural by representing service calls as futures, which can
    be composed to represent dependencies. To stay consistent with other examples
    in this book, we''ll build a small example gateway service in Java using the Spring
    Boot framework:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the project skeleton. Create a new Java project and add the following
    dependencies and plugins to the Gradle build file. We''ll be using Spring Boot
    and Hystrix in this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the JSON example in the previous section, it's clear that we are
    collecting and aggregating some distinct domain concepts. For the purposes of
    this example, we'll imagine that we have a message service that retrieves information
    about messages, including likes, comments, and attachments, and a user service.
    Our gateway service will be making a call to the message service to retrieve the
    message itself, then calls to the other services to get the associated data, which
    we'll stitch together in a single response. For the purposes of this recipe, imagine
    the message service is running on port `4567` and the user service on port `4568`.
    We'll create some stub services to mock out the data for these hypothetical microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a model to represent our `Message` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It's important that non-dependent service calls be done in a non-blocking, asynchronous
    manner. Luckily, Hystrix has an option to execute commands asynchronously, returning
    `Future<T>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new package, say, `com.packtpub.microservices.gateway.commands` with
    the following classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the `AttachmentCommand` class with the following content:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `CommentCommand` class with the following content:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `LikeCommand` class with the following content:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `MessageClient` class is a bit different than the previous examples—instead
    of returning the JSON string from the service response, it''ll return an object
    representation, in this case, an instance of our `Message` class:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `UserCommand` class with the following content:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Stitch together the execution of these Hystrix commands in a single controller
    that exposes our API as the `/message_details/:message_id` endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'There you have it. Run the service with `./gradlew bootRun` and test it by
    making a request to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Stopping cascading failures with Hystrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Failures in a complex system can be hard to diagnose. Often, the symptom can
    appear far away from the cause. Users might start experiencing higher-than-normal
    error rates during login because of some downstream service that manages profile
    pictures or something else tangentially related to user profiles. An error in
    one service can often propagate needlessly to a user request and adversely impact
    user experience and therefore trust in your application. Additionally, a failing
    service can have cascading effects, turning a small system outage into a high-severity,
    customer-impacting incident. It's important when designing microservices to consider
    failure isolation and decide how you want to handle different failure scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: A number of patterns can be used to improve the resiliency of distributed systems.
    Circuit breakers are a common pattern used to back off from making requests to
    a temporarily overwhelmed service. Circuit breakers were first described in Michael
    Nygard's book *Release It!*. A calling service defaults to a closed state, meaning
    requests are sent to the downstream service.
  prefs: []
  type: TYPE_NORMAL
- en: If the calling service receives too many failures too quickly, it can change
    the state of its circuit breaker to open, and start failing fast. Instead of waiting
    for the downstream service to fail again and adding to the load of the failing
    service, it just sends an error to upstream services, giving the overwhelmed service
    time to recover. After a certain amount of time has passed, the circuit is closed
    again and requests start flowing to the downstream service.
  prefs: []
  type: TYPE_NORMAL
- en: There are many available frameworks and libraries that implement circuit breakers.
    Some frameworks, such as Twitter's Finagle, automatically wrap every RPC call
    in a circuit breaker. In our example, we'll use the popular Netflix library, `hystrix`.
    Hystrix is a general-purpose, fault-tolerance library that structures isolated
    code as commands. When a command is executed, it checks the state of a circuit
    breaker to decide whether to issue or short circuit the request.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hystrix is made available as a Java library, so we''ll demonstrate its use
    by building a small Java Spring Boot application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Java application and add the dependencies to your `build.gradle`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll create a simple `MainController` that returns a simple message. This
    is a contrived example, but it demonstrates an upstream service making downstream
    calls. At first, our application will just return a hardcoded `Hello, World!`
    message. Next, we''ll move the string out to a Hystrix command. Finally, we''ll
    move the message to a service call wrapped in a Hystrix command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the message out to `HystrixCommand`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace the method in `MainController` to use `HystrixCommand`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the message generation to another service. We''re hardcoding the hypothetical
    message service URL here, which is not a good practice but will do for demonstration
    purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the `MainController` class to contain the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `MainController` class now makes a service call, wrapped in a Hystrix command,
    to generate a message to send back to the client. You can test this by creating
    a very simple service that generates a message string. `sinatra` is a simple-to-use
    Ruby library ideal for creating test services. Create a new file called `message-service.rb`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the service by running `ruby message-service.rb` and then make a few sample
    requests to your Hystrix-enabled service. You can simulate a failure by modifying
    the service to return a `503`, indicating that it is temporarily overwhelmed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Your Spring service should now attempt to reach the service but use the value
    in the fallback when it encounters a `503`. Furthermore, after a number of attempts,
    the command's circuit breaker will be tripped and the service will start defaulting
    to the fallback for a period of time.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to techniques such as circuit breaking, rate limiting can be an
    effective way to prevent cascading failures in a distributed system. Rate limiting
    can be effective at preventing spam, protecting against **Denial of Service**
    (**DoS**) attacks, and protecting parts of a system from becoming overloaded by
    too many simultaneous requests. Typically implemented as either a global or per-client
    limit, rate limiting is usually part of a proxy or load balancer. In this recipe,
    we'll use NGINX, a popular open source load balancer, web server, and reverse
    proxy.
  prefs: []
  type: TYPE_NORMAL
- en: Most rate-limiting implementations use the *leaky-bucket algorithm*—an algorithm
    that originated in computer network switches and telecommunications networks.
    As the name suggests, the leaky-bucket algorithm is based on the metaphor of a
    bucket with a small leak in it that controls a constant rate. Water is poured
    into the bucket in bursts, but the leak guarantees that water exists in the bucket
    at a steady, fixed rate. If the water is poured in faster than the water exits
    the bucket, eventually the bucket will overflow. In this case, the overflow represents
    requests that are dropped.
  prefs: []
  type: TYPE_NORMAL
- en: It's certainly possible to implement your own rate-limiting solution; there
    are even implementations of the algorithms out there that are open source and
    available to use. It's a lot easier, however, to use a product such as NGINX to
    do rate limiting for you. In this recipe, we'll configure NGINX to proxy requests
    to our microservice.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Install NGINX by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`nginx` has a `config` file, `nginx.conf`. On an Ubuntu-based Linux system,
    this will probably be in the `/etc/nginx/nginx.conf` directory. Open the file
    and look for the `http` block and add the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, rate limiting is implemented with two
    configuration directives. The `limit_req_zone` directive defines the parameters
    for rate limiting. In this example, we're implementing a rate limit, based on
    the client's IP address, of 10 requests per second. The `limit_req` directive
    applies our rate limiting to a specific path or location. In this case, we're
    applying it to all requests to `/auth/signin`, presumably because we don't want
    bots scripting the creation of accounts!
  prefs: []
  type: TYPE_NORMAL
- en: Using service mesh for shared concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As web services' frameworks and standards evolve, the amount of boilerplate
    or shared application concerns is reduced. This is because, collectively, we figure
    out what parts of our applications are universal and therefore shouldn't need
    to be re-implemented by every programmer or team. When people first started networking
    computers, programmers writing network-aware applications had to worry about a
    lot of low-level details that are now abstracted out by the operating system's
    networking stack. Similarly, there are certain universal concerns that all microservices
    share. Frameworks such as Twitter's Finagle wrap all network calls in a circuit
    breaker, increasing fault tolerance and isolating failures in systems. Finagle
    and Spring Boot, the Java framework we've been using for most of these recipes,
    both support exposing a standard metrics endpoint that standardizes basic network,
    JVM, and application metrics collected for microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Every microservice should consider a number of shared application concerns.
    From an observability perspective, services should strive to emit consistent metrics
    and structured logs. To improve the reliability of our systems, services should
    wrap network calls in circuit breakers and implement consistent retry and back-off
    logic. To support changes in network and service topology, services should consider
    implementing client-side load balancing and use centralized service discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of implementing all of these features in each of our services, it would
    be ideal to abstract them out to something outside our application code that could
    be maintained and operated separately. Like the features of our operating systems
    network stack, if each of these features is implemented by something our application
    could rely on being present, we would not have to worry about them being available.
    This is the idea behind a service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Running a service mesh configuration involves running each microservice in your
    system behind a network proxy. Instead of services speaking directly to one another,
    they communicate via their respective proxies, which are installed as sidecars.
    Practically speaking, your service would communicate with its own proxy running
    on localhost. As network requests are sent through a services proxy, the proxy
    can control what metrics are emitted and what log messages are output. The proxy
    can also integrate directly with your service registry and distribute requests
    evenly among active nodes, keeping track of failures and opting to fail fast when
    a certain threshold has been reached. Running your system in this kind of configuration
    can ease the operational complexity of your system while improving the reliability
    and observability of your architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Like most of the recipes discussed in this chapter, there are numerous open
    source solutions for running a service mesh. We'll focus on **Linkerd**, an open
    source proxy server built and maintained by buoyant. The original authors of Linkerd
    worked at Twitter before forming buoyant and as such, Linkerd incorporates many
    of the lessons learned by teams at Twitter. It shares many features with the Finagle
    Scala framework, but can be used with services written in any language. In this
    recipe, we'll walk through installing and configuring Linkerd and discuss how
    we can use it to control communication between our Ruby on Rails monolith API
    and our newly developed media service.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate running a service behind a proxy, we''ll install and run an
    instance of Linkerd and configure it to handle requests to and from your service.
    There are instructions on the Linkerd website for running it in Docker, Kubernetes,
    and other options. To keep things simple, we''ll focus on running Linkerd and
    our service locally:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the latest Linkerd release at [https://github.com/linkerd/linkerd/releases](https://github.com/linkerd/linkerd/releases).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the tarball by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, `linkerd` ships with a configuration that uses file-based service
    discovery. We''ll discuss alternatives to this approach next, but, for now, create
    a new file called `disco/media-service` with the following contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This maps the hostname and port to a service called `media-service`. Linkerd
    uses this file to look up services by name and determines the hostname and port
    mappings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run Linkerd as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the service on port `8080`. Change into the `media-service` directory
    and run the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Linkerd is running on port `4140`. Test that proxying is working with the following
    request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
