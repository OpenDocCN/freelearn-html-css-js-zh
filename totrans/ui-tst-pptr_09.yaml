- en: '*Chapter 9*: Scraping tools'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back in [*Chapter 1*](B16113_01_Final_SK_ePub.xhtml#_idTextAnchor014), *Getting
    started with Puppeteer*, we talked about the different uses for web automation.
    Of all those use cases, web scraping is the one that excites developers the most.
    When I give talks about automation, I know that I will get the crowd's full attention
    when I start talking about task automation, but even more when I get into the
    topic of **web scraping**.
  prefs: []
  type: TYPE_NORMAL
- en: Don't get me wrong, I think that UI testing is important. As we saw in the previous
    chapters, it's not just about running automation tests but also about taking care
    of your customers. But web scraping has that fun spark, a hacker feeling, and
    I didn't want to leave this topic out of the book.
  prefs: []
  type: TYPE_NORMAL
- en: A few months ago, I read a book about web scraping that included a chapter about
    UI testing at the end of the book. We are going to do the same, but the other
    way around. This is a UI testing book with a scraping chapter at the end.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin this chapter by defining and demystifying web scraping. Is it
    just for hackers? Is it even legal? We will also talk about scraping ethics, such
    as when it is OK to scrape and when it is not.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the chapter will discuss the different tools available to
    scrape with Puppeteer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to web scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating scrapers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running scrapers in parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to avoid being detected as a bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with authentication and authorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to apply all the concepts you have
    learned during the course of this book in a brand-new field of web automation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will find all the code of this chapter on the GitHub repository ([https://github.com/PacktPublishing/UI-Testing-with-Puppeteer](https://github.com/PacktPublishing/UI-Testing-with-Puppeteer))
    under the `Chapter9` directory. Remember to run `npm install` on that directory.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to web scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way to introduce a new concept is by giving a few concrete and straightforward
    definitions. Let's begin by defining **data scraping**. According to Wikipedia
    ([https://www.hardkoded.com/ui-testing-with-puppeteer/data-scraping](https://www.hardkoded.com/ui-testing-with-puppeteer/data-scraping)),
    "*Data scraping is a technique in which a computer program extracts data from
    human-readable output coming from another program*." Any information coming out
    from a computer can be extracted and processed. The first scrapers were called
    "screen scrapers." A screen scraper is something as simple as an application that
    can capture the screen. Then, by running **Optical Character Recognition** (**OCR**),
    it extracts the text from that image for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping takes this idea to the next level. *Web scraping is a technique
    used to extract data from one or multiple websites using a piece of software.*
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be wondering: Is that even legal? Amazon is a public site. I can
    freely navigate through the site; why wouldn''t I be able to run a script to extract
    data that is already public? Well, it depends. Let me share with you some scenarios
    from the real world that have similar ethical dilemmas to web scraping.'
  prefs: []
  type: TYPE_NORMAL
- en: '**First scenario**'
  prefs: []
  type: TYPE_NORMAL
- en: A small grocery store owner goes to a big mall to compare their products and
    prices. She can't go and get a box of milk and leave without paying, but she can
    walk around and take notes of the product prices, take that list to her shop,
    and compare their prices. The price is not a product. She's not stealing anything.
    Also, the mall is too big, and they can't control every person walking around
    taking notes. But what if the same person goes to a small grocery store on the
    next block and starts taking notes? I bet the owner already knows her, and it's
    too evident that she's taking notes, and that will probably threaten their business.
    Is it illegal? No. But she might get into a fight.
  prefs: []
  type: TYPE_NORMAL
- en: '**Second scenario**'
  prefs: []
  type: TYPE_NORMAL
- en: Some exclusive furniture stores won't let you take photos inside the store.
  prefs: []
  type: TYPE_NORMAL
- en: '**Last scenario**'
  prefs: []
  type: TYPE_NORMAL
- en: You can't count cards in the casino! They will kick you out and ban you for
    life.
  prefs: []
  type: TYPE_NORMAL
- en: These are real-life "scrapers." People are trying to extract information in
    the real world. Scraping the web is quite similar. You will be able to scrape
    a site as long as a) the site welcomes (implicitly or explicitly) scrapers, b)
    your attitude is considerate while scraping, and c) what you are scraping is allowed.
    Let's unpack these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Does the site allow scrapers?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first reaction to this question would be: "No! why would the owner of a
    website allow scrapers?" But that''s not necessarily true. What if you own a hotel?
    If an aggregator website scrapes your booking page, then it shows those results
    on their website with a link back to your site, they will make some profit from
    that, and you will get more customers: win-win. Or, if you own a non-profit site
    such as Wikipedia or a government website, scraping might not be an issue. As
    it''s a non-profit website, you shouldn''t care much about bots coming to your
    site to extract data unless they affect your website''s performance. But if your
    site is about song lyrics, you won''t want anybody to come to your site and extract
    the lyrics. Lyrics are your product, your assets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will see many techniques to bypass some validations, but
    my personal rule is: If the site doesn''t want to be scraped, I won''t scrape
    it. No means no.'
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we know whether we can scrape a website? The owner of a website can
    express that in at least four different ways.
  prefs: []
  type: TYPE_NORMAL
- en: Terms and conditions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first thing you should check before scraping a website is the terms and
    conditions. The website owners can make it very clear that they don't want it
    to be scraped.
  prefs: []
  type: TYPE_NORMAL
- en: The terms and conditions is that huge block of text we often ignore when installing
    an app. I bet you've also had emails telling you that a website has changed its
    terms and conditions, and you said "yeah, whatever" and archived the email.
  prefs: []
  type: TYPE_NORMAL
- en: But we shouldn't do that. According to iubenda ([https://www.hardkoded.com/ui-testing-with-puppeteer/iubenda-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/iubenda-terms)),
    "'*Terms and Conditions' is the document governing the contractual relationship
    between the provider of a service and its user. The Terms and Conditions are nothing
    other than a contract in which the owner clarifies the conditions of use of its
    service*." Sometimes we might think that when we buy digital content (software,
    music, e-books) on a website, we then own that product, when, in fact, if you
    read the terms and conditions, you have bought the right to use the product, but
    you don't own it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The terms and conditions also states what they allow you to do on a website.
    Many sites make that very clear. Let''s take the example of the terms and conditions
    of www.ebay.com:'
  prefs: []
  type: TYPE_NORMAL
- en: '*3\. Using eBay*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In connection with using or accessing the Services you will not:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*use any robot, spider, scraper or other automated means to access our Services
    for any purpose;*'
  prefs: []
  type: TYPE_NORMAL
- en: '*bypass our robot exclusion headers, interfere with the working of our Services,
    or impose an unreasonable or disproportionately large load on our infrastructure.*'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, eBay makes it very clear. You can't scrape their site. eBay v.
    Bidder's Edge's ([https://www.hardkoded.com/ui-testing-with-puppeteer/ebay-vs-edge](https://www.hardkoded.com/ui-testing-with-puppeteer/ebay-vs-edge))
    was a well-known case back in the 2000s. eBay alleged that Bidder's Edge activities
    constituted a trespass of eBay's chattelsÂ ([https://www.hardkoded.com/ui-testing-with-puppeteer/Trespass-to-chattels](https://www.hardkoded.com/ui-testing-with-puppeteer/Trespass-to-chattels)).
    In other words, Bidder's Edge's scraping affected the servers that are eBay's
    property. I bet you don't want to go to court against eBay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at Ryanair''s terms and conditions ([https://www.hardkoded.com/ui-testing-with-puppeteer/ryanair-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/ryanair-terms)):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Use of any automated system or software, whether operated by a third party
    or otherwise, to extract any data from this website for commercial purposes ("screen
    scraping") is strictly prohibited.*'
  prefs: []
  type: TYPE_NORMAL
- en: Ryanair doesn't like scrapers either, but it says "for commercial purposes,"
    which would mean that you could code your scraper to look for the best price for
    your next vacation.
  prefs: []
  type: TYPE_NORMAL
- en: If the terms and conditions is not clear regarding scrapers, the second way
    a site owner can express their relationship with scrapers is through the `robots.txt`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: robots.txt file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wikipedia, again, has a great definition of the robots file. According to Wikipedia
    ([https://www.hardkoded.com/ui-testing-with-puppeteer/Robots-exclusion-standard](https://www.hardkoded.com/ui-testing-with-puppeteer/Robots-exclusion-standard)),
    the robots exclusion protocol "*is a standard used by websites to communicate
    with web crawlers and other web robots. The standard specifies how to inform the
    web robot about which areas of the website should not be processed or scanned.
    Robots are often used by search engines to categorize websites*."
  prefs: []
  type: TYPE_NORMAL
- en: 'The keyword in that definition is "inform." The website owner can express in
    the robots file which parts of the site can be scraped. Most websites only use
    the `robots.txt` file to tell search engines where they can find the sitemap to
    scrape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With these two simple lines of code, they tell search engines, such as Google,
    to get that Sitemap file and scrape those pages. But you can find more complex
    definitions in that file. For instance, the `robots.txt` file on Wikipedia has
    over 700 lines! That tells us that the site is being scraped quite a lot. Let''s
    see some examples of what we can find in that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'I love that the file begins with a message to us! They expect us to come to
    this page to read it. The next section is interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, Wikipedia informs that they don''t want scrapers identified with the
    user-agent **UbiCrawler**, **DOC**, **Zao**, and **sitecheck.internetseer.com**
    to scrape the site. And the file closes with general rules for all user-agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'They basically say that all the rest (`User-agent: *`) can scrape the whole
    site except some URLs such as `/w/`, `/api/`, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: If we don't find anything useful in the terms and conditions or in the `robots.txt`
    file, we might find some hints in the page response.
  prefs: []
  type: TYPE_NORMAL
- en: Are you a human?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I bet no one in real life has asked you whether you are a human, but many websites
    ask us that question all the time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bot check by reCAPTCHA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.01_B16113.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Bot check by reCAPTCHA
  prefs: []
  type: TYPE_NORMAL
- en: 'What started as a simple "type the word you see" turned into more and more
    complicated challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Complex CAPTCHAs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.02_B16113.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Complex CAPTCHAs
  prefs: []
  type: TYPE_NORMAL
- en: Should we check the third square of the first row? Who knowsâ¦ But their goal
    is clear. In a friendly way, using the UI, they want to kick scrapers out.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a site puts a CAPTCHA, like the one in the previous screenshot, it''s
    not always because they don''t want to be scraped. Maybe they want to protect
    their users. They don''t want malicious bots to test users and passwords or stolen
    credit card numbers. But the intention is clear: that page is for humans.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I call these validations a "friendly" way to kick bots out. But you can also
    get some unfriendly responses from the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Page returning a 403 HTTP error'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.03_B16113.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Page returning a 403 HTTP error
  prefs: []
  type: TYPE_NORMAL
- en: If a page you would typically be able to access now returns a 403 when you scrape
    it, it means that the server considered your actions as an attack on the server
    and banned your IP.
  prefs: []
  type: TYPE_NORMAL
- en: That's terrible news. It means that you won't be able to access that site using
    your current IP. The ban could be for a few hours or forever. That would also
    mean that if you share your public IP with other computers, for instance, inside
    an organization, no one will be able to access that site. There are two ways to
    fix this issue. One is by reaching out to the site owner, apologizing, and asking
    them to remove your IP from the forbidden list. Or you could keep playing badly,
    trying to change your public IP or using proxies. But if you got caught once,
    you will get caught twice.
  prefs: []
  type: TYPE_NORMAL
- en: The last way a site can tell you not to scrape is by giving you an API.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Website API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this context, an API is a set of URLs that the website exposes that instead
    of returning a page, return some data, generally in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wikipedia API'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.04_B16113.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia API
  prefs: []
  type: TYPE_NORMAL
- en: The question here is, why would you waste time parsing HTML elements, wait for
    network calls, and so on, if you can hit a URL to get all the information you
    need?
  prefs: []
  type: TYPE_NORMAL
- en: 'I think this is an excellent way for a site to tell you: **hey, don''t scrape
    me, here is the data you might need. You are welcome to come here and fetch the
    information you need**.'
  prefs: []
  type: TYPE_NORMAL
- en: APIs also give the website's owner the chance to set the rules, such as the
    rate limit (how many requests you can make per second or minute), the data the
    API exposes, and what will remain hidden for consumers.
  prefs: []
  type: TYPE_NORMAL
- en: These are some ways the site can communicate what we can and can't do. But our
    attitude toward the site being scraped is also important.
  prefs: []
  type: TYPE_NORMAL
- en: Our attitude
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This part is quite simple, and it can be reduced to two words: be nice. You
    have to think that on the other side of the page you are trying to scrape, there
    are people like you whose goal is to keep the site up and running. They have to
    pay for a server and network bandwidth. Remember, they are not your enemy. They
    are your friends. They have the data you need, so be nice.'
  prefs: []
  type: TYPE_NORMAL
- en: You should look at available APIs first. If a website exposes an API with the
    data you want, there is no need to scrape the site.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you should consider your scraping rate/speed. Although you might want
    to scrape a site as fast as possible, I would try to make the scrape process close
    to real user interaction. Later in this chapter, we will see tools to scrape pages
    in parallel, but I would use those tools very carefully.
  prefs: []
  type: TYPE_NORMAL
- en: There is one thing that many scrapers using Puppeteer forget. You should always
    identify yourself as a bot. In the next section, we will see that the idea is
    telling the server that you are a bot and not a real user.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we need to consider is to evaluate what data we are extracting.
  prefs: []
  type: TYPE_NORMAL
- en: What's the data we are scraping?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I think this is common sense. Are we extracting copyrighted data? Are we extracting
    the assets of the site? For instance, if we go to a lyrics website and extract
    the lyrics, we are taking the very purpose of the website. But if we go to an
    airline's website and check for flight prices, the price is not the company's
    asset. They sell flights, not prices.
  prefs: []
  type: TYPE_NORMAL
- en: The other thing to consider is what the data we extracted will be used for.
    We should consider whether our actions will empower the scraped site, for instance,
    in the case of the hotel booking website, they might get more customers, or threaten
    it, for instance, if we scrape lyrics to create our own lyrics site.
  prefs: []
  type: TYPE_NORMAL
- en: Truth be told, lots of the sites we know today used scraping techniques to seed
    their websites. You might see that on real-estate websites. Who would want to
    go to an empty real-estate website? No one. So, these websites would be seeded
    with postings from other websites to make them more appealing to new customers.
  prefs: []
  type: TYPE_NORMAL
- en: Enough theory. Let's create some scrapers.
  prefs: []
  type: TYPE_NORMAL
- en: Creating scrapers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try to scrape book prices from the Packt site. The terms and conditions
    said nothing about scrapers ([https://www.hardkoded.com/ui-testing-with-puppeteer/packtpub-terms](https://www.hardkoded.com/ui-testing-with-puppeteer/packtpub-terms)).
    But the `robots.txt` file has some clear rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'They don''t want us to go to those pages. But the site has a pretty massive
    `sitemap.xml`, with over 9,000 lines. If `robots.txt` is the "don''t go here"
    sign for scrapers, `sitemap.xml` is the "please, check this out" sign. These are
    the first items on the `sitemap.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on this XML, we are going to build a **crawler**. A crawler is a program
    that will navigate through the website scraping all the pages. This will be our
    plan:'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to build an array of book category URLs. To make the run shorter,
    we will scrape only category pages, such as [https://www.packtpub.com/web-development](https://www.packtpub.com/web-development).
    We will also limit the list to 10 categories, so we are nice with the server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we get that list, we will navigate each page, getting book links. We don't
    want to get duplicates in there, so we need to be careful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once we get the list of books, we navigate each book page and get the price
    for the print book plus the e-book and the print book alone:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Book details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.05_B16113.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Book details
  prefs: []
  type: TYPE_NORMAL
- en: Those prices will be collected in a JSON array and sent to disk at the end of
    the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s get started! You can get the code of this section in the `crawler.js`
    file. We will get the `sitemap.xml` file and get the first 10 categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There we are using the native `https` package to download the `sitemap.xml`
    file. We create a Promise that will be resolved when we call the `resolve` function.
    Then we call the `get` function to fetch the file. We collect the information
    being downloaded in the `data` event, and when we receive an `end` event, we resolve
    the `Promise` by returning the `body` string we were collecting. It might take
    a while to get the code's flow, but it's very straightforward once you get used
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'That `sitemapxml` variable is a string. We need first to parse the XML and
    then get a JavaScript model from there. `xml2js` will do most of the job for us.
    Let''s install that module using `npm install` in our terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have that module, we can start using it in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be able to parse the sitemap by calling the `xmlParser` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are using the same `Promise` pattern we used before. When
    we call `xmlParser`, we will get the parsed `result` in the result argument of
    the callback we just passed in. Once we get the result, we prepare the output.
    It might be helpful to read the code while looking at the `sitemap.xml` file to
    get more context. We get URL elements from the `result.urlset.url` array. Then
    we `filter` URL elements with a `loc` with three slashes (such as [https://www.packtpub.com/web-development](https://www.packtpub.com/web-development)).
    Then, we grab only the first 10 elements using the `slice` function. Lastly, we
    use the `map` function to return only the resulting URL, returning an array of
    strings containing category URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to use Puppeteer. We will navigate each of the categories we grabbed
    from the `sitemap.xml` and return book URLs. We are not going to scrape only the
    first page of the category page. I'll leave that feature to you as homework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by creating a browser that we are going to use across the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: I bet you are pretty familiar with this piece of code. The first new thing we
    can see here is that we are passing the `slowMo` option. That means that we are
    going to wait 500 milliseconds after each action that Puppeteer will perform.
    We will also use the `userAgent` function to get the user-agent from the browser.
    Then, we grab that string, and we append `' UITestingWithPuppeteerDemoBot'`, so
    the server admins in the publisher will know that's us.
  prefs: []
  type: TYPE_NORMAL
- en: Let the scraping begin!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We will iterate through the `categories` list in the main function, and we will
    call `getBooks`, passing `categoryURL` and a Puppeteer page. That function will
    return a list of book URLs that we will append to our `books` array using the
    `push` function. We are using `slice(0, 10)`, so we return only the first 10 items.
  prefs: []
  type: TYPE_NORMAL
- en: We wrapped all the code into a `try/catch` block because we don't want the code
    to fail if one category has failed.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now take a look at the `getBooks` function. It looks pretty straightforward.
    We go to `categoryURL` and wait for an element using the `a.card-body` CSS selector.
    That selector will give us book URLs. Once the books are loaded, we will call
    `evaluate` so we can get all the links with `a.card-body`, and then, using the
    `map` function, we will return the `href` attribute of the link, which will give
    us the URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scraping books won''t be that different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here we are applying everything we have already learned in this book. We go
    to a page, wait for a selector, and then we will call the `evaluate` function,
    which will return an object. We haven't used the `evaluate` function in this way
    yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside `evaluate` we get the prices using the`.price-list__item .price-list__price`
    CSS selector, and we get the book title using the`.product-info__title` CSS selector.
    Then, if the product name is `"Print + eBook"`, because the site also offers videos,
    we return an object with three properties: `book`, `print`, and `ebook`.'
  prefs: []
  type: TYPE_NORMAL
- en: The last thing to highlight is that we are wrapping the code in a `try/catch`
    block. If we fail in fetching one book, we don't want the entire program to fail.
  prefs: []
  type: TYPE_NORMAL
- en: The main function will collect those results and then save them to the file
    using `fs.writeFile`. In order to use that function, you will need to import `fs`
    by adding `const fs = require('fs');` in the first line of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'If everything goes as expected, we will get a `prices.json` file with something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: And we have our first scraper. From there, you have the data in the filesystem
    ready to be analyzed by other tools.
  prefs: []
  type: TYPE_NORMAL
- en: Can this be made better? Yes, it can. We could try to see whether we can do
    some parallel scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Running scrapers in parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I'm not saying this just because I coded it, but our scraper has a pretty good
    structure. Every piece is separated into different functions, making it easy to
    identify which parts can run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: I don't want to sound repetitive, but remember, the site being scraped, in this
    case, Packt, is our friend and even my publisher. We don't affect the site; we
    want to look like normal users. We don't want to run 1,000 calls in parallel.
    We don't need to do that. So, we will try to run our scraper in parallel but with
    caution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that we don''t have to code a parallel architecture to solve
    this. We will use a package called **puppeteer-cluster** ([https://www.npmjs.com/package/puppeteer-cluster](https://www.npmjs.com/package/puppeteer-cluster)).
    This is what this library does according to the description at npmjs:'
  prefs: []
  type: TYPE_NORMAL
- en: Handles crawling errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto restarts the browser in case of a crash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can automatically retry if a job fails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offers different concurrency models to choose from (pages, contexts, browsers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is simple to use, small boilerplate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offers progress view and monitoring statistics (see the following code snippet)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sounds pretty promising. Let''s see how we can implement it. First, we need
    to install the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'That will get us the package ready to be used. You can get the code of this
    section in the `crawler-with-cluster.js` file. Let''s import the cluster in our
    scraper by calling `require` in the first line of our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have imported the `Cluster` class, we can create a new **cluster**
    in the main function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Cluster.launch` function has many options, but I think that, for now,
    we only need to know about these options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`concurrency` will tell the cluster the level of isolation we want to use.
    The default is `Cluster.CONCURRENCY_CONTEXT`. These are all the options available:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) Using `Cluster.CONCURRENCY_CONTEXT`, each job will have its own context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Using `Cluster.CONCURRENCY_PAGE`, each job will have its own page, but the
    same context will be shared across all jobs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Using `Cluster.CONCURRENCY_BROWSER`, each job will have its own browser.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`maxConcurrency` will help us set how many tasks we want to run simultaneously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `retryLimit`, we can set how many times the cluster will run a task if
    it fails. The default is 0, but we will give it one more chance to do the task,
    setting this to 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we set the `monitor` option to `true`, we will get a nice console output,
    showing the current process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last option we will cover here is `puppeteerOptions`. The cluster will pass
    this object to the `puppeteer.launch` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One thing that the package description mentions is that it supports error handling.
    Let''s add the error handling they have in the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: That looks pretty solid. When a task fails, the cluster will fire a `taskerror`
    event. There we can see the error, the data, and whether the action will be retried.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t need to change how we download and process `sitemap.xml`. There is
    nothing to change there. But once we have the category, instead of calling the
    `getBooks` function, we will use `queue` for that task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We are telling the cluster that we need to run `getBooks` by passing that `categoryURL`.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have more good news. Our scraping functions are almost ready to be used in
    a cluster â **almost ready**. We need to change four things:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: First, we changed the signature a little bit. Instead of expecting `(page, categoryURL)`,
    it will expect an object with a `page` property and a `data` property, where `page`
    will be the page created and managed by the cluster, and the `data` property will
    be the `categoryURL` instance we passed in when we queued the task.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The first argument to pass to the `queue` function doesn't need to be a URL.
    It doesn't even need to be a string. You can pass in any object, and the function
    will get that object in the `data` property.
  prefs: []
  type: TYPE_NORMAL
- en: The second thing we had to do was to add the call to `setUserAgent` as the page
    is created by the cluster itself.
  prefs: []
  type: TYPE_NORMAL
- en: Then, instead of returning the list of books, we added more tasks to the queue,
    but in this case, we enqueued the `getPrice` function, passing the `book` URL.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we had to do is remove the `try/catch` block because the cluster
    will handle that for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to update the `getPrice` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We did pretty much the same. We changed the signature, added the call to `setUserAgent`,
    removed `try/catch`, and instead of returning the price, we are pushing to the
    `prices` array inside the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to wait for the cluster to finish its work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The call to `idle` will wait for all the tasks to complete, and then the `close`
    function will close the browser and the cluster. Let's see if all this works!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output of `puppeteer-cluster` is amazing. We can see the elapsed time, the
    progress, and what the workers are processing.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we played by the rules. But what if we want to avoid being detected
    as scrapers? Let's find that out.
  prefs: []
  type: TYPE_NORMAL
- en: How to avoid being detected as a bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hesitated about adding this section after everything I mentioned about scraping
    ethics. I think I made my point clear when I said that when the owner says no,
    it means no. But if I'm writing a chapter about scraping, I think I need to show
    you these tools. It's then up to you what to do with the information you have
    learned so far.
  prefs: []
  type: TYPE_NORMAL
- en: Websites that don't want to be scraped, and are being actively scraped, will
    invest a good amount of time and money in trying not to be scraped. The effort
    would become even more important if the scrapers damage not only the site's performance
    but also the business.
  prefs: []
  type: TYPE_NORMAL
- en: Developers in charge of dealing with bots won't rely only on the user agent
    because, as we saw, that could be easily manipulated. They should rely only on
    evaluating the number of requests from an IP because, as we also saw, scrapers
    can slow down their scripts, simulating an interested user.
  prefs: []
  type: TYPE_NORMAL
- en: If the site can't stop scrapers by checking the user agent and monitoring traffic
    spikes, they would try to catch scrapers using different techniques.
  prefs: []
  type: TYPE_NORMAL
- en: They would begin by introducing CAPTCHAs. But, as we will see in this section,
    scrapers can solve some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Then, they would try to evaluate the time between requests. Did you click a
    link after 500 milliseconds? Did you fill a form in less than 1 second? You might
    be a bot.
  prefs: []
  type: TYPE_NORMAL
- en: They could also add JavaScript code to check your browser capabilities. Don't
    you have plugins? Not even the ones that come with Chrome by default? Don't you
    have a language set? You might be a bot.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, they will try to set traps to catch you. For instance, Packt knows
    that you might be scraping links using the `a.card-body` CSS selector. They could
    add a hidden link with that selector, but in that case, the link's URL could be
    https://www.packtpub.com/bot-detected. If you got to the bot-detected URL, you
    would get caught. In the case of forms, they could add hidden inputs that a typical
    user wouldn't complete because it is hidden. If the server gets a value in that
    hidden input, sorryâyou were caught again.
  prefs: []
  type: TYPE_NORMAL
- en: This is a cat-and-mouse game. The mouse will always try to find new ways to
    sneak in, and the cat will work hard to cover the holes in the wall.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, let's see what tools we have if we are the mouse in this game.
  prefs: []
  type: TYPE_NORMAL
- en: 'Antoine Vastel has a great bot detection demo page ([https://arh.antoinevastel.com/bots/areyouheadless](https://arh.antoinevastel.com/bots/areyouheadless)).
    You can get the code of this section in the `bot.js` file. Let''s try to take
    a screenshot of that page using Puppeteer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Simple Puppeteer code. We open the browser in headless mode, navigate to the
    page, and take a screenshot. Let''s see what the screenshot looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Antoine Vastelâs bot detection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.06_B16113.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Antoine Vastel's bot detection
  prefs: []
  type: TYPE_NORMAL
- en: Antoine got us. We were detected as a bot, but that's not the end of the game.
    There are a few things we can do. Let's start by incorporating `puppeteer-extra`
    ([https://github.com/berstend/puppeteer-extra](https://github.com/berstend/puppeteer-extra)).
    The `puppeteer-extra` package allows us to add plugins to Puppeteer. This package
    will allow us to use the `puppeteer-extra-plugin-stealth` plugin ([https://www.npmjs.com/package/puppeteer-extra-plugin-stealth](https://www.npmjs.com/package/puppeteer-extra-plugin-stealth)).
    This package is like a mouse master in this game. It will add all the tricks (or
    many tricks), so our code is not detected as a bot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is install those two packages from the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can replace this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We replace it with these three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: There, we import `puppeteer`, but from `puppeteer-extra`. Then, we import the
    stealth plugin and install it using the `use` function. And that's it!
  prefs: []
  type: TYPE_NORMAL
- en: '![Antoine Vastelâs bot detection bypassed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.07_B16113.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Antoine Vastel's bot detection bypassed
  prefs: []
  type: TYPE_NORMAL
- en: The `puppeteer-extra-plugin-stealth` package is not bulletproof. As I mentioned
    before, it's a cat-and-mouse game. There are many other extras you can use. You
    can see the full list in the package's repository ([https://www.hardkoded.com/ui-testing-with-puppeteer/puppeteer-extra-packages](https://www.hardkoded.com/ui-testing-with-puppeteer/puppeteer-extra-packages)).
    There, you can find `puppeteer-extra-plugin-anonymize-ua`, which will change the
    user agent in all the pages, or `puppeteer-extra-plugin-recaptcha`, which will
    try to solve reCAPTCHA ([https://www.google.com/recaptcha/about/](https://www.google.com/recaptcha/about/))
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: A scraping chapter wouldn't be complete if we didn't talk, at least a little,
    about how to deal with authorization.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with authorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authentication and authorization is a vast topic in web development. Authentication
    is how a website can identify you. To make it simple, it's the login. On the other
    hand, authorization is what you can do on the site once you are authenticated,
    for instance, checking whether you have access to a specific page.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many types of authentication modes. We covered the simplest one in
    this book: a user and password login page. But things can get more complicated.
    Testing integration with Facebook or single sign-on logins could be quite challenging,
    but they would be about automating user interaction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one authentication method that you won''t be able to perform by automating
    the DOMâ**HTTP basic authentication**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![HTTP basic authentication'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.08_B16113.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP basic authentication
  prefs: []
  type: TYPE_NORMAL
- en: 'That login popup is not popular these days. In fact, I don''t think they ever
    were popular. But you might have seen them if you have set up a router. That modal
    is like the dialogs we saw in [*Chapter 5*](B16113_05_Final_SK_ePub.xhtml#_idTextAnchor087),
    *Waiting for elements and network calls*. Puppeteer won''t help us out with this
    authentication because there is no HTML to automate there. Luckily for us, automating
    this is easy. You can get the code of this section in the `authentication.js`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The only thing we need to do to authenticate into https://ptsv2.com/t/ui-testing-puppeteer/post
    is to call the `authenticate` function before calling `goto`. The `authenticate`
    function expects an object with two properties: `username` and `password`.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we are authenticated, we need to tell the server who we are on every request,
    so they can authorize us (or not) to perform certain tasks. The web server is,
    in theory, stateless. It doesn't have a way to know who we are unless 1) they
    inject some information in their responses, using cookies, or 2) we tell them.
    The most common way is with HTTP Headers. But that can be solved by passing a
    key as a Query String argument or as part of the HTTP post data.
  prefs: []
  type: TYPE_NORMAL
- en: When you want to alter the authentication data, you need to get that information
    elsewhere. You might need to open your browser, log in to the site you want to
    scrape, and extract the authentication data from there, so you can then use that
    data in your scraper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that you want to scrape the Packt website, but this time you want
    to scrape it being logged in. So, you open your browser, log in and then you can
    use a tool such as the *Export cookie JSON file for Puppeteer* extension (you
    can find it with that name in the Chrome web store) to export all the cookies
    generated by the site. Once we have the JSON file named `account.packtpub.com.cookies.json`
    with all the cookies, you can copy that file into your workspace and do something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The new element in this code is a call to the `setCookie` function. That function
    expects a list of cookies. As we have all the cookies in a JSON file, we load
    that JSON file and pass the content to the `setCookie` function. Let''s take a
    look at what a cookie looks like inside that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The structure is quite simple and straightforward. You don't need to use an
    extension and load a cookie from a JSON file. You can call the `setCookie` function
    passing an object with the `name`, `value`, `domain`, `path`, and `expires` properties
    (the latter is a Unix time in seconds), whether it's `httpOnly`, and whether it
    should be marked as `secure`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to see how we can handle authorizations implemented using HTTP
    headers. You might find sites using the `authorization` HTTP header to pass some
    kind of user identifier. The `authorization` header would look something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'According to MDN ([https://www.hardkoded.com/ui-testing-with-puppeteer/authentication-schemes](https://www.hardkoded.com/ui-testing-with-puppeteer/authentication-schemes)),
    you can find the following types: `Basic`, `Bearer`, `Digest`, `HOBA`, `Mutual`,
    and `AWS4-HMAC-SHA256`. If those names sound scary, don''t worry about that. There
    is a high chance that you will only see the `Bearer` type. What would be the credentials?
    Well, that''s what you will need to find out while coding your scraper. You would
    need to see what information is being sent when you use a website for real and
    try to mimic that.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will use `Basic` because that''s the same HTTP basic authentication
    we saw before. When you log in using the authentication popup, the browser will
    send the authorization header passing `basic` and `username:password` in Base64\.
    In our example, the username was `user`, and the password was `password`. So,
    we can use any Base64 encoder available, for instance, [https://www.base64encode.net/](https://www.base64encode.net/),
    and get `user:password` in base64: `dXNlcjpwYXNzd29yZA==`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can inject this header in two ways. The first one is using the `setExtraHTTPHeaders`
    function. You can see this code in the `header-inject.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`setExtraHTTPHeaders` expects an object where the property name is the header
    name, and the value is the header value. Here we are adding the `authorization`
    header with the value `''basic dXNlcjpwYXNzd29yZA==''`. And that''s it. Puppeteer
    will add that header to every request that the page will make.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if the site we are trying to scrape needs an authorization header
    not in every request, but only in some of them? Well, it will be quite tricky,
    but not that hard. You can follow this code in the `header-inject2.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We are first telling Puppeteer we want to intercept every request the page will
    make. We do that by calling the `setRequestInterception` function with the first
    argument as `true`. Then we start listening to the `request` event. If the request
    meets the condition we need, in this case, if it matches our URL, we create an
    `overrides` object with a `headers` property and then call the `continue` function
    of the request object. We cannot override the headers. The `overrides` object
    can also have the `url`, `method` (the HTTP method), and `postData` properties.
  prefs: []
  type: TYPE_NORMAL
- en: The request object also has a function called `abort`. With this function, you
    can cancel that request. For instance, you could check whether the request is
    an image and `abort` it. The result will be a website with no images.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: If you call `setRequestInterception`, you need to implement a `request` event
    listener. And you need to `continue` or `abort` every request you listen to.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned when I opened this section, this doesn't cover all the different
    authentication and authorization schemes, but it will have you covered in more
    than 90% of cases. Now it's time for a wrap-up.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although this is not a scraping book, we covered a lot of ground here. I hope
    the first section gave you a good idea of what scraping is, as well as covering
    what you can do and what you shouldn''t do. We also learned how to create our
    own scrapers. We created a crawler in less than 100 lines. We added two new tools
    to our toolbox: `puppeteer-cluster` and `puppeteer-extra`. We closed this chapter
    learning a little bit about authentication and authorization, giving you almost
    everything you need to get started in the scraping world.'
  prefs: []
  type: TYPE_NORMAL
- en: If you weren't that excited about scraping before this chapter, I hope it is
    the spark that will make you start creating your own scrapers. If you knew about
    scraping, I hope this chapter gave you more tools to scrape as a professional.
  prefs: []
  type: TYPE_NORMAL
- en: Our next and final chapter will be about performance and how we can measure
    it using Puppeteer.
  prefs: []
  type: TYPE_NORMAL
