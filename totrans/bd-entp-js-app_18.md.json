["```js\n$ cat /proc/sys/kernel/pid_max\n32768\n```", "```js\n$ docker network ls\nNETWORK ID NAME DRIVER SCOPE\nd764e66872cf bridge bridge local\n61bc0ca692fc host host local\nbdbbf199a4fe none null local\n\n$ docker network inspect bridge --format='{{range $index, $container := .Containers}}{{.Name}} {{end}}'\nelasticsearch hobnob\n```", "```js\n$ docker run --env-file ./.env --name hobnob -d -p 8080:8080 hobnob:0.1.0\n```", "```js\n$ sudo swapoff -a\n```", "```js\n$ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.10.3/bin/linux/amd64/kubectl && chmod +x ./kubectl && sudo mv ./kubectl /usr/local/bin/kubectl\n```", "```js\n$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.3\", GitCommit:\"2bba0127d85d5a46ab4b778548be28623b32d0b0\", GitTreeState:\"clean\", BuildDate:\"2018-05-21T09:17:39Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n```", "```js\n$ echo \"source <(kubectl completion bash)\" >> ~/.bashrc\n```", "```js\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.27.0/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/\n```", "```js\n$ base=https://github.com/docker/machine/releases/download/v0.14.0 &&\n curl -L $base/docker-machine-$(uname -s)-$(uname -m) >/tmp/docker-machine &&\n sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n```", "```js\n$ docker-machine version\ndocker-machine version 0.14.0, build 89b8332\n```", "```js\nexport MINIKUBE_WANTUPDATENOTIFICATION=false\nexport MINIKUBE_WANTREPORTERRORPROMPT=false\nexport MINIKUBE_HOME=$HOME\nexport CHANGE_MINIKUBE_NONE_USER=true\nexport KUBECONFIG=$HOME/.kube/config\n```", "```js\n$ . .profile\n```", "```js\n$ mkdir -p $HOME/.kube\n$ touch $HOME/.kube/config\n```", "```js\n$ sudo -E minikube start --vm-driver=none\nStarting local Kubernetes v1.10.0 cluster...\nStarting VM...\nGetting VM IP address...\nMoving files into cluster...\nSetting up certs...\nConnecting to cluster...\nSetting up kubeconfig...\nStarting cluster components...\nKubectl is now configured to use the cluster.\n```", "```js\n$ kubectl get nodes\nNAME STATUS ROLES AGE VERSION\nminikube Ready master 15m v1.10.0\n```", "```js\n$ docker ps -a --format \"table {{.ID}}\\t{{.Image}}\\t{{.Command}}\\t{{.Names}}\"\nCONTAINER ID        IMAGE                        COMMAND                  NAMES\n3ff67350410a        4689081edb10                 \"/storage-provisioner\"   k8s_storage-provisioner_storage-provisioner_kube-system_4d9c2fa3-627a-11e8-a0e4-54e1ad13e25a_0\nec2922978b10        e94d2f21bc0c                 \"/dashboard --insecu…\"   k8s_kubernetes-dashboard_kubernetes-dashboard-5498ccf677-sslhz_kube-system_4d949c82-627a-11e8-a0e4-54e1ad13e25a_0\nf9f5b8fe1a41        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_storage-provisioner_kube-system_4d9c2fa3-627a-11e8-a0e4-54e1ad13e25a_0\nf5b013b0278d        6f7f2dc7fab5                 \"/sidecar --v=2 --lo…\"   k8s_sidecar_kube-dns-86f4d74b45-hs88j_kube-system_4cbede66-627a-11e8-a0e4-54e1ad13e25a_0\nf2d120dce2ed        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_kubernetes-dashboard-5498ccf677-sslhz_kube-system_4d949c82-627a-11e8-a0e4-54e1ad13e25a_0\n50ae3b880b4a        c2ce1ffb51ed                 \"/dnsmasq-nanny -v=2…\"   k8s_dnsmasq_kube-dns-86f4d74b45-hs88j_kube-system_4cbede66-627a-11e8-a0e4-54e1ad13e25a_0\na8f677cdc43b        80cc5ea4b547                 \"/kube-dns --domain=…\"   k8s_kubedns_kube-dns-86f4d74b45-hs88j_kube-system_4cbede66-627a-11e8-a0e4-54e1ad13e25a_0\nd287909bae1d        bfc21aadc7d3                 \"/usr/local/bin/kube…\"   k8s_kube-proxy_kube-proxy-m5lrh_kube-system_4cbf007c-627a-11e8-a0e4-54e1ad13e25a_0\ne14d9c837ae4        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_kube-dns-86f4d74b45-hs88j_kube-system_4cbede66-627a-11e8-a0e4-54e1ad13e25a_0\n896beface410        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_kube-proxy-m5lrh_kube-system_4cbf007c-627a-11e8-a0e4-54e1ad13e25a_0\n9f87d1105edb        52920ad46f5b                 \"etcd --listen-clien…\"   k8s_etcd_etcd-minikube_kube-system_a2c07ce803646801a9f5a70371449d58_0\n570a4e5447f8        af20925d51a3                 \"kube-apiserver --ad…\"   k8s_kube-apiserver_kube-apiserver-minikube_kube-system_8900f73fb607cc89d618630016758228_0\n87931be974c0        9c16409588eb                 \"/opt/kube-addons.sh\"    k8s_kube-addon-manager_kube-addon-manager-minikube_kube-system_3afaf06535cc3b85be93c31632b765da_0\n897928af3c85        704ba848e69a                 \"kube-scheduler --ad…\"   k8s_kube-scheduler_kube-scheduler-minikube_kube-system_31cf0ccbee286239d451edb6fb511513_0\nb3a7fd175e47        ad86dbed1555                 \"kube-controller-man…\"   k8s_kube-controller-manager_kube-controller-manager-minikube_kube-system_c871518ac418f1edf0247e23d5b99a40_0\nfd50ec94b68f        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_kube-apiserver-minikube_kube-system_8900f73fb607cc89d618630016758228_0\n85a38deae7ad        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_etcd-minikube_kube-system_a2c07ce803646801a9f5a70371449d58_0\n326fd83d6630        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_kube-addon-manager-minikube_kube-system_3afaf06535cc3b85be93c31632b765da_0\ne3dd5b372dab        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_kube-scheduler-minikube_kube-system_31cf0ccbee286239d451edb6fb511513_0\n6c2ac7c363d0        k8s.gcr.io/pause-amd64:3.1   \"/pause\"                 k8s_POD_kube-controller-manager-minikube_kube-system_c871518ac418f1edf0247e23d5b99a40_0\n```", "```js\n$ sudo systemctl status kubelet.service\n● kubelet.service - kubelet: The Kubernetes Node Agent\n Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enable\n Drop-In: /etc/systemd/system/kubelet.service.d\n └─10-kubeadm.conf\n Active: active (running) since Mon 2018-05-28 14:22:59 BST; 2h 5min ago\n Docs: http://kubernetes.io/docs/\n Main PID: 23793 (kubelet)\n Tasks: 18 (limit: 4915)\n Memory: 55.5M\n CPU: 8min 28.571s\n CGroup: /system.slice/kubelet.service\n └─23793 /usr/bin/kubelet --fail-swap-on=false --allow-privileged=true --clu\n```", "```js\n$ minikube status\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 10.122.98.148\n\n$ kubectl cluster-info\nKubernetes master is running at https://10.122.98.148:8443\nKubeDNS is running at https://10.122.98.148:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n```", "```js\n$ kubectl cluster-info\nKubernetes master is running at https://10.122.35.199:8443\nUnable to connect to the server: dial tcp 10.122.35.199:8443: connect: network is unreachable\n```", "```js\n$ minikube status\nminikube: Running\ncluster: Running\nkubectl: Misconfigured: pointing to stale minikube-vm.\nTo fix the kubectl context, run minikube update-context\n```", "```js\n$ minikube update-context\nReconfigured kubeconfig IP, now pointing at 192.168.1.11\n```", "```js\n$ kubectl cluster-info\nKubernetes master is running at https://192.168.1.11:8443\nKubeDNS is running at https://192.168.1.11:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n```", "```js\n$ sudo kubeadm reset\n[preflight] Running pre-flight checks.\n[reset] Stopping the kubelet service.\n[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n[reset] Removing kubernetes-managed containers.\n[reset] No etcd manifest found in \"/etc/kubernetes/manifests/etcd.yaml\". Assuming external etcd.\n[reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]\n[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]\n[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n```", "```js\n$ sudo -E minikube start --vm-driver=none\n$ minikube status\nminikube: Running\ncluster: Running\nkubectl: Correctly Configured: pointing to minikube-vm at 192.168.1.11\n$ kubectl cluster-info\nKubernetes master is running at https://192.168.1.11:8443\nKubeDNS is running at https://192.168.1.11:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n```", "```js\n$ docker ps -a \\\n --filter \"name=elasticsearch\" \\\n --format \"table {{.ID}}\\t{{.Image}}\\t{{.Command}}\\t{{.Names}}\"\nCONTAINER ID IMAGE COMMAND NAMES\n```", "```js\n$ kubectl run elasticsearch --image=docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2 --port=9200 --port=9300\ndeployment.apps \"elasticsearch\" created\n```", "```js\n$ kubectl get pods\nNAME READY STATUS RESTARTS AGE\nelasticsearch-656d7c98c6-s6v58 0/1 ContainerCreating 0 9s\n```", "```js\n$ kubectl get pods\nNAME READY STATUS RESTARTS AGE\nelasticsearch-656d7c98c6-s6v58 1/1 Running 0 1m\n```", "```js\ndeployment.apps \"elasticsearch\" created\n```", "```js\n$ kubectl run <name> --image=<image>\n$ kubectl create deployment <name> --image=<image>\n```", "```js\n$ kubectl get deployments\nNAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nelasticsearch   1         1         1            1           2s\n```", "```js\n$ kubectl delete deployment elasticsearch\n```", "```js\n$ kubectl get deployments\nNo resources found.\n```", "```js\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: elasticsearch\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      name: elasticsearch\n      labels:\n        app: elasticsearch\n    spec:\n      containers:\n      - name: elasticsearch\n        image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2\n        ports:\n        - containerPort: 9200\n        - containerPort: 9300\n```", "```js\nselector:\n  matchLabels:\n    app: elasticsearch\n```", "```js\n$ kubectl apply -f manifests/elasticsearch/deployment.yaml\ndeployment.apps \"elasticsearch\" created\n```", "```js\n$ kubectl get deployments\nNAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nelasticsearch   3         3         3            0           2s\n```", "```js\n$ kubectl rollout status deployment/elasticsearch\nWaiting for rollout to finish: 0 of 3 updated replicas are available...\nWaiting for rollout to finish: 1 of 3 updated replicas are available...\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\ndeployment \"elasticsearch\" successfully rolled out\n```", "```js\n$ kubectl get deployments\nNAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nelasticsearch   3         3         3            3           2m\n```", "```js\n$ kubectl get rs\nNAME                       DESIRED   CURRENT   READY     AGE\nelasticsearch-699c7dd54f   3         3         3         3m\n```", "```js\n<deployment-name>-<pod-template-hash>\n```", "```js\n$ kubectl get pods --show-labels\nNAME                             READY  STATUS    LABELS\nelasticsearch-699c7dd54f-n5tmq   1/1    Running   app=elasticsearch,pod-template-hash=2557388109\nelasticsearch-699c7dd54f-pft9k   1/1    Running   app=elasticsearch,pod-template-hash=2557388109\nelasticsearch-699c7dd54f-pm2wz   1/1    Running   app=elasticsearch,pod-template-hash=2557388109\n```", "```js\n$ kubectl describe pods elasticsearch-699c7dd54f-n5tmq\nName: elasticsearch-699c7dd54f-n5tmq\nNamespace: default\nNode: minikube/10.122.98.143\nLabels: app=elasticsearch\n pod-template-hash=2557388109\nAnnotations: <none>\nStatus: Running\nIP: 172.17.0.5\nControlled By: ReplicaSet/elasticsearch-699c7dd54f\nContainers:\n elasticsearch:\n Container ID: docker://ee5a3000a020c91a04fa02ec50b86012f2c27376b773bbf7be4c9ebce9c2551f\n Image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.4\n Image ID: docker-pullable://docker.elastic.co/elasticsearch/elasticsearch-oss@sha256:2d9c774c536bd1f64abc4993ebc96a2344404d780cbeb81a8b3b4c3807550e57\n Ports: 9200/TCP, 9300/TCP\n Host Ports: 0/TCP, 0/TCP\n State: Running\n Ready: True\n Restart Count: 0\n Environment: <none>\n Mounts:\n /var/run/secrets/kubernetes.io/serviceaccount from default-token-26tl8 (ro)\nConditions:\n Type Status\n Initialized True\n Ready True\n PodScheduled True\nVolumes:\n default-token-26tl8:\n Type: Secret (a volume populated by a Secret)\n SecretName: default-token-26tl8\n Optional: false\nQoS Class: BestEffort\nEvents:\n Type Reason Age From Message\n ---- ------ ---- ---- -------\n Normal Scheduled 1m default-scheduler Successfully assigned elasticsearch-699c7dd54f-n5tmq to minikube\n Normal SuccessfulMountVolume 1m kubelet, minikube MountVolume.SetUp succeeded for volume \"default-token-26tl8\"\n Normal Pulled 1m kubelet, minikube Container image \"docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.4\" already present on machine\n Normal Created 1m kubelet, minikube Created container\n Normal Started 1m kubelet, minikube Started container\n```", "```js\n$ curl http://172.17.0.5:9200/\n{\n \"name\" : \"CKaMZGV\",\n \"cluster_name\" : \"docker-cluster\",\n \"cluster_uuid\" : \"dCAcFnvOQFuU8pTgw4utwQ\",\n \"version\" : {\n \"number\" : \"6.3.2\",\n \"lucene_version\" : \"7.3.1\"\n ...\n },\n \"tagline\" : \"You Know, for Search\"\n}\n```", "```js\n$ kubectl get pods -l app=elasticsearch -o=custom-columns=NAME:.metadata.name,IP:.status.podIP\nNAME IP\nelasticsearch-699c7dd54f-pft9k 172.17.0.4\nelasticsearch-699c7dd54f-n5tmq 172.17.0.5\nelasticsearch-699c7dd54f-pm2wz 172.17.0.6\n\n$ curl http://172.17.0.4:9200/\n{\n \"name\" : \"TscXyKK\",\n \"cluster_name\" : \"docker-cluster\",\n \"cluster_uuid\" : \"zhz6Ok_aQiKfqYpzsgp7lQ\",\n ...\n}\n$ curl http://172.17.0.6:9200/\n{\n \"name\" : \"_nH26kt\",\n \"cluster_name\" : \"docker-cluster\",\n \"cluster_uuid\" : \"TioZ4wz4TeGyflOyu1Xa-A\",\n ...\n}\n```", "```js\n$ curl -X PUT \"172.17.0.6:9200/test/doc/1\" -H 'Content-Type: application/json' -d '{\"foo\":\"bar\"}'\n{\"_index\":\"test\",\"_type\":\"doc\",\"_id\":\"1\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":0,\"_primary_term\":1}\n```", "```js\n$ curl \"172.17.0.6:9200/test/doc/1\"\n{\"_index\":\"test\",\"_type\":\"doc\",\"_id\":\"1\",\"_version\":1,\"found\":true,\"_source\":{\"foo\":\"bar\"}}\n\n$ curl \"172.17.0.5:9200/test/doc/1\"\n{\"error\":{\"type\":\"index_not_found_exception\",\"reason\":\"no such index\",\"index\":\"test\"},\"status\":404}\n\n$ curl \"172.17.0.4:9200/test/doc/1\"\n{\"error\":{\"type\":\"index_not_found_exception\",\"reason\":\"no such index\",\"index\":\"test\"},\"status\":404}\n```", "```js\ncontainers:\n- name: elasticsearch\n  image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2\n  ports:\n  - containerPort: 9200\n  - containerPort: 9300\n  env:\n  - name: discovery.zen.ping.unicast.hosts\n    value: \"\"\n```", "```js\nkind: StatefulSet\n```", "```js\n<statefulset-name>-<ordinal>\n```", "```js\napiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch\nspec:\n  selector:\n    app: elasticsearch\n  clusterIP: None\n  ports:\n  - port: 9200\n    name: rest\n  - port: 9300\n    name: transport\n```", "```js\n$ kubectl apply -f manifests/elasticsearch/service.yaml\nservice \"elasticsearch\" created\n```", "```js\n$ kubectl get services\nNAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE\nelasticsearch   ClusterIP   None         <none>        9200/TCP,9300/TCP   46s\nkubernetes      ClusterIP   10.96.0.1    <none>        443/TCP             4h\n```", "```js\n$ kubectl delete deployment elasticsearch\n```", "```js\n...\nspec:\n  replicas: 3\n  serviceName: elasticsearch\n  ...\n```", "```js\n<service-name>.<namespace>.svc.<cluster-domain>\n```", "```js\n<pod-name>.<service-domain>\n```", "```js\n<statefulset-name>-<ordinal>.<service-name>.<namespace>.svc.<cluster-domain>\n```", "```js\nelasticsearch-0.elasticsearch.default.svc.cluster.local\nelasticsearch-1.elasticsearch.default.svc.cluster.local\nelasticsearch-2.elasticsearch.default.svc.cluster.local\n```", "```js\nenv:\n  - name: discovery.zen.ping.unicast.hosts\n    value: \"elasticsearch-0.elasticsearch.default.svc.cluster.local,elasticsearch-1.elasticsearch.default.svc.cluster.local,elasticsearch-2.elasticsearch.default.svc.cluster.local\"\n```", "```js\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch\nspec:\n  replicas: 3\n  serviceName: elasticsearch\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      name: elasticsearch\n      labels:\n        app: elasticsearch\n    spec:\n      containers:\n        - name: elasticsearch\n          image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2\n          ports:\n            - containerPort: 9200\n            - containerPort: 9300\n          env:\n            - name: discovery.zen.ping.unicast.hosts\n              value: \"elasticsearch-0.elasticsearch.default.svc.cluster.local,elasticsearch-1.elasticsearch.default.svc.cluster.local,elasticsearch-2.elasticsearch.default.svc.cluster.local\"\n```", "```js\n$ kubectl apply -f manifests/elasticsearch/stateful-set.yaml\nstatefulset.apps \"elasticsearch\" created\n```", "```js\n$ kubectl get statefulsets\nNAME            DESIRED   CURRENT   AGE\nelasticsearch   3         3         42s\n```", "```js\n$ kubectl get pods\nNAME              READY     STATUS    RESTARTS   AGE\nelasticsearch-0   1/1       Running   0          1m\nelasticsearch-1   1/1       Running   0          1m\nelasticsearch-2   1/1       Running   0          1m\n```", "```js\n$ kubectl get pods -l app=elasticsearch -o=custom-columns=NAME:.metadata.name,IP:.status.podIP\nNAME IP\nelasticsearch-0 172.17.0.4\nelasticsearch-1 172.17.0.5\nelasticsearch-2 172.17.0.6\n```", "```js\n$ curl -s $(kubectl get pod elasticsearch-0 -o=jsonpath='{.status.podIP}'):9200 | jq -r '.cluster_uuid'\neeDC2IJeRN6TOBr227CStA\n```", "```js\n$ curl -s $(kubectl get pod elasticsearch-1 -o=jsonpath='{.status.podIP}'):9200 | jq -r '.cluster_uuid'\neeDC2IJeRN6TOBr227CStA\n\n$ curl -s $(kubectl get pod elasticsearch-2 -o=jsonpath='{.status.podIP}'):9200 | jq -r '.cluster_uuid'\neeDC2IJeRN6TOBr227CStA\n```", "```js\n$ curl \"$(kubectl get pod elasticsearch-2 -o=jsonpath='{.status.podIP}'):9200/_cluster/state/master_node,nodes/?pretty\"\n{\n \"cluster_name\" : \"docker-cluster\",\n \"compressed_size_in_bytes\" : 874,\n \"master_node\" : \"eq9YcUzVQaiswrPbwO7oFg\",\n \"nodes\" : {\n \"lp4lOSK9QzC3q-YEsqwRyQ\" : {\n \"name\" : \"lp4lOSK\",\n \"ephemeral_id\" : \"e58QpjvBR7iS15FhzN0zow\",\n \"transport_address\" : \"172.17.0.5:9300\",\n \"attributes\" : { }\n },\n \"eq9YcUzVQaiswrPbwO7oFg\" : {\n \"name\" : \"eq9YcUz\",\n \"ephemeral_id\" : \"q7zlTKCqSo2qskkY8oSStw\",\n \"transport_address\" : \"172.17.0.4:9300\",\n \"attributes\" : { }\n },\n \"77CpcuDDSom7hTpWz8hBLQ\" : {\n \"name\" : \"77CpcuD\",\n \"ephemeral_id\" : \"-yq7bhphQ5mF5JX4qqXHoQ\",\n \"transport_address\" : \"172.17.0.6:9300\",\n \"attributes\" : { }\n }\n }\n}\n```", "```js\n$ curl -X PUT \"$(kubectl get pod elasticsearch-0 -o=jsonpath='{.status.podIP}'):9200/test/doc/1\" -H 'Content-Type: application/json' -d '{\"foo\":\"bar\"}'\n{\"_index\":\"test\",\"_type\":\"doc\",\"_id\":\"1\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":2,\"failed\":0},\"_seq_no\":0,\"_primary_term\":1}\n```", "```js\n$ curl \"$(kubectl get pod elasticsearch-1 -o=jsonpath='{.status.podIP}'):9200/test/doc/1\"\n{\"_index\":\"test\",\"_type\":\"doc\",\"_id\":\"1\",\"_version\":1,\"found\":true,\"_source\":{\"foo\":\"bar\"}}\n```", "```js\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: S0tL...FFDENFRJQV0\n    server: https://8b8a5720059.k8s.ondigitalocean.com\n  name: do-nyc1-hobnob\ncontexts:\n- context:\n    cluster: do-nyc1-hobnob\n    user: do-nyc1-hobnob-admin\n  name: do-nyc1-hobnob\ncurrent-context: do-nyc1-hobnob\nkind: Config\npreferences: {}\nusers:\n- name: do-nyc1-hobnob-admin\n  user:\n    client-certificate-data: LUMMmxjaJ...VElGVEo\n    client-key-data: TFyMrS2I...mhoTmV2LS05kRF\n```", "```js\n$ kubectl config current-context\nminikube\n```", "```js\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority: ~/.minikube/ca.crt\n    server: https://10.122.98.148:8443\n  name: minikube\ncontexts:\n- context:\n    cluster: minikube\n    user: minikube\n  name: minikube\ncurrent-context: minikube\nkind: Config\npreferences: {}\nusers:\n- name: minikube\n  user:\n    client-certificate: ~/.minikube/client.crt\n    client-key: ~/.minikube/client.key\n```", "```js\n$ cp downloads/hobnob-kubeconfig.yaml ~/.kube/\n```", "```js\nexport KUBECONFIG=$HOME/.kube/config:$HOME/.kube/hobnob-kubeconfig.yaml\n```", "```js\n$ . ~/.profile\n```", "```js\n$ kubectl config view\napiVersion: v1\nclusters:\n- cluster:\n certificate-authority-data: REDACTED\n server: https://8b8a5720059.k8s.ondigitalocean.com\n name: do-nyc1-hobnob\n- cluster:\n certificate-authority: ~/.minikube/ca.crt\n server: https://10.122.98.148:8443\n name: minikube\ncontexts:\n- context:\n cluster: do-nyc1-hobnob\n user: do-nyc1-hobnob-admin\n name: do-nyc1-hobnob\n- context:\n cluster: minikube\n user: minikube\n name: minikube\ncurrent-context: minikube\nkind: Config\npreferences: {}\nusers:\n- name: do-nyc1-hobnob-admin\n user:\n client-certificate-data: REDACTED\n client-key-data: REDACTED\n- name: minikube\n user:\n client-certificate: ~/.minikube/client.crt\n client-key: ~/.minikube/client.key\n```", "```js\n$ kubectl config use-context do-nyc1-hobnob\nSwitched to context \"do-nyc1-hobnob\".\n```", "```js\n$ kubectl cluster-info\nKubernetes master is running at https://8b8a5720059.k8s.ondigitalocean.com\nKubeDNS is running at https://8b8a5720059.k8s.ondigitalocean.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n```", "```js\n    [INFO ][o.e.b.BootstrapChecks ] [6tcspAO] bound or publishing to a non-loopback address, enforcing bootstrap checks\n    ERROR: [1] bootstrap checks failed\n    [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n    ```", "```js\n$ kubectl get nodes\nNAME STATUS ROLES AGE VERSION\nworker-6000 Ready <none> 17h v1.10.1\nworker-6001 Ready <none> 17h v1.10.1\n....\n```", "```js\n# sysctl -w vm.max_map_count=262144\n# ulimit -n 65536\n```", "```js\n$ sudo apt update\n$ sudo apt install pssh\n```", "```js\n$ parallel-ssh --inline-stdout --user root --host \"$(kubectl get nodes -o=jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}')\" -x \"-o StrictHostKeyChecking=no\" \"sysctl -w vm.max_map_count=262144 && ulimit -n 65536\"\n[1] 23:27:51 [SUCCESS] 142.93.126.236\nvm.max_map_count = 262144\n[2] 23:27:51 [SUCCESS] 142.93.113.224\nvm.max_map_count = 262144\n...\n```", "```js\ninitContainers:\n  - name: increase-max-map-count\n    image: busybox\n    command:\n    - sysctl\n    - -w\n    - vm.max_map_count=262144\n    securityContext:\n      privileged: true\n  - name: increase-file-descriptor-limit\n    image: busybox\n    command:\n    - sh\n    - -c\n    - ulimit -n 65536\n    securityContext:\n      privileged: true\n```", "```js\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch\n  labels:\n    app: elasticsearch\nspec:\n  replicas: 3\n  serviceName: elasticsearch\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      initContainers:\n      - name: increase-max-map-count\n        image: busybox\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        securityContext:\n          privileged: true\n      - name: increase-file-descriptor-limit\n        image: busybox\n        command:\n        - sh\n        - -c\n        - ulimit -n 65536\n        securityContext:\n          privileged: true\n      containers:\n        - name: elasticsearch\n          image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2\n          ports:\n          - containerPort: 9200\n            name: http\n          - containerPort: 9300\n            name: tcp\n          env:\n            - name: discovery.zen.ping.unicast.hosts\n              value: \"elasticsearch-0.elasticsearch.default.svc.cluster.local,elasticsearch-1.elasticsearch.default.svc.cluster.local,elasticsearch-2.elasticsearch.default.svc.cluster.local\"\n```", "```js\n$ kubectl get all\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.32.0.1    <none>        443/TCP   17h\n```", "```js\n$ kubectl apply -f manifests/elasticsearch/service.yaml\nservice \"elasticsearch\" created\n$ kubectl apply -f manifests/elasticsearch/stateful-set.yaml\nstatefulset.apps \"elasticsearch\" created\n```", "```js\n$ kubectl get all\nNAME                  READY     STATUS    RESTARTS   AGE\npod/elasticsearch-0   1/1       Running   0          1m\npod/elasticsearch-1   1/1       Running   0          1m\npod/elasticsearch-2   1/1       Running   0          10s\n\nNAME                    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE\nservice/elasticsearch   ClusterIP   None         <none>        9200/TCP,9300/TCP   1m\nservice/kubernetes      ClusterIP   10.32.0.1    <none>        443/TCP             18h\n\nNAME                             DESIRED   CURRENT   AGE\nstatefulset.apps/elasticsearch   3         3         1m\n```", "```js\n$ kubectl get pods\nNAME              READY   STATUS    RESTARTS   AGE\nelasticsearch-0   1/1     Running   0          34m\nelasticsearch-1   1/1     Running   0          34m\nelasticsearch-2   1/1     Running   0          34m\n```", "```js\n$ kubectl port-forward elasticsearch-0 9200:9200\nForwarding from 127.0.0.1:9200 -> 9200\nForwarding from [::1]:9200 -> 9200\n```", "```js\n$ curl http://localhost:9200/_cluster/state?pretty\n{\n \"cluster_name\" : \"docker-cluster\",\n \"state_uuid\" : \"rTHLkSYrQIu5E6rcGJZpCA\",\n \"master_node\" : \"TcYdL65VSb-W1ZzXPfB8aA\",\n \"nodes\" : {\n \"ns1ZaCTCS9ywDSntHz94vg\" : {\n \"name\" : \"ns1ZaCT\",\n \"ephemeral_id\" : \"PqwcVrldTOyKSfQ-ZfhoUQ\",\n \"transport_address\" : \"10.244.24.2:9300\",\n \"attributes\" : { }\n },\n \"94Q-t8Y8SJiXnwVzsGcdyA\" : {\n \"name\" : \"94Q-t8Y\",\n \"ephemeral_id\" : \"n-7ew1dKSL2LLKzA-chhUA\",\n \"transport_address\" : \"10.244.18.3:9300\",\n \"attributes\" : { }\n },\n \"TcYdL65VSb-W1ZzXPfB8aA\" : {\n \"name\" : \"TcYdL65\",\n \"ephemeral_id\" : \"pcghJOnTSgmB8xMh4DKSHA\",\n \"transport_address\" : \"10.244.75.3:9300\",\n \"attributes\" : { }\n }\n },\n \"metadata\" : {\n \"cluster_uuid\" : \"ZF1t_X_XT0q5SPANvzE4Nw\",\n ...\n },\n ...\n}\n```", "```js\napiVersion: v1\nkind: Pod\nspec:\n  ...\n  volumes:\n  - name: host-volume\n    hostPath:\n      path: /data\n      type: Directory\n```", "```js\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n    - name: elasticsearch\n      image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.4\n      ports:\n        - containerPort: 9200\n        - containerPort: 9300\n      env:\n        - name: discovery.type\n          value: single-node\n      volumeMounts:\n        - mountPath: /usr/share/elasticsearch/data\n          name: host-volume\n  ...\n```", "```js\n$ sudo mkdir data\n$ sudo chown 1000:1000 /data\n```", "```js\n$ tree /data\ndata/\n└── nodes\n └── 0\n ├── node.lock\n └── _state\n ├── global-0.st\n └── node-0.st\n\n3 directories, 3 files\n```", "```js\n$ kubectl get pods\nNAME              READY   STATUS             RESTARTS\nelasticsearch-0   1/1     Running            0 \nelasticsearch-1   0/1     CrashLoopBackOff   7 \nelasticsearch-2   0/1     CrashLoopBackOff   7       \n```", "```js\n$ kubectl logs elasticsearch-1\n[WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]\norg.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/data/docker-cluster]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?\n```", "```js\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: standard\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp2\nreclaimPolicy: Retain\n```", "```js\n$ kubectl get secret\nNAME TYPE DATA AGE\ndefault-token-2r8zr kubernetes.io/service-account-token 3 2h\n\n$ kubectl get storageclass\nNAME PROVISIONER AGE\ndo-block-storage (default) com.digitalocean.csi.dobs 2h\n```", "```js\napiVersion: apps/v1\nkind: StatefulSet\nmetadata: ...\nspec:\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: do-block-storage\n```", "```js\napiVersion: apps/v1\nkind: StatefulSet\nmetadata: ...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      initContainers: ...\n      containers: ...\n      volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n  volumeClaimTemplates: ...\n```", "```js\n- name: fix-volume-permission\n  image: busybox\n  command:\n  - sh\n  - -c\n  - chown -R 1000:1000 /usr/share/elasticsearch/data\n  securityContext:\n    privileged: true\n  volumeMounts:\n  - name: data\n    mountPath: /usr/share/elasticsearch/data\n```", "```js\napiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch\n  labels:\n    app: elasticsearch\nspec:\n  selector:\n    app: elasticsearch\n  clusterIP: None\n  ports:\n  - port: 9200\n    name: rest\n  - port: 9300\n    name: transport\n```", "```js\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch\n  labels:\n    app: elasticsearch\nspec:\n  replicas: 3\n  serviceName: elasticsearch\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      initContainers:\n      - name: increase-max-map-count\n        image: busybox\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        securityContext:\n          privileged: true\n      - name: increase-file-descriptor-limit\n        image: busybox\n        command:\n        - sh\n        - -c\n        - ulimit -n 65536\n        securityContext:\n          privileged: true\n      containers:\n        - name: elasticsearch\n          image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.2\n          ports:\n          - containerPort: 9200\n            name: http\n          - containerPort: 9300\n            name: tcp\n          env:\n            - name: discovery.zen.ping.unicast.hosts\n              value: \"elasticsearch-0.elasticsearch.default.svc.cluster.local,elasticsearch-1.elasticsearch.default.svc.cluster.local,elasticsearch-2.elasticsearch.default.svc.cluster.local\"\n      volumeMounts:\n        - name: data\n          mountPath: /usr/share/elasticsearch/data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n      storageClassName: do-block-storage\n```", "```js\n$ kubectl apply -f ./manifests/elasticsearch/service.yaml\nservice \"elasticsearch\" created\n$ kubectl apply -f ./manifests/elasticsearch/stateful-set.yaml\nstatefulset.apps \"elasticsearch\" created\n```", "```js\n$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\n```", "```js\n$ docker login --username d4nyll\n```", "```js\n$ docker build -t hobnob:0.1.0 . --no-cache\nSending build context to Docker daemon 359.4kB\nStep 1/13 : FROM node:8-alpine as builder\n...\nSuccessfully built 3f2d6a073e1a\n```", "```js\n$ docker tag <local-image-name>:<local-image-tag> <hub-namespace>/<hub-repository-name>:<hub-tag>\n```", "```js\n$ docker tag hobnob:0.1.0 d4nyll/hobnob:0.1.0\n```", "```js\n$ docker push d4nyll/hobnob\nThe push refers to repository [docker.io/d4nyll/hobnob]\n90e19b6c8d6d: Pushed\n49fb9451c65f: Mounted from library/node\n7d863d91deaa: Mounted from library/node\n8dfad2055603: Mounted from library/node\n0.1.0: digest: sha256:21610fecafb5fd8d84a0844feff4fdca5458a1852650dda6e13465adf7ee0608 size: 1163\n```", "```js\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  selector:\n    matchLabels:\n      app: backend\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: backend\n    spec:\n      containers:\n      - name: backend\n        image: d4nyll/hobnob:0.1.0\n        ports:\n        - containerPort: 8080\n          name: api\n        - containerPort: 8100\n          name: docs\n        env:\n        - name: ELASTICSEARCH_HOSTNAME\n          value: \"http://elasticsearch\"\n        - name: ELASTICSEARCH_PORT\n          value: \"9200\"\n        ...\n```", "```js\n$ kubectl apply -f ./manifests/backend/deployment.yaml\ndeployment.apps \"backend\" created\n```", "```js\n$ kubectl get all\nNAME                           READY     STATUS    RESTARTS   AGE\npod/backend-6d58f66658-6wx4f   1/1       Running   0          21s\npod/backend-6d58f66658-rzwnl   1/1       Running   0          21s\npod/backend-6d58f66658-wlsdz   1/1       Running   0          21s\npod/elasticsearch-0            1/1       Running   0          18h\npod/elasticsearch-1            1/1       Running   0          20h\npod/elasticsearch-2            1/1       Running   0          20h\n\nNAME                    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S) \nservice/elasticsearch   ClusterIP   None         <none>        9200/TCP,9300/TCP\nservice/kubernetes      ClusterIP   10.32.0.1    <none>        443/TCP \n\nNAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/backend   3         3         3            3           21s\n\nNAME                                 DESIRED   CURRENT   READY     AGE\nreplicaset.apps/backend-6d58f66658   3         3         3         21s\n\nNAME                             DESIRED   CURRENT   AGE\nstatefulset.apps/elasticsearch   3         3         20h\n```", "```js\n$ kubectl logs pod/backend-6d58f66658-6wx4f\nHobnob API server listening on port 8080!\n```", "```js\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend\n  labels:\n    app: backend\nspec:\n  selector:\n    app: backend\n  ports:\n  - port: 8080\n    name: api\n  - port: 8100\n    name: docs\n```", "```js\n$ kubectl apply -f ./manifests/backend/service.yaml\nservice \"backend\" created\n\n$ kubectl get services\nNAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\nbackend         ClusterIP   10.32.187.38   <none>        8080/TCP,8100/TCP   4s\nelasticsearch   ClusterIP   None           <none>        9200/TCP,9300/TCP   1d\nkubernetes      ClusterIP   10.32.0.1      <none>        443/TCP             1d\n```", "```js\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml\n```", "```js\n$ kubectl get pods --namespace=ingress-nginx\nNAME READY STATUS RESTARTS AGE\ndefault-http-backend-5c6d95c48-8tjc5 1/1 Running 0 1m\nnginx-ingress-controller-6b9b6f7957-7tvp7 1/1 Running 0 1m\n```", "```js\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: api.hobnob.social\n    http:\n      paths:\n      - backend:\n          serviceName: backend\n          servicePort: 8080\n  - host: docs.hobnob.social\n    http:\n      paths:\n      - backend:\n          serviceName: backend\n          servicePort: 8100\n```", "```js\n$ kubectl apply -f ./manifest/backend/ingress.yaml\ningress.extensions \"backend-ingress\" created\n$ kubectl describe ingress backend-ingress\nName: backend-ingress\nNamespace: default\nAddress: 174.138.126.169\nDefault backend: default-http-backend:80 (<none>)\nRules:\n Host Path Backends\n ---- ---- --------\n api.hobnob.social\n backend:8080 (<none>)\n docs.hobnob.social\n backend:8100 (<none>)\nAnnotations:\n kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"nginx.ingress.kubernetes.io/rewrite-target\":\"/\"},\"name\":\"backend-ingress\",\"namespace\":\"default\"},\"spec\":{\"rules\":[{\"host\":\"api.hobnob.social\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"backend\",\"servicePort\":8080}}]}},{\"host\":\"docs.hobnob.social\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"backend\",\"servicePort\":8100}}]}}]}}\n\n nginx.ingress.kubernetes.io/rewrite-target: /\nEvents:\n Type Reason Age From Message\n ---- ------ ---- ---- -------\n Normal UPDATE 2s nginx-ingress-controller Ingress default/backend-ingress\n```"]