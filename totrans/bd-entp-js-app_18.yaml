- en: Robust Infrastructure with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we used Docker to pre-build and package different parts
    of our application, such as Elasticsearch and our API server, into Docker images.
    These images are portable and can be deployed independently onto any environment.
    Although this revised approach automated some aspects of our workflow, we are
    still **manually** deploying our containers on a **single** server.
  prefs: []
  type: TYPE_NORMAL
- en: This lack of automation presents the risk of human error. Deploying on a single
    server introduces a **single point of failure** (**SPOF**), which reduces the
    reliability of our application.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we should provide redundancy by spawning multiple instances of each
    service, and deploying them across different physical servers and data centers.
    In other words, we should deploy our application on a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters allow us to have high availability, reliability, and scalability. When
    an instance of a service becomes unavailable, a failover mechanism can redirect
    unfulfilled requests to the still-available instances. This ensures that the application,
    as a whole, remains responsive and functional.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, coordinating and managing this distributed, redundant cluster is non-trivial,
    and requires many moving parts to work in concert. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global configuration store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '...and many more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster Management Tools is a platform which manages these tools and provides
    a layer of abstraction for developers to work with. A prime example is *Kubernetes*,
    which was open-sourced by Google in 2014.
  prefs: []
  type: TYPE_NORMAL
- en: Because most Cluster Management Tools also use containers to deploy, they are
    often also called **Container Orchestration** **systems**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Make our application more robust by deploying it with Kubernetes on DigitalOcean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the features of a robust system; namely **availability**, **reliability**,
    **throughput**, and **scalability**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examine the types of components a **Cluster Management Tool** would normally
    manage, how they work together, and how they contribute to making our system more
    robust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get hands-on and deploy and manage our application as a distributed Kubernetes
    cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Availability is a measure of the proportion of time that a system is able to
    fulfill its intended function. For an API, it means the percentage of time that
    the API can respond correctly to a client's requests.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Availability is usually measured as the percentage of time the system is functional
    (*Uptime*) over the total elapsed time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7816e04c-3bb0-484f-8900-0b6d50eff372.png)'
  prefs: []
  type: TYPE_IMG
- en: This is typically represented as "nines". For example, a system with an availability
    level of "four nines" will have an uptime of 99.99% or higher.
  prefs: []
  type: TYPE_NORMAL
- en: Following the industry standard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally speaking, the more complex a system, the more things can go wrong;
    this translates to a lower availability. In other words, it is much easier to
    have a 100% uptime for a static website than for an API.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what is the industry standard for availability for common APIs? Most online
    platforms offer a **service level agreement** (**SLA**) that includes a clause
    for the minimum availability of the platform. Here are some examples (accurate
    at the time of writing):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Compute Engine Service Level Agreement (SLA): 99.99%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon Compute Service Level Agreement: 99.99%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'App Engine Service Level Agreement (SLA): 99.95%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google Maps—Service Level Agreement (“Maps API SLA”): 99.9%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon S3 Service Level Agreement: 99.9%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evidently, these SLAs provide minimum availability guarantees that range from
    "three nines" (99.9%) to "four nines" (99.99%); this translates to a maximum downtime
    of between 52.6 minutes and 8.77 hours per year. Therefore, we should also aim
    to provide a similar level of availability for our API.
  prefs: []
  type: TYPE_NORMAL
- en: Eliminating single points of failure (SPOF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most fundamental step to ensure high availability is eliminating (SPOF).
    A SPOF is a component within a system which, if fails, causes the entire system
    to fail.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we deploy only one instance of our backend API, the single Node
    process running the instance becomes a SPOF. If that Node process exits for whatever
    reason, then our whole application goes down.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, eliminating a SPOF is relatively simple—replication; you simply
    have to deploy multiple instances of that component. However, that comes with
    challenges of its own—when a new request is received, which instance should handle
    it?
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing versus failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Conventionally, there are two methods to route requests to replicated components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load balancing**: A load balancer sits in-between the client and the server
    instances, intercepting the requests and distributing them among all instances:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/fc306757-1c22-4a71-8d75-6796dfeb55c4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The way requests are distributed depends on the load balancing algorithm used.
    Apart from "random" selection, the simplest algorithm is the *round-robin* algorithm.
    This is where requests are sequentially routed to each instance in order. For
    example, if there are two backend servers, A and B, the first request will be
    routed to A, the second to B, the third back to A, the fourth to B, and so on.
    This results in requests being evenly distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/340684e2-6628-466d-8e53-ef0fde637b6f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: While round-robin is the simplest scheme to implement, it assumes that all nodes
    are equal – in terms of available resources, current load, and network congestion.
    This is often not the case. Therefore, *dynamic round-robin* is often used, which
    will route more traffic to hosts with more available resources and/or lower load.
  prefs: []
  type: TYPE_NORMAL
- en: '**Failover**: Requests are routed to a single *primary* instance. If and when
    the primary instance fails, subsequent requests are routed to a different *secondary*,
    or *standby*, instance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/30a4e467-8e2b-4831-bc8f-7d39ebe30ea2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As with all things, there are pros and cons of each method:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource Utilization**: With the failover approach, only a single instance
    is running at any one time; this means you''ll be paying for server resources
    that do not contribute to the normal running of your application, nor improve
    its performance or throughput. On the other hand, the objective of load balancing
    is to maximize resource usage; providing high availability is simply a useful
    side effect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statefulness**: Sometimes, failover is the only viable method. Many real-world,
    perhaps legacy, applications are stateful, and the state can become corrupted
    if multiple instances of the application are running at the same time. Although
    you can refactor the application to cater for this, it''s still a fact that not
    all applications can be served behind a load balancer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: With failover, to improve performance and throughput, you
    must scale the primary node vertically (by increasing its resources). With load
    balancing, you can scale both vertically and horizontally (by adding more machines).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since our application is stateless, using a distributed load balancer makes
    more sense as it allows us to fully utilize all resources and provide better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load balancing can be done in multiple ways—using DNS for load distribution,
    or employing a Layer 4 or Layer 7 load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: DNS load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A domain can configure its DNS settings so that multiple IP addresses are associated
    with it. When a client tries to resolve the domain name to an IP address, it returns
    a list of all IP addresses. Most clients would then send its requests to the first
    IP address in the list.
  prefs: []
  type: TYPE_NORMAL
- en: DNS load balancing is where the DNS changes the order of these addresses each
    time a new name resolution request is made. Most commonly, this is done in a round-robin
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this method, client requests should be distributed equally among all
    backend servers. However, load balancing at the DNS level has some major disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of health-checks**: The DNS does not monitor the health of the servers.
    Even if one of the servers in the list goes down, it will still return with the
    same list of IP addresses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating and propagating DNS records to all *root servers*, intermediate DNS
    servers (*resolvers*), and clients can take anything from minutes to hours. Furthermore,
    most DNS servers cache their DNS records. This means that requests may still be
    routed to failed servers long after the DNS records are updated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 4/7 load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to load balance client requests is to use a *load balancer*. Instead
    of exposing the backend servers on multiple IP addresses and letting the client
    pick which server to use, we can instead keep our backend servers hidden behind
    a private local network. When a client wants to reach our application, it would
    send the request to the load balancer, which will forward the requests to the
    backends.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, there are two types of load balancers—Layer 4 (L4), Layer
    7 (L7). Their names relate to the corresponding layer inside the **Open Systems
    Interconnection** (**OSI**) reference model—a standard conceptual model that partitions
    a communication system into abstraction layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bd73b37-b690-4015-a261-411fac6ff45a.png)'
  prefs: []
  type: TYPE_IMG
- en: There are numerous standard protocols at each layer that specify how data should
    be packaged and transported. For example, FTP and MQTT are both application layer
    protocols. FTP is designed for file transfer, whereas MQTT is designed for publish-subscribe-based
    messaging.
  prefs: []
  type: TYPE_NORMAL
- en: When a load balancer receives a request, it will make a decision as to which
    backend server to forward a request to. These decisions are made using information
    embedded in the request. An L4 load balancer would use information from the transport
    layer, whereas an L7 load balancer can use information from the application layer,
    including the request body itself.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally speaking, L4 load balancers use information defined at the **Transport layer**
    (layer 4) of the OSI model. In the context of the internet, this means that L4
    load balancers should use information from Transmission Control Protocol (TCP)
    data packets. However, as it turns out, L4 load balancers also use information
    from Internet Protocol (IP) packets, which is a layer 3 - the *network* layer.
    Therefore, the name "Layer 4" should be considered a misnomer.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, an L4 load balancer routes requests based on the source/destination
    IP addresses and ports, with zero regards to the contents of the packets.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, an L4 load balancer comes in the form of a dedicated hardware device
    running proprietary chips and/or software.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 7 load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Layer 7 (L7) load balancer is similar to an L4 load balancer, but uses information
    from the highest layer on the OSI model – the *application* layer. For web services
    like our API, the Hypertext Transfer Protocol (HTTP) is used.
  prefs: []
  type: TYPE_NORMAL
- en: An L7 load balancer can use information from the URL, HTTP headers (for example, `Content-Type`),
    cookies, contents of the message body, client's IP address, and other information
    to route a request.
  prefs: []
  type: TYPE_NORMAL
- en: 'By working on the application layer, an L7 load balancer has several advantages
    over an L4 load balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Smarter**: Because L7 load balancers can base their routing rules on more
    information, such as the client''s geolocation data, they can offer more sophisticated
    routing rules than L4 load balancers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More capabilities**: Because L7 load balancers have access to the message
    content, they are able to alter the message, such as encrypting and/or compressing
    the body.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Load Balancing**: Because L4 load balancers are typically hardware
    devices, cloud providers usually do not allow you to configure them. In contrast,
    L7 load balancers are typically software, which can be fully managed by the developer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of debugging**: They can use cookies to keep the same client hitting
    the same backend server. This is a must if you implement stateful logic such as
    "sticky" sessions, but is also otherwise advantageous when debugging—you only
    have to parse logs from one backend server instead of all of them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, L7 load balancers are not always "better" than their L4 counterparts.
    L7 load balancers require more system resources and have high latency, because
    it must take into consideration more parameters. However, this latency is not
    significant enough for us to worry about.
  prefs: []
  type: TYPE_NORMAL
- en: There are currently a few production-ready L7 load balancers on the market—**High
    Availability Proxy** (**HAProxy**), NGINX, and Envoy. We will look into deploying
    a distributed load balancer in front of our backend servers later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: High reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reliability is a measure of the confidence in a system, and is inversely proportional
    to the probability of failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reliability is measured using several metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean time between failures** (**MTBF**): Uptime/number of failures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean time to repair **(**MTTR**): The average time it takes the team to fix
    a failure and return the system online'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing for reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to increase reliability is to increase test coverage of the
    system. This is, of course, assuming that those tests are meaningful tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tests increase reliability by:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Increasing MTBF: The more thorough your tests, the more likely you''ll catch
    bugs before the system is deployed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reducing MTTR: This is because historical test results inform you of the last
    version which passes all tests. If the application is experiencing a high level
    of failures, then the team can quickly roll back to the last-known-good version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High throughput
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughput is a measure of the number of requests that can be fulfilled in a
    given time interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'The throughput of a system depends on several factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network Latency**: The amount of time it takes for the message to get from
    the client to our application, as well as between different components of the
    application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: The computation speed of the program itself'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism**: Whether requests can be processed in parallel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can increase throughput using the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying our application geographically close to the client: Generally, this
    reduces the number of hops that a request must make through proxy servers, and
    thus reduces network latency. We should also deploy components that depend on
    each other close together, preferably within the same data center. This also reduces
    network latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ensure servers have sufficient resources: This makes sure that the CPU on your
    servers are sufficiently fast, and that the servers have enough memory to perform
    their tasks without having to use swap memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deploy multiple instances of an application behind a load balancer: This allows
    multiple requests to the application to be processed at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ensure your application code is non-blocking: JavaScript is an asynchronous
    language. If you write synchronous, blocking code, it will prevent other operations
    from executing while you wait for the synchronous operation to complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scalability is a measure of how well a system can grow in order to handle higher
    demands, while still maintaining the same levels of performance.
  prefs: []
  type: TYPE_NORMAL
- en: The demand may arise as part of a sustained growth in user uptake, or it may
    be due to a sudden peak of traffic (for example, a food delivery application is
    likely to receive more requests during lunch hours).
  prefs: []
  type: TYPE_NORMAL
- en: A highly scalable system should constantly monitor its constituent components
    and identify components which are working above a "safe" resource limit, and scale
    that component either *horizontally* or *vertically*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can increase scalability in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scale Vertically or *scaling Up*: Increase the amount of resources (for example,
    CPU, RAM, storage, bandwidth) to the existing servers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scale Horizontally or *scaling out*: Adding servers to the existing cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling vertically is simple, but there''ll always be a limit as to how much
    CPU, RAM, bandwidth, ports, and even processes the machine can handle. For example,
    many kernels have a limit on the number of processes it can handle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Scaling horizontally allows you to have higher maximum limits for resources,
    but comes with challenges of its own. An instance of the service may hold some
    temporary state that must be synchronized across different instances.
  prefs: []
  type: TYPE_NORMAL
- en: However, because our API is "stateless" (in the sense that all states are in
    our database and not in memory), scaling horizontally poses less of an issue.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters and microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to make our system be highly available, reliable, scalable, and produce
    high throughput, we must design a system that is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Resilient/Durable: Able to sustain component failures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elastic: Each service and resource can grow and shrink quickly based on demand'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such systems can be achieved by breaking monolithic applications into many smaller *stateless* components
    (following the microservices architecture) and deploying them in a *cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead of having a monolithic code base that caters to many concerns, you
    can instead break the application down into many services which, when working
    together, make up the whole application. Each service should:'
  prefs: []
  type: TYPE_NORMAL
- en: Have one or very few concerns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be de-coupled from other services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be stateless (if possible)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/22f95aca-df87-4acc-913b-40657a9c3cab.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With a monolithic application, all the components must be deployed together
    as a single unit. if you want to scale your application, you must scale by deploying
    more instances of the monolith. Furthermore, because there're no clear boundaries
    between different service, you'd often find tightly-coupled code in the code base. On
    the other hand, a microservices architecture places each service as a separate,
    standalone entity. You can scale by replicating only the service that is required.
    Furthermore, you can deploy the services on varied architecture, even using different
    vendors.
  prefs: []
  type: TYPE_NORMAL
- en: A service should expose an API for other services to interact with, but would
    otherwise be independent of other services. This means services could be independently
    deployed and managed.
  prefs: []
  type: TYPE_NORMAL
- en: Writing an application that allows for a microservice architecture allows us
    to achieve high scalability—administrators can simply spawn more instances of
    an in-demand service. Because the services are independent of each other, they
    can be deployed independently, where the more in-demand services have more instances
    deployed.
  prefs: []
  type: TYPE_NORMAL
- en: We have made our application stateless and containerized, both of which makes
    implementing a microservices architecture much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement a reliable and scalable infrastructure, we must provide redundancy.
    This means redundancy in:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware**: We must deploy our application across multiple physical hosts,
    each (ideally) at different geographical locations. This is so that if one data
    center is offline or destroyed, services deployed at the other data centers can
    keep our application running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software**: We must also deploy multiple instances of our services; this
    is so that the load of handling requests can be distributed across them. Consequently,
    this yields the following benefits:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can route users to the server which provides them with the quickest response
    time (usually the one closest geographically to the user)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can put one service offline, update it, and bring it back online without
    affecting the uptime of the entire application
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying applications on a cluster allows you to have hardware redundancy,
    and load balancers provide software redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: A cluster consists of a network of hosts/servers (called *nodes*). Once these
    nodes are provisioned, you can then deploy instances of your services inside them.
    Next, you'll need to configure a load balancer that sits in front of the services
    and distribute requests to the node with the most available service.
  prefs: []
  type: TYPE_NORMAL
- en: 'By deploying redundant services on a cluster, it ensures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High Availability**: If a server becomes unavailable, either through failure
    or planned maintenance, then the load balancer can implement a *failover* mechanism
    and redistribute requests to the healthy instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Reliability**: Redundant instances remove the *single point of failure*.
    It means our whole system becomes *fault-tolerant*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Throughput**: By having multiple instances of the service across geographical
    regions, it allows for low latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This may be implemented as a **Redundant Array Of Inexpensive Servers** (**RAIS**),
    the server equivalent of RAID, or *Redundant Arrays Of Inexpensive Disks*. Whenever
    a server fails, the service will still be available by serving them from the healthy
    servers.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you are using a cloud provider like DigitalOcean, they would take
    care of the hardware redundancy for you. All that's left for us to do is deploy
    our cluster and configure our load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying our application in a microservices manner inside a cluster is simple
    enough in principle, but actually quite complex to implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you must *provision* servers to act as nodes inside your cluster. Then,
    we''ll need to set up a handful of tools that work in concert with each other
    to manage your cluster. These tools can be categorized into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster-level tools**: Works at the cluster level, and makes global decisions
    that affect the whole cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node-level tools**: Resides within each node. It takes instructions from,
    and feedback to, cluster-level tools in order to coordinate the management of
    services running inside the node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the cluster-level tools, you''ll need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A scheduler**: This dictates which node a particular service will be deployed
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A Discovery Service**: This keeps a record of how many instances of each
    service are deployed, their states (for example, starting, running, terminating
    and so on.), where they''re deployed, and so on. It allows for *service discovery.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A Global Configuration Store**: Stores cluster configurations such as common
    environment variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the node-level, you''ll need the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local configuration management tools**: To keep local configuration states
    and to synchronize with cluster-level configurations. We have our cluster configurations
    stored inside the Global Configuration Store; however, we also need a way to retrieve
    those settings into each node. Furthermore, when those configurations are changed,
    we need a way to fetch the updated configuration and reload the application/services
    if required. `confd` ([https://github.com/kelseyhightower/confd](https://github.com/kelseyhightower/confd)) is
    the most popular tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container runtime**: Given that a node is assigned to run a service, it must
    have the necessary programs to do so. Most services deployed on modern microservice
    infrastructures use containers to encapsulate the service. Therefore, all major
    cluster management tools will be bundled with some kind of container runtime,
    such as Docker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's take a look at each cluster-level tool in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster-level tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, cluster-level tools work at the cluster level, and
    make global decisions that affect the whole cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Discovery service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the moment, our API container can communicate with our Elasticsearch container
    because, under the hood, they''re connected to the same network, on the same host
    machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: However, if these containers are deployed on separate machines, using different
    networks, how can they communicate with each other?
  prefs: []
  type: TYPE_NORMAL
- en: Our API container must obtain network information about the Elasticsearch container
    so that it can send requests to it. One way to do this is by using a *service
    discovery* tool.
  prefs: []
  type: TYPE_NORMAL
- en: With service discovery, whenever a new container (running a service) is initialized,
    it registers itself with the **Discovery Service**, providing information about
    itself, which includes its IP address. The **Discovery Service** then stores this
    information in a simple key-value store.
  prefs: []
  type: TYPE_NORMAL
- en: The service should update the **Discovery Service** regularly with its status,
    so that the Discovery Service always has an up-to-date state of the service at
    any time.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new service is initiated, it will query the **Discovery Service** to
    request information about the services it needs to connect with, such as their
    IP address. Then, the **Discovery Service** will retrieve this information from
    its key-value store and return it to the new service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/343e6df5-2202-41e1-8c4d-1c71079a7520.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, when we deploy our application as a cluster, we can employ a service
    discovery tool to facilitate our API's communication with our Elasticsearch service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Popular service discovery tools include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd`, by CoreOS ([https://github.com/coreos/etcd](https://github.com/coreos/etcd))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consul, by HashiCorp ([https://www.consul.io/](https://www.consul.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zookeeper, by Yahoo, now an Apache Software Foundation ([https://zookeeper.apache.org/](https://zookeeper.apache.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the **Discovery Service** holds information about the state and location
    of each service, it does not make the decision of which host/node the service
    should be deployed on. This process is known as **host selection** and is the
    job of a *scheduler*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/671cb9c9-069b-42b8-8cd6-75dbcc6c0fcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The scheduler''s decision can be based on a set of rules, called **policies**,
    which takes into account the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The nature of the request.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster configuration/settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Host density**: An indication of how busy a the host system on the node is.
    If there are multiple nodes inside the cluster, we should prefer to deploy any
    new services on a node with the lowest host density. This information can be obtained
    from the Discovery Service, which holds information about all deployed services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service (anti-)affinity**: Whether two services should be deployed together
    on the same host. This depends on:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redundancy requirements**: The same application should not be deployed on
    the same node(s) if there are other nodes that are not running the service. For
    instance, if our API service has already been deployed on two of three hosts,
    the scheduler may prefer to deploy on the remaining host to ensure maximum redundancy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data locality**: The scheduler should try placing computation code next to
    the data it needs to consume to reduce network latency.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource requirements**: Of existing services running on nodes, as well as
    the service to be deployed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware/software constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other policies/rules set by the cluster administrator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global configuration store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Oftentimes, as is the case with our services, environment variables need to
    be set before the service can run successfully. So far, we''ve specified the environment
    variables to use by using the `docker run` `--env-file` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, when deploying services on a cluster, we no longer run each container
    manually—we let the scheduler and node-level tools do this for us. Furthermore,
    we need all our services to share the same environment variables. Therefore, the
    most obvious solution is to provide a *Global Configuration Store* that stores
    configuration that is to be shared among all of the nodes and services.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Provisioning means starting new hosts (be it physical or virtual) and configuring
    them in a way that allows them to run Cluster Management Tools. After provisioning,
    the host is ready to become a node inside the cluster and can receive work.
  prefs: []
  type: TYPE_NORMAL
- en: This may involve using Infrastructure Management tools like Terraform to spin
    up new hosts, and Configuration Management tools like Puppet, Chef, Ansible or
    Salt, to ensure the configuration set inside each host are consistent with each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: While provisioning can be done before deploying our application, most Cluster
    Management software has a provisioning component built into it.
  prefs: []
  type: TYPE_NORMAL
- en: Picking a cluster management tool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having to manage these different cluster management components individually
    is tedious and error-prone. Luckily, cluster management tools exist that provides
    a common API that allows us to configure these tools in a consistent and automated
    manner. You'd use the Cluster management tool's API instead of manipulating each
    component individually.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster management tools are also known as *cluster orchestration tools* or *container
    orchestration tools*. Although there may be slight nuances between the different
    terms, we can regard them as the same for the purpose of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few popular cluster management tools available today:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Marathon ([https://mesosphere.github.io/marathon/](https://mesosphere.github.io/marathon/)):
    By Mesosphere and runs on Apache Mesos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Swarm ([https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/)):
    The Docker engine includes a *swarm mode* that manages Docker containers in clusters
    called *swarms*. You may also group certain containers together using Docker Compose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes: The *de facto* cluster management tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be using Kubernetes because it has the most mature ecosystem, and is
    the *de facto* industry standard.
  prefs: []
  type: TYPE_NORMAL
- en: Control Planes and components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The components we described previously—scheduler, Discovery Service, Global
    Configuration Store, and so on—are common to all Cluster Management Tools that
    exist today. The difference between them is how they package these components
    and abstract away the details. In Kubernetes, these components are aptly named
    Kubernetes *Components*.
  prefs: []
  type: TYPE_NORMAL
- en: We will distinguish between generic "components" with Kubernetes Components
    by using the capital case for the latter.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes terminology, a "component" is a process that implements some part
    of the Kubernetes cluster system; examples include the `kube-apiserver` and `kube-scheduler`.
    The sum of all components forms what you think of as the "Kubernetes system",
    which is formally known as the *Control Plane*.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to how we categorized the cluster tools into cluster-level tools and
    node-level tools, Kubernetes categorizes Kubernetes Components into *Master Components* and *Node
    Components*, respectively. Node Components operates within the node it is running
    on; Master Components work with multiple nodes or the entire cluster, hold cluster-level
    settings, configuration, and state, and make cluster-level decisions. Master Components
    collectively makes up the *Master Control Plane*.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes also has *Addons—*components which are not strictly required but
    provide useful features such as Web UI, metrics, and logging.
  prefs: []
  type: TYPE_NORMAL
- en: With this terminology in mind, let's compare the generic cluster architecture
    we've described with Kubernetes'.
  prefs: []
  type: TYPE_NORMAL
- en: Master components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discovery service and global configuration store and scheduler are implemented
    with the `etcd` and `kube-scheduler` master components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd` is a consistent and highly-available **key-value** (**KV**) store used
    as both the Discovery Service and Global Configuration Store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the discovery service and global configuration store both hold information
    about the services, and each are accessible by all nodes, `etcd` can serve both
    purposes. Whenever a service registers itself with the discovery service, it will
    also be returned a set of configuration settings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`kube-scheduler` is a scheduler. It keeps track of which applications are unassigned
    to a node (and thus not running) and makes the decision as to which node to assign
    it to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these essential cluster management components, Kubernetes also
    provides additional Master Components to make working with Kubernetes easier.
  prefs: []
  type: TYPE_NORMAL
- en: By default, all Master Components run on a single *Master Node*, which runs
    only the Master Components and not other containers/services. However, they can
    be configured to be replicated in order to provide redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: kube-apiserver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes runs as a daemon that exposes a RESTful Kubernetes API server—`kube-apiserver`. `kube-apiserver` acts
    as the interface to the Master Control Plane. Instead of communicating with each
    Kubernetes Component individually, you''d instead make calls to `kube-apiserver`,
    which will communicate with each component on your behalf:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70dc31f4-3fce-42e9-9937-f0421f46f4ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are many benefits to this, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You have a central location where all changes pass through. This allows you
    to record a history of everything that has happened in your cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API provides a uniform syntax.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kube-control-manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we will demonstrate later, a central concept of Kubernetes, and the reason
    why you'd use a Cluster Management Tool in the first place, is that you don't
    have to *manually* manipulate the cluster yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Doing so will consist of sending a request to one component, receiving a response,
    and based on that response, sending another request to another component. This
    is the *imperative* approach, and is time-consuming because it requires you to
    manually write programs to implement this logic.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Kubernetes allows us to specify the desired state of our cluster using
    configuration files, and Kubernetes will automatically coordinate the different
    Kubernetes Components to make it happen. This is the *declarative *approach and
    is what Kubernetes recommends.
  prefs: []
  type: TYPE_NORMAL
- en: Linking this to what we already know, the job of the whole Kubernetes system
    (the Control Plane) then becomes a system that tries to align the current state
    of the cluster with the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes does this through *Controllers*. Controllers are the processes that
    actually carry out the actions of keeping the state of the cluster with the desired
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many types of controllers; here are two examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Node Controllers, for ensuring that the cluster has the desired number of nodes.
    For example, when a node fails, the Node Controller is responsible for spawning
    a new node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication Controllers, for ensuring each application has the desired number
    of replicas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of controllers for `kube-controller-manager` become clearer once we've
    explained Kubernetes Objects and deploy our first service on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Node components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Node-level tools are implement as Node Components in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Container runtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes runs applications and services inside containers, and it expects
    that each node in the cluster already has the respective container runtime installed;
    this can be done with a provisioning tool like Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: However, it does not dictate any particular container format, as long as it
    is a format that abides by the **Open Container Initiative** (**OCI**)'s runtime
    specification ([https://github.com/opencontainers/runtime-spec](https://github.com/opencontainers/runtime-spec)).
    For instance, you can use Docker, rkt (by CoreOS), or runc (by OCI) / CRI-O (by
    Kubernetes team) as the container format and runtime.
  prefs: []
  type: TYPE_NORMAL
- en: kubelet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the generic cluster architecture, our cluster needs a local configuration
    management tool like `confd` to pull updates from the Discovery Service and Global
    Configuration Stores. This ensures applications running on the node are using
    the most up-to-date parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, this is the job of `kubelet`. However, `kubelet` does a lot more
    than just updating the local configuration and restarting services. It also monitors
    each service, make sure they are running *and* healthy, and reports their status
    back to `etcd` via `kube-apiserver`.
  prefs: []
  type: TYPE_NORMAL
- en: kube-proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each application (including replicas) deployed in the cluster is assigned virtual
    IPs. However, as applications are shut down and re-deployed elsewhere, their virtual
    IPs can change. We will go into more details later, but Kubernetes provides a *Services* Object
    that provides a static IP address for our end users to call. `kube-proxy` is a
    network proxy that runs on each node, and acts as a simple load balancer that
    forwards (or proxies) requests from the static IP address to the virtual IP address
    of one of the replicated applications.
  prefs: []
  type: TYPE_NORMAL
- en: The role of `kube-proxy` will become more apparent when we create services.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you understand the different Components that make up the Kubernetes
    system, let's shift our attention to *Kubernetes API Objects*, or *Objects* (with
    a capital O), for short.
  prefs: []
  type: TYPE_NORMAL
- en: As you already know, with Kubernetes, you don't need to interact directly with
    individual Kubernetes Components; instead, you interact with `kube-apiserver` and
    the API server will coordinate actions on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: The API abstracts away raw processes and entities into abstract concepts called
    Objects. For instance, instead of asking the API server to "Run these groups of
    related containers on a node", you'd instead ask "Add this Pod to the cluster".
    Here, the group of containers is abstracted to a *Pod* Object. When we work with
    Kubernetes, all we're doing is sending requests to the Kubernetes API to manipulate
    these Objects.
  prefs: []
  type: TYPE_NORMAL
- en: The four basic objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are four basic Kubernetes Objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pod**: A group of closely-related containers that should be managed as a
    single unit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service**: An abstraction that proxies requests from a static IP to the dynamic,
    virtual IPs of one of the Pods running the application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume**: This provides shared storage for all containers inside the same
    Pod'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Namespace**: This allows you to separate a single physical cluster into multiple
    virtual clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-level objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These basic Objects may then be built upon to form higher-level Objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ReplicaSet**: Manages a set of Pods so that a specified number of replicas
    are maintained within the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment:** An even higher-level abstraction than ReplicaSet, a Deployment
    Object will manage a ReplicaSet to ensure that the right number of replicas are
    running, but also allows you update your configuration to update/deploy a new
    ReplicaSet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**StatefulSet**: Similar to a Deployment, but in a Deployment, when a Pod restarts
    (for example, due to scheduling), the old Pod is destroyed and a new Pod is created.
    Although these Pods are created using the same specification, they are different
    Pods because data from the previous Pod is not persisted. In a StatefulSet, the
    old Pod can persist its state across restarts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DaemonSet**: Similar to ReplicaSet, but instead of specifying the number
    of replicas to run, a DaemonSet is intended to run on every node in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Job**: Instead of keeping Pods running indefinitely, a Job object spawns
    new Pods to carry out tasks with a finite timeline, and ensures that the Pods
    terminates successfully after the task completes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aforementioned higher-level Objects rely on the four basic Objects.
  prefs: []
  type: TYPE_NORMAL
- en: Controllers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These higher-level Objects are ran and managed by *Controllers*, which actually
    perform the actions that manipulate the Objects.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when we create a Deployment, a *Deployment controller* manages
    the Pods and ReplicaSet specified from the configuration. It is the controller
    who is responsible for making changes to get the actual state to the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Most Objects have a corresponding Controller—a ReplicaSet object is managed
    by a ReplicaSet controller, a DaemonSet is managed by the DaemonSet controller,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from these, there are numerous other Controllers, with the most common
    ones listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node Controller**: Responsible for noticing and responding when nodes go
    down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replication Controller**: Responsible for maintaining the correct number
    of pods for every replication controller object in the system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route Controller**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume Controller**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service Controller**: Works on the load balancer and direct requests to the
    corresponding Pods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoints Controller**: Populates the Endpoints object that links Service
    Objects and Pods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service Account and Token Controllers**: Creates default accounts and API
    access tokens for new namespaces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These higher-level objects, and the Controllers that implements them, manage
    basic Objects on your behalf, providing additional conveniences that you'd come
    to expect when working with Cluster Management Tools. We will demonstrate the
    use of these Objects as we migrate our application to run on Kubernetes later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the local development environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you understand the different Components of Kubernetes and the abstractions
    (Objects) that the API provides, we are ready to migrate the deployment of our
    application to using Kubernetes. In this section, we will learn the basics of
    Kubernetes by running it on our local machine. Later on in this chapter, we will
    build on what we've learned and deploy our application on multiple VPSs, managed
    by a cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: Checking hardware requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run Kubernetes locally, your machine needs to fulfill the following hardware
    requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Have 2 GB or more of available RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have two or more CPU cores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swap space is disabled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure you are using a machine which satisfies those requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning our environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because Kubernetes manages our application containers for us, we no longer need
    to manage our own Docker containers. Therefore, let's provide a clean working
    environment by removing any Docker containers and images related to our application.
    You can do this by running `docker ps -a` and `docker images` to see a list of
    all containers and images, and then using `docker stop <container>`, `docker rm
    <container>`, and `docker rmi <image>` to remove the relevant ones.
  prefs: []
  type: TYPE_NORMAL
- en: Disabling swap memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Running Kubernetes locally requires you to turn Swap Memory off. You can do
    so by running `swapoff -a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Installing kubectl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we can interact with the Kubernetes API by sending raw HTTP requests
    using a program like `curl`, Kubernetes provides a convenient command-line client
    called `kubectl`. Let''s install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can find alternate installation methods at [kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check that the installation was successful by running `kubectl version`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `kubectl` provides autocompletion; to activate it, simply run
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Installing Minikube
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Minikube is a free and open source tool by the Kubernetes team that enables
    you to easily run a single-node Kubernetes cluster locally. Without Minikube,
    you'd have to install and configure `kubectl` and `kubeadm` (used for provisioning)
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's install Minikube by following the instructions found at [https://github.com/kubernetes/minikube/releases](https://github.com/kubernetes/minikube/releases).
    For Ubuntu, we can choose to either run the install script or install the `.deb` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing this book, the `.deb` package installation is still
    experimental, so we will opt for the install script instead. For example, to install
    Minikube v0.27.0, we can run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can use the same command to update `minikube`.
  prefs: []
  type: TYPE_NORMAL
- en: Installing a Hypervisor or Docker Machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, Minikube runs a single-node cluster inside a virtual machine (VM),
    and this requires the installation of a hypervisor like VirtualBox or KVM. This
    requires a lot of setting up and is not great for performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we can instruct Minikube to run Kubernetes components directly on
    our machine outside of any VMs. This requires the Docker runtime and Docker Machine
    to be installed on our machine. Docker runtime should already be installed if
    you followed our previous chapter, so let''s install Docker Machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After installation, run `docker-machine version` to confirm that the installation
    was successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Running your cluster with Minikube on Docker is only available on Linux machines.
    If you are not using a Linux machine, go to the Minikube documentation to follow
    instructions on setting up a VM environment and using a VM driver. The rest of
    the chapter will still work for you. Just remember to use the correct `--vm-driver` flag
    when running `minikube start`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the Kubernetes daemon (installed and ran by `minikube`) and the Kubernetes
    client (`kubectl`) installed, we can now run `minikube start` to create and start
    our cluster. We'd need to pass in `--vm-driver=none` as we are not using a VM.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a VM, remember to use the correct `--vm-driver` flag.
  prefs: []
  type: TYPE_NORMAL
- en: We need to run the `minikube start` command as `root` because the `kubeadm` and `kubelet` binaries
    need to be downloaded and moved to `/usr/local/bin`, which requires root privileges.
  prefs: []
  type: TYPE_NORMAL
- en: However, this usually means that all the files created and written during the
    installation and initiation process will be owned by `root`. This makes it hard
    for a normal user to modify configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Kubernetes provides several environment variables that we can set
    to change this.
  prefs: []
  type: TYPE_NORMAL
- en: Setting environment variables for the local cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inside `.profile` (or its equivalents, such as `.bash_profile` or `.bashrc`),
    add the following lines at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`CHANGE_MINIKUBE_NONE_USER` tells `minikube` to assign the current user as
    the owner of the configuration files. `MINIKUBE_HOME` tells `minikube` to store
    the Minikube-specific configuration on `~/.minikube`, and `KUBECONFIG` tells `minikube` to
    store the Kubernetes-specific configuration on `~/.kube/config`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply these environment variables to the current shell, run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we''ll need to actually create a `.kube/config` configuration file
    to our home directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Running minikube start
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our environment variables set, we''re finally ready to run `minikube start`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This command performs several operations under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: Provisions any VMs (if we're using VM). This is done internally by libmachine
    from Docker Machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets up configuration files and certificates under `./kube` and `./minikube`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starts up the local Kubernetes cluster using `localkube`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configures `kubectl` to communicate with this cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we are developing locally using the `--vm-driver=none` flag, our machine
    becomes the only node within the cluster. You can confirm this by using `kubectl` to
    see whether the node is registered with the Kubernetes API and `etcd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'All Master Components, such as the scheduler (`kube-scheduler`), as well as
    Node Components, such as `kubelet`, are running on the same node, inside Docker
    containers. You can check them out by running `docker ps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As a last check, run `systemctl status kubelet.service` to ensure that `kubelet` is
    running as a daemon on the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything is now set up. You can confirm this by running `minikube status` and `kubectl
    cluster-info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Updating the context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you change the local network that your computer is connected to, the cluster''s
    IP may change. If you try to use `kubectl` to connect to the cluster after this
    change, you''ll see an error saying that the `network is unreachable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever you see an error like this, run `minikube status` to check the state
    of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, it informs us that `kubectl` is "pointing to the stale `minikube-vm`"
    and we should run `minikube update-context` to update `kubectl` to point to the
    new cluster IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After doing this, check that `kubectl` is able to communicate with the Kubernetes
    API server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Resetting the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Working with Kubernetes can be tricky, especially at the beginning. If you
    ever get stuck with a problem and can''t resolve it, you can use `kubeadm reset` to
    reset everything related to our Kubernetes cluster, and start again from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Try it now. Then, run the same `minikube start` command as before to recreate
    the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Creating our first Pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a cluster running locally, let's deploy our Elasticsearch service
    on it. With Kubernetes, all services run inside containers. Conveniently for us,
    we are already familiar with Docker, and Kubernetes supports the Docker container
    format.
  prefs: []
  type: TYPE_NORMAL
- en: However, Kubernetes doesn't actually deploy containers individually, but rather,
    it deploys *Pods*. As already mentioned, Pods are a type of basic Kubernetes Objects—abstractions
    provided by the Kubernetes API. Specifically, Pods are a logical grouping of containers
    that should be deployed and managed together. In Kubernetes, Pods are also the
    lowest-level unit that Kubernetes manages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers inside the same Pod share the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lifecycle**: All containers inside a Pod are managed as a single unit. When
    a pod starts, all the containers inside the pod will start (this is known as a **shared
    fate**). When a Pod needs to be relocated to a different node, all containers
    inside the pod will relocate (also known as **co-scheduling**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context**: A Pod is isolated from other Pods similar to how one Docker container
    is isolated from another Docker container. In fact, Kubernetes uses the same mechanism
    of namespaces and groups to isolate a pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared network**: All containers within the pod share the same IP address
    and port space, and can communicate with each other using `localhost:<port>`. They
    can also communicate with each other using inter-process communications (IPC).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared storage**: Containers can access a shared volume that will be persisted
    outside of the container, and will survive even if the containers restart:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0e055736-d842-4cb4-b4e0-a8c0ef3b38c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Running Pods with kubelet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pods are run by the `kubelet` service that runs inside each node. There are
    three ways to instruct `kubelet` to run a Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: By directly passing it the Pod configuration file (or a directory container
    configuration files) using `kubelet --config <path-to-pod-config>`. `kubelet` will
    poll this directory every 20 seconds for changes, and will start new containers
    or terminate containers based on any changes to the configuration file(s).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By specifying an HTTP endpoint which returns with the Pod configuration files.
    Like the file option, `kubelet` polls the endpoint every 20 seconds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using the Kubernetes API server to send any new pod manifests to `kubelet`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first two options are not ideal because:'
  prefs: []
  type: TYPE_NORMAL
- en: It relies on polling, which means that the nodes cannot react quickly to changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kubernetes API server is not aware of these pods, and thus cannot manage
    them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, we should use `kubelet` to communicate our intentions to the Kubernetes
    API server, and let it coordinate how to deploy our Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Running Pods with kubectl run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, confirm that no Elasticsearch containers are currently running on our
    machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use `kubectl run` to run an image inside a Pod, and deploy it onto
    our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we check the Pods that have been deployed onto our cluster, we can
    see a new `elasticsearch-656d7c98c6-s6v58` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'It may take some time for the Pod to initiate, especially if the Docker image
    is not available locally and needs to be downloaded. Eventually, you should see
    the `READY` value become `1/1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Understanding high-level Kubernetes objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The more observant of you might have noticed the following output after you
    ran `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run `kubectl run`, Kubernetes does not create a Pod directly; instead,
    Kubernetes automatically creates a Deployment Object that will manage the Pod
    for us. Therefore, the following two commands are functionally equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate this, you can see a list of active Deployments using `kubectl
    get deployments`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The benefit of using a Deployment object is that it will manage the Pods under
    its control. This means that if the Pod fails, the Deployment will automatically
    restart the Pod for us.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we should not *imperatively* instruct Kubernetes to create a low-level
    object like Pods, but *declaratively *create a higher-level Kubernetes Object
    and let Kubernetes manage the low-level Objects for us.
  prefs: []
  type: TYPE_NORMAL
- en: This applies to ReplicaSet as well—you shouldn't deploy a ReplicaSet; instead,
    deploy a Deployment Object that uses ReplicaSet under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Declarative over imperative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pods, Deployments, and ReplicaSet are examples of Kubernetes Objects. Kubernetes
    provides you with multiple approaches to run and manage them.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl run`—imperative: You provide instructions through the command line
    to the Kubernetes API to carry out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl create`—imperative: You provide instructions, in the form of a configuration
    file, to the Kubernetes API to carry out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl apply`—declarative: You tell the Kubernetes API the desired state
    of your cluster using configuration file(s), and Kubernetes will figure out the
    operations required to reach that state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl create` is a slight improvement to `kubectl run` because the configuration
    file(s) can now be version controlled; however, it is still not ideal due to its
    imperative nature.'
  prefs: []
  type: TYPE_NORMAL
- en: If we use the imperative approach, we'd be manipulating the Kubernetes object(s)
    directly, and thus be responsible for monitoring all Kubernetes objects. This
    essentially defeats the point of having a Cluster Management Tool.
  prefs: []
  type: TYPE_NORMAL
- en: The preferred pattern is to create Kubernetes Objects in a declarative manner
    using a version-controlled *manif**est *file.
  prefs: []
  type: TYPE_NORMAL
- en: '| Management technique | Operates on | Recommended environment | Supported
    writers | Learning curve |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Imperative commands | Live objects | Development projects | 1+ | Lowest |'
  prefs: []
  type: TYPE_TB
- en: '| Imperative object configuration | Individual files | Production projects
    | 1 | Moderate |'
  prefs: []
  type: TYPE_TB
- en: '| Declarative object configuration | Directories of files | Production projects
    | 1+ | Highest |'
  prefs: []
  type: TYPE_TB
- en: You should also note that the imperative and declarative approaches are mutually
    exclusive—you cannot have Kubernetes manage everything based on your configuration,
    and also manipulate objects on your own. Doing so will cause Kubernetes to detect
    the changes you've made as deviations from the desired state, and will work against
    you and undo your changes. Therefore, we should consistently use the declarative
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With this in mind, let''s redeploy our Elasticsearch service in a declarative
    manner, using `kubectl apply`. But first, we must delete our existing Deployment.
    We can do that with `kubectl delete`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Creating a deployment manifest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, create a new directory structure at `manifests/elasticsearch`, and in
    it, create a new file called `deployment.yaml`. Then, add the following Deployment
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration file consists of several fields (fields marked `*` are required):'
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion*`: The version of the API. This affects the scheme expected for
    the configuration file. The API is broken into modular API Groups. This allows
    Kubernetes to develop newer features independently. It also provides Kubernetes
    cluster administrators more fine-grained control over which API features they
    want to be enabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The core Kubernetes objects are available in the *core* group (the *legacy* group),
    and you can specify this by using `v1` as the `apiVersion` property value. Deployments
    are available under the `apps` group, and we can enable this by using `apps/v1` as
    the `apiVersion` property value. Other groups include `batch` (provides the `CronJob` object), `extensions`, `scheduling.k8s.io`, `settings.k8s.io`, and
    many more.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`kind*`: The type of resource this manifest is specifying. In our case, we
    want to create a Deployment, so we should specify `Deployment` as the value. Other
    valid values for `kind` include `Pod` and `ReplicaSet`, but for reasons mentioned
    previously, you wouldn''t normally use them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: Metadata about the Deployment, such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespace`: With Kubernetes, you can split a single physical cluster into
    multiple *virtual clusters*. The default namespace is `default`, which is sufficient
    for our use case.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A name to identify the Deployment within the cluster.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec`: Details the behavior of the Deployment, such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replicas`: The number of replica Pods, specified in the `spec.template`, to
    deploy'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`template`: The specification for each Pod in the ReplicaSet'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: The metadata about the Pod, including a `label` property'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec`: The specification for each individual Pod:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`containers`: A list of containers that belong in the same Pod and should be
    managed together.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`selector`: The method by which the Deployment controller knows which Pods
    it should manage. We use the `matchLabels` criteria to match all Pods with the
    label `app: elasticsearch`. We then set the label at `spec.template.metadata.labels`.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A note on labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our manifest file, under `spec.template.metadata.labels`, we''ve specified
    that our Elasticsearch Pods should carry the label `app: elasticsearch`.'
  prefs: []
  type: TYPE_NORMAL
- en: Label is one of two methods to attach arbitrary metadata to Kubernetes Objects,
    with the other being *annotations*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both labels and annotations are implemented as key-value stores, but they serve
    different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Labels: Used to identify an Object as belonging to a certain group of similar
    Objects. In other words, it can be used to select a subset of all Objects of the
    same type. This can be used to apply Kubernetes commands to only a subset of all
    Kubernetes Objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Annotations: Any other arbitrary metadata not used to identify the Object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A label key consists of two components—an optional prefix, and a name—separated
    by a forward slash (`/`).
  prefs: []
  type: TYPE_NORMAL
- en: The prefix exists as a sort of namespace, and allows third-party tools to select
    only the Objects that it is managing. For instance, the core Kubernetes components
    have a label with a prefix of `kubernetes.io/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Labeled Objects can then be selected using *label selectors*, such as the one
    specified in our Deployment manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This selector instructs the Deployment Controller to manage only these Pods
    and not others.
  prefs: []
  type: TYPE_NORMAL
- en: Running pods declaratively with kubectl apply
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the Deployment manifest ready, we can run `kubectl apply` to update the
    desired state of our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This will trigger a set of events:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` sends the Deployment manifest to the Kubernetes API server (`kube-apiserver`). `kube-apiserver` will
    assign it a unique ID, and adds it on to `etcd`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API server will also create the corresponding ReplicaSet and Pod Objects
    and add it to `etcd`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The scheduler watches `etcd` and notices that there are Pods that have not been
    assigned to a node. Then, the scheduler will make a decision about where to deploy
    the Pods specified by the Deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once a decision is made, it will inform `etcd` of its decision; `etcd` records
    the decision.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `kubelet` service running on each node will notice this change on `etcd`,
    and pull down a PodSpec – the Pod's manifest file. It will then run and manage
    a new Pod according to the PodSpec.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During the entire process, the scheduler and kubelets keep `etcd` up to date *at
    all times* via the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we query for the state of the Deployment in the first few seconds after
    we run `kubectl apply`, we will see that `etcd` has updated its records with our
    desired state, but the Pods and containers will not be available yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '**What do the numbers m****ean?** `DESIRED`—the desired number of replicas; `CURRENT`—the
    current number of replicas; `UP-TO-–` the current number of replicas that has
    the most up-to-date configuration (has the copy of the latest Pod template/manifest); `AVAILABLE`—the
    number of replicas available to users'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then run `kubectl rollout status` to be notified, in real-time, when
    each Pod is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can check the deployment again, and we can see that all three replica
    Pods are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We have now successfully switched our approach from an imperative one (using `kubectl
    run`), to a declarative one (using manifest files and `kubectl apply`).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Object management hierarchy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To solidify your understanding that our Deployment object is managing a ReplicaSet
    object, you can run `kubectl get rs` to get a list of ReplicaSet in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The name of a ReplicaSet is automatically generated from the name of the Deployment
    object that manages it, and a hash value derived from the Pod template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, we know that the `elasticsearch-699c7dd54f` ReplicaSet is managed
    by the `elasticsearch` Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same logic, you can run `kubectl get pods` to see a list of Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Again, the name of the Pod is the name of its controlling ReplicaSet and a unique
    hash.
  prefs: []
  type: TYPE_NORMAL
- en: You can also see that the Pods have a `pod-template-hash=2557388109` label applied
    to them. The Deployment and ReplicaSet use this label to identify which Pods it
    should be managing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find out more information about an individual Pod, you can run `kubectl
    describe pods <pod-name>`, which will produce a human-friendly output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can get information about a Pod in a more structured JSON
    format by running `kubectl get pod <pod-name>`.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Elasticsearch cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the output of `kubectl describe pods` (or `kubectl get pod`), we can see
    that the IP address of the Pod named `elasticsearch-699c7dd54f-n5tmq` is listed
    as `172.17.0.5`. Since our machine is the node that this Pod runs on, we can access
    the Pod using this private IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Elasticsearch API should be listening to port `9200`. Therefore, if we
    make a `GET` request to `http://172.17.0.5:9200/`, we should expect Elasticsearch
    to reply with a JSON object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do the same for Pods `elasticsearch-699c7dd54f-pft9k` and `elasticsearch-699c7dd54f-pm2wz`,
    which have the IPs `172.17.0.4` and `172.17.0.6`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Although these Elasticsearch instances are deployed inside the same Kubernetes
    cluster, they are each inside their own Elasticsearch cluster (there are currently
    three Elasticsearch clusters, running independently from each other). We know
    this because the value of `cluster_uuid` for the different Elasticsearch instances
    are all different.
  prefs: []
  type: TYPE_NORMAL
- en: However, we want our Elasticsearch nodes to be able to communicate with each
    other, so that data written to one instance will be propagated to, and accessible
    from, other instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s confirm that this is not the case with our current setup. First, we
    will index a simple document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Already, we can see that the desired total number of shards is `2`, but we only
    have one shard.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm that the document is now indexed and accessible from the same
    Elasticsearch instance (running at `172.17.0.6:9200`), but not from any other
    Elasticsearch instances on our Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Before we continue, it's important to make the distinction between an Elasticsearch
    cluster and a Kubernetes cluster. Elasticsearch is a distributed data storage
    solution, where all data is distributed among one or more shards, deployed among
    one or more nodes. An Elasticsearch cluster can be deployed on any machines, and
    is completely unrelated to a Kubernetes cluster. However, because we are deploying
    a distributed Elasticsearch services on Kubernetes, the Elasticsearch cluster
    now resides within the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Networking for distributed databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the ephemeral nature of Pods, the IP addresses for Pods running a particular
    service (such as Elasticsearch) may change. For instance, the scheduler may kill
    Pods running on a busy node, and redeploy it on a more available node.
  prefs: []
  type: TYPE_NORMAL
- en: 'This poses a problem for our Elasticsearch deployment because:'
  prefs: []
  type: TYPE_NORMAL
- en: An Elasticsearch instance running on one Pod would not know the IP addresses
    of other instances running on other Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if an instance obtains a list of IP addresses of other instances, this
    list will quickly become obsolete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that Elasticsearch nodes cannot discover each other (this process
    is called **Node Discovery**), and is the reason why changes applied to one Elasticsearch
    node is not propagated to the others.
  prefs: []
  type: TYPE_NORMAL
- en: To resolve this issue, we must understand how Node Discovery works in Elasticsearch,
    and then figure out how we can configure Kubernetes to enable discovery for Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Elasticsearch's Zen discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch provides a discovery module, called **Zen Discovery**, that allows
    different Elasticsearch nodes to find each other.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Zen Discovery achieves this by pinging ports `9300` to `9305` on
    each loopback address (`127.0.0.0/16`), and tries to find Elasticsearch instances
    that respond to the ping. This default behavior provides auto-discovery for all
    Elasticsearch nodes running on the same machine.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the nodes reside on different machines, they won't be available
    on the loopback addresses. Instead, they will have IP addresses that are private
    to their network. For Zen Discovery to work here, we must provide a *seed list* of
    hostnames and/or IP addresses that other Elasticsearch nodes are running on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This list can be specified under the `discovery.zen.ping.unicast.hosts` property
    inside Elasticsearch''s configuration file `elasticsearch.yaml`. But this is difficult
    because:'
  prefs: []
  type: TYPE_NORMAL
- en: The Pod IP address that these Elasticsearch nodes will be running on is very
    likely to change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every time the IP changes, we'd have to go inside each container and update `elasticsearch.yaml`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fortunately, Elasticsearch allows us to specify this setting as an environment
    variable. Therefore, we can modify our `deployment.yaml` and add an `env` property
    under `spec.template.spec.containers`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Attaching hostnames to Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But what should the value of this environment variable be? Currently, the IP
    addresses of the Elasticsearch Pods is random (within a large range) and may change
    at any time.
  prefs: []
  type: TYPE_NORMAL
- en: To resolve this issue, we need to give each Pod a unique hostname that sticks
    to the Pod, even if it gets rescheduled.
  prefs: []
  type: TYPE_NORMAL
- en: When you visit a website, you usually won't type the site's IP address directly
    onto the browser; instead, you'd use the website's domain name. Even if the host
    of the website changes to a different IP address, the website will still be reachable
    on the same domain name. This is similar to what happens when we attach a hostname
    to a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we need to do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide each Pod with an identity using another Kubernetes Object called *StatefulSet.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attach a DNS subdomain to each Pod using a *Headless Service*, where the value
    of the subdomain is based on the Pod's identity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Working with StatefulSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've been using the Deployment object to deploy our Elasticsearch service.
    The Deployment Controller will manage the ReplicaSets and Pods under its control
    and ensure that the correct numbers are running and healthy.
  prefs: []
  type: TYPE_NORMAL
- en: However, a Deployment assumes that each instance is stateless and works independently
    from each other. More importantly, it assumes that instances are fungible—that
    one instance is interchangeable with any other. **Pods managed by a Deployment
    have identical identities.**
  prefs: []
  type: TYPE_NORMAL
- en: This is not the case for Elasticsearch, or other distributed databases, which
    must hold stateful information that distinguishes one Elasticsearch node from
    another. These Elasticsearch nodes need individual identities so that they can
    communicate with each other to ensure data is consistent across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides another API Object called **StatefulSet**. Like the Deployment
    object, StatefulSet manages the running and scaling of Pods, but it also guarantees
    the ordering and uniqueness of each Pod. **Pods managed by a StatefulSet have
    individual identities.**
  prefs: []
  type: TYPE_NORMAL
- en: 'StatefulSets are similar to Deployments in terms of definition, so we only
    need to make minimal changes to our `manifests/elasticsearch/deployment.yaml`.
    First, change the filename to `stateful-set.yaml`, and then change the `kind` property
    to StatefulSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, all the Pods within the StatefulSet can be identified with a name. The
    name is composed of the name of the StatefulSet, as well as the *ordinal index* of
    the Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Ordinal index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ordinal index, also known as **ordinal number** in set theory, is simply
    a set of numbers that are used to order a collection of objects, one after the
    other. Here, Kubernetes is using them to order, as well as identify each Pod.
    You can think of it akin to an auto-incrementing index in a SQL column.
  prefs: []
  type: TYPE_NORMAL
- en: The "first" Pod in the StatefulSet has an ordinal number of `0`, the "second"
    Pod has the ordinal number of `1`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Our StatefulSet is named `elasticsearch` and we indicated `3` replicas, so our
    Pods will now be named `elasticsearch-0`, `elasticsearch-1`, and `elasticsearch-2`.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, a Pod's cardinal index, and thus its identity, is *sticky—*if
    the Pod gets rescheduled onto another Node, it will keep this same ordinal and
    identity.
  prefs: []
  type: TYPE_NORMAL
- en: Working with services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By using a StatefulSet, each Pod can now be uniquely identified. However, the
    IP of each Pod is still randomly assigned; we want our Pods to be accessible from
    a stable IP address. Kubernetes provides the *Service* Object to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: The Service Object is very versatile, in that it can be used in many ways. Generally,
    it is used to provide an IP address to Kuberentes Objects like Pods.
  prefs: []
  type: TYPE_NORMAL
- en: The most common use case for a Service Object is to provide a single, stable,
    externally-accessible *Cluster IP* (also known as the *Service IP*) for a distributed
    service. When a request is made to this Cluster IP, the request will be proxied
    to one of the Pods running the service. In this use case, the Service Object is
    acting as a load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: However, that's not what we need for our Elasticsearch service. Instead of having
    a single cluster IP for the entire service, we want each Pod to have its own stable
    subdomain so that each Elasticsearch node can perform Node Discovery.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this use case, we want to use a special type of Service Object called** Headless
    Service**. As with other Kubernetes Objects, we can define a Headless Service
    using a manifest file. Create a new file at `manifests/elasticsearch/service.yaml` with
    the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go through what some of the fields mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '`metadata.name`: Like other Kuberentes Objects, having a name allows us to
    identify the Service by name and not ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec.selector`: This specifies the Pods that should be managed by the Service
    Controller. Specifically for Services, this defines the selector to select all
    the Pods that constitute a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec.clusterIP`: This specifies the Cluster IP for the Service. Here, we set
    it to `None` to indicate that we want a Headless Service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec.ports`: A mapping of how requests are mapped from a port to the container''s
    port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s deploy this Service into our Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: We don't need to actually run the Pods before we define a Service. A Service
    will frequently evaluate its selector to find new Pods that satisfy the selector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run `kubectl get service` to see a list of running services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Linking StatefulSet to a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s remove our existing `elasticsearch` Deployment Object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the final step is to create our StatefulSet, which provides each Pod with
    a unique identity, and link it to the Service, which gives each Pod a subdomain.
    We do this by specifying the name of the Service as the `spec.serviceName` property
    in our StatefulSet manifest file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the Service linked to the StatefulSet will get a domain with the following
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Our Service's name is `elasticsearch`. By default, Kubernetes will use the `default` namespace,
    and `cluster.local` as the Cluster Domain. Therefore, the Service Domain for our
    Headless Service is `elasticsearch.default.svc.cluster.local`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each Pod within the Headless Service will have its own subdomain, which has
    the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Or if we expand this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, our three replicas would have the subdomains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Updating Zen Discovery configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now combine these subdomains into a comma-separated list, and use it
    as the value for the `discovery.zen.ping.unicast.hosts` environment variable we
    are passing into the Elasticsearch containers. Update the `manifests/elasticsearch/stateful-set.yaml` file
    to read the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The final `stateful-set.yaml` should read as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can add this StatefulSet to our cluster by running `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check that the StatefulSet is deployed by running `kubectl get statefulset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We should also check that the Pods are deployed and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Note how each Pod now has a name with the structure `<statefulset-name>-<ordinal>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s `curl` port `9200` of each Pod and see if the Elasticsearch Nodes
    have discovered each other and have collectively formed a single cluster. We will
    be using the `-o` flag of `kubectl get pods` to extract the IP address of each
    Pod. The `-o` flag allows you to specify custom formats for your output. For example,
    you can get a table of Pod names and IPs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We will run the following command to get the Cluster ID of the Elasticsearch
    node running on Pod `elasticsearch-0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl get pod elasticsearch-0 -o=jsonpath=''{.status.podIP}''` returns the
    IP address of the Pod. This is then used to `curl` the port `9200` of this IP;
    the `-s` flag silences the progress information that cURL normally prints to `stdout`.
    Lastly, the JSON returned from Elasticsearch is parsed by the `jq` tool which
    extracts the `cluster_uuid` field from the JSON object.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The end result gives a Elasticsearch Cluster ID of `eeDC2IJeRN6TOBr227CStA`.
    Repeat the same step for the other Pods to confirm that they''ve successfully
    performed Node Discovery and are part of the same Elasticsearch Cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Perfect! Another way to confirm this is to send a `GET /cluster/state` request
    to any one of the Elasticsearch nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Validating Zen Discovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once all ES nodes have been discovered, most API operations are propagated from
    one ES node to another in a peer-to-peer manner. To test this, let's repeat what
    we did previously and add a document to one Elasticsearch node and test whether
    you can access this newly indexed document from a different Elasticsearch node.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s index a new document on the Elasticsearch node running inside
    the `elasticsearch-0` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try to retrieve this document from another Elasticsearch node (for
    example, the one running inside Pod `elasticsearch-1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Try repeating the same command for `elasticsearch-0` and `elasticsearch-2` and
    confirm that you get the same result.
  prefs: []
  type: TYPE_NORMAL
- en: Amazing! We've now successfully deployed our Elasticsearch service in a distributed
    manner inside our Kubernetes cluster!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying on cloud provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've deployed everything locally so that you can experiment freely
    without costs. But for us to make our service available to the wider internet,
    we need to deploy our cluster remotely, with a cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean supports running Kubernetes clusters, and so we will sign in to
    our DigitalOcean dashboard and create a new cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new remote cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After signing into your DigitalOcean account, click on the Kubernetes tab on
    your dashboard. You should be greeted with the message Get started with Kubernetes
    on DigitalOcean. Click on the Create a Cluster button and you will be shown a
    screen similar to how you configured your droplet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6b273ac-87cf-4bc0-9e9a-33a9dd3b42dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Make sure you select at least three Nodes, where each node has at least 4 GB
    of RAM. Then, click Create Cluster. You''ll be brought back to the main Kubernetes
    tab, where you can see that the cluster is being provisioned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0dc405da-9af2-4c4a-bb4e-cd9bed20c444.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the cluster and you''ll be brought to the Overview section for the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6c41811-0166-4695-b74e-3b9fd128978c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the Download Config button to download the configuration required
    to connect with our newly-created cluster on DigitalOcean. When you open it up,
    you should see something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s examine the fields to understand why they''re there:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion`, `kind`: These fields have the same meaning as before'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusters`: Define different clusters to be managed by `kubectl`, including
    the cluster''s server''s hostname, and certificates required to verify the identity
    of the server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`users`: Defines user credentials that are used to connect to a cluster; this
    may be certificates and keys, or simple usernames and passwords. You can use the
    same user to connect to multiple clusters, although normally you''d create a separate
    user for each cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context`: A grouping of clusters, users, and namespaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will take a few minutes for the nodes to initialize; in the meantime, let's
    see how we can configure `kubectl` to interact with our new remote cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Switching contexts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using `kubectl`, a context is a grouping of clusters, user credentials,
    and namespaces. `kubectl` uses information stored in these contexts to communicate
    with any cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we set up our local cluster using Minikube, it creates a default `minikube` context
    for us. We can confirm this by running `kubectl config current-context`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl` gets its configuration from the file specified by the `KUBECONFIG` environment
    variable. This was set in our `.profile` file to `$HOME/.kube/config`. If we look
    inside it, we will see that it is very similar to the config we downloaded from
    DigitalOcean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The `~/.kube/config` file records the IP address of the cluster's master API
    server, the credentials for our client to interact with it, and grouped the cluster
    information and user credentials together in the context object.
  prefs: []
  type: TYPE_NORMAL
- en: For `kubectl` to interact with our new DigitalOcean Hobnob cluster, we must
    update the `KUBECONFIG` environment variable to include our new configuration
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, copy the configuration file from DigitalOcean to a new file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, edit your `~/.profile` file and update the `KUBECONFIG` environment variable
    to include the new configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Save and source the file to make it apply to the current shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we run `kubectl config view`, we will see that configuration from
    both of our files has merged together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to make `kubectl` interact with our DigitalOcean cluster instead of our
    local cluster, all we have to do is change the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we run `kubectl cluster-info`, we get information about the remote
    cluster instead of the local one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Configuring nodes for Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned in the official Elasticsearch Guide ([https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults)),
    we must configure the node running Elasticsearch in a certain way when deploying
    on production. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Elasticsearch uses a `mmapfs` directory to store its indices. However,
    most systems set a limit of `65530` on mmap counts, which means Elasticsearch
    may run out of memory for its indices. If we do not change this setting, you''ll
    encounter the following error when trying to run Elasticsearch:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Therefore, we should change the `vm.max_map_count` kernel setting to at least `262144`.
    This can be done temporarily by running `sysctl -w vm.max_map_count=262144`, or
    permanently by adding it to a new file at `/etc/sysctl.d/elasticsearch.conf`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: UNIX systems impose an upper limit on the number of open files, or more specifically,
    the number of file descriptors. If you go over that limit, the process which is
    trying to open a new file will encounter the error `Too many open files`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a global limit for the kernel, which is stored at `/proc/sys/fs/file-max`;
    on most systems, this is a large number like `2424348`. There's also a hard and
    soft limit per user; hard limits can only be raised by the root, while soft limits
    can be changed by the user, but never go above the hard limit. You can check the
    soft limit on file descriptors by running `ulimit -Sn`; on most systems, this
    defaults to `1024`. You can check the hard limit by running `ulimit -Hn`; the
    hard limit on my machine is `1048576`, for example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Elasticsearch recommends that we change the soft and hard limit to at least `65536`.
    This can be done by running `ulimit -n 65536` as `root`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We need to make these changes for every node in our cluster. But first, let's
    return to our DigitalOcean dashboard to see if our nodes have been created successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Running commands on multiple servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When on your DigitalOcean dashboard, click into your cluster and go to the
    Nodes tab. Here, you should see that the nodes in your cluster have successfully
    been provisioned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30628f97-6df1-4e10-ac6d-7b0f2377a2f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can confirm this from the command line by running `kubectl get nodes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Because our current context is set to `do-nyc1-hobnob`, it will get the nodes
    on our remote cluster, and not the local cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the nodes are ready, how do we go about updating the Elasticsearch-specific
    settings mentioned previously? The simplest way is to SSH into each server and
    run the following three sets of commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: However, this becomes unmanageable once we have a large number of servers. Instead,
    we can use a tool called `pssh`.
  prefs: []
  type: TYPE_NORMAL
- en: Using pssh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tools such as `pssh` (parallel **ssh**, [https://github.com/robinbowes/pssh](https://github.com/robinbowes/pssh)), `pdsh` ([https://github.com/chaos/pdsh](https://github.com/chaos/pdsh)),
    or `clusterssh` ([https://github.com/duncs/clusterssh](https://github.com/duncs/clusterssh))
    allow you to issue commands simultaneously to multiple servers at once. Out of
    all of them, `pssh` is the easiest to install.
  prefs: []
  type: TYPE_NORMAL
- en: '`pssh` is listed in the APT registry, so we can simply update the registry
    cache and install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: This will actually install `pssh` under the name `parallel-ssh`; this was done
    to avoid conflict with the `putty` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use `kubectl get nodes` to programmatically get the IPs of all nodes
    in the cluster, and pass it to `parallel-ssh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: We are setting the `ssh` parameter `StrictHostKeyChecking` to `no` to temporarily
    disable `ssh` checking the authenticity of the nodes. This is insecure but offers
    convenience; otherwise, you'll have to add each node's key to the `~/.ssh/known_hosts` file.
  prefs: []
  type: TYPE_NORMAL
- en: Using init containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using `pssh` is acceptable, but it's an extra command we need to remember. Ideally,
    this configuration should be recorded inside `stateful-set.yaml`, so that the
    commands only run on nodes that have our Elasticsearch StatefulSet deployed there.
    Kuberentes provides a special type of Container called Init Containers, which
    allows us to do just that.
  prefs: []
  type: TYPE_NORMAL
- en: Init Containers are special Containers that run and exit before your "normal" *app
    Containers* are initiated. When multiple Init Containers are specified, they run
    in a sequential order. Also, if the previous Init Container exits with a non-zero
    exit status, then the next Init Container is not ran and the whole Pod fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows you to use Init Containers to:'
  prefs: []
  type: TYPE_NORMAL
- en: Poll for the readiness of other services. For instance, if your service X depends
    on another service Y, you can use an Init Container to poll service Y, and this
    exits only when service Y responds correctly. After the Init Container exits,
    the app container can begin its initialization steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update configurations on the node running the Pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, we can define Init Containers inside `stateful-set.yaml`, which will
    update the configurations on nodes running our Elasticsearch StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside `stateful-set.yaml`, under `spec.template.spec`, add a new field called `initContainers` with
    the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `busybox` Docker image. `busybox` is an image that "combines
    tiny versions of many common UNIX utilities into a single small executable". Essentially,
    it is an extremely lightweight (<5 MB) image that allows you to run many of the
    utility commands you'd expect from the GNU operating system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final `stateful-set.yaml` file should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: This configures our nodes in the same way as `pssh`, but with the added benefit
    of configuration-as-code, since it's now part of our `stateful-set.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Elasticsearch service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our `stateful-set.yaml` ready, it's time to deploy our Service and StatefulSet
    onto our remote cloud cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the moment, our remote cluster is not running anything apart from the Kubernetes
    Master Components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The Kubernetes Master Components are automatically deployed when we create a
    new cluster using DigitalOcean.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy our Service and StatefulSet, we will use `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Give it a minute or so, and run `kubectl get all` again. You should see that
    the Pods, StatefulSet, and our headless Service are running successfully!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Validating Zen Discovery on the remote cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's validate that all three Elasticsearch nodes has been successfully added
    to the Elasticsearch cluster once more. We can do this by sending a `GET` request
    to `/_cluster/state?pretty` and checking the output.
  prefs: []
  type: TYPE_NORMAL
- en: But since we want to keep the database service internal, we haven't exposed
    it to an external-reachable URL, so the only way to validate this is to SSH into
    one of the VPS and query Elasticsearch using its private IP.
  prefs: []
  type: TYPE_NORMAL
- en: However, `kubectl` provides a more convenient alternative. `kubectl` has a `port-forward` command,
    which forwards requests going into a port on `localhost` to a port on one of the
    Pods. We can use this feature to send requests from our local machine to each
    Elasticsearch instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that we have three Pods running Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'We can set up port forward on `elasticsearch-0` by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, on a separate terminal, send a `GET` request to `http://localhost:9200/_cluster/state?pretty`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `node` field contains three objects, representing each of
    our Elasticsearch instances. They are all part of the cluster, with a `cluster_uuid` value
    of `ZF1t_X_XT0q5SPANvzE4Nw`. Try port forwarding to the other Pods, and confirm
    that the `cluster_uuid` for those nodes are the same.
  prefs: []
  type: TYPE_NORMAL
- en: If everything worked, we have now successfully deployed the same Elasticsearch
    service on DigitalOcean!
  prefs: []
  type: TYPE_NORMAL
- en: Persisting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, we're not finished yet! Right now, if all of our Elasticsearch containers
    fail, the data stored inside them would be lost.
  prefs: []
  type: TYPE_NORMAL
- en: This is because containers are *ephemeral*, meaning that any file changes inside
    the container, be it addition or deletion, only persist for as long as the container
    persists; once the container is gone, the changes are gone.
  prefs: []
  type: TYPE_NORMAL
- en: This is fine for stateless applications, but our Elasticsearch service's primary
    purpose is to hold state. Therefore, similar to how we persist data using Volumes
    in Docker, we need to do the same with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kubernetes Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like Docker, Kubernetes has an API Object that's also called Volume, but there
    are several differences between the two.
  prefs: []
  type: TYPE_NORMAL
- en: With both Docker and Kubernetes, the storage solution that backs a Volume can
    be a directory on the host machine, or it can be a part of a cloud solution like
    AWS.
  prefs: []
  type: TYPE_NORMAL
- en: And for both Docker and Kubernetes, a Volume is an abstraction for a piece of
    storage that can be attached or mounted. The difference is which resource it is
    mounted to.
  prefs: []
  type: TYPE_NORMAL
- en: With Docker Volumes, the storage is mounted on to a directory inside the container.
    Any changes made to the contents of this directory would be accessible by both
    the host machine and the container.
  prefs: []
  type: TYPE_NORMAL
- en: With Kubernetes Volumes, the storage is mapped to a directory inside a Pod.
    Containers within the same Pod has access to the Pod's Volume. This allows containers
    inside the same Pod to share information easily.
  prefs: []
  type: TYPE_NORMAL
- en: Defining Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Volumes are created by specifying information about the Volume in the `.spec.volumes` field
    inside a Pod manifest file. The following manifest snippet will create a Volume
    of type `hostPath`, using the parameters defined in the `path` and `type` properties.
  prefs: []
  type: TYPE_NORMAL
- en: '`hostPath` is the Volume type most similar to a Docker Volume, where the Volume
    exists as a directory from the host node''s filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: This Volume will now be available to all containers within the Pod. However,
    the Volume is not automatically mounted onto each container. This is done by design
    because not all containers may need to use the Volume; it allows the configuration
    to be explicit rather than implicit.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mount the Volume to a container, specify the `volumeMounts` option in the
    container''s specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: The `mountPath` specifies the directory inside the container where the Volume
    should be mounted at.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this Pod, you first need to create a `/data` directory on your host
    machine and change its ownership to having a `UID` and `GID` of `1000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we run this Pod, you should be able to query it on `<pod-ip>:9200` and
    see the content written to the `/data` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Problems with manually-managed Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While you can use Volumes to persists data for individual Pods, this won''t
    work for our StatefulSet. This is because each of the replica Elasticsearch nodes
    will try to write to the same files at the same time; only one will succeed, the
    others will fail. If you tried, the following hanged state is what you''ll encounter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use `kubectl logs` to inspect one of the failing Pods, you''ll see the
    following error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Basically, before an Elasticsearch instance is writing to the database files,
    it creates a `node.lock` file. Before other instances try to write to the same
    files, it will detect this `node.lock` file and abort.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from this issue, attaching Volumes directly to Pods is not good for another
    reason—Volumes persist data at the Pod-level, but Pods can get rescheduled to
    other Nodes. When this happens, the "old" Pod is destroyed, along with its associated
    Volume, and a new Pod is deployed on a different Node with a blank Volume.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, scaling storage this way is also difficult—if the Pod requires more
    storage, you'll have to destroy the Pod (so it doesn't write anything to the Volume,
    create a new Volume, copy contents from the old Volume to the new, and then restart
    the Pod).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing PersistentVolume (PV)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To tackle these issues, Kubernetes provides the PersistentVolume (PV) object.
    PersistentVolume is a variation of the Volume Object, but the storage capability
    is associated with the entire cluster, and not with any particular Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Consuming PVs with PersistentVolumeClaim (PVC)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an administrator wants a Pod to use storage provided by a PV, the administrator
    would create a new **PersistentVolumeClaim** (**PVC**) object and assign that
    PVC Object to the Pod. A PVC object is simply a request for a suitable PV to be
    bound to the PVC (and thus the Pod).
  prefs: []
  type: TYPE_NORMAL
- en: After the PVC has been registered with the Master Control Plane, the Master
    Control Plane would search for a PV that satisfies the criteria laid out in the
    PVC, and bind the two together. For instance, if the PVC requests a PV with at
    least 5 GB of storage space, the Master Control Plane will only bind that PVC
    with PVs which have at least 5 GB of space.
  prefs: []
  type: TYPE_NORMAL
- en: After the PVC has been bound to the PV, the Pod would be able to read and write
    to the storage media backing the PV.
  prefs: []
  type: TYPE_NORMAL
- en: A PVC-to-PV binding is a one-to-one mapping; this means when a Pod is rescheduled,
    the same PV would be associated with the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a PersistentVolumeClaim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a Pod no longer needs to use the PersistentVolume, the PVC can simply be
    deleted. When this happens, what happens to the data stored inside the storage
    media depends on the PersistentVolume's Reclaim Poli*cy*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the Reclaim Policy is set to:'
  prefs: []
  type: TYPE_NORMAL
- en: Retain, the PV is retained—the PVC is simply released/unbounded from the PV.
    The data in the storage media is retained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete, it deletes both the PV and the data in the storage media.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleting a PersistentVolume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you no longer need a PV, you can delete it. But because the actually data
    is stored externally, the data will remain in the storage media.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with manually provisioning PersistentVolume
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Whil a PersistentVolume decouples storage from individual Pods, it still lacks
    the automation that we''ve come to expect from Kubernetes, because the cluster
    administrator (you) must manually interact with their cloud provider to provision
    new storage spaces, and then create a PersistentVolume to represent them in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b4d4ff5-c69d-437c-9c3f-f996836024c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Furthermore, a PVC to PV binding is a one-to-one mapping; this means we must
    take care when creating our PVs. For instance, let's suppose we have 2 PVCs—one
    requesting 10 GB and the other 40 GB. If we register two PVs, each of size 25GB,
    then only the 10 GB PVC would succeed, even though there is enough storage space
    for both PVCs.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic volume provisioning with StorageClass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To resolve these issues, Kubernetes provides another API Object called `StorageClass`.
    With `StorageClass`, Kubernetes is able to interact with the cloud provider directly.
    This allows Kubernetes to provision new storage volumes, and create `PersistentVolumes`
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, a `PersistentVolume` is a representation of a piece of storage, whereas
    `StorageClass` is a specification of *how* to create `PersistentVolumes` *dynamically*.
    `StorageClass` abstracts the manual processes into a set of fields you can specify
    inside a manifest file.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a StorageClass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For example, if you want to create a `StorageClass` that will create Amazon
    EBS Volume of type General Purpose SSD (`gp2`), you''d define a StorageClass manifest
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what each field means (required fields are marked with an asterik (`*`):'
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion`: The `StorageClass` object is provided in the `storage.k8s.io` API
    group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*provisioner`: The name of a *provisioner* that would prepare new storage
    spaces on-demand. For instance, if a Pod requests 10 GB of block storage from
    the `standard` StorageClass, then the `kubernetes.io/aws-ebs` provisioner will
    interact directly with AWS to create a new storage volume of at least 10 GB in
    size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*parameters`: The parameters that are passed to the provisioner so it knows
    how to provision the storage. Valid parameters depends on the provisioner. For
    example, both `kubernetes.io/aws-ebs` and `kubernetes.io/gce-pd` support the `type` parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*reclaimPolicy`: As with `PersistentVolumes,` the Reclaim Policy determines
    whether the data written to the storage media is retained or deleted. This can
    be either `Delete` or `Retain`, but it defaults to `Delete`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many types of provisioners available. Amazon EBS provisions *Block
    storage* on AWS, but there are other types of storage, namely file and object
    storage. We will be using block storage here because it provides the lowest latency,
    and is suitable for use with our Elasticsearch database.
  prefs: []
  type: TYPE_NORMAL
- en: Using the csi-digitalocean provisioner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DigitalOcean provides its own provisioner called CSI-DigitalOcean ([https://github.com/digitalocean/csi-digitalocean](https://github.com/digitalocean/csi-digitalocean)).
    To use it, simply follow the instructions in the `README.md` file. Essentially,
    you have go to the DigitalOcean dashboard, generate a token, use that to generate
    a Secret Kubernetes Object, and then deploy the StorageClass manifest file found
    at [https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-latest-stable.yaml](https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-latest-stable.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, because we are using the DigitalOcean Kubernetes platform, our Secret
    and the `csi-digitaloceanstorage` class is already configured for us, so we don''t
    actually need to do anything! You can check both the Secret and StorageClass using `kubectl
    get`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Note down the name of the StorageClass (`do-block-storage` here).
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning PersistentVolume to StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now need to update our `stateful-set.yaml` file to use the `do-block-storage` StorageClass.
    Under the StatefulSet spec (`.spec`), add a new field called `volumeClaimTemplates` with
    the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: This will use the `do-block-storage` class to dynamically provision 2 GB `PersistentVolumeClaim`
    Objects for any containers which mount it. The PVC is given the name `data` as
    a reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mount it to a container, add a `volumeMounts` property under the `spec` property
    of the container spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Elasticsearch writes its data to `/usr/share/elasticsearch/data`, so that's
    the data we want to persist.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring permissions on a bind-mounted directory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Elasticsearch runs inside the Docker container as the user `elasticsearch`,
    with both a `UID` and `GID` of `1000`. Therefore, we must ensure that the data
    directory (`/usr/share/elasticsearch/data`) and all its content is going to be owned
    by this the `elasticsearch` user so that Elasticsearch can write to them.
  prefs: []
  type: TYPE_NORMAL
- en: When Kubernetes bind-mounted the `PersistentVolume` to our `/usr/share/elasticsearch/data`,
    it was done using the `root` user. This means that the `/usr/share/elasticsearch/data` directory
    is no longer owned by the `elasticsearch` user.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, to complete our deployment of Elasticsearch, we need to use an Init
    Container to fix our permissions. This can be done by running `chown -R 1000:1000
    /usr/share/elasticsearch/data` on the node as `root`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following entry to the `initContainers` array inside `stateful-set.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'This basically mounts the `PersistentVolume` and updates its owner before the
    app Container starts initializing, so that the correct permissions would already
    be set when the app container executes. To summarize, your final `elasticsearch/service.yaml` should
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'And your final `elasticsearch/stateful-set.yaml` should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete your existing Services, StatefulSets, and Pods and try deploying them
    from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing Kubernetes Objects using the Web UI Dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You've been introduced to *a lot* of Kubernetes in this chapter—Namespaces,
    Nodes, Pods, Deployments, ReplicaSet, StatefulSet, DaemonSet, Services, Volumes,
    PersistentVolumes, and StorageClasses. So, let's take a mini-breather before we
    continue.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've been using `kubectl` for everything. While `kubectl` is great,
    sometimes, visual tools can help. The Kubernetes project provides a convenient
    Web UI Dashboard that allows you to visualize all Kubernetes Objects easily.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes Web UI Dashboard is different from the DigitalOcean Dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Both `kubectl` and the Web UI Dashboard make calls to the `kube-apiserver`,
    but the former is a command-line tool, whereas the latter provides a web interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the Web UI Dashboard is not deployed automatically. We''d normally
    need to run the following to get an instance of the Dashboard running on our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: However, both DigitalOcean and Minikube deploy this Dashboard feature by default,
    so we don't need to deploy anything.
  prefs: []
  type: TYPE_NORMAL
- en: Launching the Web UI Dashboard locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To launch the Web UI Dashboard for your local cluster, run `minikube dashboard`.
    This will open a new tab on your web browser with an Overview screen like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77b2e134-9ad8-4da4-b156-31fc7c23c35a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can use the menu on the left to navigate and view other Kubernetes Objects
    currently running in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d1dcb8b-1096-4beb-8124-f0feb93737f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Launching the Web UI Dashboard on a remote cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To access the Web UI Dashboard deployed on the remote cluster, the easier method
    is to use kubectl proxy to access the remote cluster's Kubernetes API. Simply
    run `kubectl proxy`, and the Web UI Dashboard should be available at [http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/](http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/).
  prefs: []
  type: TYPE_NORMAL
- en: We will continue using `kubectl` for the rest of this chapter, but feel free
    to switch to the Web UI Dashboard to get a more intuitive view of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the backend API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've deployed Elasticsearch, so let's carry on with the rest of the deployment—of
    our backend API and our frontend application.
  prefs: []
  type: TYPE_NORMAL
- en: The `elasticsearch` Docker image used in the deployment was available publicly.
    However, our backend API Docker image is not available anywhere, and thus our
    remote Kubernetes cluster won't be able to pull and deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to build our Docker images and make it available on a Docker
    registry. If we don't mind our image being downloaded by others, we can publish
    it on a public registry like Docker Hub. If we want to control access to our image,
    we need to deploy it on a private registry.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity's sake, we will simply publish our images publicly on Docker
    Hub.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing our image to Docker Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, go to [https://hub.docker.com/](https://hub.docker.com/) and create an
    account with Docker Hub. Make sure to verify your email.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click on Create | create Repository at the top navigation. Give the repository
    a unique name and press Create. You can set the repository to Public or Private
    as per your own preferences (at the time of writing this book, Docker Hub provides
    one free private repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6031fa93-6807-4158-8e27-de32a7e36ce0.png)'
  prefs: []
  type: TYPE_IMG
- en: The repository can be identified using `<namespace>/<repository-name>`, where
    the namespace is simply your Docker Hub username. You can find it on Docker Hub
    via the URL `hub.docker.com/r/<namespace>/<repository-name>/`.
  prefs: []
  type: TYPE_NORMAL
- en: If you have an organization, the namespace may be the name of the organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, return to your terminal and login using your Docker Hub credentials.
    For example, my Docker Hub username is `d4nyll`, so I would run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter your password when prompted, and you should see a message informing you
    of your `Login Succeeded`. Next, build the image (if you haven''t already):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, tag the local image with the full repository name on Docker Hub, as well
    as a tag that''ll appear on Docker Hub to distinguish between different versions
    of your image. The `docker tag` command you should run will have the following
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'In my example, I would run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, push the image onto Docker Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Confirm it has been successfully pushed by going to `https://hub.docker.com/r/<namespace>/<repository-name>/tags/`.
    You should see the tagged image appear there.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since our backend API is a stateless application, we don't need to deploy a
    StatefulSet like we did with Elasticsearch. We can simply use a simpler Kubernetes
    Object that we've encountered already—Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new manifest at `manifests/backend/deployment.yaml` with the following
    content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: For the `.spec.template.spec.containers[].env` field, add in the same environment
    variables that we passed in to our Docker image from the previous chapter (the
    ones we stored inside our `.env` file). However, for the `ELASTICSEARCH_PORT` variable,
    hard-code it to `"9200"`, and for `ELASTICSEARCH_HOSTNAME`, use the value `"http://elasticsearch"`.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering Services using kube-dns/CoreDNS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Kubernetes Components constitutes the essential parts of the Kubernetes
    platform, there are also *add-ons*, which extend the core functionalities. They
    are optional, but some are highly recommended and are often included by default.
    In fact, the Web UI Dashboard is an example of an add-on.
  prefs: []
  type: TYPE_NORMAL
- en: Another such add-on is `kube-dns`, a DNS server which is used by Pods to resolve
    hostnames.
  prefs: []
  type: TYPE_NORMAL
- en: '*CoreDNS* is an alternative DNS server which reached **General Availability**
    (**GA**) status in Kubernetes 1.11, replacing the existing `kube-dns` addon as
    the default. For our purposes, they achieve the same results.'
  prefs: []
  type: TYPE_NORMAL
- en: This DNS server watches the Kubernetes API for new Services. When a new Service
    is created, a DNS record is created that would route the name `<service-name>.<service-namespace>` to
    the Service's Cluster IP. Or, in the case of a Headless Service (without a cluster
    IP), a list of IPs of the Pods that constitutes the Headless Service.
  prefs: []
  type: TYPE_NORMAL
- en: This is why we can use `"http://elasticsearch"` as the value of the `ELASTICSEARCH_HOSTNAME` environment
    variable, because the DNS server will resolve it, even if the Service changes
    its IP.
  prefs: []
  type: TYPE_NORMAL
- en: Running Our backend Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our Deployment manifest ready, let''s deploy it onto our remote cluster.
    You should be familiar with the drill by now—simply run `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the status of the Deployment using `kubectl get all`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also check the logs for the backend Pods. If you get back a message
    saying the server is listening on port `8080`, the deployment was successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: Creating a backend Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we should deploy a Service that sits in front of the backend Pods. As
    a recap, every `backend` Pod inside the `backend` Deployment will have its own
    IP address, but these addresses can change as Pods are destroyed and created.
    Having a Service that sits in front of these Pods allow other parts of the application
    to access these backend Pods in a consistent manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new manifest file at `./manifests/backend/service.yaml` with the following
    content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'And deploy it using `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: Our `backend` Service is now reachable through its Cluster IP (`10.32.187.38`, in
    our example). However, that is a private IP address, accessible only within the
    cluster. We want our API to be available externally – to the wider internet. To
    do this, we need to look at one final Kubernetes Object—Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: Exposing services through Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Ingress is a Kubernetes Object that sits at the edge of the cluster and manages
    external access to Services inside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The Ingress holds a set of rules that takes inbound requests as parameters and
    routes them to the relevant Service. It can be used for routing, load balancing,
    terminate SSL, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the NGINX Ingress Controller
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An Ingress Object requires a Controller to enact it. Unlike other Kubernetes
    controllers, which are part of the `kube-controller-manager` binary, the Ingress
    controller is not. Apart from the GCE/Google Kubernetes Engine, the Ingress controller
    needs to be deployed separately as a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular Ingress controller is the NGINX controller ([https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx)),
    which is officially supported by Kubernetes and NGINX. Deploy it by running `kubectl
    apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mandatory.yaml` file contains a Deployment manifest that deploys the NGINX
    Ingress controller as a Pod with the label `app: ingress-nginx`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cloud-generic.yaml` file contains a Service manifest of type `LoadBalancer`,
    with a label selector for the label `app: ingress-nginx`. When deployed, this
    will interact with the DigitalOcean API to spin up an L4 network load balancer
    (note that this load balaner is *outside* our Kubernetes cluster):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfe922fc-ec13-44db-b9f3-5357ae2ddaf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The L4 load balancer will provide an external IP address for our end users
    to hit. The Kubernetes service controller will automatically populate the L4 load
    balancer with entries for our Pods, and set up health checks and firewalls. The
    end result is that any requests that hits the L4 load balancer will be forwarded
    to Pods that matches the Service''s selector, which, in our case, is the Ingress
    controller Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c12f1ca-0d08-448d-92b3-6b1fb5f087a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the request reaches the Ingress controller Pod, it can then examine the
    host and path of the request, and proxy the request to the relevant Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a1f616c-c407-4cd1-8c34-1e9d6673ea2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Give it a minute or two, and then check that the controller is created successfully
    by running `kubectl get pods`, specifying `ingress-nginx` as the namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: If you see a Pod named `nginx-ingress-controller-XXX` with the status `Running`,
    you're ready to go!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Ingress resource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our Ingress controller is running, we are ready to deploy our Ingress
    resource. Create a new manifest file at `./manifests/backend/ingress.yaml` with
    the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: The important parts lies at `.spec.rules`. This is a list of rules that checks
    the request's host and path, and if it matches, proxies the request to a specified
    Service.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we are matching any requests for the domain `api.hobnob.social` to
    our `backend` service, on port `8080`; likewise, we'll also forward requests for
    the host `docs.hobnob.social` to our `backend` Service, but on the `8100` port
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, deploy it with `kubectl apply`, and then wait for the address of the L4
    load balancer to appear in the `kubectl describe output`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: This means any requests with the hosts `api.hobnob.social` and `docs.hobnob.social` can
    now reach our distributed service!
  prefs: []
  type: TYPE_NORMAL
- en: Updating DNS records
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the `api.hobnob.social` and `docs.hobnob.social` domains can both
    be accessed through the load balancer, it''s time to update our DNS records to
    point those subdomains to the load balancer''s external IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a8987e9-3be9-4c40-901d-0a611c729ccb.png)'
  prefs: []
  type: TYPE_IMG
- en: After the DNS records have been propagated, go to a browser and try `docs.hobnob.social`.
    You should be able to see the Swagger UI documentation!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have successfully deployed our Elasticsearch instance and
    backend API on Kubernetes. We have learned the roles of each Component and the
    types of Objects each manages.
  prefs: []
  type: TYPE_NORMAL
- en: You've come a long way since we started! To finish it off, let's see if you
    can use what you've learned to deploy the frontend application on Kubernetes on
    your own.
  prefs: []
  type: TYPE_NORMAL
