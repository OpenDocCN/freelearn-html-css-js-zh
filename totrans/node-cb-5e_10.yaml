- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimizing Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance optimization is an endless activity. Further optimizations can always
    be made. The recipes in this chapter will demonstrate typical performance optimization
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: The performance optimization workflow starts with establishing a baseline. Often,
    this involves benchmarking our application in some way. In the case of a web server,
    this could be measuring how many requests our server can handle per second. A
    baseline measure must be recorded for us to have evidence of any performance improvements
    that have been made.
  prefs: []
  type: TYPE_NORMAL
- en: Once the baseline has been determined, the next step is to identify the bottleneck.
    The recipes in this chapter will cover using tools such as flame graphs and memory
    profilers to help us identify the specific bottlenecks in an application. Using
    these performance tools will ensure that our optimization efforts are invested
    in the correct place.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying a bottleneck is the first step to understanding where the optimization
    work should begin, and performance tools can help us determine the starting point.
    For instance, a flame graph can identify a specific function responsible for causing
    the bottleneck. After making the necessary optimizations, the changes must be
    verified by rerunning the initial baseline test. This allows us to have numerical
    evidence supporting whether the optimization has improved the application’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking HTTP requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting flame graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting memory leaks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing synchronous functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing asynchronous functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with worker threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should have the latest version of Node.js 22 installed, as well as access
    to a terminal. You will also need access to an editor and browser of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: The *Optimizing synchronous functions* recipe will require the use of MongoDB.
    We’ll be using Docker to provision a containerized MongoDB instance. Please refer
    to [*Chapter 7*](B19212_07.xhtml#_idTextAnchor212) , for detailed technical setup
    information regarding how to use MongoDB via Docker.
  prefs: []
  type: TYPE_NORMAL
- en: The code samples that will be used in this chapter can be found in this book’s
    GitHub repository at [https://github.com/PacktPublishing/Node.js-Cookbook-Fifth-Edition](https://github.com/PacktPublishing/Node.js-Cookbook-Fifth-Edition)
    , in the **Chapter10** directory.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking HTTP requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve seen throughout this book, HTTP communications are the foundation of
    many Node.js applications and microservices. For these applications, the HTTP
    requests should be handled as efficiently as possible. To be able to optimize,
    we must first record a baseline measure of our application’s performance. Once
    we’ve recorded the baseline, we’ll be able to determine the impact of our optimization
    efforts.
  prefs: []
  type: TYPE_NORMAL
- en: To create a baseline, it’s necessary to simulate the load on the application
    and record how it responds. For an HTTP-based application, we must simulate HTTP
    requests being sent to the server.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we’ll capture a baseline performance measure for an HTTP web
    server using a tool named **autocannon** ( [https://github.com/mcollina/autocannon](https://github.com/mcollina/autocannon)
    ), which will simulate HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll be using the **autocannon** tool to benchmark an Express.js
    web server. Instead of creating a web server from scratch, we’ll use the Express.js
    generator to create one. The web server will return an HTML page at **http://localhost:3000**
    :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following commands to use the Express.js generator to generate a
    sample web server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The **autocannon** tool is available on the **npm** registry. Globally install
    the **autocannon** module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we’ve created a web server to test, we’re ready to start this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll learn how to use the **autocannon** tool to benchmark
    HTTP requests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Express.js web server with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Navigate to **http://localhost:3000** in your browser. You should see the following
    output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Browser window showing the “Welcome to Express” web page](img/Figure_10.1_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Browser window showing the “Welcome to Express” web page
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve confirmed our server has started and is responding to requests at **http://localhost:3000**
    . Now, we can use the **autocannon** tool to benchmark our HTTP requests. Open
    a new terminal window and enter the following command to run a load test with
    **autocannon** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While the **autocannon** load test is running, switch to the terminal window
    where you started the web server. You should see a mass of incoming requests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.2 – The Express.js server receiving many HTTP GET requests](img/Figure_10.2_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – The Express.js server receiving many HTTP GET requests
  prefs: []
  type: TYPE_NORMAL
- en: 'Switch back to the terminal window where you’re running the **autocannon**
    load test. Once the load test is complete, you should see an output similar to
    the following, detailing the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.3 – autocannon results summary](img/Figure_10.3_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – autocannon results summary
  prefs: []
  type: TYPE_NORMAL
- en: Observe the table of results. The first table details the request latency. The
    average was recorded as **12.74** ms. The second table details the request volume.
    Here, it was recorded that our server handled an average of **7,555.2** requests
    per second, with an average throughput of **3.71** MB per second.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that, we’ve learned how to use the **autocannon** tool to benchmark HTTP
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **autocannon** tool is a cross-platform HTTP benchmarking tool written in
    Node.js and published to the **npm** registry.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we used **autocannon** to load test our Express.js web server
    at the **http://localhost:3000** endpoint. We passed **autocannon** the **--connections
    100** flag. This flag instructs **autocannon** to allocate a pool of **100** concurrent
    connections to our server. Had we omitted this flag, **autocannon** would have
    defaulted to allocating **10** concurrent connections. The number of concurrent
    connections should be altered to best represent the anticipated load on your server
    so that you can simulate production workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This recipe used the full-form command-line flags for **autocannon** for readability.
    However, as with many command-line flags, it’s possible to use an abbreviated
    form. The **--connections** flag can be abbreviated to **-c** and the **--duration**
    flag can be abbreviated to **-d** .
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that **autocannon** defaults to running the load test for **10** seconds,
    immediately sending a new request on each socket after the previous request has
    been completed. It’s possible to extend the length of the load test using the
    **--duration** flag. For example, you could use the following command to extend
    the load test shown in this recipe to **20** seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By default, **autocannon** outputs the data from the load test in two tables.
    The first table details the request latency, while the second table details the
    request volume.
  prefs: []
  type: TYPE_NORMAL
- en: '**Request latency** is the amount of time that’s elapsed between when a request
    is made, and a response is received. The request latency table is broken down
    into various percentiles. The **2.5%** percentile records the fastest **2.5%**
    of requests, whereas the **99%** percentile records the slowest **1%** of requests.
    When benchmarking requests, it can be useful to record and consider both the best
    and worst-case scenarios. The latency table also details the average, standard
    deviation, and maximum recorded latency. Generally, the lower the latency, the
    better.'
  prefs: []
  type: TYPE_NORMAL
- en: The request volume table details the number of requests per second ( **Req/Sec**
    ) and the throughput, which is recorded as the number of bytes processed per second
    ( **Bytes/Sec** ). Again, the results are broken down into percentiles so that
    the best and worst cases can be interpreted. For these two measures, the higher
    the number, the better, as it indicates more requests were processed by the server
    in the given timeframe.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about the available **autocannon** command-line flags,
    please refer to the *Usage* documentation on GitHub: [https://github.com/mcollina/autocannon#usage](https://github.com/mcollina/autocannon#usage)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we’ll cover how to use **autocannon** to benchmark HTTP **POST** requests.
    We’ll also consider how we can best replicate a production environment during
    our benchmarks and how this can change our latency and throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking HTTP POST requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this recipe, we benchmarked an HTTP **GET** request. The **autocannon** tool
    provides allows you to send requests using other HTTP methods, such as HTTP **POST**
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can use **autocannon** to send an HTTP **POST** request with
    a JSON payload:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same directory ( **benchmarking-http** ), create a file named **post-server.js**
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to define an endpoint on an Express.js server that will handle
    an HTTP **POST** request with a JSON payload. Add the following to **post-server.js**
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to start **post-server.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In a separate terminal window, enter the following command to load test the
    HTTP **POST** request. Note that we pass **autocannon** the **--method** , **--headers**
    , and **--** **body** flags:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As in the main recipe, **autocannon** will run the load test and output a results
    summary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This demonstrates how we can use **autocannon** to simulate other HTTP method
    requests, including requests with a payload.
  prefs: []
  type: TYPE_NORMAL
- en: Replicating a production environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When measuring performance, it’s important to replicate the production environment
    as closely as possible; otherwise, we may produce misleading results. The behavior
    of applications in development and production may differ, which can result in
    performance differences.
  prefs: []
  type: TYPE_NORMAL
- en: We can use an Express.js-generated application to demonstrate how performance
    results may differ, depending on the environment we’re running in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use **express-generator** to generate an Express.js application in a new directory
    named **benchmarking-views** . For more information on the Express.js generator,
    please refer to the *Creating an Express.js web application* recipe in [*Chapter
    6*](B19212_06.xhtml#_idTextAnchor178) . In this example, we’ll be using the **pug**
    view engine to generate a simple HTML page:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following command in your terminal to generate the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the server with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In a new terminal window, use **autocannon** to load test **http://localhost:3000**
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the load test has been completed, **autocannon** will output the load
    test results summary:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4 – autocannon result summary from the development mode run](img/Figure_10.4_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – autocannon result summary from the development mode run
  prefs: []
  type: TYPE_NORMAL
- en: In this load test, the average number of requests per second was around 1,584,
    and the average throughput was around 632 kB per second. This is considerably
    slower than the HTTP **GET** request that we benchmarked in the main recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why the requests are slower is that when in development mode, the
    pug templating engine will reload the template for every request. This is useful
    in development mode because changes to the template can be reflected without having
    to restart the server. When the mode is set to production, Express.js will no
    longer reload the template for every request. This will result in performance
    differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Restart the Express.js server in production mode using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, in your other terminal window, rerun the same benchmark test using **autocannon**
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compare the output between the two runs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.5 – autocannon result summary from the production mode run](img/Figure_10.5_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – autocannon result summary from the production mode run
  prefs: []
  type: TYPE_NORMAL
- en: In the second load test, we can see that the average number of requests per
    second has increased to approximately **8744** (up from **1584** ), and the throughput
    has increased to **3.49** MB per second (up from **632** kB). This performance
    increase is due to the template being cached when in production mode.
  prefs: []
  type: TYPE_NORMAL
- en: This highlights the need to benchmark our application in an environment that
    best represents the expected production environment.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Interpreting flame graphs* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Detecting memory leaks* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Optimizing synchronous functions* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Optimizing asynchronous functions* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting flame graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A flame graph is a visual tool that allows us to identify “hot code paths” within
    our application. The term “hot code path” is used to describe execution paths
    in the program that consume a relatively large amount of time, which can indicate
    a bottleneck in an application.
  prefs: []
  type: TYPE_NORMAL
- en: Flame graphs provide a visualization of an application’s call stack during execution.
    From this visualization, it’s possible to determine which functions are spending
    the most time on the CPU while the application is running.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we’re going to use the **0x** flame graph tool ( [https://github.com/davidmarkclements/0x](https://github.com/davidmarkclements/0x)
    ) to generate a flame graph for our Node.js application.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to create an application that we can profile. **Profiling** is a type
    of program analysis that measures how frequently and for how long functions or
    methods in our program are being used. We’ll use the Express.js generator to create
    a base application. Our application will use the **pug** view engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve generated an application, we’re ready to start generating a flame
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll be using the **0x** tool to profile our server and generate
    a flame graph. We’ll also need to use the **autocannon** tool, which we covered
    in the *Benchmarking HTTP requests* recipe of this chapter, to generate a load
    on our application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to ensure that we have both the **autocannon** and **0x** tools
    installed globally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, instead of starting our server with the **node** binary, we need to start
    it with the **0x** executable. If we open the **package.json** file, we’ll see
    that the **npm start** script is **node ./bin/www** . We need to substitute the
    **node** binary in the terminal command with **0x** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to generate some load on the server. In a new terminal window,
    use the **autocannon** benchmarking tool to generate a load by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expect to see the following output when the **autocannon** load test has been
    completed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.6 – autocannon result summary](img/Figure_10.6_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – autocannon result summary
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this load test, our server was handling **1512** requests per second
    on average.
  prefs: []
  type: TYPE_NORMAL
- en: Return to the terminal window where the server was started and press *Ctrl*
    + *C* . This will stop the server. At this point, **0x** will convert the captured
    stacks into a flame graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expect to see the following output after pressing *Ctrl* + *C* . This output
    details the location where **0x** has generated the flame graph. Observe that
    the **0x** tool has created a directory named **96552.0x** , where **96552** is
    the **process identifier** ( **PID** ) of the server process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.7 – The 0x tool generating a flame graph](img/Figure_10.7_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – The 0x tool generating a flame graph
  prefs: []
  type: TYPE_NORMAL
- en: Open the **flamegraph.html** file that’s been generated in the **flamegraph-app**
    directory with Google Chrome. You can do this by copying the path to the flame
    graph and pasting it into the Google Chrome address bar. Expect to see the generated
    flame graph and some controls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe that the bars in the flame graph are of different shades. A darker (redder)
    shade indicates a hot code path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Each generated flame graph may be slightly different, even when running the
    same load test. The flame graph that’s generated on your device is likely to look
    different from the output shown in this recipe. This is due to the non-deterministic
    nature of the profiling process, which may have subtle impacts on the flame graph’s
    output. However, generally, the flame graph’s overall results and bottlenecks
    are identified consistently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Identify one of the darker frames. In the example flame graph, we can see that
    the **readFileSync()** frame method has a darker shade – indicating that that
    function has spent a relatively large amount of time on the CPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 10.8 – \uFEFFAn overview of the 0x flame graph highlighting readFileSync()\
    \ as a hot frame](img/Figure_10.8_B19212.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – An overview of the 0x flame graph highlighting readFileSync()
    as a hot frame
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the darker frame. If it’s difficult to identify the frame, you can
    enter **readFileSync** into the **search** bar (top right), after which the frame
    will be highlighted. Upon clicking on the frame, **0x** will expand the parent
    and child stacks of the selected frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 10.9 – \uFEFFAn overview of the 0x flame graph showing a drilled-down\
    \ view of readFileSync()](img/Figure_10.9_B19212.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – An overview of the 0x flame graph showing a drilled-down view
    of readFileSync()
  prefs: []
  type: TYPE_NORMAL
- en: From the drilled-down view, we can see the hot code path. From the flame graph,
    we can make an educated guess about which functions it would be worthwhile to
    invest time in optimizing. In this case, we can see references to **handleTemplateCache()**
    . In the previous recipe, *Benchmarking HTTP requests* , we learned how **pug**
    reloads a template for each request when in development mode. This is the cause
    of this bottleneck. Let’s change the application so that it runs in production
    mode and see what the impact is on the load test results and flame graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Restart the Express.js server in production mode with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rerun the load test using the **autocannon** tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the results of the load test, we can see that our server is handling more
    requests per second. In this run, our load test reported that our server handled
    an average of around **7688** requests per second, up from around **1512** before
    we changed the Express.js server so that it runs in production mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.10 – autocannon result summary from the production mode run](img/Figure_10.10_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – autocannon result summary from the production mode run
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, once the **autocannon** load test is complete, stop your server
    using *Ctrl* + *C* . A new flame graph will be generated. Open the new flame graph
    in your browser and observe that the new flame graph is a different shape from
    the first. Observe that the second flame graph highlights a different set of darker
    frames. This is because we’ve resolved our first bottleneck. Hot code paths are
    relative. Despite having increased the performance of our application, the flame
    graph will identify the next set of hot code paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 10.11 – \uFEFFAn overview of the 0x flame graph from production mode](img/Figure_10.11_B19212.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – An overview of the 0x flame graph from production mode
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve used **0x** to generate a flame graph, which has enabled us
    to identify a bottleneck in our application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used the **0x** tool to profile and generate a flame graph
    for our application. Our application was a small, generated Express.js web server.
    The **autocannon** tool was used to add load to our web server so that we could
    produce a flame graph that’s representative of a production workload.
  prefs: []
  type: TYPE_NORMAL
- en: To use the **0x** tool, we had to start our server with **0x** . When we start
    an application with **0x** , two processes are started.
  prefs: []
  type: TYPE_NORMAL
- en: The first process uses the Node.js binary, **node** , to start our program.
    When **0x** starts the node process, it passes the **--perf-basic-prof** command-line
    flag to the process. This command-line flag allows C++ V8 function calls to be
    mapped to the corresponding JavaScript function calls.
  prefs: []
  type: TYPE_NORMAL
- en: The second process starts the local system’s stack tracing tool. On Linux, the
    **perf** tool will be invoked, whereas on macOS and SmartOS, the **dtrace** tool
    will be invoked. These tools capture the underlying C-level function calls.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying system stack tracing tool will take samples. A **sample** is
    a snapshot of all the functions being executed by the CPU at the time the sample
    was taken, which will also record the parent function calls.
  prefs: []
  type: TYPE_NORMAL
- en: The sampled stacks are grouped based on the call hierarchy, grouping the parent
    and child function calls together. These groups are what’s known as a **flame**
    , hence the name **flame graph** . The same function may appear in multiple flames.
  prefs: []
  type: TYPE_NORMAL
- en: Each line in a flame is known as a frame. A **frame** represents a function
    call. The width of the frame corresponds to the amount of time that that function
    was observed by the profiler on the CPU. The time representation of each frame
    aggregates the time that all child functions take as well, hence the triangular
    or *flame* shape of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Darker (redder) frames indicate that a function has spent more time at the top
    of the stack relative to the other functions. This means that this function is
    spending a lot of time on the CPU, which indicates a potential bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Chrome DevTools can also be used to profile the CPU, which can help identify
    bottlenecks. Using the **--inspect** command-line flag, the Node.js process can
    be debugged and profiled using Chrome DevTools. Please refer to the *Debugging
    with Chrome DevTools* recipe in [*Chapter 12*](B19212_12.xhtml#_idTextAnchor388)
    for more information on using Chrome DevTools to debug a Node.js program.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Creating an Express.js web application* recipe in [*Chapter 6*](B19212_06.xhtml#_idTextAnchor178)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Benchmarking HTTP requests* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Detecting memory leaks* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Optimizing synchronous functions* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Optimizing asynchronous functions* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Debugging with Chrome DevTools* recipe in [*Chapter 12*](B19212_12.xhtml#_idTextAnchor388)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting memory leaks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Memory leaks can drastically reduce your application’s performance and can lead
    to crashes. V8 manages objects and dynamic data in its heap, a binary tree-based
    structure designed to manage parent-child node relationships. The V8 **Garbage
    Collector** ( **GC** ) is responsible for managing the heap. It reclaims any memory
    that is no longer in use – freeing the memory so that it can be reused.
  prefs: []
  type: TYPE_NORMAL
- en: A memory leak occurs when a block of memory is never reclaimed by the GC and
    is therefore idle and inefficient. This results in pieces of unused memory remaining
    on the heap. The performance of your application can be impacted when many of
    these unused memory blocks accumulate in the heap. In the worst cases, the unused
    memory could consume all the available heap space, which, in turn, can cause your
    application to crash.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we’ll learn how to use Chrome DevTools to profile memory, enabling
    us to detect and fix memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe will require you to have Chrome DevTools installed, which is integrated
    into the Google Chrome browser. Visit [https://www.google.com/chrome/](https://www.google.com/chrome/)
    to download Google Chrome:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll be using the **autocannon** tool to direct load to our application. Install
    **autocannon** from the **npm** registry with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to create a directory to work in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a file named **leaky-server.js** . This HTTP server will intentionally
    contain a memory leak:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following to **leaky-server.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we’ve installed the necessary tools and created a sample application
    containing a memory leak, we’re ready to move on to this recipe’s steps.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll use Chrome DevTools to identify a memory leak:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory leaks can get progressively worse the longer an application is running.
    Sometimes, it can take several days or weeks of an application running before
    the memory leak causes the application to crash. We can use the Node.js process
    **--max-old-space-size** command-line flag to increase or reduce the maximum V8
    old memory size (in MB). To demonstrate the presence of the memory leak, we’ll
    set this to a very small value. Start **leaky-server.js** with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In a second terminal window, use the **autocannon** tool to direct load to
    the server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Back in the terminal window where you started the server, observe that the
    server crashed with **JavaScript heap out** **of memory** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.12 – JavaScript heap out of memory error](img/Figure_10.12_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – JavaScript heap out of memory error
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we’ll start using Chrome DevTools to profile our application. First, we
    must restart the server with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Navigate to **chrome://inspect** in Google Chrome and click **inspect** (underneath
    **leaky-server.js** ). This should open the Chrome DevTools interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ensure you’re on the **Memory** tab and that **Heap snapshot** is selected.
    Click **Take snapshot** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.13 – The Chrome DevTools Memory interface](img/Figure_10.13_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – The Chrome DevTools Memory interface
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see **Snapshot 1** appear on the left of the interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Chrome DevTools memory snapshot interface](img/Figure_10.14_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Chrome DevTools memory snapshot interface
  prefs: []
  type: TYPE_NORMAL
- en: 'Return to your second terminal window and rerun the **autocannon** benchmark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the load test has been completed, return to your Chrome DevTools window.
    Return to the **Profiles** interface of the **Memory** tab and take another snapshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Chrome DevTools memory snapshot interface](img/Figure_10.15_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – Chrome DevTools memory snapshot interface
  prefs: []
  type: TYPE_NORMAL
- en: Note **MaxListenersExceededWarning** in the **Console** tab – this will be covered
    in more detail in the *There’s* *more…* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have two snapshots, we can use Chrome DevTools to compare them.
    To do this, change the drop-down window from **Summary** to **Comparison** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Chrome DevTools memory snapshot comparison interface](img/Figure_10.16_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – Chrome DevTools memory snapshot comparison interface
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe that the constructors are now sorted by delta – the difference between
    two snapshots. Expand the **(array)** constructor and the **(object elements)
    [ ]** object within it; you should see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.17 – Chrome DevTools memory snapshot comparison interface expanded](img/Figure_10.17_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 – Chrome DevTools memory snapshot comparison interface expanded
  prefs: []
  type: TYPE_NORMAL
- en: 'The expanded view indicates that there are masses of **connectionListener()**
    events stemming from *line 4* of **leaky-server.js** . If we take a look at that
    line, we’ll see that it starts on the **server.on(''connection'',...** block.
    This is our memory leak. We’re registering a listener for the connected event
    upon every request, causing our server to eventually run out of memory. We need
    to move this event listener outside of our request handler function. Create a
    new file named **server.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following to **server.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Close the Chrome DevTools window and then rerun the same experiment. Start
    the server with **$ node --inspect server.js** and take a snapshot. In a second
    terminal window, direct load to the server with **$ autocannon http://localhost:3000**
    and take another snapshot. Now, when we compare the two, we’ll see that the **#
    Delta** value of the **(array)** constructors has significantly reduced:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.18 – Chrome DevTools memory snapshot comparison interface](img/Figure_10.18_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 – Chrome DevTools memory snapshot comparison interface
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the **MaxListenersExceededWarning** warning is no longer appearing,
    indicating that we’ve fixed our memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve learned how to take heap snapshots of our application, enabling
    us to diagnose a memory leak in our application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The V8 JavaScript engine is used by both Google Chrome and Node.js. The common
    underlying engine means that we can use Chrome DevTools to debug and profile Node.js
    applications. To enable the debugging client, we must pass the **--inspect** command-line
    flag to the **node** process. Passing this flag instructs the V8 inspector to
    open a port that accepts WebSocket connections. The WebSocket connection allows
    the client and V8 inspector to interact.
  prefs: []
  type: TYPE_NORMAL
- en: The V8 JavaScript engine retains a heap of all the objects and primitives referenced
    in our JavaScript code. The JavaScript heap can be exposed via an internal V8
    API ( **v8_inspector** ). Chrome DevTools uses this internal API to provide tooling
    interfaces, including the **Memory Profiler** interface we used in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We used the **Memory** interface of Chrome DevTools to take an initial heap
    snapshot of the server. This snapshot is considered our baseline. Then, we generated
    load on the server using the **autocannon** tool to simulate usage over time.
    For our server, the memory leak could be observed with the default **autocannon**
    load ( **10** connections for **10** seconds). Some memory leaks may only be observable
    under considerable load; in these cases, we’d need to simulate a more extreme
    load on the server, potentially for a longer period.
  prefs: []
  type: TYPE_NORMAL
- en: autocannon
  prefs: []
  type: TYPE_NORMAL
- en: The *Benchmarking HTTP requests* recipe in this chapter goes into more detail
    about how we can simulate more extreme server loads with the **autocannon** tool.
  prefs: []
  type: TYPE_NORMAL
- en: Once we directed the load to our server, we took a second heap snapshot. This
    showed how much impact the load had on the heap size. Our second snapshot was
    much larger than the first, which is an indication of a memory leak. The heap
    snapshot **Comparison** view can be utilized to identify which constructors have
    the largest deltas.
  prefs: []
  type: TYPE_NORMAL
- en: From inspecting and expanding the **(array)** constructor, we found a long list
    of **connection** **Listener()** events stemming from *line 4* of our **leaky-server.js**
    file. This enabled us to identify the memory leak. Note that the **(array)** constructor
    refers to an internal structure used by V8. For a JavaScript array, the constructor
    would be named **Array** .
  prefs: []
  type: TYPE_NORMAL
- en: Once the memory leak has been identified and fixed, it’s prudent to rerun the
    test and confirm that the new heap snapshot shows a reduction in deltas. The snapshot
    is still likely to be larger than the initial baseline snapshot because of the
    load. However, it shouldn’t be as drastically large as it was with our **leaky-server.js**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, when under load, **leaky-server.js** emitted **MaxListenersExceededWarning**
    before crashing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, Node.js allows a maximum of **10** listeners to be registered for
    a single event. In **leaky-server.js** , we were registering a new listener for
    each request. Once our application registered the 11th request, it emitted **MaxListenersExceededWarning**
    . This is an early warning sign of a memory leak. It’s possible to change the
    maximum number of listeners. To change the threshold for an individual **EventEmitter**
    instance, we can use the **emitter.setMaxListeners()** method. For example, to
    lower the maximum number of listeners on our server to **1** , we could change
    **leaky-server.js** to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, if we were to run the same experiment, we’d see the following error after
    just two event listeners were registered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s also possible to use the **EventEmitter.defaultMaxListeners** property
    to change the default maximum listeners for all **EventEmitter** instances. This
    should be done with caution as it will impact all **EventEmitter** instances.
    You could use the following to set the **EventEmitter.defaultMaxListeners** value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note that **emitter.setMaxListeners()** will always take precedence over the
    global default set via **EventEmitter.defaultMaxListeners** . Before raising the
    maximum threshold of listeners, it’s worth considering whether you’re inadvertently
    masking a memory leak in your application.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Interpreting flame graphs* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Optimizing synchronous functions* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Optimizing asynchronous functions* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Debugging with Chrome DevTools* recipe in [*Chapter 12*](B19212_12.xhtml#_idTextAnchor388)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing synchronous functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous recipes of this chapter covered how to detect hot code paths in
    our applications. Once a hot code path is identified, we can focus our optimization
    efforts on it to reduce the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to optimize any hot code paths as any function that takes a long
    time to process can prevent I/O and other functions from executing, impacting
    the overall performance of your application.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will cover how to micro-benchmark and optimize a synchronous function.
    A **micro-benchmark** is a type of performance test that focuses on a small, specific
    piece of code or functionality within a larger system. We’ll use Benchmark.js
    ( [https://benchmarkjs.com/](https://benchmarkjs.com/) ) to create a micro-benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In real applications, we’d use tooling such as flame graphs or profilers to
    identify slow functions in our applications. For this recipe, we’ll create a single
    slow function that we can learn how to micro-benchmark and optimize:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a directory for this recipe’s code and initialize the project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to install Benchmark.js:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we’ve initialized our directory, we can start this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s assume that we’ve identified a bottleneck in our code base and it happens
    to be a function called **sumOfSquares()** . Our task is to make this function
    faster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a file named **slow.js** , which will hold our unoptimized
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following to **slow.js** to create the slow **sumOfSquares()** implementation.
    This uses the **Array.from()** method to generate an array of integers. The **map**
    function is used to square each number in the array, while the **reduce** function
    is used to sum the elements of the array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have a slow version of our function, let’s turn it into a module
    so that we can benchmark it with ease. If our function formed part of a larger
    script or application, it would be worthwhile trying to extract it into a standalone
    script or module to enable it to be benchmarked in isolation. Add the following
    line to the bottom of **slow.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can write a micro-benchmark for our **sumOfSquares()** function using
    Benchmark.js. Create a file named **benchmark.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code to **benchmark.js** to create a benchmark for our **sumOfSquares()**
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This file contains the configuration of Benchmark.js, a single benchmark that
    calls our **slow.js** module, and a **printResults()** function, which outputs
    the benchmark run information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can run the benchmark with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s generate a flame graph using the **0x** tool. A flame graph can help
    us identify which of the lines of our code are spending the most time on the CPU.
    Generate a flame graph with **0x** by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open the flame graph in your browser. In the following example, there’s one
    pink frame, indicating a hot code path. Hover over the hotter frames to identify
    which line of the application they’re referring to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 10.19 – \uFEFFAn overview of the 0x flame graph showing a hot frame\
    \ on line 9 of slow.js](img/Figure_10.19_B19212.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 – An overview of the 0x flame graph showing a hot frame on line
    9 of slow.js
  prefs: []
  type: TYPE_NORMAL
- en: In the flame graph, we can see that the hottest function is an anonymous function
    on *line 9* of **slow.js** . If we look at our code, we’ll see that this points
    to our use of **Array.reduce()** . Note that the line number may be different
    should you have formatted this recipe’s code differently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we suspect that it’s the use of **Array.reduce()** that’s slowing our operations
    down, we should try rewriting the function in a procedural form (using a **for**
    loop) to see whether it improves the performance. Create a file named **loop.js**
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following to **loop.js** to create a procedural implementation of the
    **sumOfSquares()** function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s add a benchmark for the implementation of the **sumOfSquares()**
    function in **loop.js** . First, import the **loop.js** module by adding the following
    line below the **slow.js** import in **benchmark.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, add a new benchmark to the suite, below the slow run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rerun the benchmark. This time, it will run both of our implementations and
    determine which one is fastest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, we’ve confirmed that our procedural/loop implementation of the **sumOfSquares()**
    function is much faster than the original implementation.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe stepped through the process of optimizing a synchronous function
    call, starting with the slow implementation of a **sumOfSquares()** function.
  prefs: []
  type: TYPE_NORMAL
- en: We created a micro-benchmark using Benchmark.js to create a baseline measure
    of our initial **sumOfSquares()** implementation in **slow.js** . This baseline
    measure is called a micro-benchmark. **Micro-benchmarks** are used to benchmark
    a small facet of an application. In our case, it was for the single **sumOfSquares()**
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Once our micro-benchmark was created, we ran the benchmark via **0x** to generate
    a flame graph. This flame graph enabled us to identify which frames were spending
    the most time on the CPU, which provided us with an indication of which specific
    line of code within our **sumOfSquares()** function was the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: From the flame graph, we determined that the use of the **map** and **reduce**
    functions of **sumOfSquares()** was slowing the operation down. Therefore, we
    created a second implementation of **sumOfSquares()** . The second implementation
    used traditional procedural code (a **for** loop). Once we had the second implementation
    of the function, in **loop.js** , we added it to our benchmarks. This allowed
    us to compare the two implementations to see which was faster.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the number of operations that could be handled per second, **loop.js**
    was found to be significantly faster than the initial **slow.js** implementation.
    The benefit of writing a micro-benchmark is that you have evidence and confirmation
    of your optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Benchmarking HTTP requests* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Interpreting flame graphs* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Detecting memory leaks* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Optimizing asynchronous functions* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Working with worker threads* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing asynchronous functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Node.js runtime was built with I/O in mind, hence its asynchronous programming
    model. In the previous recipes of this chapter, we explored how to diagnose performance
    issues within synchronous JavaScript functions.
  prefs: []
  type: TYPE_NORMAL
- en: However, a performance bottleneck may occur as part of an asynchronous workflow.
    In this recipe, we’ll cover profiling and optimizing an asynchronous performance
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll diagnose a bottleneck in an Express.js web server that
    communicates with a MongoDB database. For more information on MongoDB, please
    refer to the *Storing and retrieving data with MongoDB* recipe in [*Chapter 5*](B19212_05.xhtml#_idTextAnchor139)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start MongoDB, we’ll use Docker (as we did in [*Chapter 5*](B19212_05.xhtml#_idTextAnchor139)
    ). Ensuring that you have Docker running, enter the following command in your
    terminal to initialize a MongoDB database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to create a directory to work in. We’ll also install the **express**
    and **mongodb** modules from **npm** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To simulate a real application, some data needs to be present in MongoDB. Create
    a file named **values.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following to **values.js** . This creates a load script that will enter
    a series of numbers into our MongoDB database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the **values.js** script to populate the database for this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure the **0x** and **autocannon** performance tools are installed globally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that our directory has been initialized and a MongoDB database is available
    with some sample data, let’s start this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’re going to diagnose a bottleneck in a web application that
    communicates with a MongoDB database. We’ll build a sample application that calculates
    the average of all the values stored in the database:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file named **server.js** . This will store our server that calculates
    the average of the values in the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A dd the following code to **server.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the server by entering the following command in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Navigate to **http://localhost:3000** in your browser to check that the server
    is running. Expect to see a message printing the average of the random values
    we persisted to the database in the *Getting* *ready* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a second terminal, we’ll use the **autocannon** benchmarking tool to simulate
    a load on the server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expect to see the following **autocannon** result summary once the load test
    has been completed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.20 – autocannon result summary for server.js](img/Figure_10.20_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 – autocannon result summary for server.js
  prefs: []
  type: TYPE_NORMAL
- en: This load test shows an average of around **317** requests per second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see where the bottlenecks are in our application. We’ll use the
    **0x** tool to generate a flame graph. Restart the server with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the second terminal, let’s simulate a load on the server again using the
    **autocannon** tool:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Stop the server and open the generated flame graph in your browser. Expect
    a flame graph similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 10.21 – \uFEFFAn overview of the 0x flame graph showing deserializeObject()\
    \ hot frames](img/Figure_10.21_B19212.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.21 – An overview of the 0x flame graph showing deserializeObject()
    hot frames
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in the *Interpreting flame graphs* recipe of this chapter, the
    darker/more red frames can indicate bottlenecks in our application. In our example,
    the **deserializeObject()** function appears to be the hottest, meaning it was
    spending the most amount of time on the CPU. This is a commonly observed bottleneck
    in MongoDB-based applications. The bottleneck in **deserializeObject()** is related
    to the large amount of data we’re querying and receiving from our MongoDB instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s try and solve this bottleneck by precomputing and storing the average
    in the database. This should help by reducing the amount of data we request from
    MongoDB and removing the need to calculate the average. We’ll create a script
    called **calculate-average.js** that calculates the average and stores it in MongoDB.
    Create the **calculate-average.js** file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following code to **calculate-average.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the **calculate-averages.js** script to calculate and store the average
    in the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can rewrite the server so that it returns the stored average, rather
    than calculating it upon each request. Create a new file named **server-no-processing.js**
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following to **server-no-processing.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s rerun the **autocannon** benchmark. Start the server with **$ node server-no-process.js**
    . Then, in a second terminal window, rerun the **autocannon** load test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expect to see the **autocannon** result summary once the load test has been
    completed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.22 – autocannon result summary for server-no-processing.js](img/Figure_10.22_B19212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.22 – autocannon result summary for server-no-processing.js
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the average number of requests per second has increased
    from around **317** in **server.js** to **6430** using the precomputed average
    in **server-no-processing.js** .
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we learned how obtaining and processing large amounts of data
    from MongoDB can introduce bottlenecks in our application. We solved the bottleneck
    showcased in this recipe by precomputing and storing the average.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe demonstrated a bottleneck in an application that communicated with
    a MongoDB database.
  prefs: []
  type: TYPE_NORMAL
- en: The slowness was caused by both the large amount of data being requested and
    the calculation of the average upon each request. By using the **0x** tool to
    generate a flame graph, it was possible to diagnose the specific function that
    was causing the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the bottleneck was solved by precomputing the average and storing
    it in the database. This meant that instead of having to query the database for
    all values and computing the average on each request, it was possible to just
    query and obtain the average directly. This showed a significant increase in performance.
  prefs: []
  type: TYPE_NORMAL
- en: It was worthwhile amending the data model to store the precomputed average so
    that it didn’t need to be calculated on each request. However, in a real application,
    it may not always be possible to edit the data model to store computed values.
    When building a new application, it’s worth considering what data should be stored
    in the data model to minimize computation on the live server.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-optimizations, such as precomputing an average, can enhance performance
    by reducing runtime computation. These small improvements can boost efficiency,
    especially under heavy load. However, premature optimizations can complicate code,
    making maintenance harder. As such, it’s usually recommended to prioritize optimizations
    that offer substantial performance gains for your application and end users.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Creating an Express.js web application* recipe in [*Chapter 6*](B19212_06.xhtml#_idTextAnchor178)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Storing and retrieving data with MongoDB* recipe in [*Chapter 7*](B19212_07.xhtml#_idTextAnchor212)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Benchmarking HTTP requests* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Detecting memory leaks* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Optimizing synchronous functions* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Working with worker threads* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with worker threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JavaScript is a single-threaded programming language, meaning that it executes
    one task at a time within a process. Node.js also runs on a single thread, but
    it uses an event loop to handle asynchronous operations, enabling non-blocking
    I/O calls. Despite this, the event loop processes one task at a time. As a result,
    CPU-intensive tasks can block the event loop and degrade the overall performance
    of your application.
  prefs: []
  type: TYPE_NORMAL
- en: To handle CPU-intensive tasks in Node.js efficiently, you should consider using
    worker threads. Worker threads were declared stable in Node.js version 12 and
    later and are accessible through the core **worker_threads** core module. The
    worker threads API allows you to run JavaScript code in parallel across multiple
    threads, making it well-suited for CPU-intensive operations.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial will introduce the **worker_threads** module and demonstrate how
    to use it to manage CPU-intensive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, ensure you’re using Node.js 22. Then, create a project directory to
    work in named **worker-app** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve created a directory to work in, we can start this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll learn how to leverage worker threads to handle a CPU-intensive
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by creating a simplified worker that returns the **Hello <name>!**
    string. Create a file named **hello-worker.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In **hello-worker.js** , we need to import the necessary class and methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to create an **if** statement using the **isMainThread()** method
    from the **worker_threads** module. Anything within the **if** block will be executed
    on the main thread. Code within the **else** block will be executed in the worker.
    Add the following to **hello-worker.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s populate the main thread code. First, create a new worker and pass
    the **Worker** constructor two arguments. The first argument will be the filename
    of the worker’s main script or module. In this case, we’ll use **__filename**
    to reference our current file. The second parameter will be an **options** object,
    which will specify a **workerData** property that holds the name we want to pass
    through to the worker thread. The **workerData** property is used to share values
    with the worker thread. Add the following line under the **// Main thread** **code**
    comment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, expect the worker thread to pass a value back to the main thread. To capture
    this, we can create a worker message event listener. Add the following line below
    the worker initialization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can write the worker code that will construct the greeting. Using the
    **parentPort.postMessage()** method will return the value to our main thread.
    Add the following code below the **// Worker** **code** comment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, run the program with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s try something CPU-intensive and compare the behaviors when using
    and not using worker threads. First, create a file named **fibonacci.js** . This
    will contain a Fibonacci calculator program that returns the Fibonacci number
    at a given index. Create the **fibonacci.js** file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following to **fibonacci.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the script with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, the **fibonacci()** function blocks the execution of **console.log("...");**
    until the **fibonacci()** function has finished running.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s try writing it using worker threads to see how we can avoid blocking
    the main thread. Create a file named **fibonacci-worker.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start by adding the following imports to **fibonacci-worker.js** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, as we did in **fibonacci.js** in *Step 8* , add the **Fibonacci** **calculator**
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can implement the structure that enables us to use the **worker**
    thread. Add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, run this script with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Observe that **console.log("...");** is being printed before the result of the
    **fibonacci()** function returns. The **fibonacci()** function has been offloaded
    to the worker thread, meaning work on the main thread can continue.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With that, we’ve learned how to offload tasks to a worker thread using the Node.js
    core **worker_threads** module.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe served as an introduction to worker threads. As we’ve seen, worker
    threads can be used to handle CPU-intensive computations. Offloading CPU-intensive
    computations to a worker thread can help avoid blocking the Node.js event loop.
    This means the application can continue to handle other work – for example, I/O
    operations – while CPU-intensive tasks are being processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Worker threads are exposed via the core Node.js **worker_threads** module.
    To use a worker thread in this recipe, we imported the following four assets from
    the **worker_threads** core module:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Worker** : The worker thread class, which represents an independent JavaScript
    thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**isMainThread** : A property that returns **true** if the code isn’t running
    in a worker thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parentPort** : This is a message port that allows communication from the
    worker to the parent thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**workerData** : This property clones the data that’s passed in the worker
    thread constructor. This is how the initial data from the main thread is passed
    to the worker thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this recipe, we initialized a worker thread with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The **Worker** constructor requires a mandatory first argument – that is, a
    filename. This filename is the path to the worker thread’s main script or module.
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is an **options** object, which can accept many different
    configuration options. In **fibonacci-worker.js** , we provided just one configuration
    option, **workerData** , to pass the value of **n** to the worker thread. The
    full list of options that can be passed via the worker thread’s **options** object
    is listed in the Node.js **worker_threads** API documentation ( [https://nodejs.org/api/worker_threads.html#worker_threads_new_worker_filename_options](https://nodejs.org/api/worker_threads.html#worker_threads_new_worker_filename_options)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the worker has been initialized, we can register event listeners on it.
    In this recipe, we registered a message event listener function that executes
    every time a message is received from the worker. The following events can be
    listened for on a worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '**error** : Emitted when the worker thread throws an uncaught exception'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**exit** : Emitted once the worker thread has stopped executing code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**message** : Emitted when the worker thread emits a message using **parentPort.postMessage()**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**messagerror** : Emitted when deserializing the message fails'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**online** : Emitted when the worker thread starts executing JavaScript code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use **parentPort.postMessage()** to send the value of **fibonacci(n)** back
    to the parent thread. In the parent thread, we register a message event listener
    to detect incoming messages from the worker thread.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve introduced worker threads and showcased how they can be used
    to handle CPU-intensive tasks.
  prefs: []
  type: TYPE_NORMAL
