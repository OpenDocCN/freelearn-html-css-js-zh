<html><head></head><body>
        

                            
                    <h1 class="header-title">Colors, Depth Testing, and Alpha Blending</h1>
                
            
            
                
<p class="mce-root">In the previous chapter, we covered global versus local transformations, matrix stacks, animation timers, and various interpolation techniques. In this chapter, we start by examining how colors are structured and handled in WebGL and ESSL. We will discuss the use of colors in objects, lights, and scenes. Then, we will see how WebGL leverages the depth buffer for object occlusion when one object is in front of another, blocking it from view. Lastly, we will cover alpha blending, which allows us to combine the colors of objects when one is occluding the other, while also allowing us to create translucent objects.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Using colors in objects.</li>
<li>Assigning colors to light sources.</li>
<li>Working with several light sources in the ESSL program.</li>
<li>Learning how to use the depth test and the z-buffer.</li>
<li>Learning how to blend functions and equations.</li>
<li>Creating transparent objects with face-culling.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Colors in WebGL</h1>
                
            
            
                
<p>WebGL supplies a fourth attribute to the RGB model. This attribute is called the <strong>alpha channel</strong>. The extended model then is known as the <strong>RGBA</strong> model, where A stands for alpha. The alpha channel contains a value between the range of <kbd>0.0</kbd> to <kbd>1.0</kbd>, just like the other three channels (red, green, and blue). The following diagram shows the RGBA color space. On the horizontal axis, you can see the different colors that can be obtained by combining the <kbd>R</kbd>, <kbd>G</kbd>, and <kbd>B</kbd> channels. The vertical axis corresponds to the alpha channel:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/704d0e52-ba79-4864-b414-e77852adb878.png" style="width:36.83em;height:22.17em;"/></p>
<p>The alpha channel carries extra information about a color. This information affects the way the color is rendered on the screen. In most cases, the alpha value will refer to the amount of opacity that the color contains. A completely opaque color will have an alpha value of <kbd>1.0</kbd>, whereas a completely transparent color will have an alpha value of <kbd>0.0</kbd>. This is the general case, but as we will see, we need to take other factors into account when we obtain translucent colors.</p>
<p>Transparent Versus Translucent<br/>
<br/>
Glass, for example, is transparent to all visible light. Translucent objects allow some light to travel through them. Materials such as frosted glass and some plastics are called translucent. When light strikes translucent materials, only some of the light passes through them.<br/></p>
<p>We use colors everywhere in our WebGL 3D scenes:</p>
<ul>
<li><strong>Objects</strong>: 3D objects can be colored by selecting one color for every pixel (fragment) or by selecting the color that the object will have based on the material's diffuse property.</li>
<li><strong>Lights</strong>: Even though we have been using white lights, there is no reason we can't have lights whose ambient or diffuse properties contain other colors.</li>
<li><strong>Scene</strong>: The background of our scene has a color that we can change by calling <kbd>gl.clearColor</kbd>. Also, as we will see later on, there are special operations that need to be performed on objects' colors to achieve translucent effects.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Use of Color in Objects</h1>
                
            
            
                
<p>As earlier chapters have addressed, the final color of a pixel is assigned in the fragment shader by setting an out ESSL variable. If all the fragments in the object have the same color, we can say the object has a constant color. Otherwise, the object is generally understood as having per-vertex color.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Constant Coloring</h1>
                
            
            
                
<p>To obtain a constant color, we store the desired color in a uniform that is passed to the fragment shader. This uniform is usually called the object's <strong>diffuse material property</strong>. We can also combine object normals and light-source information to obtain a Lambert coefficient. We can use the Lambert coefficient to proportionally change the reflecting color depending on the angle on which the light hits the object.</p>
<p>As the following diagram demonstrates, we lose depth perception when we do not use information about the normals to obtain a Lambert coefficient. Please note that we are using a diffusive lighting model:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/7ae4717b-fd2b-48e5-b0b3-42c9dc57f24a.png" style="width:33.83em;height:18.17em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Per-Vertex Coloring</h1>
                
            
            
                
<p>In medical and engineering visualization applications, it is common to find color maps that are associated with the vertices of the models we are rendering. These maps assign each vertex a color depending on its scalar value. An example of this idea includes the temperature charts that indicate cold temperatures as blue, and hot temperatures as red overlaid on a map.</p>
<p>To implement per-vertex coloring, we need to define an attribute that stores the color for the vertex in the vertex shader:</p>
<div><pre>in vec4 aVertexColor;</pre></div>
<p>The next step is to assign the <kbd>aVertexColor</kbd> attribute to a varying so that it can be passed to the fragment shader. Remember that varyings are automatically interpolated. Therefore, each fragment will have a color that is the weighted result of its contributing vertices.</p>
<p>If we want our color map to be sensitive to lighting conditions, we can multiply each vertex color by the diffuse component of the light. The result is then assigned to the varying that will transfer the result to the fragment shader.</p>
<p>The following diagram demonstrates two different possibilities for this case: on the left, the vertex color is multiplied by the light diffuse term without any weighting from the position of the light; on the right, the Lambert coefficient generates the expected shadows and gives information about the relative location of the light source:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2b06ccc1-13ff-4af5-9c29-3b75e32dd3a1.png" style="width:34.58em;height:20.08em;"/></p>
<p>Here, we are using a vertex buffer object that is mapped to the <kbd>aVertexColor</kbd> vertex shader attribute. We learned how to map VBOs in <a href="d2019a49-9e84-448c-8799-e296187476d1.xhtml" target="_blank">Chapter 2</a>, <em>Rendering</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Per-Fragment Coloring</h1>
                
            
            
                
<p>We can also assign a random color to each pixel of the object we are rendering. Although ESSL does not have a pre-built random function, there are algorithms we can use to generate pseudo-random numbers. That being said, the purpose and usefulness of this technique are outside the scope of this book.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Coloring the Cube</h1>
                
            
            
                
<p>Let's cover a simple example of coloring a geometry:</p>
<ol>
<li>Open the <kbd>ch06_01_cube.html</kbd> file using your browser. You will see a page similar to the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/401dcbc1-d1ef-4cbd-8df5-8bd4e4a6eac4.png"/></p>
<ol start="2">
<li>In this exercise, weâ€™re going to compare constant versus per-vertex coloring. Let's talk about the page's widgets:
<ul>
<li>Lambert: When selected, it will include the Lambert coefficient in the calculation of the final color.</li>
<li>Per Vertex: The two different coloring methods explained before: per-vertex or constant.</li>
<li>Complex Cube: Loads a JSON object where the vertices are repeated with the goal of obtaining multiple normals and multiple colors per vertex. We will explain how this works later on.</li>
<li>Alpha Value: This slider is mapped to the <kbd>uAlpha</kbd> float uniform in the vertex shader. <kbd>uAlpha</kbd> sets the alpha value for the vertex color.</li>
</ul>
</li>
</ol>
<ol start="3">
<li>Disable the use of the Lambert coefficient by clicking on Lambert. Rotate the cube by clicking and dragging. Notice the loss of depth perception when the Lambert coefficient is not included in the final color calculation. The Lambert button is mapped to the <kbd>uUseLambert</kbd> Boolean uniform. The code that calculates the Lambert coefficient can be found in the vertex shader included in the page:</li>
</ol>
<div><pre style="padding-left: 60px">float lambertTerm = 1.0;<br/><br/>if (uUseLambert) {<br/>  vec3 normal = vec3(uNormalMatrix * vec4(aVertexNormal, 1.0));<br/>  vec3 lightDirection = normalize(-uLightPosition);<br/>  lambertTerm = max(dot(normal, -lightDirection), 0.20);<br/>}</pre></div>
<ol start="4">
<li>If the <kbd>uUseLambert</kbd> uniform is <kbd>false</kbd>, then <kbd>lambertTerm</kbd> remains as <kbd>1.0</kbd>, not affecting the final diffuse term here:</li>
</ol>
<div><pre style="padding-left: 60px">Id = uLightDiffuse * uMaterialDiffuse * lambertTerm;</pre></div>
<ol start="5">
<li>Otherwise, <kbd>Id</kbd> will have the Lambert coefficient factored in.</li>
<li>Having Lambert disabled, click on the Per Vertex button. Rotate the cube to see how ESSL interpolates the vertex colors. The vertex shader key code fragment that allows us to switch from a constant diffuse color to per-vertex colors uses the <kbd>uUseVertexColors</kbd> Boolean uniform and the <kbd>aVertexColor</kbd> attribute. This fragment is shown here:</li>
</ol>
<div><pre style="padding-left: 60px">if (uUseVertexColor) {<br/>  Id = uLightDiffuse * aVertexColor * lambertTerm;<br/>}<br/>else {<br/>  Id = uLightDiffuse * uMaterialDiffuse * lambertTerm;<br/>}</pre></div>
<ol start="7">
<li>Take a look at the <kbd>common/models/geometries/cube-simple.json</kbd> file. There, the eight vertices of the cube are defined in the vertices array and there is an element in the scalars array for every vertex. As you may expect, each one of these elements corresponds to a respective vertex color, as shown in the following diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/736f4d9b-81e0-4180-81ee-f3d13bf0f369.png" style="width:40.67em;height:23.42em;"/></p>
<ol start="8">
<li>Make sure that the Lambert button is not active and then click on the Complex Cube button. By repeating vertices in the vertex array in the corresponding JSON file, <kbd>common/models/geometries/cube-complex.json</kbd>, we can achieve independent face-coloring. The following diagram explains how the vertices are organized in <kbd>cube-complex.json</kbd>. Note that as the definition of colors occurs by vertex (since we are using the shader attribute), we need to repeat each color four times, because each face has four vertices. This idea is depicted in the following diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/700279ed-b6ed-45c6-8799-5cdd1b5ea217.png" style="width:41.17em;height:24.08em;"/></p>
<ol start="9">
<li>Activate the Lambert button to see how the Lambert coefficient affects the color of the object. Try different button configurations to see what happens.</li>
<li>Let's quickly explore the effect of changing the alpha channel to a value less than <kbd>1.0</kbd>. What do you see? Notice that the object does not become transparent but instead starts losing its color. To obtain transparency, we need to activate blending. We will discuss blending in depth later in this chapter. For now, uncomment these lines in the <kbd>configure</kbd> function, in the source code:</li>
</ol>
<div><pre style="padding-left: 60px">// gl.disable(gl.DEPTH_TEST);<br/>// gl.enable(gl.BLEND);<br/>// gl.blendFunc(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA);</pre></div>
<ol start="11">
<li>Save the page and reload it in your internet browser. If you select Per-Vertex, Complex Cube and reduce the alpha value to <kbd>0.5</kbd>, you will see something like the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/85d1a4d8-b3ee-4108-ab5a-a2aff15d10a3.png"/></p>
<p><em><strong>What just happened?</strong></em></p>
<p>We have studied two different ways of coloring objects: <strong>constant coloring</strong> and <strong>per-vertex coloring</strong>. In both cases, the final color of each fragment is assigned by exporting a color variable via an <kbd>out</kbd> qualifier in the fragment shader.</p>
<p>We saw that by activating the calculation of the Lambert coefficient, we obtain sensory depth information.</p>
<p>We also saw that repeating vertices in our object allows us to obtain different coloring effects. For instance, we can color an object by faces instead of by vertices.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Use of Color in Lights</h1>
                
            
            
                
<p>Colors are light properties. In <a href="0dcbfd9d-5446-48e9-90c1-841f4d160232.xhtml" target="_blank">Chapter 3</a>, <em>Lights</em>, we learned that the number of light properties depends on the lighting-reflection model selected for a scene. For instance, using a Lambertian reflection model, we would only need to model one shader uniform: the light diffuse property/color. In contrast, if the Phong reflection model were selected, each light source would need to have three properties: the ambient, diffuse, and specular colors.</p>
<p>Positional Lights<br/>
<br/>
The light position is usually modeled as a uniform when the shader needs to know the position of the light. Therefore, a Phong model with a positional light would have four uniforms: ambient, diffuse, specular, and position.<br/>
<br/>
For directional lights, the fourth uniform is the light direction. For more information, refer to <a href="0dcbfd9d-5446-48e9-90c1-841f4d160232.xhtml" target="_blank">Chapter 3</a>, <em>Lights</em>.</p>
<p>We have seen that each light property is represented by a JavaScript four-element array with mappings to <kbd>vec4</kbd> uniforms in the shaders.</p>
<p>As a quick reminder, which WebGL methods should we use to retrieve and set a uniform? In our case, the two methods we use to pass lights to the shaders are as follows:</p>
<ul>
<li><kbd>getUniformLocation</kbd>: Locates the uniform in the program and returns an index we can use to set the value.</li>
<li><kbd>uniform4fv</kbd>: Since the light components are RGBA, we need to pass a four-element float vector.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Scalability Problem</h1>
                
            
            
                
<p>Given the desire to use more than one light in our scene, we need to define and map the number of appropriate uniforms of the lighting model of choice. If we have four properties per light (ambient, diffuse, specular, and location), we need to define four uniforms for each light. If we want to have three lights, we need to write, use, and map twelve uniforms! We need to resolve this complexity before it gets out of hand.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How Many Uniforms Can We Use?</h1>
                
            
            
                
<p>The OpenGL Shading Language ES specification delineates the number of uniforms that we are allowed to use.</p>
<p>Section 4.3.4 Uniforms<br/>
<br/>
There is an implementation dependent limit on the amount of storage for uniforms that can be used for each type of shader. If this is exceeded, it will cause a compile-time or link-time error.</p>
<p>To find out the limit for your WebGL implementation, you can query WebGL using the <kbd>gl.getParameter</kbd> function with these constants:</p>
<div><pre>gl.MAX_VERTEX_UNIFORM_VECTORS<br/>gl.MAX_FRAGMENT_UNIFORM_VECTORS</pre></div>
<p>The implementation limit is given by your browser and heavily depends on your graphics hardware. That said, even though your machine may have enough variable space, it does not necessarily mean that the problem is solved. We still have to define and map each of the uniforms and that can often lead to brittle and verbose code, as we will see in a later exercise.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Simplifying the Problem</h1>
                
            
            
                
<p>In order to simplify the problem, we can assume that the ambient component is the same for all of the lights. This will reduce the number of uniformsâ€”one fewer uniform for each light. However, this is not an extensible solution for more general cases where we cannot assume that the ambient light is constant.</p>
<p>Before we start diving deeper into our scene with multiple lights, let's update our architecture to cover some of the concepts we've addressed.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Architectural Updates</h1>
                
            
            
                
<p>As we progress through this book, we continue to refine our architecture where appropriate to reflect what weâ€™ve learned. On this occasion, we will improve how we pass uniforms to our program and add support for handling a large number of uniforms to define multiple lights.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding Support for Light Objects</h1>
                
            
            
                
<p>Let's cover these changes in detail. We have created a new JavaScript module, <kbd>Lights.js</kbd>, that has two objects:</p>
<ul>
<li><kbd>Light</kbd>: Aggregates light properties (position, diffuse, specular, and so on) in a single entity.</li>
<li><kbd>LightsManager</kbd>: Contains the lights in our scene. This allows us to retrieve each light by <kbd>index</kbd> or <kbd>name</kbd>.</li>
</ul>
<p><kbd>LightsManager</kbd> also contains the <kbd>getArray</kbd> method to flatten the arrays of properties by type:</p>
<div><pre>getArray(type) {<br/>  return this.list.reduce((result, light) =&gt; {<br/>    result = result.concat(light[type]);<br/>    return result;<br/>  }, []);<br/>}</pre></div>
<p>This will be useful when we use uniform arrays later on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Improving How We Pass Uniforms to the Program</h1>
                
            
            
                
<p>We have also improved the way we pass uniforms to the program. In <kbd>configure</kbd>, we can see how we pass attributes and uniforms to <kbd>program.load</kbd> rather than manually attaching them to the instance like in the introductory chapters.</p>
<p>The <kbd>configure</kbd> function is the appropriate place to load the program. We are also going to create a dynamic mapping between JavaScript variables and uniforms. With this in mind, we have updated the <kbd>program.load</kbd> method to receive two arrays:</p>
<ul>
<li><kbd>attributes</kbd>: An array containing the names of the attributes that we will map between JavaScript and ESSL.</li>
<li><kbd>uniforms</kbd>: An array containing the names of the uniforms that we will map between JavaScript and ESSL.</li>
</ul>
<p>The implementation of the function now looks as follows:</p>
<div><pre>// Load up the given attributes and uniforms from the given values<br/>load(attributes, uniforms) {<br/>  this.useProgram();<br/>  this.setAttributeLocations(attributes);<br/>  this.setUniformLocations(uniforms);<br/>}</pre></div>
<p>The last two lines correspond to the two new functions, <kbd>setAttributeLocations</kbd> and <kbd>setUniformLocations</kbd>:</p>
<div><pre>// Set references to attributes onto the program instance<br/>setAttributeLocations(attributes) {<br/>  attributes.forEach(attribute =&gt; {<br/>    this[attribute] = this.gl.getAttribLocation(this.program, attribute);<br/>  });<br/>}<br/><br/>// Set references to uniforms onto the program instance<br/>setUniformLocations(uniforms) {<br/>  uniforms.forEach(uniform =&gt; {<br/>    this[uniform] = this.gl.getUniformLocation(this.program, uniform);<br/>  });<br/>}</pre></div>
<p>As you can see, these functions look up the attribute and uniform lists, respectively, and then attach the location as a property to the <kbd>Program</kbd> instance.</p>
<p>In short, if we include the <kbd>uLightPosition</kbd> uniform name in the <kbd>uniforms</kbd> list to be passed to <kbd>program.load</kbd>, we will then have a <kbd>program.uLightPosition</kbd> property that will contain the location of the respective uniform! Neat!</p>
<p>Once we load the program in the <kbd>configure</kbd> function, we can initialize the values of the uniforms immediately with the following:</p>
<div><pre>gl.uniform3fv(program.uLightPosition, value);</pre></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Adding a Blue Light to a Scene</h1>
                
            
            
                
<p>Weâ€™re ready to take a look at the first example in this chapter. We will work on a scene with <strong>per-fragment</strong> lighting that has three light sources.</p>
<p>Each light has a position and a diffuse color property. This means we have two uniforms per light. Perform the following steps:</p>
<ol>
<li>To keep things simple, we have assumed that the ambient color is the same for all three light sources. We have also removed the specular property. Open the <kbd>ch06_02_wall_initial.html</kbd> file in your browser.</li>
</ol>
<ol start="2">
<li>You will see a scene such as the one displayed in the following screenshot where two lights (red and green) illuminate a black wall:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/93aa40c3-3f69-485a-b1e6-247ca3900007.png"/></p>
<ol start="3">
<li>Open the <kbd>ch06_02_wall-initial.html</kbd> file using your code editor. We will update the vertex shader, the fragment shader, the JavaScript code, and the HTML code to add the blue light.</li>
<li><strong>Updating the vertex shader</strong>: Go to the vertex shader where you can see these two uniforms:</li>
</ol>
<div><pre style="padding-left: 60px">uniform vec3 uPositionRedLight; <br/>uniform vec3 uPositionGreenLight; </pre></div>
<ol start="5">
<li>Let's add the third uniform here:</li>
</ol>
<div><pre style="padding-left: 60px">uniform vec3 uPositionBlueLight;</pre></div>
<ol start="6">
<li>We also need to define a varying to carry the interpolated light ray direction to the fragment shader. Remember here that we are using per-fragment lighting. Check where the varyings are defined:</li>
</ol>
<div><pre style="padding-left: 60px">out vec3 vRedRay;<br/>out vec3 vGreenRay;</pre></div>
<ol start="7">
<li>And, add the third varying there:</li>
</ol>
<div><pre style="padding-left: 60px">out vec3 vBlueRay;</pre></div>
<ol start="8">
<li>Let's take a look at the body of the vertex shader. We need to update each of the light locations according to our position in the scene. We achieve this by writing the following:</li>
</ol>
<div><pre style="padding-left: 60px">vec4 blueLightPosition = uModelViewMatrix * vec4(uPositionBlueLight, 1.0);</pre></div>
<ol start="9">
<li>Notice that the positions for the other two lights are also being calculated.</li>
<li>Let's calculate the light ray for the updated position from our blue light to the current vertex. We do that by writing the following code:</li>
</ol>
<div><pre style="padding-left: 60px">vBlueRay = vertex.xyz - blueLightPosition.xyz;</pre></div>
<ol start="11">
<li>That is all we need to modify in the vertex shader.</li>
<li>So far, weâ€™ve included a new light position and we have calculated the light rays in the vertex shader. These rays will be interpolated by the fragment shader.</li>
<li>Let's work out how the colors on the wall will change by including our new blue source of light. Scroll down to the fragment shader and add a new uniformâ€”the blue diffuse property. Look for these uniforms declared right before the <kbd>main</kbd> function:</li>
</ol>
<div><pre style="padding-left: 60px">uniform vec4 uDiffuseRedLight;<br/>uniform vec4 uDiffuseGreenLight;</pre></div>
<ol start="14">
<li>Insert the following line of code:</li>
</ol>
<div><pre style="padding-left: 60px">uniform vec4 uDiffuseBlueLight;</pre></div>
<ol start="15">
<li>To calculate the contribution of the blue light to the final color, we need to obtain the light ray we defined previously in the vertex shader. This varying is available in the fragment shader. You also need to declare it before the <kbd>main</kbd> function. Look for the following:</li>
</ol>
<div><pre style="padding-left: 60px">in vec3 vRedRay;<br/>in vec3 vGreenRay;</pre></div>
<ol start="16">
<li>Insert the following code right under it:</li>
</ol>
<div><pre style="padding-left: 60px">in vec3 vBlueRay;</pre></div>
<ol start="17">
<li>It is assumed that the ambient component is the same for all the lights. This is reflected in the code by having only one <kbd>uLightAmbient</kbd> variable. The ambient term, <kbd>Ia</kbd>, is the product of <kbd>uLightAmbient</kbd> and the wall's material ambient property:</li>
</ol>
<div><pre style="padding-left: 60px">// ambient Term<br/>vec4 Ia = uLightAmbient * uMaterialAmbient;</pre></div>
<ol start="18">
<li>If <kbd>uLightAmbient</kbd> is set to <kbd>(1.0, 1.0, 1.0, 1.0)</kbd> and <kbd>uMaterialAmbient</kbd> is set to <kbd>(0.1, 0.1, 0.1, 1.0)</kbd>, then the resulting ambient term, <kbd>Ia</kbd>, will be really small. This means the contribution of the ambient light will be low in this scene. In contrast, the diffuse component will be different for every light.</li>
<li>Let's add the effect of the blue diffuse term. In the fragment shader main function, look for the following code:</li>
</ol>
<div><pre style="padding-left: 60px">// diffuse Term<br/>vec4 Id1 = vec4(0.0, 0.0, 0.0, 1.0);<br/>vec4 Id2 = vec4(0.0, 0.0, 0.0, 1.0);</pre></div>
<ol start="20">
<li>Add the following line immediately under it:</li>
</ol>
<div><pre style="padding-left: 60px">vec4 Id3 = vec4(0.0, 0.0, 0.0, 1.0);</pre></div>
<ol start="21">
<li>Scroll down to the following:</li>
</ol>
<div><pre style="padding-left: 60px">float lambertTermOne = dot(N, -normalize(vRedRay));<br/>float lambertTermTwo = dot(N, -normalize(vGreenRay));</pre></div>
<ol start="22">
<li>Add the following line of code right under it:</li>
</ol>
<div><pre style="padding-left: 60px">float lambertTermThree = dot(N, -normalize(vBlueRay));</pre></div>
<ol start="23">
<li>Scroll to the following:</li>
</ol>
<pre style="padding-left: 60px">if (lambertTermOne &gt; uCutOff) {<br/>  Id1 = uDiffuseRedLight * uMaterialDiffuse * lambertTermOne;<br/>}<br/><br/>if (lambertTermTwo &gt; uCutOff) {<br/>  Id2 = uDiffuseGreenLight * uMaterialDiffuse * lambertTermTwo;<br/>}</pre>
<ol start="24">
<li>Insert the following code after it:</li>
</ol>
<div><pre style="padding-left: 60px">if (lambertTermThree &gt; uCutOff) {<br/>  Id3 = uDiffuseBlueLight * uMaterialDiffuse * lambertTermTwo;<br/>}</pre></div>
<ol start="25">
<li>Update <kbd>fragColor</kbd> so that it includes <kbd>Id3</kbd>:</li>
</ol>
<div><pre style="padding-left: 60px">fragColor = vec4(vec3(Ia + Id1 + Id2 + Id3), 1.0);</pre></div>
<ol start="26">
<li>That's all we need to do in the fragment shader. Let's move on to our JavaScript code. So far, we have written the code that is needed to handle one more light inside our shaders. Let's see how we create the blue light from the JavaScript side and how we map it to the shaders. Scroll down to the <kbd>configure</kbd> function and look for the following code:</li>
</ol>
<pre style="padding-left: 60px">const redLight = new Light('redLight');<br/>redLight.setPosition(redLightPosition);<br/>redLight.setDiffuse([1, 0, 0, 1]);<br/><br/>const greenLight = new Light('greenLight');<br/>greenLight.setPosition(greenLightPosition);<br/>greenLight.setDiffuse([0, 1, 0, 1]);</pre>
<ol start="27">
<li>Insert the following code:</li>
</ol>
<div><pre style="padding-left: 60px">const blue = new Light('blueLight');<br/>blue.setPosition([-2.5, 3, 3]);<br/>blue.setDiffuse([0.0, 0.0, 1.0, 1.0]);</pre></div>
<ol start="28">
<li>Scroll to the point where the <kbd>uniforms</kbd> list is defined. As mentioned earlier, this new mechanism makes it easier to obtain locations for the uniforms. Add the two new uniforms that we are using for the blue light: <kbd>uDiffuseBlueLight</kbd> and <kbd>uPositionBlueLight</kbd>. The list should look like the following code:</li>
</ol>
<div><pre style="padding-left: 60px">const uniforms = [<br/>  'uProjectionMatrix',<br/>  'uModelViewMatrix',<br/>  'uNormalMatrix',<br/>  'uMaterialDiffuse',<br/>  'uMaterialAmbient',<br/>  'uLightAmbient',<br/>  'uDiffuseRedLight',<br/>  'uDiffuseGreenLight',<br/><strong>  'uDiffuseBlueLight',<br/></strong>  'uPositionRedLight',<br/>  'uPositionGreenLight',<br/><strong>  'uPositionBlueLight',<br/></strong>  'uWireframe',<br/>  'uLightSource',<br/>  'uCutOff'<br/>];</pre></div>
<ol start="29">
<li>Let's pass the position and diffuse values of our newly defined light to <kbd>program</kbd>. Find the following lines, after the line that loads <kbd>program</kbd>, and make these necessary changes:</li>
</ol>
<div><pre style="padding-left: 60px">gl.uniform3fv(program.uPositionRedLight, redLight.position);<br/>gl.uniform3fv(program.uPositionGreenLight, greenLight.position);<br/><strong>gl.uniform3fv(program.uPositionBlueLight, blueLight.position);<br/></strong><br/>gl.uniform4fv(program.uDiffuseRedLight, redLight.diffuse);<br/>gl.uniform4fv(program.uDiffuseGreenLight, greenLight.diffuse);<br/><strong>gl.uniform4fv(program.uDiffuseBlueLight, blueLight.diffuse);</strong></pre></div>
<p>Uniform Arrays<br/>
<br/>
Coding one uniform per light makes the code quite verbose. Later on, we will cover how to simplify the code using uniform arrays.</p>
<ol start="30">
<li>Let's update the <kbd>load</kbd> function. We need a new sphere to represent the blue light, the same way we have two spheres in the scene: one for the red light and the other for the green light. Append the following line:</li>
</ol>
<div><pre style="padding-left: 60px">scene.load('/common/models/geometries/sphere3.json', 'blueLight');</pre></div>
<ol start="31">
<li>As we saw in the <kbd>load</kbd> function, we are loading the same geometry (sphere) three times. To differentiate the sphere that represents the light source, we are using local transforms for the sphere (initially centered at the origin). Scroll to the <kbd>render</kbd> function and find the following lines of code:</li>
</ol>
<div><pre style="padding-left: 60px">const modelViewMatrix = transforms.modelViewMatrix;<br/><br/>if (object.alias === 'redLight') {<br/>  mat4.translate(<br/>    modelViewMatrix, modelViewMatrix, <br/>    program.getUniform(program.uPositionRedLight)<br/>  );<br/>  object.diffuse = program.getUniform(program.uDiffuseRedLight);<br/>  gl.uniform1i(program.uLightSource, true);<br/>}<br/><br/>if (object.alias === 'greenLight') {<br/>  mat4.translate(<br/>    modelViewMatrix, modelViewMatrix,<br/>    program.getUniform(program.uPositionGreenLight)<br/>  );<br/>  object.diffuse = program.getUniform(program.uDiffuseGreenLight);<br/>  gl.uniform1i(program.uLightSource, true);<br/>}</pre></div>
<ol start="32">
<li>Add the following code:</li>
</ol>
<div><pre style="padding-left: 60px">if (object.alias === 'blueLight') {<br/>  mat4.translate(<br/>    modelViewMatrix, modelViewMatrix,<br/>    program.getUniform(program.uPositionBlueLight)<br/>  );<br/>  object.diffuse = program.getUniform(program.uDiffuseBlueLight);<br/>  gl.uniform1i(program.uLightSource, true);<br/>}</pre></div>
<ol start="33">
<li>Thatâ€™s it! Save the page with a different name and test it in your browser:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/3a945b06-e22f-48aa-8eaa-aae7adc1d908.png"/></p>
<ol start="34">
<li>If you do not obtain the expected result, please go back and check the steps. You will find the completed exercise in the <kbd>ch06_03_wall-final.html</kbd> file.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We have modified our sample scene by adding one more light: a blue light. We have updated the following:</p>
<ul>
<li>The vertex shader</li>
<li>The fragment shader</li>
<li>The <kbd>configure</kbd> function</li>
<li>The <kbd>load</kbd> function</li>
<li>The <kbd>draw</kbd> function</li>
</ul>
<p>As you can see, handling light properties one uniform at a time is not very efficient. Later in this chapter, we will study a more effective way to handle lights in a WebGL scene.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Have a Go: Adding Interactivity</h1>
                
            
            
                
<p>We are going to add an additional slider to our controls widget to interactively change the position of the blue light we just added.</p>
<p>We will use <strong>dat.GUI</strong>, one for each one of the blue light coordinates.</p>
<p>dat.GUI<br/>
<br/>
You can find more information about dat.GUI on GitHub: <a href="https://github.com/dataarts/dat.gui">https://github.com/dataarts/dat.gui</a>.</p>
<ol>
<li>Create three sliders: one for the <kbd>X</kbd> coordinate, one for the <kbd>Y</kbd> coordinate, and a third one for the <kbd>Z</kbd> coordinate for the blue light.</li>
</ol>
<ol start="2">
<li>The final GUI should include the new blue light sliders, which should look like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/5d92cd7e-8479-404e-ade0-37c642d68ea6.png"/></p>
<ol start="3">
<li>Use the sliders present in the page to guide your work.</li>
<li>You will find the completed exercise in the <kbd>ch06_03_wall-final.html</kbd> file.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Uniform Arrays to Handle Multiple Lights</h1>
                
            
            
                
<p>As we've seen, handling light properties with individual uniforms makes the code verbose and difficult to maintain. Fortunately, ESSL provides several mechanisms we can use to solve the problem of handling multiple lights. One of them is <strong>uniform arrays</strong>.</p>
<p>This technique allows us to handle multiple lights by introducing enumerable arrays of vectors in the shaders. This allows us to calculate light contributions by iterating through the light arrays in the shaders. We still need to define each light in JavaScript, but the mapping to ESSL becomes simpler since we arenâ€™t defining one uniform per light property. Let's see how this technique works. We just need to make two simple changes in our code.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Uniform Array Declaration</h1>
                
            
            
                
<p>First, we need to declare the light uniforms as arrays inside our ESSL shaders. For example, the light position that contains three lights would look like this:</p>
<div><pre>uniform vec3 uPositionLight[3];</pre></div>
<p>Itâ€™s important to note that ESSL does not support dynamic initialization of uniform arrays. We could try something such as this:</p>
<div><pre>uniform int numLights;<br/>uniform vec3 uPositionLight[numLights]; // will not work</pre></div>
<p>If so, the shader will not compile, and you will obtain the following error:</p>
<div><pre>ERROR: 0:12 â€” constant expression required<br/>ERROR: 0:12 â€” array size must be a constant integer expression</pre></div>
<p>However, this construct is valid:</p>
<div><pre>const int numLights = 3;<br/>uniform vec3 uPositionLight[numLights]; // will work </pre></div>
<p>We declare one uniform array per light property, regardless of how many lights weâ€™re going to have. As a result, if we want to pass information about diffuse and specular components of five lights, for example, we need to declare two uniform arrays, as follows:</p>
<pre>uniform vec4 uDiffuseLight[5];<br/>uniform vec4 uSpecularLight[5];</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">JavaScript Array Mapping</h1>
                
            
            
                
<div><p>Next, we need to map the JavaScript variables where we have the light property information to the program. For example, we may want to map these three light positions:</p>
</div>
<div><pre>const lightPosition1 = [0, 7, 3];<br/>const lightPosition2 = [2.5, 3, 3];<br/>const lightPosition3 = [-2.5, 3, 3];</pre></div>
<p>If so, we need to retrieve the uniform array location (just like in any other case):</p>
<div><pre>const location = gl.getUniformLocation(program, 'uPositionLight');</pre></div>
<p>The one difference is that we map these positions as a concatenated flat array:</p>
<div><pre>gl.uniform3fv(location, [0, 7, 3, 2.5, 3, 3, -2.5, 3, 3]);</pre></div>
<p>There are two important points here:</p>
<ul>
<li>The name of the uniform that is passed to <kbd>getUniformLocation</kbd> is the same as before. The fact that <kbd>uPositionLight</kbd> is now an array does not change a thing when you locate the uniform with <kbd>getUniformLocation</kbd>.</li>
<li>The JavaScript array that we are passing to the uniform is a flat array. If you write something as follows, the mapping will not work:</li>
</ul>
<div><pre>gl.uniform3fv(location, [<br/>  [0, 7, 3],<br/>  [2.5, 3, 3],<br/>  [-2.5, 3, 3]<br/>]);</pre></div>
<p>So, if you have one variable per light, you should ensure that you concatenate them before passing them to the shader.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Adding a White Light to a Scene</h1>
                
            
            
                
<p>Let's cover an example of how we'd add a new light to our scene:</p>
<ol>
<li>Open the <kbd>ch06_04_wall-light-arrays.html</kbd> file in your browser. This scene looks exactly like <kbd>ch06_03_wall-final.html</kbd>; however, the code is far less complex since we are now using uniform arrays. Let's see how using uniform arrays changes our code.</li>
<li>Open the <kbd>ch06_04_wall-light-arrays.html</kbd> file in your code editor. Let's take a look at the vertex shader. Note the use of the constant integer expression const int, <kbd>numLights = 3;</kbd>, to declare the number of lights that the shader will handle.</li>
</ol>
<ol start="3">
<li>There, you can also see that a uniform array is being used to operate on light positions. Note that we are using a varying array to pass the light rays (for each light) to the fragment shader:</li>
</ol>
<div><pre style="padding-left: 60px">for(int i = 0; i &lt; numLights; i++) {<br/>  vec4 lightPosition = uModelViewMatrix * vec4(uLightPosition[i], <br/>   1.0);<br/>  vLightRay[i] = vertex.xyz - lightPosition.xyz;<br/>}</pre></div>
<ol start="4">
<li>This fragment of code calculates one varying light ray per light. Recall that the same code in the <kbd>ch06_03_wall-final.html</kbd> file looks like the following code:</li>
</ol>
<pre style="padding-left: 60px">vec4 redLightPosition = uModelViewMatrix * vec4(uPositionRedLight, <br/> 1.0);<br/>vec4 greenLightPosition = uModelViewMatrix * <br/> vec4(uPositionGreenLight, 1.0);<br/>vec4 blueLightPosition = uModelViewMatrix * <br/> vec4(uPositionBlueLight, 1.0);<br/><br/>vRedRay = vertex.xyz - redLightPosition.xyz;<br/>vGreenRay = vertex.xyz - greenLightPosition.xyz;<br/>vBlueRay = vertex.xyz - blueLightPosition.xyz;</pre>
<ol start="5">
<li class="mce-root">Once you compare these two snippets, the advantage of using uniform arrays (and varying arrays) should be clear.</li>
<li>The fragment shader also uses uniform arrays. In this case, the fragment shader iterates through the light diffuse properties to calculate the contribution of each one to the final color on the wall:</li>
</ol>
<div><pre style="padding-left: 60px">for(int i = 0; i &lt; numLights; i++) {<br/>  L = normalize(vLightRay[i]);<br/>  lambertTerm = dot(N, -L);<br/>  if (lambertTerm &gt; uCutOff) {<br/>    finalColor += uLightDiffuse[i] * uMaterialDiffuse * lambertTerm;<br/>  }<br/>}</pre></div>
<ol start="7">
<li>For the sake of brevity, we wonâ€™t cover the verbose version from the <kbd>ch06_03_wall-final.html</kbd> exercise, but you should check it out for yourself and compare it with this one.</li>
</ol>
<ol start="8">
<li>In the <kbd>configure</kbd> function, the size of the JavaScript array containing the uniform names has decreased considerably by omitting the other unnecessary light attributes:</li>
</ol>
<div><pre style="padding-left: 60px">const uniforms = [<br/>  'uPerspectiveMatrix',<br/>  'uModelViewMatrix',<br/>  'uNormalMatrix',<br/>  'uMaterialDiffuse',<br/>  'uMaterialAmbient',<br/>  'uLightAmbient',<br/>  'uLightDiffuse',<br/>  'uLightPosition',<br/>  'uWireframe',<br/>  'uLightSource',<br/>  'uCutOff'<br/>];</pre></div>
<ol start="9">
<li>The mapping between JavaScript lights and uniform arrays is now simpler because of the <kbd>getArray</kbd> method from the <kbd>LightsManager</kbd> class. As we described earlier, the <kbd>getArray</kbd> method concatenates the lights' data into one flat array.</li>
<li>The <kbd>load</kbd> and <kbd>render</kbd> functions look exactly the same. If we want to add a new light, we still need to load a new sphere with the <kbd>load</kbd> function (to represent the light source in our scene), and we still need to translate the sphere to the appropriate location in the <kbd>render</kbd> function.</li>
<li>Let's see how much effort we need to add a new light. Go to the <kbd>configure</kbd> function and create a new light object, as follows:</li>
</ol>
<div><pre style="padding-left: 60px">const whiteLight = new Light('whiteLight');<br/>whiteLight.setPosition([0, 10, 2]);<br/>whiteLight.setDiffuse([1.0, 1.0, 1.0, 1.0]);</pre></div>
<ol start="12">
<li>Add <kbd>whiteLight</kbd> to the <kbd>lights</kbd> instance:</li>
</ol>
<div><pre style="padding-left: 60px">lights.add(whiteLight);</pre></div>
<ol start="13">
<li>Move to the <kbd>load</kbd> function and append this line:</li>
</ol>
<div><pre style="padding-left: 60px">scene.load('/common/models/geometries/sphere3.json', 'whiteLight');</pre></div>
<ol start="14">
<li>Just like in the previous <em>Time for Action</em> section, add this to the <kbd>render</kbd> function:</li>
</ol>
<div><pre style="padding-left: 60px">if (object.alias === 'whiteLight') {<br/>  const whiteLight = lights.get(object.alias);<br/>  mat4.translate(modelViewMatrix, modelViewMatrix, <br/>   whiteLight.position);<br/>  object.diffuse = whiteLight.diffuse;<br/>  gl.uniform1i(program.uLightSource, true);<br/>}</pre></div>
<ol start="15">
<li>Save the webpage with a different name and open it using your browser. We have also included the completed exercise in <kbd>ch06_05_wall-light-arrays-final.html</kbd>, including some minor improvements on keeping the light configuration more declarative. The following diagram shows the final result:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/86bfe41a-08aa-4cac-b642-ba986109db16.png"/></p>
<p>Thatâ€™s all you need to do! If you want to control the white light properties with the controls widget, you would need to write the corresponding code.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Directional Point Lights</h1>
                
            
            
                
<p>In <a href="0dcbfd9d-5446-48e9-90c1-841f4d160232.xhtml" target="_blank">Chapter 3</a>, <em>Lights</em>, we compared directional and positional lights:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8f69327e-711a-452e-8a94-e6aafcd98af6.png" style="width:39.92em;height:19.33em;"/></p>
<p>In point lighting, for every point on the surface of our object, we compute the direction from the light to that point on the surface. We then do the same thing we did for directional lighting. Remember that we took the dot product of the surface normal (the direction the surface is facing) and the light direction. This gave us a value of <kbd>1</kbd> if the two directions matched, which means the fragment should be fully lit, <kbd>0</kbd> if the two directions were perpendicular, and <kbd>-1</kbd> if they were opposite. We directly used that value to multiply the color of the surface, which gave us lighting.</p>
<p>In this section, we will combine directional and positional lights. We are going to create a third type of light: a <strong>directional point light</strong>, commonly referred to as a <strong>spot light</strong>. This light has both positional and directional properties. We are ready to do this since our shaders can easily handle lights with multiple properties:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/440f89a6-bcc1-4444-97bb-89827e6cb9dd.png" style="width:39.67em;height:28.75em;"/></p>
<p>The trick to creating these lights is to subtract the light-direction vector from the normal for each vertex. The resulting vector will create a different Lambert coefficient that will reflect into the cone generated by the light source:</p>
<ol>
<li>Open <kbd>ch06_06_wall-spot-light.html</kbd> in your browser. As you can see, the three light sources now have a direction:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/47c83dde-1688-4bdd-9428-4c54e6c5cd90.png"/></p>
<ol start="2">
<li>Open <kbd>ch06_06_wall-spot-light.html</kbd> in your source code editor.</li>
<li>To create a light cone, we need to obtain one Lambert coefficient per fragment. As we did in previous examples, we obtain these coefficients in the fragment shader by calculating the dot product between the inverted light ray and the normal that has been interpolated. Thus far, we have been using one varying to do this: <kbd>vNormal</kbd>.</li>
<li>So far, one varying has sufficed since we did not need to update the normals, regardless of how many lights we have in the scene. However, to create directional point lights, we do have to update the normals as the direction of each light will create a different normal. Therefore, we replace <kbd>vNormal</kbd> with a <strong>varying array</strong>:</li>
</ol>
<div><pre style="padding-left: 60px">out vec3 vNormal[numLights];</pre></div>
<ol start="5">
<li>The line that subtracts the light direction from the normal occurs inside the <kbd>for</kbd> loop. This is because we do this for every light in the scene, as every light has its own light direction:</li>
</ol>
<div><pre style="padding-left: 60px">for(int i = 0; i &lt; numLights; i++) {<br/>  vec4 positionLight = uModelViewMatrix * vec4(uLightPosition[i], <br/>   1.0);<br/>  vec3 directionLight = vec3(uNormalMatrix * <br/>   vec4(uLightDirection[i], 1.0));<br/>  vNormal[i] = normal - directionLight;<br/>  vLightRay[i] = vertex.xyz - positionLight.xyz;<br/>}</pre></div>
<ol start="6">
<li>Here, the light direction is transformed by the Normal matrix while the light position is transformed by the Model-View matrix.</li>
<li>In the fragment shader, we calculate the Lambert coefficients: one per light and fragment. The key difference is this line in the fragment shader:</li>
</ol>
<div><pre style="padding-left: 60px">N = normalize(vNormal[i]);</pre></div>
<ol start="8">
<li>Here, we obtain the interpolated updated normal per light.</li>
<li>Let's create a cut-off by restricting the allowed Lambert coefficients. There are at least two different ways to obtain a light cone in the fragment shader. The first one consists of restricting the Lambert coefficient to be higher than the <kbd>uCutOff</kbd> uniform (cut-off value). Let's take a look at the fragment shader:</li>
</ol>
<div><pre style="padding-left: 60px">if (lambertTerm &gt; uCutOff) {<br/>  finalColor += uLightDiffuse[i] * uMaterialDiffuse;<br/>}</pre></div>
<ol start="10">
<li>The Lambert coefficient is the cosine of the angle between the reflected light and the surface normal. If the light ray is perpendicular to the surface, we obtain the highest Lambert coefficient, and as we move away from the center, the Lambert coefficients change following the cosine function until the light rays are completely parallel to the surface. This creates a cosine of <kbd>90</kbd> degrees between the normal and the light ray. This produces a Lambert coefficient of zero:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/6b57e091-9bea-41b5-893f-7166d1123723.png" style="width:34.17em;height:21.00em;"/></p>
<ol start="11">
<li>Open <kbd>ch06_06_wall-spot-light.html</kbd> in your browser if you havenâ€™t done so yet. Use the cut-off slider on the page. Notice how this affects the light cone by making it wider or narrower. After playing with the slider, youâ€™ll probably notice that these lights donâ€™t look very realistic. The reason is that the final color is the same regardless of what Lambert coefficient you obtained: as long as the Lambert coefficient is higher than the set cut-off value, you will obtain the full diffuse contribution from the three light sources.</li>
<li>To refine the result, open the web page using your source code editor. Then, go to the fragment shader and multiply the Lambert coefficient in the line that calculates the final color:</li>
</ol>
<div><pre style="padding-left: 60px">finalColor += uLightDiffuse[i] * uMaterialDiffuse * lambertTerm;</pre></div>
<ol start="13">
<li>Save the web page with a different name (so you can keep the original) and then load it in your web browser. You will notice that the light colors are attenuated as you depart from the center of each light reflection on the wall. This may look better, but thereâ€™s a simpler way to create more realistic light cut-offs:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/4a2fd092-74bf-4e9e-8572-1300cc67dd78.png"/></p>
<ol start="14">
<li>Let's create a cut-off by using an <strong>exponential attenuation factor</strong>. In the fragment shader, find the following code:</li>
</ol>
<div><pre style="padding-left: 60px">if (lambertTerm &gt; uCutOff) {<br/>  finalColor += uLightDiffuse[i] * uMaterialDiffuse * lambertTerm;<br/>}</pre></div>
<ol start="15">
<li>Replace it with the following:</li>
</ol>
<div><pre style="padding-left: 60px">finalColor += uLightDiffuse[i] * uMaterialDiffuse * pow(lambertTerm, 10.0 * uCutOff);</pre></div>
<ol start="16">
<li>Notice that weâ€™ve removed the <kbd>if</kbd> condition. This time, the attenuation factor is <kbd>pow(lambertTerm, 10.0 * uCutOff);</kbd>.</li>
<li>This modification works as the factor attenuates the final color exponentially. If the Lambert coefficient is close to zero, the final color will be heavily attenuated:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/a826c9ba-bc82-4ed7-b8eb-07be54f5b86e.png" style="width:40.00em;height:25.00em;"/></p>
<ol start="18">
<li>Save the web page with a different name and load it in your browser. The improvement is dramatic:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/2302d070-2526-44fb-bbf2-aeff5ac0a930.png" style="width:54.42em;height:28.83em;"/></p>
<p>Weâ€™ve included the completed exercises here:</p>
<ul>
<li><kbd>ch06_07_wall-spot-light-proportional.html</kbd></li>
<li><kbd>ch06_08_wall-spot-light-exponential.html</kbd></li>
</ul>
<p><em><strong>What just happened?</strong></em></p>
<p>Weâ€™ve learned how to implement directional point lights. We have also discussed attenuation factors that improve lighting effects.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Use of Color in the Scene</h1>
                
            
            
                
<p>Itâ€™s time to discuss transparency and alpha blending. As mentioned before, the alpha channel can carry information about the opacity of the object color. However, as we saw in the cube example, itâ€™s not possible to obtain a translucent object unless alpha blending is activated. Things get a bit more complicated when we have several objects in the scene. To manage these difficulties, we need to learn what to do in order to have a consistent scene with translucent and opaque objects.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Transparency</h1>
                
            
            
                
<p>The first approach to render transparent objects is to use <strong>polygon stippling</strong>. This technique consists of discarding some fragments so that you can see through the object. Think of this as punching little holes in the surface of your object.</p>
<p>OpenGL supports polygon stippling through the <kbd>glPolygonStipple</kbd> function. This function is not available in WebGL. You could try to replicate this functionality by dropping some fragments in the fragment shader using the ESSL discard command.</p>
<p>More commonly, we can use the alpha channel information to obtain translucent objects. However, as weâ€™ve seen in the cube example, modifying the alpha values does not produce transparency automatically.</p>
<p>Creating transparency corresponds to altering the fragments that weâ€™ve already written to the framebuffer. Think, for instance, of a scene where there is one translucent object in front of an opaque object (from our camera view). In order for the scene to be rendered correctly, we need to be able to see the opaque object through the translucent object. Therefore, the fragments that overlap between the far and near objects need to be combined somehow to create the transparency effect.</p>
<p>The same idea applies when there is only one translucent object in the scene. The only difference is that the far fragments correspond to the back face of the object and the near fragments correspond to the front face of the object. To produce the transparency effect in this case, the far and near fragments need to be combined.</p>
<p>To properly render transparent surfaces, we need to learn about two important WebGL concepts: <strong>depth testing</strong> and <strong>alpha blending</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Updated Rendering Pipeline</h1>
                
            
            
                
<p>Depth testing and alpha blending are two optional stages for fragments once theyâ€™ve been processed by the fragment shader. If the depth test is not activated, all the fragments are automatically available for alpha blending. If the depth test is enabled, those fragments that fail the test will automatically be discarded by the pipeline and will no longer be available for any other operation. This means that discarded fragments will not be rendered. This behavior is similar to using the ESSL discard command.</p>
<p>The following diagram shows the order in which depth testing and alpha blending are performed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c72ca62c-eab7-4c54-bc9a-019313562a72.png"/></p>
<p>Now, let's see what depth testing is about and why itâ€™s relevant to alpha blending.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Depth Testing</h1>
                
            
            
                
<p>Each fragment that has been processed by the fragment shader carries an associated depth value. Though fragments are two-dimensional since theyâ€™re rendered on the screen, the depth value keeps the information of how far the fragment is from the camera (screen). Depth values are stored in a special WebGL buffer named <strong>depth buffer</strong> or <strong>z-buffer</strong>. The <kbd>z</kbd> comes from the fact that <kbd>x</kbd> and <kbd>y</kbd> values correspond to the screen coordinates of the fragment, while the <kbd>z</kbd> value measures distance perpendicular to the screen.</p>
<p>After the fragment has been calculated by the fragment shader, it becomes available for depth testing. This only occurs if the depth test is enabled. Assuming that <kbd>gl</kbd> is the JavaScript variable that contains our WebGL context, we can enable depth testing by writing the following:</p>
<div><pre>gl.enable(gl.DEPTH_TEST);</pre></div>
<p>The depth test takes the depth value of a fragment into consideration and compares it to the depth value for the same fragment coordinates already stored in the depth buffer. The depth test determines whether that fragment is accepted for further processing in the rendering pipeline.</p>
<p>Only the fragments that pass the depth test will be processed. Any fragment that does not pass the depth test will be discarded.</p>
<p>In normal circumstances, when the depth test is enabled, only those fragments with a lower depth value than the corresponding fragments present in the depth buffer will be accepted.</p>
<p>Depth testing is a commutative operation with respect to the rendering order. This means that no matter which object gets rendered first, as long as depth testing is enabled, we will always have a consistent scene.</p>
<p>Let's illustrate this with an example. The following diagram contains a cone and a sphere. The depth test is disabled using the following code:</p>
<div><pre>gl.disable(gl.DEPTH_TEST);</pre></div>
<p>The sphere is rendered first. As expected, the cone fragments that overlap the cone are not discarded when the cone is rendered. This occurs because there is no depth test between the overlapping fragments:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/25fad2cf-dab4-4ab0-89aa-f0926626d235.png" style="width:37.92em;height:26.25em;"/></p>
<p>Now, let's enable the depth test and render the same scene. The sphere is rendered first. Since all the cone fragments that overlap the sphere have a higher depth value (they are farther from the camera), these fragments fail the depth test and are discarded, creating a consistent scene.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Depth Function</h1>
                
            
            
                
<p>In some applications, we may be interested in changing the default behavior of depth testing, which discards fragments with a higher depth value than those fragments in the depth buffer. For that purpose, WebGL provides the <kbd>gl.depthFunc(function)</kbd> method.</p>
<p>This method has only one parameter, the <kbd>function</kbd> to use:</p>
<table border="1" style="border-collapse: collapse;width: 100%;border-color: #000000">
<tbody>
<tr>
<td><strong>Parameter</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>gl.NEVER</kbd></td>
<td>The depth test always fails.</td>
</tr>
<tr>
<td><kbd>gl.LESS</kbd></td>
<td>Only fragments with a depth lower than current fragments on the depth buffer will pass the test.</td>
</tr>
<tr>
<td><kbd>gl.LEQUAL</kbd></td>
<td>Fragments with a depth less than or equal to corresponding current fragments in the depth buffer will pass the test.</td>
</tr>
<tr>
<td><kbd>gl.EQUAL</kbd></td>
<td>Only fragments with the same depth as current fragments on the depth buffer will pass the test.</td>
</tr>
<tr>
<td><kbd>gl.NOTEQUAL</kbd></td>
<td>Only fragments that do not have the same depth value as fragments on the depth buffer will pass the test.</td>
</tr>
<tr>
<td><kbd>gl.GEQUAL</kbd></td>
<td>Fragments with greater or equal depth value will pass the test.</td>
</tr>
<tr>
<td><kbd>gl.GREATER</kbd></td>
<td>Only fragments with a greater depth value will pass the test.</td>
</tr>
<tr>
<td><kbd>gl.ALWAYS</kbd></td>
<td>The depth test always passes.</td>
</tr>
</tbody>
</table>
<p>Â </p>
<p>The depth test is disabled by default in WebGL. When enabled, if no depth function is set, the <kbd>gl.LESS</kbd> function is selected by default.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Alpha Blending</h1>
                
            
            
                
<p>A fragment is available for alpha blending only if it has passed the depth test. By default, depth testing is disabled and makes all fragments available for alpha blending.</p>
<p>Alpha blending is enabled using the following line of code:</p>
<div><pre>gl.enable(gl.BLEND);</pre></div>
<p>For each available fragment, the alpha blending operation reads the color from the framebuffer by the appropriate fragment coordinates and creates a new color based on a linear interpolation between the previously calculated color in the fragment shader and the color from the framebuffer.</p>
<p>Alpha Blending<br/>
<br/>
Alpha blending is disabled by default in WebGL.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Blending Function</h1>
                
            
            
                
<p>With blending enabled, the next step is to define a blending function. This function will determine how fragment colors from the object (source) are combined with the fragment colors present in the framebuffer (destination).</p>
<p>We combine source and destination colors as follows:</p>
<div><pre>color = S * sW + D * dW;</pre></div>
<p>More precisely:</p>
<ul>
<li><kbd>S</kbd>: Source color (vec4)</li>
<li><kbd>D</kbd>: Destination color (vec4)</li>
<li><kbd>sW</kbd>: Source scaling factor</li>
<li><kbd>dW</kbd>: Destination scaling factor</li>
<li><kbd>S.rgb</kbd>: RGB components of the source color</li>
<li><kbd>S.a</kbd>: Alpha component of the source color</li>
<li><kbd>D.rgb</kbd>: RGB components of the destination color</li>
<li><kbd>D.a</kbd>: Alpha component of the destination color</li>
</ul>
<p>Itâ€™s important to note that the rendering order will determine the source and the destination fragments. Following the example from the previous section, if the sphere is rendered first, it will then become the destination of the blending operation because the sphere fragments are stored in the framebuffer at the time that the cone is rendered. In other words, alpha blending is a non-commutative operation with respect to rendering order:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/feef0be1-26ef-465c-b513-28e00b8bc4ec.png" style="width:39.17em;height:24.75em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Separate Blending Functions</h1>
                
            
            
                
<p>It is also possible to determine how the RGB channels are going to be combined independently from the alpha channels. For that, we use the <kbd>gl.blendFuncSeparate</kbd> function.</p>
<p>We define two independent functions this way:</p>
<div><pre>color = S.rgb * sW.rgb + D.rgb * dW.rgb;<br/>alpha = S.a * sW.a + D.a * dW.a;</pre></div>
<p>More precisely:</p>
<ul>
<li><kbd>sW.rgb</kbd>: Source scaling factor (only RGB)</li>
<li><kbd>dW.rgb</kbd>: Destination scaling factor (only RGB)</li>
<li><kbd>sW.a</kbd>: Source scaling factor for the source alpha value</li>
<li><kbd>dW.a</kbd>: Destination scaling factor for the destination alpha value</li>
</ul>
<p>Then, we could have something such as the following:</p>
<div><pre>color = S.rgb * S.a + D.rbg * (1.0 - S.a);<br/>alpha = S.a * 1.0 + D.a * 0.0;</pre>
<p>This would be translated into code as follows:</p>
</div>
<div><pre>gl.blendFuncSeparate(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA, gl.ONE, gl.ZERO);</pre></div>
<p>The parameters for the <kbd>gl.blendFuncSeparate</kbd> function are the same as <kbd>gl.blendFunc</kbd>. You can find more information on these functions later in this section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Blend Equation</h1>
                
            
            
                
<p>We could have a case where we do not want to interpolate the source and destination fragment colors with scale or add operations. For example, we may want to subtract one from the other. In this case, WebGL provides the <kbd>gl.blendEquation</kbd> function. This function receives one parameter that determines the operation on the scaled source and destination fragment colors. For example, <kbd>gl.blendEquation(gl.FUNC_ADD)</kbd> is calculated as such:</p>
<div><pre>color = S * sW + D * dW;</pre></div>
<p>And, <kbd>gl.blendEquation(gl.FUNC_SUBTRACT)</kbd> corresponds to the following:</p>
<div><pre>color = S * sW - D  * dW;</pre></div>
<p>There is a third option, <kbd>gl.blendEquation(gl.FUNC_REVERSE_SUBTRACT)</kbd>, that corresponds to the following:</p>
<div><pre>color = D* dw - S * sW;</pre></div>
<p>As expected, you can define the blending equation separately for the RGB channels and for the alpha channel. For that, we use the <kbd>gl.blendEquationSeparate</kbd> function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Blend Color</h1>
                
            
            
                
<p>WebGL provides the <kbd>gl.CONSTANT_COLOR</kbd> and <kbd>gl.ONE_MINUS_CONSTANT_COLOR</kbd> scaling factors. These scaling factors can be used with <kbd>gl.blendFunc</kbd> and <kbd>gl.blendFuncSeparate</kbd>. However, we need to first establish the blend color. We do so by invoking <kbd>gl.blendColor</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">WebGL Alpha-Blending API</h1>
                
            
            
                
<p>The following table summarizes the WebGL functions that are relevant to performing alpha-blending operations:</p>
<table border="1" style="border-collapse: collapse;width: 100%;border-color: #000000">
<tbody>
<tr>
<td><strong>WebGL function</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>gl.enable|disable(gl.BLEND)</kbd></td>
<td>Enable/disable blending.</td>
</tr>
<tr>
<td><kbd>gl.blendFunc(sW, dW)</kbd></td>
<td>
<p>Specify pixel arithmetic. Accepted values for <kbd>sW</kbd> and <kbd>dW</kbd> are as follows:</p>
<ul>
<li><kbd>ZERO</kbd></li>
<li><kbd>ONE</kbd></li>
<li><kbd>SRC_COLOR</kbd></li>
<li><kbd>DST_COLOR</kbd></li>
<li><kbd>SRC_ALPHA</kbd></li>
<li><kbd>DST_ALPHA</kbd></li>
<li><kbd>CONSTANT_COLOR</kbd></li>
<li><kbd>CONSTANT_ALPHA</kbd></li>
<li><kbd>ONE_MINUS_SRC_ALPHA</kbd></li>
<li><kbd>ONE_MINUS_DST_ALPHA</kbd></li>
<li><kbd>ONE_MINUS_SRC_COLOR</kbd></li>
<li><kbd>ONE_MINUS_DST_COLOR</kbd></li>
<li><kbd>ONE_MINUS_CONSTANT_COLOR</kbd></li>
<li><kbd>ONE_MINUS_CONSTANT_ALPHA</kbd></li>
</ul>
<p>In addition, <kbd>sW</kbd> can also be <kbd>SRC_ALPHA_SATURATE</kbd>.</p>
</td>
</tr>
<tr>
<td><kbd>gl.blendFuncSeparate(sW_rgb, dW_rgb, sW_a, dW_a)</kbd></td>
<td>
<p>Specify pixel arithmetic for RGB and alpha components separately.</p>
</td>
</tr>
<tr>
<td><kbd>gl.blendEquation(mode)</kbd></td>
<td>
<p>Specify the equation used for both the RGB blend equation and the alpha blend equation. Accepted values for mode are as follows:</p>
<ul>
<li><kbd>gl.FUNC_ADD</kbd></li>
<li><kbd>gl.FUNC_SUBTRACT</kbd></li>
<li><kbd>gl.FUNC_REVERSE_SUBTRACT</kbd></li>
</ul>
</td>
</tr>
<tr>
<td><kbd>gl.blendEquationSeparate(modeRGB, modeAlpha)</kbd></td>
<td>Set the RGB blend equation and the alpha blend equation separately.</td>
</tr>
<tr>
<td><kbd>gl.blendColor(red, green, blue, alpha)</kbd></td>
<td>Set the blend color.</td>
</tr>
<tr>
<td><kbd>gl.getParameter(name)</kbd></td>
<td>
<p>Just like with other WebGL variables, it is possible to query blending parameters using <kbd>gl.getParameter</kbd>. Relevant parameters are as follows:</p>
<ul>
<li><kbd>gl.BLEND</kbd></li>
<li><kbd>gl.BLEND_COLOR</kbd></li>
<li><kbd>gl.BLEND_DST_RGB</kbd></li>
<li><kbd>gl.BLEND_SRC_RGB</kbd></li>
<li><kbd>gl.BLEND_DST_ALPHA</kbd></li>
<li><kbd>gl.BLEND_SRC_ALPHA</kbd></li>
<li><kbd>gl.BLEND_EQUATION_RGB</kbd></li>
<li><kbd>gl.BLEND_EQUATION_ALPHA</kbd></li>
</ul>
</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Alpha Blending Modes</h1>
                
            
            
                
<p>Depending on the parameter selection for <kbd>sW</kbd> and <kbd>dW</kbd>, we can create different blending modes. In this section, we will see how to create additive, subtractive, multiplicative, and interpolative blending modes. All blending modes are derived from the previous formula:</p>
<div><pre>color = S * (sW) + D * dW;</pre></div>


            

            
        
    

        

                            
                    <h1 class="header-title">The Blending Function</h1>
                
            
            
                
<p>Additive blending simply adds the colors of the source and destination fragments, creating a lighter image. We obtain additive blending by writing the following:</p>
<div><pre>gl.blendFunc(gl.ONE, gl.ONE);</pre></div>
<p>This assigns the weights for source and destination fragments <kbd>sW</kbd> and <kbd>dW</kbd> to <kbd>1</kbd>. The color output will be as follows:</p>
<div><pre>color = S * 1.0 + D * 1.0;<br/>color = S + D;</pre></div>
<p>Since each color channel is in the <kbd>[0, 1]</kbd> range, blending will clamp all values over <kbd>1</kbd>. When all channels are <kbd>1</kbd>, this results in a white color.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Subtractive Blending</h1>
                
            
            
                
<p>Similarly, we can obtain subtractive blending by writing the following:</p>
<div><pre>gl.blendEquation(gl.FUNC_SUBTRACT);<br/>gl.blendFunc(gl.ONE, gl.ONE);</pre></div>
<p>This will change the blending equation to the following:</p>
<div><pre>color = S * 1.0 - D * 1.0;<br/>color = S - D;</pre></div>
<p>All negative values will be set to <kbd>0</kbd>. When all channels are negative, the result is black.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Multiplicative Blending</h1>
                
            
            
                
<p>We obtain multiplicative blending by writing the following:</p>
<div><pre>gl.blendFunc(gl.DST_COLOR, gl.ZERO);</pre></div>
<p>This will be reflected in the blending equation as the following:</p>
<div><pre>color = S * D + D * 0.0;<br/>color = S * D;</pre></div>
<p>The result will always be a darker blending.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Interpolative Blending</h1>
                
            
            
                
<p>If we set <kbd>sW</kbd> to <kbd>S.a</kbd> and <kbd>dW</kbd> to <kbd>1 - S.a</kbd>, then we get the following:</p>
<div><pre>color = S * S.a + D *(1 - S.a);</pre></div>
<p>This will create a linear interpolation between the source and destination color using the source alpha color, <kbd>S.a</kbd>, as the scaling factor. In code, this is translated as the following:</p>
<div><pre>gl.blendFunc(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA);</pre></div>
<p>Interpolative blending allows us to create a transparency effect as long as the destination fragments have passed the depth test. As expected, this requires that the objects be rendered from back to front.</p>
<p>In the next section, we will play with different blending modes on a simple scene composed of a cone and sphere.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Blending Workbench</h1>
                
            
            
                
<p>Let's cover an example of these various blending functions in action:</p>
<ol>
<li>Open the <kbd>ch06_09_blending.html</kbd> file in your browser. You will see an interface like the one in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/26723570-509d-46d5-95fe-a2093208bd24.png" style="width:78.67em;height:53.33em;"/></p>
<ol start="2">
<li>This interface has most of the parameters that allow you to configure alpha blending. The default settings are source <kbd>gl.SRC_ALPHA</kbd> and destination <kbd>gl.ONE_MINUS_SRC_ALPHA</kbd>. These are parameters for interpolative blending. Which slider do you need to use to change the scaling factor for interpolative blending? Why?</li>
</ol>
<ol start="3">
<li>Change the sphere alpha slider to <kbd>0.5</kbd>. You will see some shadow-like artifacts on the surface of the sphere. This occurs because the sphere back face is now visible. To get rid of the back face, click on Back Face Culling.</li>
<li>Click on the Reset button.</li>
<li>Disable the Lambert Term and Floor buttons.</li>
<li>Enable the Back Face Culling button.</li>
<li>Let's implement multiplicative blending. What values do source and destination need to have?</li>
<li>Click and drag the <kbd>canvas</kbd>. Check that the multiplicative blending creates dark regions where the objects overlap.</li>
<li>Change the blending function to <kbd>gl.FUNC_SUBTRACT</kbd> using the provided drop-down menu.</li>
<li>Change Source to <kbd>gl.ONE</kbd> and Destination to <kbd>gl.ONE</kbd>.</li>
<li>What blending mode is this? Click and drag the <kbd>canvas</kbd> to check the appearance of the overlapped regions.</li>
<li>Try different parameter configurations. Remember you can also change the blending function. If you decide to use a constant color or constant alpha, please use the color widget and the respective slider to modify the values of these parameters.</li>
</ol>
<p><strong><em>What just happened?</em></strong></p>
<p>You have seen how the additive, multiplicative, subtractive, and interpolative blending modes work with a simple exercise.</p>
<p>You have seen that the combination of <kbd>gl.SRC_ALPHA</kbd> and <kbd>gl.ONE_MINUS_SRC_ALPHA</kbd> produces transparency.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating Transparent Objects</h1>
                
            
            
                
<p>Weâ€™ve learned that in order to create transparency, we need to:</p>
<ul>
<li>Enable alpha blending and select the interpolative blending function</li>
<li>Render the faces of objects back to front</li>
</ul>
<p>How do we create transparent objects when there is nothing to blend them against? In other words, if thereâ€™s only one object, how can we make it transparent? One solution is to use <strong>face-culling</strong>.</p>
<p>Face-culling allows us to <em>only</em> render the back or front face of an object. We used this technique in the previous section when we only rendered the front face by enabling the Back Face Culling button.</p>
<p>Let's use the color cube from earlier in this chapter. We are going to make it transparent. For that effect, we will perform the following:</p>
<ol>
<li>Enable alpha blending and use the interpolative blending mode.</li>
<li>Enable face-culling.</li>
<li>Render the back face (by culling the front face).</li>
<li>Render the front face (by culling the back face).</li>
</ol>
<p>Similar to other options in the pipeline, culling is disabled by default. We enable it by calling the following:</p>
<div><pre>gl.enable(gl.FACE_CULLING);</pre></div>
<p>To render only the back faces of an object, we call <kbd>gl.cullFace(gl.FRONT)</kbd> before we call <kbd>drawArrays</kbd> or <kbd>drawElements</kbd>.</p>
<p>Similarly, to render only the front face, we use <kbd>gl.cullFace(gl.BACK)</kbd> before the draw call.</p>
<p>The following diagram summarizes the steps needed to create a transparent object with alpha blending and face-culling:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/28bfd5b0-2acb-4e74-9e47-37c2673177dd.png" style="width:42.58em;height:25.75em;"/></p>
<p>In the following section, we will see the transparent cube in action and take a look at the code that makes it possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Culling</h1>
                
            
            
                
<p>Let's cover an example showcasing culling in action:</p>
<ol>
<li>Open the <kbd>ch06_10_culling.html</kbd> file in your browser.</li>
<li>You will see that the interface is similar to the blending workbench exercise. However, on the top row, you will see these three options:
<ul>
<li>Alpha Blending: Enables or disables alpha blending.</li>
<li>Render Front Face: If active, renders the front face.</li>
<li>Render Back Face: If active, renders the back face.</li>
</ul>
</li>
</ol>
<ol start="3">
<li>Remember that for blending to work, objects need to be rendered back to front. Therefore, the back face of the cube is rendered first. This is reflected in the <kbd>draw</kbd> function:</li>
</ol>
<div><pre style="padding-left: 60px">if (showBackFace) {<br/>  gl.cullFace(gl.FRONT);<br/>  gl.drawElements(gl.TRIANGLES, object.indices.length, <br/>   gl.UNSIGNED_SHORT, 0);<br/>}<br/><br/>if (showFrontFace) {<br/>  gl.cullFace(gl.BACK);<br/>  gl.drawElements(gl.TRIANGLES, object.indices.length, <br/>   gl.UNSIGNED_SHORT, 0);<br/>}</pre></div>
<ol start="4">
<li>Going back to the web page, notice how the interpolative blending function produces the expected transparent effect. Move the alpha value slider that appears under the button options to adjust the scaling factor for interpolative blending.</li>
<li>Review the interpolative blending function. In this case, the destination is the back face (rendered first) and the source is the front face. If the alpha source equals <kbd>1</kbd>, what would you obtain according to the function? Test the result by moving the alpha slider to zero.</li>
</ol>
<ol start="6">
<li>Let's visualize the back face only. For that, disable the Render Front Face button. Increase the alpha value using the alpha value slider. Your screen should look like this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/0f601e12-975a-4aa6-b666-9ead2fbc6c8a.png"/></p>
<ol start="7">
<li>Click and drag the cube on the <kbd>canvas</kbd>. Notice how the back face is calculated every time you move the camera around.</li>
<li>Click on the Render Front Face again to activate it. Change the blending function so you can obtain subtractive blending.</li>
<li>Try different blending configurations using the controls provided in this exercise.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We have seen how face-culling and the alpha-blending interpolative mode can help us properly blend the faces of translucent objects.</p>
<p>Now, let's see how to implement transparency when there are two objects on the screen. In this case, we have a wall that we want to make transparent. Behind it is a cone.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Creating a Transparent Wall</h1>
                
            
            
                
<p>Let's cover an example of how we'd make an object transparent:</p>
<ol>
<li>Open <kbd>ch06_11_transparency-initial.html</kbd> in your browser. We have two completely opaque objects: a cone behind a wall. Click and drag the <kbd>canvas</kbd> to move the camera behind the wall and see the cone, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/a5ebfd61-ef0b-48fa-b084-e039c0ff0e7a.png"/></p>
<ol start="2">
<li>Change the wall alpha value by using the provided slider.</li>
<li>As you can see, modifying the alpha value does not produce any transparency. The reason for this is that alpha blending is not enabled. Let's edit the source code to include alpha blending. Open the <kbd>ch06_11_transparency-initial.html</kbd> file in your source code editor. Scroll to the <kbd>configure</kbd> function and find these lines:</li>
</ol>
<pre style="padding-left: 60px">gl.enable(gl.DEPTH_TEST);<br/>gl.depthFunc(gl.LESS);</pre>
<ol start="4">
<li>Below them, append the following lines:</li>
</ol>
<pre style="padding-left: 60px">gl.enable(gl.BLEND);<br/>gl.blendFunc(gl.SRC_ALPHA, gl.ONE_MINUS_SRC_ALPHA);</pre>
<ol start="5">
<li>Save your changes as <kbd>ch06_12_transparency-final.html</kbd> and load this page on your web browser.</li>
<li>As expected, the wall changes its transparency as you modify its alpha value using the respective slider.</li>
<li>Remember that in order for transparency to be effective, the objects need to be rendered back to front. Let's take a look at the source code. Open <kbd>ch06_12_transparency-final.html</kbd> in your source code editor.</li>
<li>The cone is the farthest object in the scene. Hence, it is loaded first. You can check that by looking at the <kbd>load</kbd> function:</li>
</ol>
<div><pre style="padding-left: 60px">function load() {<br/>  scene.add(new Floor(80, 20));<br/>  scene.load('/common/models/ch6/cone.json', 'cone');<br/>  scene.load('/common/models/ch6/wall.json', 'wall', {<br/>    diffuse: [0.5, 0.5, 0.2, 1.0],<br/>    ambient: [0.2, 0.2, 0.2, 1.0]<br/>  });<br/>}</pre></div>
<ol start="9">
<li>It occupies a lower index in the <kbd>scene.objects</kbd> list. In the <kbd>render</kbd> function, the objects are rendered in the order in which they appear in the <kbd>scene.objects</kbd> list:</li>
</ol>
<div><pre style="padding-left: 60px">scene.traverse(object =&gt; {<br/>  // ...<br/>});</pre></div>
<ol start="10">
<li>What happens if we rotate the scene so that the cone is closer to the camera and the wall is farther away?</li>
<li>Open <kbd>ch06_12_transparency-final.html</kbd> in your browser and rotate the scene such that the cone appears in front of the wall. Decrease the alpha value of the cone while the alpha value of the wall remains at <kbd>1.0</kbd>.</li>
<li>As you can see, the blending is inconsistent. This does not have to do with alpha blending because in <kbd>ch06_12_transparency-final.html</kbd>, the blending is enabled. It has to do with the <strong>rendering order</strong>. Click on the Wall First button. The scene should appear consistent now:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/9dee8429-19a6-42ab-bd95-067b32bfe57f.png"/></p>
<ol start="13">
<li>The Cone First and Wall First buttons use a couple of new functions that we have included in the <kbd>Scene</kbd> class to change the rendering order. These functions are <kbd>renderSooner</kbd> and <kbd>renderFirst</kbd>.</li>
<li>In total, we have added these functions to the <kbd>Scene</kbd> object to deal with rendering order:
<ul>
<li><kbd>renderSooner(objectName)</kbd>: Moves the <kbd>objectName</kbd> object one position higher in the <kbd>Scene.objects</kbd> list.</li>
<li><kbd>renderLater(objectName)</kbd>: Moves the <kbd>objectName</kbd> object one position lower in the <kbd>Scene.objects</kbd> list.</li>
<li><kbd>renderFirst(objectName)</kbd>: Moves the <kbd>objectName</kbd> object to the first position of the list (index 0).</li>
<li><kbd>renderLast(objectName)</kbd>: Moves the <kbd>objectName</kbd> object to the last position of the list.</li>
<li><kbd>renderOrder()</kbd>: Lists the objects in the <kbd>Scene.objects</kbd> list in the order in which they are rendered. This is the same order in which they are stored in the list. For any two given objects, the object with the lower index will be rendered first.</li>
</ul>
</li>
</ol>
<ol start="15">
<li>You can use these functions from the JavaScriptÂ consoleÂ in your browser and see what effect these have on the scene.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We took a simple scene where we implemented alpha blending. After that, we analyzed the importance of the rendering order in creating consistent transparencies. Finally, we presented the new methods of the <kbd>Scene</kbd> object that control the rendering order.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Letâ€™s summarize what we learned in this chapter:</p>
<ul>
<li>We learned how to extensively use colors with objects, lights, and in the scene. Specifically, weâ€™ve learned that an object can be colored per vertex, per fragment, or can have a constant color.</li>
<li>We reviewed lights and the various approaches to lighting models.</li>
<li>We covered how to create lights of different colors and leveraged concepts from directional and point lights to create spot lights. By introducing several light sources in our scene, we updated our architectural patterns and used uniform arrays to reduce the complexity of creating and mapping uniforms between JavaScript and ESSL.</li>
<li>We learned that proper translucency requires more than just using alpha values in our color vectors. Because of this, we explored various blending behaviors, render sequences, and WebGL functions to create translucent objects.</li>
<li>We learned how face-culling can help produce better results when there are multiple translucent objects present in the scene.</li>
</ul>
<p>In the next chapter, we will learn how to leverage textures to help us render images in our scene.</p>


            

            
        
    </body></html>