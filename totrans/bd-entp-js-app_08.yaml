- en: Writing Unit/Integration Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now done as much as we can to modularize our code base, but how much
    confidence do we have in each of the modules? If one of the E2E tests fails, how
    would we pinpoint the source of the error? How do we know which module is faulty?
  prefs: []
  type: TYPE_NORMAL
- en: We need a lower level of testing that works at the module level to ensure they
    work as distinct, standalone units—we need **unit**** tests**. Likewise, we should
    test that multiple units can work well together as a larger logical unit; to do
    that, we need to also implement some **integration tests**.
  prefs: []
  type: TYPE_NORMAL
- en: 'By following this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Write unit and integration tests using **Mocha**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Record function calls with **spies**, and simulate behavior with **stubs**,
    both provided by the **Sinon** library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stub out dependencies in unit tests using **dependency injection** (**DI**)
    or **monkey patching**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring **test coverage** with **Istanbul**/**nyc**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Picking a testing framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there's only one *de facto* testing framework for E2E tests for JavaScript
    (Cucumber), there are several popular testing frameworks for unit and integration
    tests, namely Jasmine ([jasmine.github.io](https://jasmine.github.io/)), Mocha
    ([mochajs.org](https://mochajs.org/)), Jest ([jestjs.io](https://jestjs.io/)),
    and AVA ([github.com/avajs/ava](https://github.com/avajs/ava)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using Mocha for this book, but let''s understand the rationale behind
    that decision. As always, there are pros and cons for each choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Maturity**: Jasmine and Mocha have been around for the longest, and for many
    years were the only two viable testing frameworks for JavaScript and Node. Jest
    and AVA are the new kids on the block. Generally, the maturity of a library correlates
    with the number of features and the level of support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Popularity**: Generally, the more popular a library is, the larger the community,
    and the higher likelihood of receiving support when things go awry. In terms of
    popularity, let''s examine several metrics (correct as of September 7, 2018):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub stars@ Jest (20,187), Mocha (16,165), AVA (14,633), Jasmine (13,816)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposure (percentage of developers who have heard of it): Mocha (90.5%), Jasmine
    (87.2%), Jest (62.0%), AVA (23.9%)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Developer satisfaction (percentage of developers who have used the tool *and
    would use it again*): Jest (93.7%), Mocha (87.3%), Jasmine (79.6%), AVA (75.0%).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism**: Mocha and Jasmine both run tests serially (meaning one after
    the other), which means they can be quite slow. Instead, AVA and Jest, by default,
    run unrelated tests in parallel, as separate processes, making tests run faster
    because one test suite doesn''t have to wait for the preceding one to finish in
    order to start.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backing**: Jasmine is maintained by developers at Pivotal Labs, a software
    consultancy from San Francisco. Mocha was created by TJ Holowaychuk and is maintained
    by several developers; although it is not maintained by a single company, it is
    backed by larger companies such as Sauce Labs, Segment, and Yahoo!. AVA was started
    in 2015 by Sindre Sorhus and is maintained by several developers. Jest is developed
    by Facebook, and so has the best backing of all the frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Composability**: Jasmine and Jest have different tools bundled into one framework,
    which is great to get started quickly, but it means we can''t see how everything
    fits together. Mocha and AVA, on the other hand, simply run the tests, and you
    can use other libraries such as `Chai`, `Sinon`, and `nyc` for assertions, mocking,
    and coverage reports, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exposure and developer satisfaction figures are derived from The State of
    JavaScript survey, 2017 ([2017.stateofjs.com/2017/testing/results](https://2017.stateofjs.com/2017/testing/results/)).
  prefs: []
  type: TYPE_NORMAL
- en: We have chosen to use Mocha for this book, as it allows us to compose a custom
    testing stack. By doing this, it allows us to examine each testing tool individually,
    which is beneficial for your understanding. However, once you understand the intricacies
    of each testing tool,  I do encourage you to try Jest, as it is easier to set
    up and use.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Mocha
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s install Mocha as a development dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will install an executable, `mocha`, at `node_modules/mocha/bin/mocha`,
    which we can execute later to run our tests.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring our test files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we are going to write our unit tests, but where should we put them? There
    are generally two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Placing all tests for the application in a top-level `test/` directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Placing the unit tests for a module of code next to the module itself, and using
    a generic `test` directory only for application-level integration tests (for example,
    testing integration with external resources such as databases)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second approach (as shown in the following example) is better as it keeps
    each module *truly* separated in the filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore, we're going to use the `.test.js` extension to indicate that a
    file contains tests (although using `.spec.js` is also a common convention). We
    will be even more explicit and specify the *type* of test in the extension itself;
    that is, using `unit.test.js` for unit test, and `integration.test.js` for integration
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: Writing our first unit test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s write unit tests for the `generateValidationErrorMessage` function.
    But first, let''s convert our `src/validators/errors/messages.js` file into its
    own directory so that we can group the implementation and test code together in
    the same directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, in `index.unit.test.js`, import the `assert` library and our `index.js`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to write our tests.
  prefs: []
  type: TYPE_NORMAL
- en: Describing the expected behavior
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we installed the `mocha` npm package, it provided us with the `mocha` command
    to execute our tests. When we run `mocha`, it will inject several functions, including `describe` and `it`, as
    global variables into the test environment. The `describe` function allows us
    to group relevant test cases together, and the `it` function defines the actual
    test case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside `index.unit.tests.js`, let''s define our first `describe` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Both the `describe` and `it` functions accept a string as their first argument,
    which is used to describe the group/test. The description has no influence on
    the outcome of the test, and is simply there to provide context for someone reading
    the tests.
  prefs: []
  type: TYPE_NORMAL
- en: The second argument of the `it` function is another function where you'd define
    the assertions for your tests. The function should throw an `AssertionError` if
    the test fails; otherwise, Mocha will assume that the test should pass.
  prefs: []
  type: TYPE_NORMAL
- en: In our test, we have created a dummy `errors` array that mimics the `errors`
    array, which is typically generated by Ajv. We then passed the array into the `generateValidationErrorMessage` function
    and capture its returned value. Lastly, we compare the actual output with our
    expected output; if they match, the test should pass; otherwise, it should fail.
  prefs: []
  type: TYPE_NORMAL
- en: Overriding ESLint for test files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding test code should have caused some ESLint errors. This is because
    we violated three rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`func-names`: Unexpected unnamed function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefer-arrow-callback`: Unexpected function expression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no-undef`: `describe` is not defined'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's fix them before we continue.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding arrow functions in Mocha
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already encountered the `func-names` and `prefer-arrow-callback` rules
    before when we wrote our E2E tests with `cucumber-js`. Back then, we needed to
    keep using function expressions instead of arrow functions because `cucumber-js`
    uses `this` inside each function to maintain context between different steps of
    the same scenario. If we'd used arrow functions, `this` would be bound, in our
    case, to the global context, and we'd have to go back to using file-scope variables
    to maintain state between steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, Mocha also uses `this` to maintain a "context". However, in
    Mocha''s vocabulary, a "context" is not used to persist state between steps; rather,
    a Mocha context provides the following methods, which you can use to control the
    flow of your tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`this.timeout()`: To specify how long, in milliseconds, to wait for a test
    to complete before marking it as failed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`this.slow()`: To specify how long, in milliseconds, a test should run for
    before it is considered "slow"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`this.skip()` : To skip/abort a test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`this.retries()`: To retry a test a specified number of times'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also impractical to give names to every test function; therefore, we should
    disable both the `func-names` and `prefer-arrow-callback` rules.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we disable these rules for our test files? For our E2E tests, we
    created a new `.eslintrc.json` and placed it inside the `spec/` directory. This
    would apply those configurations to all files under the `spec/` directory. However,
    our test files are not separated into their own directory, but interspersed between
    all our application code. Therefore, creating a new `.eslintrc.json` won't work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we can add an `overrides` property to our top-level `.eslintrc.json`,
    which allows us to override rules for files that match the specified file glob(s).
    Update `.eslintrc.json` to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are indicating that files with the extension `.test.js` should have
    the `func-names` and `prefer-arrow-callback` rules turned off.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying ESLint environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, ESLint will still complain that we are violating the `no-undef` rule.
    This is because when we invoke the `mocha` command, it will inject the `describe`
    and `it` functions as global variables. However, ESLint doesn't know this is happening
    and warns us against using variables that are not defined inside the module.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can instruct ESLint to ignore these undefined globals by specifying an **environment**. An
    environment defines global variables that are predefined. Update our overrides
    array entry to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, ESLint should not complain anymore!
  prefs: []
  type: TYPE_NORMAL
- en: Running our unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run our test, we''d normally just run `npx mocha`. However, when we try
    that here, we get a warning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is because, by default, Mocha will try to find a directory named `test` at
    the root of the project and run the tests contained inside it. Since we placed
    our test code next to their corresponding module code, we must inform Mocha of
    the location of these test files. We can do this by passing a **glob** matching
    our test files as the second argument to `mocha`. Try running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We got another error. We had already encountered this when we worked with `cucumber-js`.
    This error occurs because Mocha is not using Babel to transpile our test code
    before running it. With `cucumber-js`, we used the `--require-module` flag to
    require the `@babel/register` package, which ??. We can do the same with Mocha
    using its `--require` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you've forgotten about the different Babel packages (for example, `@babel/node`, `@babel/register`, `@babel/polyfill`,
    and so on), refer back to [Chapter 6](76e42f28-9731-49ca-9e87-fab7b2b6a7e8.xhtml), *Setting
    Up Development Tools*, under the *Different faces of Babel* section.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the test description we passed into `describe` and `it` is displayed
    in the test output.
  prefs: []
  type: TYPE_NORMAL
- en: Running unit tests as an npm script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Typing out the full `mocha` command each time can be tiresome. Therefore, we
    should create an npm script just like we did with the E2E tests. Add the following
    to the scripts object inside our `package.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, let''s also update our existing `test` npm script to run all our
    tests (both unit and E2E):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run our unit tests by running `yarn run test:unit`, and run all
    our tests with `yarn run test`. We have now completed our first unit test, so
    let''s commit the changes and move on to writing even more tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Completing our first unit test suite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have only covered a single scenario with our first unit test. Therefore,
    we should write more tests to cover every scenario. Try completing the unit test
    suite for `generateValidationErrorMessage` yourself; once you are ready, compare
    your solution with the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the tests again, and note how the tests are grouped under the `describe` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now completed the unit tests for `generateValidationErrorMessage`,
    so let''s commit it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Unit testing ValidationError
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, let''s focus on testing the `ValidationError` class. Once again, we will
    move the `validation.js` file into its own director:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a new file at `src/validators/errors/validation-error/index.unit.test.js` to
    house our unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the tests and make sure they pass. Then, commit it into the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Unit testing middleware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we are going to test our middleware functions, starting with the `checkEmptyPayload` middleware.
    Like we did previously, move the middleware module into its own directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, inside `src/middlewares/check-content-type.js/index.unit.test.js`, lay
    out the skeleton of our first test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The purpose of the `checkEmptyPayload` middleware is to ensure that the `POST`,
    `PATCH`, and `PUT` requests always carry a non-empty payload. Therefore, if we
    pass in a request with a different method, say `GET`, we should be able to assert
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: That the `res` object is not modified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That the `next` function is invoked once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asserting deep equality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To assert that the `res` object has not been modified, we need to perform a
    deep comparison of the `res` object before and after `checkEmptyPayload` has been
    called.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of implementing this function ourselves, we can save time by using existing
    utility libraries. For instance, Lodash provides the `cloneDeep` method ([lodash.com/docs/#cloneDeep](https://lodash.com/docs/#cloneDeep)) for
    deep cloning, and the `isEqual` method ([lodash.com/docs/#isEqual](https://lodash.com/docs/#isEqual)) for
    deep object comparison.
  prefs: []
  type: TYPE_NORMAL
- en: To use these methods in our code, we can install the `lodash` package from npm,
    which contains hundreds of utility methods. However, we won't be using most of
    these methods in our project; if we install the entire utility library, most of
    the code would be unused. We should always try to be as lean as possible, minimizing
    the number, and size, of our project's dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, Lodash provides a separate npm package for each method, so let''s
    add them to our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You can use an online tool called Bundlephobia ([bundlephobia.com](https://bundlephobia.com/))
    to find out the file size of an npm package, without downloading it.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can see from [bundlephobia.com/result?p=lodash@4.17.10](https://bundlephobia.com/result?p=lodash@4.17.10)
    that the `lodash` package is 24.1 KB in size after it's been minified and gzipped.
    Similarly, the `lodash.isequal` and `lodash.clonedeep` packages have a size of
    3.7 KB and 3.3 KB, respectively. Therefore, by installing the more specific packages,
    we have reduced the amount of unused code in our project by 17.1 KB.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's use the `deepClone` method to clone the `res` object before passing
    it to `checkEmptyPayload`. Then, after `checkEmptyPayload` has been called, use `deepEqual`
    to compare the `res` object and its clone, and assert whether the `res` object
    has been modified or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a go at implementing it yourself, and compare your solution with ours,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need a way to assert that the `next` function has been called once.
    We can do that by using test **spies**.
  prefs: []
  type: TYPE_NORMAL
- en: Asserting function calls with spies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A spy is a function that records information about every call made to it. For
    example, instead of assigning an empty function to `next`, we can assign a spy
    to it. Whenever `next` is invoked, information about each invocation is stored
    inside the spy object. We can then use this information to determine the number
    of times the spy has been called.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *de facto* spy library in the ecosystem is Sinon ([sinonjs.org](http://sinonjs.org/)),
    so let''s install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in our unit test, import the `spy` **named export** from the `sinon`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in our test function, instead of assigning an empty function to `next`,
    assign it a new spy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When the spy function is called, the spy will update some of its properties
    to reflect the state of the spy. For example, when it''s been called once, the
    spy''s `calledOnce` property will be set to `true`; if the spy function is invoked
    again, the `calledOnce` property will be set to `false` and the `calledTwice`
    property will be set to `true`. There are many other useful properties such as
    `calledWith`, but let''s update our `it` block by checking the `calledOnce` property
    of our spy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll define more tests to examine what happens when `req.method` is
    one of `POST`, `PATCH`, or `PUT`. Implement the following tests, which test what
    happens when the `content-length` header is not `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`beforeEach` is another function that is injected into the global scope by
    Mocha. `beforeEach` will run the function passed into it, prior to running each
    `it` block that resides on the same or lower level as the `beforeEach` block.
    Here, we are using it to invoke `checkEmptyPayload` before each assertion.'
  prefs: []
  type: TYPE_NORMAL
- en: '`beforeEach` is a type of **hook** function. There are also `afterEach`, `before`,
    and `after`. See how you can use them by referring to the documentation at [mochajs.org/#hooks](https://mochajs.org/#hooks).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, when the `content-type` header is `0`, we want to assert that the `res.status`,
    `res.set`, and `res.json` methods are called correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we need to test that `checkEmptyPayload` will return the output of `res.json()`.
    To do that, we need to use another test construct called **stubs**.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating behavior with stubs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stubs are functions that simulate the behavior of another component.
  prefs: []
  type: TYPE_NORMAL
- en: In Sinon, stubs are an extension to spies; this means that all the methods that
    are available to spies are also available to stubs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of our tests, we don''t really care about the returned value
    of `res.json()` – we only care that our `checkEmptyPayload` middleware function
    relays this value back faithfully. Therefore, we can turn our `res.json` spy into
    a stub, and make it return a reference to an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then add another assertion step to compare the value returned by the `checkEmptyPayload` function,
    and the value returned by our `res.json` stub; they should be strictly identical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the unit tests by executing `yarn run test:unit`, fix any errors that cause
    the tests to fail, and then commit the unit tests to the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Testing all middleware functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it's time for you to write some unit tests yourself. Try following the
    same approach to test the `checkContentTypeIsJson`, `checkContentTypeIsSet`, and `errorHandler`
    middleware functions. Refer to the code bundle for help if needed. As always,
    run the tests and commit your code!
  prefs: []
  type: TYPE_NORMAL
- en: Once all of our middleware functions have been unit tested, we will move on
    to testing the request handlers and the engine.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing the request handler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we'll move the `src/handlers/users/create.js` module into its own directory.
    Then, we will correct the file paths specified in the `import` statements to point
    to the correct file. Lastly, we will create an `index.unit.test.js` file next
    to our module to house the unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `createUser` function inside our request handler
    module. It has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: First, it will call the `create` function that was imported from `src/engines/users/create/index.js`.
    Based on the result, it will invoke either the `onFulfilled` or `onRejected` callbacks
    inside the `then` block.
  prefs: []
  type: TYPE_NORMAL
- en: Although our `createUser` function depends on the `create` function, when writing
    a unit test, our test should test only the relevant unit, not its dependencies.
    Therefore, if the result of our tests relies on the `create` function, we should
    use a stub to control its behavior. Otherwise, our test would, in fact, be an
    *integration test*.
  prefs: []
  type: TYPE_NORMAL
- en: Stubbing create
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can create different stubs that return different results, each mimicking
    the possible return values of the `create` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we invoke `createStubs.success()`, it will always resolve to the `{
    _id: ''foo''}` object; therefore, we can use this stub to test for scenarios where
    the `req` object we pass into the `createUser` function is valid. Likewise, we
    can use `createStubs.validationError()` to mimic a situation where the `req` object
    causes `createUser` to reject with `ValidationError`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we know how to stub out the `create` function, but how do we actually replace
    it inside the `createUser` function? When testing the `createUser` function, the
    only variables we can change in our test are the parameters we pass into the function,
    and the `createUser` method accepts only three parameters: `req`, `res`, and `db`.
  prefs: []
  type: TYPE_NORMAL
- en: There are two approaches to this: **dependency injection** and **monkey patching**.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency injection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of dependency injection is to make every dependency a parameter of
    the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the moment, our `createUser` function relies on entities outside of its
    parameters; this includes the `create` function and the `ValidationError` class.
    If we were to use dependency injection, we''d modify our `createUser` function
    to have the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we would be able to inject the following dependencies from our tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Monkey patching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative approach to dependency injection is monkey patching, where we
    dynamically modify the system at runtime. In our example, we might want to replace
    the `create` function with our stub functions, but *only* when we are running
    our tests.
  prefs: []
  type: TYPE_NORMAL
- en: Implementations of monkey patching libraries tend to be hacky and usually involves
    reading the module code into a string, injecting custom code into the string,
    and then loading it. Thus, the entities being monkey patched would be modified
    in some way.
  prefs: []
  type: TYPE_NORMAL
- en: There are several libraries that allow us to apply monkey patches when running
    tests; the most popular library is `rewire` ([npmjs.com/package/rewire](https://www.npmjs.com/package/rewire)).
    It also has a Babel plugin equivalent called `babel-plugin-rewire` ([github.com/speedskater/babel-plugin-rewire](https://github.com/speedskater/babel-plugin-rewire)).
  prefs: []
  type: TYPE_NORMAL
- en: 'This plugin will add the `__set__`, `__get__`, and `__with__` methods to every
    top-level file-scoped entity in the module being "rewired". Now, we can use the `__set__` method
    of our `createUser` module to monkey patch our `create` function, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `__set__` method returns a function that we can use to revert the `create` function
    back to its original state. This is useful when you want to run tests using different
    variants of `create`. In that case, you'd simply `revert` the create function
    after each test run, and patch it again at the beginning of the next run.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency injection versus monkey patching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both approaches have their pros and cons, so let's compare their differences
    and see which one is the most appropriate for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: Modularity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dependency injection has the benefit of keeping every module as decoupled as
    possible, as modules do not have predefined dependencies; every dependency is
    passed in (injected) at runtime. This makes unit testing a lot easier, as we can
    replace any dependencies with stubs, keeping our unit tests truly unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Readability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With dependency injection, every dependency must be a parameter of the function.
    Thus, if the module has 20 dependencies, it'll need to have 20 parameters. This
    can make the module hard to read.
  prefs: []
  type: TYPE_NORMAL
- en: Often, you'll have a single root file where every dependency is imported, instantiated,
    and injected; these dependencies would then be passed down to child functions,
    and their child functions, and so on. This means for a developer to find the source
    of the dependency, he/she would have to follow the trail of function calls leading
    up to the root where the dependency is originally injected. This could be three
    or four function calls, or it might be a dozen.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, the more abstraction layers there are in a project, the
    harder it is for developers to read the code, but this is especially true when
    using the dependency injection approach.
  prefs: []
  type: TYPE_NORMAL
- en: With monkey patching, the signature of the module functions can be much leaner.
    Only dynamic dependencies would be included in the function parameters list; utility
    functions and static dependencies can be imported at the top of the file.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the `req`, `res`, and `db` parameters of the `createUser` function
    are dynamic – `req` and `res` would be different for each request, and `db` is
    only instantiated at startup. On the other hand, the `create` function and `ValidationError`
    class are static – you know their exact value before you run the code.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, using monkey patching can improve the readability of our application
    code, at the expense of making our test code a bit more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Reliance on third-party tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dependency injection is a simple concept to implement and does not require any
    third-party tools. On the other hand, monkey patching is hard to implement and
    you'd normally use `babel-plugin-rewire` or a similar library. This means that
    our test would now have to depend on the `babel-plugin-rewire` package.
  prefs: []
  type: TYPE_NORMAL
- en: This can become an issue if `babel-plugin-rewire` becomes unmaintained, or if
    maintenance is slow. At the time of writing this book, the `babel-plugin-rewire` plugin
    still lacks support for Babel 7\. If a developer is using the `babel-plugin-rewire` plugin,
    he/she won't be able to upgrade their Babel version, and for developers who are
    already using Babel 7, they won't be able to monkey patch until support is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Following the dependency injection pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the preceding discussion, it seems like dependency injection is the better
    choice. Readability should not be too much of an issue, as we only have two layers
    of abstraction – handlers and engines. Therefore, let's migrate our code to use
    the dependency injection pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, remove the `import` statements from `src/handlers/users/create/index.js` and
    change the signature of the `createUser` function to include the `create` engine
    function and the `ValidationError` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to inject these dependencies into the handler. In `src/index.js`,
    we are already using the `injectHandlerDependencies` function to inject the database
    client into the handler, so let's modify it to also inject the corresponding engine
    function and `ValidationError` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import all the dependencies inside `src/index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s create a mapping of handler functions to engine functions, and
    call it `handlerToEngineMap`. We will pass this `handlerToEngineMap` function
    into the `injectHandlerDependencies` function, so that it knows which engine to
    inject:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `Map` object, which was introduced in ECMAScript 2015 (ES6).
    A `Map` is a key-value store, where the keys and values can be of any type – primitives,
    objects, arrays, or functions (the last two are just special types of object).
    This is unlike an object literal, where the keys must be either a string or a
    Symbol. Here, we are storing the handler function as the key, and the engine function
    as the value.
  prefs: []
  type: TYPE_NORMAL
- en: 'All that''s left to do in `src/index.js` is to add `handlerToEngineMap` and
    `ValidationError` into `injectHandlerDependencies`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, update the `injectHandlerDependencies` function to relay these dependencies
    into the handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve made a lot of changes in many files, so you should run all of our existing
    tests again to make sure that we didn''t break anything. You may also want to
    commit these changes to the Git repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Promises and Mocha
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're now ready to get back to our original task – writing unit tests for our
    Create User request handler! You should have enough understanding to implement
    the unit tests for the handler yourself, but we'd like to first give you some
    hints with regards to promises.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the function we are testing perform asynchronous operations, there''s no
    guarantee that the asynchronous operations would complete before our assertion
    code is run. For instance, if our `create` engine function is actually very slow
    to resolve, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the following test would fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Mocha can deal with asynchronous code in two ways – using callbacks or promises.
    Since we'd generally avoid using callbacks, let's focus on working with promises.
    In Mocha, if we return a promise in the preceding `beforeEach` block, Mocha will
    wait for the promise to settle before running the relevant `describe` and `it`
    blocks. Therefore, when writing functions that involve asynchronous operations,
    we should *always return a promise*. Not only does it make the function easier
    to test, but it also allows you to chain multiple promises together should you
    have that need in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we must update our `createUser` function to a promise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, make sure that all of our `beforeEach` blocks also return a promise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Dealing with rejected promises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, another limitation of Mocha is that you cannot return a rejected promise
    inside the hook functions. If you do, Mocha will think the test has failed. In
    those cases, you should move the function that you expect to fail inside the `it`
    block, and make any assertions inside a `catch` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Completing the unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You now have enough understanding of unit tests, Mocha, and working with promises
    to complete the unit tests for the Create User handler. Have a go at implementing
    this yourself, referring back to the reference code sample only if you need to.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, don''t forget to run the unit and E2E tests to make sure you haven''t
    introduced any regression, and then commit the changes to our repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Unit testing our engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, let''s test our `create` engine function. Like our previous `createUser`
    request handler, the `src/engines/users/create/index.js` module contains two `import`
    statements, which makes it difficult to test. Therefore, just like before, we
    must pull these dependencies out, and import them back into `src/index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, update the `injectHandlerDependencies` function to inject the validator
    function into the handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, inside the handler, relay the validator function and `ValidationError`
    class into the engine function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, update the unit tests to cater for this change. Once all tests pass,
    commit this change to Git:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Once that''s committed, let''s move on to writing the unit tests themselves.
    There are only two cases to test for – when the validator returns with a `ValidationError`,
    or when it returns with `undefined`. Again, because we don''t want our unit tests
    to depend on the validator, and so we will use stubs to simulate its functionality.
    Attempt to implement it yourself and compare it with our implementation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, run the tests and commit the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Integration testing our engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have been retrofitting our code with unit tests, which test each
    unit individually, independent of external dependencies. However, it's also important
    to have confidence that different units are compatible with each other. This is
    where integration tests are useful. So, let's add some integration tests to our
    User Create engine that'll test its interaction with the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s update our npm scripts to include a `test:integration` script.
    We''ll also update the glob file in our `test:unit` npm to be more specific and
    select only unit tests. Lastly, update the `test` script to run the integration
    tests after the unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `dotenv mocha` part will run Mocha after loading all the environment variables.
    We are then using a double dash (`--`) to signify to our *bash* shell that this
    is the end of the options for the `dotenv` command; anything after the double
    dash is passed into the `mocha` command, like it did previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'You write your integration tests in the same way as your unit tests, the only
    difference being instead of stubbing everything, you supply the unit you''re testing
    with genuine parameters. Let''s take a look at the signature of our create function
    once again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Previously, we used stubs to simulate the real `db` object and `createUserValidator`
    function. For an integration test, you''d actually import the real validator function
    and instantiate a real Elasticsearch JavaScript client. Once again, try to implement
    the integration tests yourself, and check back here for our solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, run all the tests to make sure they all pass, then commit these changes
    to the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Adding test coverage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of our TDD process, we wrote E2E tests first and used them
    to drive development. However, for unit and integration tests, we actually retrofitted
    them back into our implementation. Therefore, it's very likely that we missed
    some scenarios that we should have tested for.
  prefs: []
  type: TYPE_NORMAL
- en: To remedy this practical problem, we can summon the help of **test coverage**
    tools. A test coverage tool will run your tests and record all the lines of code
    that were executed; it will then compare this with the total number of lines in
    your source file to return a percentage coverage. For example, if my module contains
    100 lines of code, and my tests only ran 85 lines of my module code, then my test
    coverage is 85%. This may mean that I have dead code or that I missed certain
    use cases. Once I know that some of my tests are not covering all of my code,
    I can then go back and add more test cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *de facto* test coverage framework for JavaScript is `istanbul` ([github.com/gotwarlost/istanbul](https://github.com/gotwarlost/istanbul)).
    We will be using istanbul via its command line interface, `nyc` ([github.com/istanbuljs/nyc](https://github.com/istanbuljs/nyc)).
    So, let''s install the `nyc` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, add the following npm script to `package.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run `yarn run test:unit:coverage` to get a report of our code coverage.
    Because we specified the `--reporter=text` option, `nyc` which will print the
    results to stdout in a text table format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b292ab9-3c72-40fc-82e7-f2de0962bfd6.png)'
  prefs: []
  type: TYPE_IMG
- en: The `--reporter=html` flag will also instruct `nyc` to create an HTML report,
    which is stored at a new `coverage` directory at the root of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Reading a test coverage report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inside the `coverage` directory, you should find an `index.html` file; open
    it up in a web browser to continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4850beca-4bde-4ce5-b632-41320b7ac81c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At the top, you can see different percentages of test coverage. Here''s what
    they mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lines**: Percentage of the total lines of code (LoC) that were run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statements**: Percentage of total statements that were executed. If you always
    use a separate line for each statement (as is the case in our project), then Statements
    and Lines would have the same value. If you have multiple statements per line
    (for example, `if (condition) { bar = 1; }`), then there''ll be more statements
    than lines, and the Statements coverage may be lower. The Statements coverage
    is more useful than Lines coverage; the Lines coverage exists for interoperability
    with line-oriented coverage tools like `lcov`. Note that you can use ESLint to
    enforce having one statement per line by enabling the `max-statements-per-line`
    rule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Branches**: Imagine our code as a set of paths – if certain conditions are
    met, the execution of our program will follow a certain path; when a different
    set of conditions is employed, the execution will follow a different path. These
    paths diverge at conditional statements into *branches*. The branch coverage indicates
    how many of these branches are covered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functions** : The percentage of total functions that were called.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that our overall Statements coverage is 91.84%, which is pretty good
    already. However, our `handlers/users/create/index.js` file seems to have only
    66.67% coverage. Let's investigate why!
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the handlers/users/create link until you arrive at the screen showing
    the source code of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90679309-0891-4780-847b-ec29341915c0.png)'
  prefs: []
  type: TYPE_IMG
- en: A green bar on the left-hand side indicates that the line is covered. Furthermore,
    `nyc` will give you a count for how many times that line was executed over the
    entire run of our unit test suite. For example, the preceding `res.status(201)`
    line has been executed 8 times.
  prefs: []
  type: TYPE_NORMAL
- en: 'A red bar indicates that the line has not been executed. This can mean one
    of a few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Our tests are insufficient and do not test all possible scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's unreachable code in our project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other gaps in the coverage are indicated in the code itself as a letter
    enclosed inside a black box; when you hover over it, it will provide a more descriptive
    reason. In our case, there's a letter E, which stands for "else path not taken",
    meaning there's no test that covers what happens when the `create` function rejects
    with an error that is *not* an instance of `ValidationError`.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, it actually highlights an error in our code. Inside the `onRejected`
    function of our `then` block, we are returning `undefined` if the error is not
    an instance of `ValidationError`. This will, in effect, return a resolved promise,
    and thus the `catch` block will never catch the error. Furthermore, we are also
    not testing for the case where the `create` function returns a generic error.
    Therefore, let's increase the test coverage for this module by fixing these two
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do, let''s commit our existing changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Improving test coverage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, inside the `/home/dli/.d4nyll/.beja/final/code/9/src/handlers/users/create/index.js`
    file, change the `return undefined;` statement to propagate the error down the
    promise chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, add unit tests to `src/handlers/users/create/index.unit.test.js` to cover
    this missed scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Now, when we run our `test:unit:coverage` script and look at the report again,
    you will be glad to see that coverage is now 100%!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e3f73fe-4f42-4530-b43a-095629bbb786.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, commit this refactoring step into your repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Code coverage versus test quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As illustrated in the preceding section, code coverage tools can help you uncover
    mistakes in your code. However, they should be used as a diagnostic tool only;
    you shouldn't be chasing after 100% code coverage as a goal in itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because code coverage has no relation to the quality of your tests.
    You can define test cases that cover 100% of your code, but if the assertions
    are wrong, or if the tests have errors in it, then the perfect coverage means
    nothing. For instance, the following test block will always pass, even though
    one of the assertions suggests it would fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This highlights the point that *code coverage cannot detect bad tests*. Instead,
    you should focus on writing meaningful tests that will actually show bugs when
    they arise; if you do that, the test coverage will naturally remain high, and
    you can use the reports to improve whatever you've missed in your tests.
  prefs: []
  type: TYPE_NORMAL
- en: You don't have to test everything, all the time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After we updated our unit tests to cover the missed `catch` block, our Statements
    coverage is now 100%. However, if we examine our code, we''ll find two modules
    that still lack unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`validate`: User validation function at `src/validators/users/create.js`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`injectHandlerDependencies`: Utility function at `src/utils/inject-handler-dependencies.js`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They did not show up in the coverage report because the unit tests never imported
    those files. But do we need to write unit tests for every unit? To answer this
    question, you should ask yourself – "Do I have confidence that this block of code
    works?" If the answer is "yes", then writing additional tests may be unnecessary.
  prefs: []
  type: TYPE_NORMAL
- en: Code coverage for a unit should not be analyzed based on unit tests alone, since
    there may be integration and E2E tests that use that unit. If these other tests
    cover what the unit tests don't, and the tests are passing, then that should give
    you confidence that your unit is working as intended.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a more useful metric is to analyze the code coverage of *all tests*,
    not just unit tests.
  prefs: []
  type: TYPE_NORMAL
- en: Unifying test coverage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Therefore, let''s add coverage scripts for integration and E2E tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'However, when we run the `test:e2e:coverage` script, the coverage report shows
    results for compiled files in the `dist/` directory, rather than the source files
    from `src/`. This is because our E2E test script (`scripts/e2e.test.sh`) is running the
    `serve` npm script, which transpiles our code before running it. To fix this,
    let''s add a new `test:serve` script, which uses `babel-node` to directly run
    our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, update `scripts/e2e.test.sh` to use this modified script instead of `serve`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Now, when we run the `test:coverage` or `test:e2e:coverage` again, it will show
    coverage for files under `src/` instead of `dist/`.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, you may have also noticed that our step definitions are showing up
    in our coverage report. Istanbul is not smart enough to figure out that our step
    definition files are part of the tests, and not the code; therefore, we need to
    manually instruct Istanbul to ignore them. We can do with by adding a `.nycrc`
    file and specifying the `exclude` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Now, when we run the `test:coverage` script, the step definition files are excluded
    from the results. All that's left to do is commit our code!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Finishing up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have now modularized and tested the code for the Create User feature. Therefore,
    now is a good time to merge our current `create-user/refactor-modules` branch
    into the `create-user/main` branch. Since this also completes the Create User
    feature, we should merge the `create-user/main` feature branch back into the `dev`
    branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of the preceding three chapters, we have shown you how to write
    E2E tests, use them to drive the development of your feature, modularize your
    code wherever possible, and then increase confidence in your code by covering
    modules with unit and integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will be tasked with implementing the rest of the features
    by yourself. We will outline some principles of API design that you should follow,
    and you can always reference our sample code bundle, but the next chapter is where
    you truly get to practice this process independently.
  prefs: []
  type: TYPE_NORMAL
- en: '"Learning is an active process. We learn by doing. Only knowledge that is used
    sticks in your mind."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Dale Carnegie, author of the book How to Win Friends and Influence People'
  prefs: []
  type: TYPE_NORMAL
