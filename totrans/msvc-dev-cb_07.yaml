- en: Monitoring and Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Structured JSON logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting metrics with StatsD and Graphite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting metrics with Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making debugging easier with tracing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerting when something goes wrong
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices add complexity to an architecture. With more moving parts in a
    system, monitoring and observing the behavior of the system becomes more important
    and more challenging. In a microservice architecture, failure conditions impacting
    one service can cascade in unexpected ways, impacting the system as a whole. A
    faulty switch somewhere in a datacenter may be causing unusually high latency
    for a service, perhaps resulting in intermittent timeouts in requests originating
    from the API Gateway, which may result in unexpected user impact, which results
    in an alert being fired. This kind of scenario is not uncommon in a microservice
    architecture and requires forethought so that engineers can easily determine the
    nature of customer-impacting incidents. Distributed systems are bound to experience
    certain failures and special consideration must be taken to build observability
    into systems.
  prefs: []
  type: TYPE_NORMAL
- en: Another shift that microservices have necessitated is the move to DevOps. Many
    traditional monitoring solutions were developed at a time when operations were
    the sole responsibility of a special and distinct group of system administrators
    or operations engineers. System administrators and operations engineers are often
    interested in system-level or host-level metrics, such as CPU, memory disk, and
    network usage. These metrics are important but only make up a small part of observability.
    **Observability** must also be considered by engineers writing microservices. It's
    equally important to use metrics to be able to observe events unique to a system,
    such as certain types of exceptions being thrown or the number of events emitted
    to a queue.
  prefs: []
  type: TYPE_NORMAL
- en: Planning for observability also gives us the information we need to effectively
    test systems in production. Ephemeral environments for staging and integration
    testing can be useful, but there are entire classes of failure states that they
    are unable to test for. As discussed in [Chapter 5](b569ef24-285f-40bf-97b0-0ac9c1a79494.xhtml),
    *Reliability Patterns*, Gamedays and other forms of failure injection are critical
    for improving the resilience of systems. Observable systems lend themselves to
    this kind of testing, allowing engineers to gain confidence in our understanding
    of the system.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll introduce several tenants of monitoring and observability.
    We'll demonstrate how to modify our services to emit structured logs. We'll also
    take a look at metrics, using a number of different systems for collecting, aggregating,
    and visualizing metrics. Finally we'll look at tracing, a way to look at requests
    as they travel through various components of a system and alert us when user-impacting
    error conditions are detected.
  prefs: []
  type: TYPE_NORMAL
- en: Structured JSON logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outputting useful logs is a key part of building an observable service. What
    constitutes a useful log is subjective, but a good set of guidelines is that logs
    should contain timestamped information about key events in a system. A good logging
    system supports the notion of configurable log levels, so the amount of information
    sent to logs can be dialed up or down for a specific amount of time depending
    on the needs of engineers working with the system. For example, when testing a
    service against failure scenarios in production, it may be useful to turn up the
    log level and get more detail about events in the system.
  prefs: []
  type: TYPE_NORMAL
- en: The two most popular logging libraries for Java applications are **Log4j** ([https://logging.apache.org/log4j/2.x/](https://logging.apache.org/log4j/2.x/))
    and **Logback** ([https://logback.qos.ch/](https://logback.qos.ch/)). By default,
    both of these libraries will emit log entries in an unstructured format, usually
    space-separated fields including information such as a timestamp, log level, and
    message. This is useful, but especially so in a microservices architecture, where
    multiple services are emitting event logs possibly to a centralized log store;
    it's extremely useful to emit structured logs with some consistency.
  prefs: []
  type: TYPE_NORMAL
- en: JSON has become a common standard for passing messages between systems. Nearly
    every popular language has libraries for parsing and generating JSON. It's lightweight,
    yet structured, making it a good choice for data, such as event logs. Emitting
    event logs in JSON makes it easier to feed your service's logs into a centralized
    store and have log data analyzed and queried.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll modify our message-service to emit logs using the popular
    `logback` library for Java applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the message-service project from [Chapter 6](5c67f295-78fb-4ae9-a596-39f384f6e9f2.xhtml),
    *Security*. The first change we''ll make is to add the `logback` library to the
    `build.gradle` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `logback.xml` configuration file. In the configuration file, we''ll
    create a single logger, called `jsonLogger`, that references a single appender,
    called `consoleAppender`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a single sample log message to `Application.java` to test our new logging
    configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the application and see that log messages are now emitted in JSON:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Collecting metrics with StatsD and Graphite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metrics are numeric measurements over time. The most common types of metrics
    collected in our systems are counters, timers, and gauges. A counter is exactly
    what it sounds like, a value that is incremented a number of times over some time
    period. A timer can be used to measure recurring events in a system, such as the
    amount of time it takes to serve a request or perform a database query. Gauges
    are just arbitrary numeric values that can be recorded.
  prefs: []
  type: TYPE_NORMAL
- en: '**StatsD** is an open source network daemon invented in 2011 at Etsy. Metrics
    data is pushed to a `statsd` server, often on the same server, which aggregates
    data before sending it on to a durable backend. One of the most common backends
    used with `statsd` is **Graphite**, an open source time-series storage engine
    and graphing tool. Together, Graphite and StatsD make up a very popular metrics
    stack. They''re easy to get started with and enjoy large communities and a large
    selection of tools and libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: Spring Boot has a sub-project called **Actuator** that adds a number of production
    readiness features to a service. Actuator gives us our services certain metrics
    for free, together with a project called micrometer, Actuator enables a vendor-neutral
    API to various metric's backends. We'll use Actuator and micrometer in this recipe
    and the next one.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll add Actuator to the message-service we've worked with
    in previous recipes. We'll create a few custom metrics and demonstrate using `statsd`
    and `graphite` to graph metrics from our application. We'll run `statsd` and `graphite`
    locally in docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the message-service project from previous recipes. We''re going to upgrade
    the version of Spring Boot and add `actuator` and `micrometer` to our list of
    dependencies. Modify the `build.gradle` file to look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Open `application.yml` in the `src/main/resources` directory and add the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our application now supports emitting metrics to a locally-running instance
    of `statsd`. Open `MessageController.java` and add the `Timed` annotation to the
    class as well as the `get` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to demonstrate that metrics are actually being emitted, we''ll run
    `statsd` and graphite locally in a docker container. Having installed `docker`,
    run the following command, which will pull down an image from `dockerhub` and
    run a container locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, visit `http://localhost` to see your metrics!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collecting metrics with Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Prometheus** is an open source monitoring and alerting toolkit originally
    developed in 2012 at **SoundCloud**. It was inspired by Borgmon at Google. In
    contrast to the push model employed by systems such as `statsd`, Prometheus uses
    a pull model for collecting metrics. Instead of each service being responsible
    for pushing metrics to a `statsd` server, Prometheus is responsible for scraping
    an endpoint exposed by services that have metrics. This inversion of responsibilities
    provides some benefits when operating metrics at scale. Targets in Prometheus
    can be configured manually or via service discovery.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the hierarchical format that systems such as Graphite use to
    store metrics data, Prometheus employs a multidimensional data model. Time-series
    data in Prometheus is identified by a metric name (such as `http_request_duration_seconds`)
    and one or more labels (such as `service=message-service` and `method=POST`).
    This format can make it easier to standardize metrics across a number of different
    applications, which is particularly valuable in a microservices architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll continue to use message-service and the Actuator and micrometer
    libraries. We'll configure micrometer to use the Prometheus metrics registry and
    we'll expose an endpoint that Prometheus can scrape in order to collect metrics
    from our service. We'll then configure Prometheus to scrape the message-service
    (running locally) and run Prometheus locally to verify that we can query our metrics.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Open the message-service and edit `build.gradle` to include actuator and the
    micrometer-prometheus dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following to `application.yml`. This will enable an endpoint that exposes
    metrics collected in the Prometheus metrics registry. Notice that we''re opening
    another port for the management endpoints added by `actuator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now test that our service is exposing metrics on the `/manage/prometheus`
    endpoint. Run the service and make the following `curl` request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure and run Prometheus in a docker container. Create a new file in the
    `/tmp` directory, called `prometheus.yml`, with information about our target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Download and extract the version of Prometheus for your platform. Instructions
    are on the Prometheus website ([https://prometheus.io/docs/introduction/first_steps/](https://prometheus.io/docs/introduction/first_steps/)).
    Run Prometheus with the configuration file we created in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Open `http://localhost:9090` in your browser to issue Prometheus queries and
    see your metrics! Until you start making requests to your service, the only metrics
    you'll see will be the JVM and system metrics, but this should give you an idea
    of the kind of querying you can do with Prometheus and demonstrate how the scraper
    works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Making debugging easier with tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a microservices architecture, a single request can go through several different
    services and result in writes to several different data stores and event queues.
    When debugging a production incident, it isn't always clear whether a problem
    exists in one system or another. This lack of specificity means metrics and logs
    only form a small part of the picture. Sometimes we need to zoom out and look
    at the complete life cycle of a request from the user agent to a terminal service
    and back again.
  prefs: []
  type: TYPE_NORMAL
- en: In 2010, engineers at Google published a paper describing **Dapper** ([https://research.google.com/archive/papers/dapper-2010-1.pdf](https://research.google.com/archive/papers/dapper-2010-1.pdf)),
    a large-scale distributed systems tracing infrastructure. The paper described
    how Google had been using an internally developed tracing system to aid in observing
    system behavior and debugging performance issues. This work inspired others, including
    engineers at Twitter who, in 2012, introduced an open source distributed tracing
    system called **Zipkin** ([https://blog.twitter.com/engineering/en_us/a/2012/distributed-systems-tracing-with-zipkin.html](https://blog.twitter.com/engineering/en_us/a/2012/distributed-systems-tracing-with-zipkin.html)).
    Zipkin started out as an implementation of the Dapper paper but evolved into a
    full set of tools for analyzing performance and inspecting requests to Twitter
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: All of the work going on in the tracing space made apparent a need for some
    kind of standardized API. The **OpenTracing** ([http://opentracing.io/](http://opentracing.io/))
    framework is an attempt to do just that. OpenTracing defines a specification detailing
    a pan-language standard for traces. Many engineers from different companies have
    contributed to this effort, including the engineers at Uber who originally created
    Jaeger ([https://eng.uber.com/distributed-tracing/](https://eng.uber.com/distributed-tracing/)),
    an open source, end-to-end distributed tracing system that conforms to the OpenTracing
    specification.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll modify our message-service to add support for tracing.
    We'll then run Jaeger in a docker container so that we can see a few traces in
    practice.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Open the message-service project and replace the contents of `build.gradle`
    with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Open `application.yml` in the `src/main/resources` directory and add a section
    for `opentracing` configuration. Here we''re configuring our `opentracing` implementation
    to connect to an instance of Jaeger running locally on port `6831`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to collect traces, we''ll run an instance of Jaeger locally. Docker
    makes this easy with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Run message-service and make a few example requests (even if they result in
    a 404). Open `http://localhost:16686` in your browser and you'll see Jaeger's
    web UI. Hit search and explore the trace data collected so far!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alerting us when something goes wrong
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're seriously looking at microservices, you're probably running a 24/7
    service. Customers demand that your service is available to use at any time. Contrast
    this increase in the need for availability with the reality that distributed systems
    are constantly experiencing some kind of failure. No system is ever completely
    healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you have a monolith or microservices architecture, it is pointless to
    try to avoid production incidents altogether. Instead, you should try to optimize
    how you are able to respond to failures, limiting their impact on customers by
    reducing the time it takes to resolve them.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the time it takes to resolve incidents (often measured as mean time
    to resolve or MTTR) involves first reducing the **Mean Time To Detect** (**MTTD**).
    Being able to accurately alert the right on-call engineer when a service is in
    a customer-impacting failure state is paramount to being able to maintain uptime.
    Good alerts should be actionable and urgent; if your system notifies on-call engineers
    when failures are either unactionable or non-urgent (not customer-impacting),
    you risk burning out on-call engineers and creating what is commonly referred
    to as alert fatigue. Alert fatigue is very real and can have a more catastrophic
    impact on uptime than any amount of software bugs or failing hardware. It is essential
    to continuously improve your system's alerting to get thresholds and other factors
    just right, to prevent false positives while maintaining alerting for truly customer-impacting
    incidents.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting infrastructure is not something you want to build yourself. **PagerDuty**
    is an SaaS tool that allows you to create escalation policies and schedules for
    teams of engineers who are on-call for specific services. Using PagerDuty, you
    can set up a rotating schedule so that an engineer on a team of five, for example,
    can expect to be on-call one week in every five. Escalation policies allow you
    to configure a set of steps in case the on-call engineer is unavailable (perhaps
    they're driving their car on the freeway). Escalation policies are often configured
    to page a secondary on-call schedule, a manager, or even the entire team in the
    event that an incident goes unacknowledged for a certain amount of time. Using
    a system such as PagerDuty allows engineers on a team to enjoy much-needed off-call
    time while knowing that customer-impacting incidents will be responded to promptly.
  prefs: []
  type: TYPE_NORMAL
- en: Alerts can be configured manually using any number of supporting integrations,
    but this is time-consuming and error-prone. Instead, it's desirable to have a
    system that allows you to automate the creation and maintenance of alerts for
    your services. The Prometheus monitoring and alerting toolkit covered in this
    chapter includes a tool called Alertmanager which allows you to do just that.
    In this recipe, we'll modify our message-service to add alerts using Alertmanager.
    Specifically, we'll configure a single alert that fires when the average response
    time exceeds 500 ms for at least 5 minutes. We'll work from the version of message-service
    that already includes Prometheus metrics. We won't add any PagerDuty integration
    in this recipe, since that would require a PagerDuty account in order to follow
    along. PagerDuty has an excellent integration guide on its website. We'll configure
    `alertmanager` to send a simple WebHook-based alert.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s have a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a previous recipe, we configured Prometheus with a file called `prometheus.yml`.
    We''ll need to add the `alertmanager` configuration to this file, so open it again
    and add the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new file called `/tmp/rules.yml`. This file defines the rules we want
    Prometheus to be able to creates alerts for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new file called `/tmp/alertmanager.yml`. This is the file that will
    describe our alerting configuration. It is broken into a few different sections, global
    sets of certain configuration options that impact how `alertmanager` works. The
    section called receivers is where we configure our alert notification systems. In
    this case, it''s a WebHook to a service running locally. This is just for demo
    purposes; we''ll write a small ruby script that listens for HTTP requests and
    prints the payload to the standard output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the source code for the small ruby service that will print out our
    alerts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the ruby script, restart `prometheus`, and start `alertmanager`. With these
    three systems running, we''ll be ready to test our alert:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to get our alert to fire, open message-service and add the following
    line to `MessageController.java`. It''s a single line that will force the controller
    to sleep for 600 milliseconds before returning a response. Note that this is above
    our threshold described in our rules configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With that in place, run your updated message service and make a number of requests
    to it. After one minute, Prometheus should notify Alertmanager, which should then
    notify your local debug ruby service. Your alert is working!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
