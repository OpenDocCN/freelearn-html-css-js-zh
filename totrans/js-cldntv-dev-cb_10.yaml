- en: Deploying to Multiple Regions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing latency-based routing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a regional health check
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triggering regional failover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing regional replication with DynamoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing round-robin replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not a matter of if but when will a given cloud provider experience a news-worthy
    regional disruption. It is inevitable. In my experience, this happens approximately
    every two years or so. When such an event does occur, many systems have no recourse
    and become unavailable during the disruption because they are only designed to
    work across multiple availability zones within a single region. Meanwhile, other
    systems barely experience a blip in availability because they have been designed
    to run across multiple regions. The bottom line is that truly cloud-native systems
    capitalize on regional bulkheads and run in multiple regions. Fortunately, we
    leverage fully managed, value-added cloud services that already run across availability
    zones. This empowers teams to refocus that effort on creating an active-active,
    multi-regional system. The recipes in this chapter cover multi-regional topics
    from three interrelated perspectives—synchronous requests, database replication,
    and asynchronous event streams.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing latency-based routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many systems make the conscious decision not to run across multiple regions
    because it is simply not worth the additional effort and cost. This is completely
    understandable when running in an active-passive mode because the additional effort
    does not produce an easily visible benefit until there is a regional disruption.
    It is also understandable when running in active-active mode doubles the monthly
    runtime cost. Conversely, serverless cloud-native systems are easily deployed
    to multiple regions and the increase in cost is nominal since the cost of a given
    transaction volume is spread across the regions. This recipe demonstrates how
    to run an AWS API Gateway and Lambda-based service in multiple regions and leverage
    Route53 to route traffic across these active-active regions to minimize latency.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need a registered domain name and a **Route53 Hosted Zone** that you
    can use in this recipe to create a subdomain for the service that will be deployed,
    such as we discussed in the *Associating a custom domain name with a CDN* recipe.
    You will also need a wildcard certificate for your domain name in the `us-east-1`
    and `us-west-2` regions, such as we discussed in the *Creating an SSL certificate
    for encryption in transit* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-latency-based-routing` directory with `cd cncb-latency-based-routing`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the following fields in the `serverless.yml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`custom.dns.hostedZoneId`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom.dns.domainName`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom.dns.us-east-1.acmCertificateArn`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom.dns.us-west-2.acmCertificateArn`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Review the file named `handler.js`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack in the `us-west-2` region with `npm run dp:lcl:**w** -- -s
    $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS Console for the `us-west-2` region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test the `regional` endpoint of the service and note the region returned in
    the payload:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Deploy the stack in the `us-east-1` region with `npm run dp:lcl:**e** -- -s
    $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying a CloudFront distribution can take 20 minutes or longer.
  prefs: []
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS console for the `us-east-1` region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test the `global` endpoint of the service and note the region returned in the
    payload, which should be different from above if you are not closest to the `us-west-2`
    region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the stacks once you have finished:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we are taking a single service and deploying it to two regions—`us-east-1`
    and `us-west-2`. From the perspective of the API Gateway and the Lambda function,
    we are simply just creating two different CloudFormation stacks, one in each region.
    We have two scripts—`dp:lcl:**e**` and `dp:lcl:**w**`, and the only difference
    between the two is that they specify different regions. As a result, the effort
    to deploy to two regions is marginal, and there is no additional cost because
    we only pay per transaction. One thing of note in the `serverless.yml` file is
    that we are defining the `endpointType` for the API Gateway as `REGIONAL`, which
    will allow us to leverage the Route53 regional routing capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb2eaaad-8517-4439-8413-3d2ff7999a62.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, we need to configure Route53 to perform *latency-*based
    routing between the two regions. This means that Route53 will route requests to
    the region that is closest to the requester. The `serverless-multi-regional-plugin`
    encapsulates the majority of these configuration details, so we only need to specify
    the variables under `custom.dns`. First, we provide the `hostedZoneId` for the
    zone that hosts the top-level domain name, such as `example.com`. Next, we define
    the `domainName` that will be used as the `alias` to access the service globally
    via **CloudFront**. For this, we use the service name (that is, `${self:service}`)
    as a subdomain of the top-level domain to uniquely identify a service.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to define a `regionalDomainName` to provide a common name across
    all the regions so that CloudFront can rely on Route53 to pick the best region
    to access. For this, we are using the stage (that is, `${opt:stage}-${self:custom.dns.domainName}`)
    as a prefix, and note that we are concatenating this with a dash so that it works
    with a simple wildcard certificate, such as `*.example.com`. The regional `acmCertificateArn`
    variables point to copies of your wildcard certificate in each region, as mentioned
    in the *Getting ready* section. API Gateway requires that the certificates live
    in the same region as the service. CloudFront requires that the certificate lives
    in the `us-east-1` region. CloudFront is a global service, so we only need to
    deploy the CloudFront distribution from the `us-east-1` region.
  prefs: []
  type: TYPE_NORMAL
- en: All requests to the global endpoint (`service.example.com`) will be routed to
    the closest CloudFront edge location. CloudFront then forwards the requests to
    the regional endpoint (`stage-service.example.com`), and Route53 will route the
    requests to the closest region. Once a request is in a region, all requests to
    services, such as Lambda, DynamoDB and Kinesis, will stay within the region to
    minimize latency. All changes to state will be replicated to the other regions,
    as we discuss in the *Implementing regional replication with DynamoDB* and *Implementing
    round-robin replication* recipes.
  prefs: []
  type: TYPE_NORMAL
- en: I highly recommend looking at the generated *CloudFormation* template to see
    the details of all the resources that are created.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a regional health check
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Health checks in a cloud-native system have a different focus from traditional
    health checks. Traditional health checks operate at the instance level to identify
    when a specific instance in a cluster needs to be replaced. Cloud-native systems,
    however, use fully managed, value-added cloud services, so there are no instances
    to manage. These serverless capabilities provide high availability across the
    availability zones within a specific region. As a result, cloud-native systems
    can focus on providing high availability across regions. This recipe demonstrates
    how to assert the health of the value-added cloud services that are used within
    a given region.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this recipe in full, you will need a Pingdom ([https://www.pingdom.com](https://www.pingdom.com))
    account.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-regional-health-check` directory with `cd cncb-regional-health-check`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack in the `us-east-1` and `us-west-2` regions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS console for both regions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each region, invoke the endpoint shown in the stack output as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Create an **Uptime** check in your **Pingdom** account for each regional endpoint
    with an interval of `1 minute`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove the stacks once you have finished:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A traditional health check typically asserts that an instance is able to access
    all the resources that it needs to operate properly. A regional health check does
    the same thing but from the perspective of the region as a whole. It asserts that
    all the value-added cloud services (that is, resources) used by the system are
    operating properly within the given region. If any one resource is unavailable,
    we will failover the entire region, as discussed in the *Triggering regional failover*
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The health check service is implemented as a `REGIONAL` API Gateway based service
    and deployed to each region. We then need to periodically invoke the health check
    in each region to check that the region is healthy. We could have Route53 ping
    these regional endpoints, but it will ping them so frequently that the health
    check service could easily become the most expensive service in your entire system.
    Alternatively, we can use an external service, such as **Pingdom**, to invoke
    the health check in each region once per minute. Once a minute is sufficient for
    many systems, but extremely high traffic systems may benefit from the higher frequency
    provided by Route53.
  prefs: []
  type: TYPE_NORMAL
- en: The health check needs to assert that the required resources are available.
    The health check itself implicitly asserts that the API Gateway and Lambda services
    are available because it is built on those services. For all other resources,
    it will need to perform some sort of ping operation. In this recipe, we assume
    that DynamoDB is the required resource. The health check service defines its own
    DynamoDB table and performs a `readCheck` and a `writeCheck` on each invocation
    to assert that the service is still available. If either request fails, then the
    health check service will fail and return a `503` status code. For testing, the
    service provides an `UNHEALTHY` environment variable that can be used to simulate
    a failure, which we will use in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Triggering regional failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed, in the *Creating a regional health check* recipe, our regional
    health checks assert that the fully managed, value-added cloud services that are
    used by the system are all up and running properly. When any of these services
    are down or experiencing a sufficiently high error rate, it is best to fail the
    entire region over to the next-best active region. This recipe demonstrates how
    to connect a regional health check to Route53, using **CloudWatch Alarms**, so
    that Route53 can direct traffic to healthy regions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need a registered domain name and a **Route53 Hosted Zone** that you
    can use in this recipe to create a subdomain for the service that will be deployed,
    such as we discussed in the *Associating a custom domain name with a CDN* recipe.
    You will also need a wildcard certificate for your domain name in the `us-east-1`
    and `us-west-2` regions, such as we discussed in the *Creating an SSL certificate
    for encryption in transit* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the service and check projects from the following templates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-regional-failover-check` directory with `cd cncb-regional-failover-check`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Review the file named `handler.js`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack in the `us-east-1` and `us-west-2` regions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS console for both regions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the `cncb-regional-failover-service` directory with `cd cncb-regional-failover-service`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the following fields in the `serverless.yml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`custom.dns.hostedZoneId`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom.dns.domainName`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom.dns.us-east-1.acmCertificateArn`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom.dns.us-east-1.healthCheckId` from the output of the `east` health check
    stack'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom.dns.us-west-2.acmCertificateArn`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom.dns.us-west-2.healthCheckId` from the output of the `west` health check
    stack'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack in the `us-west-2` and `us-east-1` regions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Deploying a CloudFront distribution can take 20 minutes or longer.
  prefs: []
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS console for both regions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test the `global` endpoint of the service and note the region returned in the
    payload:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Go to the `cncb-regional-failover-check-<stage>-check` Lambda function in the
    AWS console for the region that your request was routed to and change the `UNHEALTHY`
    environment variable to `true` and s*ave* the function. For example, the previous
    output shows `us-east-1`, so update the function in `us-east-1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the health check endpoint for that region multiple times over the course
    of several minutes to trigger a failover:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the `global` endpoint of the service and note that the region has changed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Review the status of the Route53 health checks in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove the stacks once you are finished:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our regional health check service is designed to return a `5xx` status code
    when one or more of the required services returns an error. We add a CloudWatch
    alarm, named `Api5xxAlarm`, to the health check service that monitors the API
    Gateway `5xxError` metric in the given region and raises an alarm when there is
    at least one `5xx` in a minute. You will want to adjust the sensitivity of the
    alarm to your specific requirements. Next, we add a Route53 health check, named
    `ApiHealthCheck`, to the service that depends on the `Api5xxAlarm` and outputs
    the `ApiHealthCheckId` for use by other services. Finally, we associate the `healthCheckId`
    with the Route53 RecordSet for each service in each region, such as the `cncb-regional-failover-service`.
    When the alarm status is `Unhealthy`, Route53 will stop routing traffic to the
    region until the status is `Healthy` again.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we used the `UNHEALTHY` environment variable to simulate a regional
    failure and manually invoked the service to trigger the alarm. As we discussed
    in the *Creating a regional health check* recipe, the health check will typically
    be invoked on a regular basis by another service, such as Pingdom, to ensure that
    there is a constant flow of traffic asserting the health of the region. To increase
    coverage, we could also expand the alarm to check the `5xx` metric of all services
    in a region by removing the `ApiName` dimension from the alarm but still rely
    on pinging the health check service to assert the status when there is no other
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing regional replication with DynamoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Timely replication of data across regions is important to facilitate a seamless
    user experience when a regional failover occurs. During normal execution, regional
    replication will occur in near real time. During a regional failure, it should
    be expected that data would replicate more slowly. We can think of this as protracted
    eventual consistency. Fortunately, our cloud-native systems are designed to be
    eventually consistent. This means they are tolerant of stale data, regardless
    of how long it takes to become consistent. This recipe shows how to create global
    tables to replicate DynamoDB tables across regions and discusses why we do not
    replicate event streams.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need an AWS Kinesis Stream in the `us-east-1`
    and `us-west-2` regions, such as the one created in the *Creating an event stream*
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-dynamodb-global-table` directory with `cd cncb-dynamodb-global-table`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack in the `us-east-1` and `us-west-2` regions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS console in both regions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the `command` function to save data to the `us-east-1` region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the `query` function to retrieve the data from the `us-west-2` region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs for the `trigger` and `listener` functions in both
    regions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove both stacks once you are finished:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **DynamoDB Global Table** is responsible for replicating data across all the
    regional tables that have been associated with the global table and keep the data
    synchronized, all in near real time. The `serverless-dynamodb-global-table-plugin`
    will create the global tables and is designed to work with the `serverless-dynamodb-autoscaling-plugin`.
    For each table that has the `global` flag set to true, the plugin will create
    the global table when the service is deployed to the first region. For each successive
    regional deployment the plugin will add the regional table to the global table.
    Each regional table must have the same name, have streams enabled, and have the
    same autoscaling policies, which is handled by the plugins. One thing that is
    not handled by the plugins is that the tables must all be empty when the global
    table is initially deployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with the happy-path scenario, where there is no regional disruption
    and everything is working smoothly, and walk through the following diagram. When
    data is written to the table in a region, such as `us-east-1`, then the data is
    replicated to the `us-west-2` region. The `trigger` in the `us-east-1` region
    is also executed. The trigger has a `forOrigin` filter that will ignore all events
    where the `aws:rep:updateregion` field is not equal to the current `AWS_REGION`.
    Otherwise, the trigger will publish an event to the Kinesis Stream in the current
    region and all subscribers to the event will execute in the current region and
    replicate their own data to the other regions. The `listener` for the current
    service will ignore any events that it produced itself. In the `us-west-2` region
    the `trigger` will also be invoked after the replication, but the `forOrigin`
    filter will short-circuit the logic so that a duplicate event is not published
    to Kinesis and reprocessed by all the subscribers in that region. The inefficiency
    of duplicate event processing and the potential for infinite replication loops
    are two reasons why it is best not to replicate event streams and instead reply
    on replication at the leaf data stores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0942f19e-c3d5-442b-b233-5e9a57539aa1.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that this recipe builds on the code from the *Implementing bi-directional
    synchronization* recipe, so you can review that recipe for additional details
    about the code.
  prefs: []
  type: TYPE_NORMAL
- en: During a regional failover, in the best-case scenario, a user's data will have
    already been replicated and the failover process will be completely seamless.
    The user's next commands will execute in the new region, the chain of events will
    process in the new region, and the results will eventually replicate back to the
    failed region. When there is some replication latency, **session consistency**
    helps make the failover process appear seamless, as we discussed in the *Leveraging
    session consistency* recipe. However, during a regional failover, it is likely
    that some subscribers in the failing region will fall behind on processing the
    remaining events in the regional stream. Fortunately, a regional disruption typically
    means that there is lower throughput in the failing region, as opposed to no throughput.
    This means that there will be a higher latency for replicating the results of
    event processing to the other regions but they will eventually become consistent.
    A user experience that is designed for eventual consistency, such as an email
    app on a mobile device, will handle this protracted eventual consistentcy in its
    stride.
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of trying to keep track of which events have processed and which
    events are stuck in a failing region is another reason why it is best not to replicate
    event streams. In cases where this protracted eventual consistency cannot be tolerated,
    then the latest events in the new region can rely on session consistency for more
    up-to-date information and use the techniques discussed in the *Implementing idempotency
    with an inverse oplock* and *Implementing idempotency with event sourcing* recipes
    to handle the older events that are received out of order from the slowly recovering
    region.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing round-robin replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not all of the databases in our polyglot persistence architecture will support
    turnkey regional replication as we have with AWS DynamoDB, yet we still need to
    replicate their data to multiple regions to improve latency and support regional
    failover. This recipe demonstrates how to use AWS S3 as a surrogate to add regional
    replication to any database.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-round-robin-replication` directory with `cd cncb-round-robin-replication`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `replicator.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack in the `us-east-1` and `us-west-2` regions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Update the `replicationBucketName` variables in the `serverless.yml` so that
    `us-east-1` replicates to `us-west-2` and visa versa, and then redeploy the stacks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stacks and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the following curl command to search the data in the `us-west-2` region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs for the `trigger` and `listener` functions in both
    regions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Empty the bucket in each region before removing the stacks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove both stacks once you have finished:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we are creating a **materialized view** in Elasticsearch, and
    we want to allow users to search against the data in the region that they are
    closest to. However, Elasticsearch does not support regional replication. As we
    discussed in the *Implementing regional replication with DynamoDB* recipe, we
    do not want to replicate the event stream because that solution is complex and
    too difficult to reason about. Instead, as shown in the following diagram, we
    will place an S3 bucket in front of Elasticsearch in each region and leverage
    S3 triggers to update Elasticsearch and to implement a round-robin replication
    scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e50354ed-46ed-4af0-92f6-e13e47b85f06.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that this recipe builds on the code from the *Implementing a search BFF*
    recipe, so you can review that recipe for additional details about the code.
  prefs: []
  type: TYPE_NORMAL
- en: The service listens to the Kinesis Stream in the current region and writes the
    data to an S3 bucket in the current region, which generates an S3 trigger that
    is routed to an SNS topic. A function reacts to the topic and creates the materialized
    view in Elasticsearch in the current region. Meanwhile, a `replicator` function
    also reacts to the same topic. The replicator copies the contents of the object
    from the S3 bucket to the matching bucket in the next region, as specified by
    the `REPLICATION_BUCKET_NAME` environment variable. This in turn generates a trigger
    in that region. Once again, a function responds to the topic and creates the materialized
    view in Elasticsearch in that region as well. The `replicator` in that region
    also responds and looks to copy the object to the next region. This process of
    `trigger` and `replicate` will round robin for as many regions as necessary, until
    the `forOrigin` filter sees that the origin bucket (that is, `uow.object.Metadata.origin`)
    is equal to the target of the current replicator (that is, `process.env.REPLICATION_BUCKET_NAME`).
    In this recipe, we have two regions—`us-east-1` and `us-west-2`. The data originates
    in the east region, so the east replicator copies the data to the west bucket
    (`1cqxst40pvog4`). The west replicator does not copy the data to the east bucket
    (`1a3rh4v9tfedw`) because the origin is the east bucket.
  prefs: []
  type: TYPE_NORMAL
- en: This round robin replication technique is a simple and cost-effective approach
    that builds on the event architecture that is already in place. Note that we cannot
    leverage the built-in S3 replication feature for this purpose because it only
    replicates to a single region and does not create a chain reaction. However, we
    could add S3 replication to these buckets for backup and disaster recovery, as
    we discussed in the *Replicating the data lake for disaster recovery* recipe.
  prefs: []
  type: TYPE_NORMAL
