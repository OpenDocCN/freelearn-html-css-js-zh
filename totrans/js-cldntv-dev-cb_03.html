<html><head></head><body>
        

                            
                    <h1 class="header-title">Implementing Autonomous Services</h1>
                
            
            
                
<p>In this chapter, the following recipes will be covered:</p>
<ul>
<li class="mce-root">Implementing a GraphQL CRUD BFF</li>
<li class="mce-root">Implementing a search BFF</li>
<li class="mce-root">Implementing an analytics BFF</li>
<li class="mce-root">Implementing an inbound External Service Gateway</li>
<li class="mce-root">Implementing an outbound External Service Gateway</li>
<li class="mce-root">Orchestrating collaboration between services</li>
<li class="mce-root">Implementing a Saga</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction</h1>
                
            
            
                
<p>In <a href="a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml">Chapter 1</a>, <em>Getting Started with Cloud-Native</em>, we began our journey to understand why cloud-native is lean and autonomous. We focused on recipes that demonstrate how leveraging fully managed cloud services empower self-sufficient, full-stack teams to rapidly and continuously deliver innovation with confidence. In <a href="129afdee-aa82-4ba6-9d80-5ec70c4a766e.xhtml">Chapter 2</a>, <em>Applying the Event Sourcing and CQRS Patterns</em>, we worked through recipes that showcase how these patterns establish the bulkheads that enable the creation of autonomous services.</p>
<p>In this chapter, we bring all these foundational pieces together with recipes for implementing autonomous service patterns. In my book, <a href="https://www.packtpub.com/application-development/cloud-native-development-patterns-and-best-practices">Cloud Native Development Patterns and Best Practices</a>, I discuss various approaches for decomposing a cloud-native system into bound, isolated, and autonomous services.</p>
<p>Every service should certainly have a bounded context and a single responsibility, but we can decompose services along additional dimensions as well. The life cycle of data is an important consideration for defining services, because the users, requirements, and persistence mechanisms will likely change as data ages. We also decompose services based on boundary and control patterns. Boundary services, such as a <strong>Backend for Frontend</strong> (<strong>BFF</strong>) or an <strong>External Service Gateway</strong> (<strong>ESG</strong>), interact with things that are external to the system, such as humans and other systems. Control services orchestrate the interactions between these decoupled boundary services. The recipes in this chapter demonstrate common permutations of these decomposition strategies.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing a GraphQL CRUD BFF</h1>
                
            
            
                
<p>The BFF pattern accelerates innovation because the team that implements the frontend also owns and implements the backend service that supports the frontend. This enables teams to be self-sufficient and unencumbered by competing demands for a shared backend service. In this recipe, we will create a CRUD BFF service that supports data at the beginning of its life cycle. The single responsibility of this service is authoring data for a specific bounded context. It leverages <em>database-first</em> Event Sourcing to publish domain events to downstream services. The service exposes a GraphQL-based API.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>Before starting this recipe, you will need an AWS Kinesis Stream, such as the one created in the <em>Creating an event stream</em> recipe.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch3/bff-graphql-crud --path cncb-bff-graphql-crud</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-bff-graphql-crud</kbd> directory with <kbd>cd cncb-bff-graphql-crud</kbd>.</li>
</ol>
<ol start="3">
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-bff-graphql-crud<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/><br/>functions:<br/>  <strong>graphql</strong>:<br/>    handler: handler.<strong>graphql</strong><br/>    events:<br/>      - http:<br/>          path: <strong>graphql</strong><br/>          method: post<br/>          cors: true<br/>    environment:<br/>      TABLE_NAME:<br/>        Ref: Table<br/>  graphiql:<br/>    handler: handler.graphiql<br/>    ...<br/>  <strong>trigger</strong>:<br/>    handler: handler.<strong>trigger</strong><br/>    events:<br/>      - stream:<br/>          type: dynamodb<br/>          ...<br/><br/>resources:<br/>  Resources:<br/>    <strong>Table</strong>:<br/>      Type: AWS::DynamoDB::Table<br/>      ...</pre>
<ol start="4">
<li>Review the file named <kbd>./schema/thing/typedefs.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports = `<br/>  type <strong>Thing</strong> {<br/>    id: String!<br/>    name: String<br/>    description: String<br/>  }<br/><br/>  type <strong>ThingConnection</strong> {<br/>    items: [Thing!]!<br/>    cursor: String<br/>  }<br/><br/>  extend type <strong>Query</strong> {<br/>    <strong>thing</strong>(id: String!): Thing<br/>    <strong>things</strong>(name: String, limit: Int, cursor: String): ThingConnection<br/>  }<br/><br/>  input <strong>ThingInput</strong> {<br/>    id: String<br/>    name: String!<br/>    description: String<br/>  }<br/><br/>  extend type <strong>Mutation</strong> {<br/>    <strong>saveThing</strong>(<br/>      input: ThingInput<br/>    ): Thing<br/>    <strong>deleteThing</strong>(<br/>      id: ID!<br/>    ): Thing<br/>  }<br/>`;</pre>
<ol start="5">
<li>Review the file named <kbd>./schema/thing/resolvers.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports = {<br/>  <strong>Query</strong>: {<br/>    <strong>thing</strong>(_, { id }, ctx) {<br/>      return ctx.<strong>models</strong>.Thing.<strong>getById</strong>(id);<br/>    },<br/>    <strong>things</strong>(_, { name, limit, cursor }, ctx) {<br/>      return ctx.models.Thing.<strong>queryByName</strong>(name, limit, cursor);<br/>    },<br/>  },<br/>  <strong>Mutation</strong>: {<br/>    <strong>saveThing</strong>: (_, { input }, ctx) =&gt; {<br/>      return ctx.models.Thing.<strong>save</strong>(input.id, input);<br/>    },<br/>    <strong>deleteThing</strong>: (_, args, ctx) =&gt; {<br/>      return ctx.models.Thing.<strong>delete</strong>(args.id);<br/>    },<br/>  },<br/>};</pre>
<ol start="6">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">...<br/>const { graphqlLambda, graphiqlLambda } = require('<strong>apollo-server-lambda</strong>');<br/>const <strong>schema</strong> = require('./schema');<br/>const <strong>Connector</strong> = require('./lib/connector');<br/>const { <strong>Thing</strong> } = require('./schema/thing');<br/><br/>module.exports.<strong>graphql</strong> = (event, context, cb) =&gt; {<br/>  <strong>graphqlLambda</strong>(<br/>    (event, context) =&gt; {<br/>      return {<br/>        <strong>schema</strong>, context: { <strong>models</strong>: {<br/>            Thing: new <strong>Thing</strong>( new <strong>Connector</strong>(process.env.<strong>TABLE_NAME</strong>) )<br/>          } }<br/>      };<br/>    }<br/>  )(event, context, (error, output) =&gt; {<br/>    cb(error, ...);<br/>  });<br/>};<br/>. . .</pre>
<ol start="7">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-bff-graphql-crud@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-bff-graphql-crud<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>endpoints:<br/>  POST - <strong>https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/john/graphql</strong><br/>  GET - <strong>https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/john/graphiql</strong><br/>functions:<br/>  graphql: cncb-bff-graphql-crud-john-graphql<br/>  graphiql: cncb-bff-graphql-crud-john-graphiql<br/>  trigger: cncb-bff-graphql-crud-john-trigger<br/><br/>Stack Outputs<br/>...<br/><strong>ServiceEndpoint</strong>: https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/john<br/>...</pre>
<ol start="11">
<li>Review the stack in the AWS Console.</li>
<li>Invoke the function with the following <kbd>curl</kbd> commands:</li>
</ol>
<p>Make sure to replace the API Gateway ID (that is, <kbd>ac0n4oyzm6</kbd>) in the endpoints with the value output during deployment.</p>
<pre style="padding-left: 30px"><strong>$</strong> <strong>curl -X POST -H 'Content-Type: application/json' -d '{"query":"mutation { saveThing(input: { id: \"33333333-1111-1111-1111-000000000000\", name: \"thing1\", description: \"This is thing one of two.\" }) { id } }"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "saveThing" : {<br/>         "id" : "33333333-1111-1111-1111-000000000000"<br/>      }<br/>   }<br/>}<br/><br/>$ <strong>curl -X POST -H 'Content-Type: application/json' -d '{"query":"mutation { saveThing(input: { id: \"33333333-1111-1111-2222-000000000000\", name: \"thing2\", description: \"This is thing two of two.\" }) { id } }"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "saveThing" : {<br/>         "id" : "33333333-1111-1111-2222-000000000000"<br/>      }<br/>   }<br/>}<br/><br/><strong>$</strong> <strong>curl -X POST -H 'Content-Type: application/json' -d '{"query":"query { thing(id: \"33333333-1111-1111-1111-000000000000\") { id name description }}"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "thing" : {<br/>         "description" : "This is thing one of two.",<br/>         "id" : "33333333-1111-1111-1111-000000000000",<br/>         "name" : "thing1"<br/>      }<br/>   }<br/>}<br/><br/><strong>$</strong> <strong>curl -X POST -H 'Content-Type: application/json' -d '{"query":"query { things(name: \"thing\") { items { id name } cursor }}"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "things" : {<br/>         "items" : [<br/>            {<br/>               "id" : "33333333-1111-1111-1111-000000000000",<br/>               "name" : "thing1"<br/>            },<br/>            {<br/>               "name" : "thing2",<br/>               "id" : "33333333-1111-1111-2222-000000000000"<br/>            }<br/>         ],<br/>         "cursor" : null<br/>      }<br/>   }<br/>}<br/><br/><strong>$</strong> <strong>curl -X POST -H 'Content-Type: application/json' -d '{"query":"query { things(name: \"thing\", limit: 1) { items { id name } cursor }}"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "things" : {<br/>         "items" : [<br/>            {<br/>               "id" : "33333333-1111-1111-1111-000000000000",<br/>               "name" : "thing1"<br/>            }<br/>         ],<br/>         "cursor" : "eyJpZCI6IjMzMzMzMzMzLTExMTEtMTExMS0xMTExLTAwMDAwMDAwMDAwMCJ9"<br/>      }<br/>   }<br/>}<br/><br/><strong>$</strong> <strong>curl -X POST -H 'Content-Type: application/json' -d '{"query":"query { things(name: \"thing\", limit: 1, cursor:\"CURSOR VALUE FROM PREVIOUS RESPONSE\") { items { id name } cursor }}"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "things" : {<br/>         "items" : [<br/>            {<br/>               "id" : "33333333-1111-1111-2222-000000000000",<br/>               "name" : "thing2"<br/>            }<br/>         ],<br/>         "cursor" : "eyJpZCI6IjMzMzMzMzMzLTExMTEtMTExMS0yMjIyLTAwMDAwMDAwMDAwMCJ9"<br/>      }<br/>   }<br/>}<br/><br/><strong>$ curl -X POST -H 'Content-Type: application/json' -d '{"query":"mutation { deleteThing( id: \"33333333-1111-1111-1111-000000000000\" ) { id } }"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "deleteThing" : {<br/>         "id" : "33333333-1111-1111-1111-000000000000"<br/>      }<br/>   }<br/>}<br/><br/><strong>$</strong> <strong>curl -X POST -H 'Content-Type: application/json' -d '{"query":"mutation { deleteThing( id: \"33333333-1111-1111-2222-000000000000\" ) { id } }"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "deleteThing" : {<br/>         "id" : "33333333-1111-1111-2222-000000000000"<br/>      }<br/>   }<br/>}<br/><br/><strong>$</strong> <strong>curl -X POST -H 'Content-Type: application/json' -d '{"query":"query { things(name: \"thing\") { items { id } }}"}' https://ac0n4oyzm6.execute-api.us-east-1.amazonaws.com/$MY_STAGE/graphql | json_pp</strong><br/><br/>{<br/>   "data" : {<br/>      "things" : {<br/>         "items" : []<br/>      }<br/>   }<br/>}</pre>
<ol start="13">
<li>Perform the same mutations and queries using GraphiQL by using the endpoint output during deployment:</li>
</ol>
<div><img src="img/e8117ea3-faa6-45bd-98fb-35c1a5e0e7a2.png"/></div>
<ol start="14">
<li>Take a look at the <kbd>trigger</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong></pre>
<ol start="15">
<li>Review the events collected in the data lake bucket.</li>
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>This recipe builds on the <em>Applying the database-first variant of the Event Sourcing pattern with DynamoDB</em> recipe in <a href="129afdee-aa82-4ba6-9d80-5ec70c4a766e.xhtml">Chapter 2</a>, <em>Applying the Event Sourcing and CQRS Patterns</em> by exposing the ability to author data in a bounded context through a GraphQL API. <em>GraphQL</em> is becoming increasingly popular because of the flexibility of the resulting API and the power of client libraries, such as the Apollo Client. We implement a single <kbd>graphql</kbd> function to support our API and then add the necessary functionality through the <kbd>schema</kbd>, <kbd>resolvers</kbd>, <kbd>models</kbd>, and <kbd>connectors</kbd>.</p>
<p>The GraphQL schema is where we define our <kbd>types</kbd>, <kbd>queries</kbd>, and <kbd>mutations</kbd>. In this recipe, we can query <kbd>thing</kbd> types by ID and by name, and <kbd>save</kbd> and <kbd>delete</kbd>. The <kbd>resolvers</kbd> map the GraphQL requests to <kbd>model</kbd> objects that encapsulate the business logic. The <kbd>models</kbd>, in turn, talk to <kbd>connectors</kbd> that encapsulate the details of the database API. The <kbd>models</kbd> and <kbd>connectors</kbd> are registered with the <kbd>schema</kbd> in the <kbd>handler</kbd> function with a very simple but effective form of constructor-based dependency injection. We don't use dependency injection very often in cloud-native, because functions are so small and focused that it is overkill and can impede performance. With GraphQL, this simple form is very effective for facilitating testing. The <kbd>Graphiql</kbd> tool is very useful for exposing the self-documenting nature of APIs.</p>
<p>The single responsibility of this service is authoring data and publishing the events, using database-first Event Sourcing, for a specific bounded context. The code within the service follows a very repeatable coding convention of <kbd>types</kbd>, <kbd>resolvers</kbd>, <kbd>models</kbd>, <kbd>connectors</kbd>, and <kbd>triggers</kbd>. As such, it is very easy to reason about the correctness of the code, even as the number of business domains in the service increases. Therefore, it is reasonable to have a larger number of domains in a single authoring BFF services, so long as the domains are cohesive, part of the same bounded context, and authored by a consistent group of users.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing a search BFF</h1>
                
            
            
                
<p>In the <em>Implementing a GraphQL CRUD BFF</em> recipe, we discussed how the <em>BFF</em> pattern accelerates innovation. We have also discussed how different user groups interact with data at different stages in the data life cycle, and how different persistent mechanisms are more appropriate at the different stages. In this recipe, we will create a BFF service that supports the read-only consumption of data. The single responsibility of this service is <em>indexing</em> and retrieving data for a specific bounded context. It applies the <em>CQRS</em> pattern to create two <em>materialized views</em> that work in tandem, one in Elasticsearch and another in S3. The service exposes a RESTful API.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch3/bff-rest-search --path cncb-bff-rest-search</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-bff-rest-search</kbd> directory with <kbd>cd cncb-bff-rest-search</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-bff-rest-search<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/>  environment:<br/>    BUCKET_NAME:<br/>      Ref: Bucket<br/>    DOMAIN_ENDPOINT:<br/>      Fn::GetAtt: [ Domain, DomainEndpoint ]<br/>...<br/><br/>functions:<br/>  listener:<br/>    handler: handler.listener<br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>  <strong>trigger</strong>:<br/>    handler: handler.<strong>trigger</strong><br/>    events:<br/>      - sns:<br/>          arn: <br/>            Ref: <strong>BucketTopic</strong><br/>          topicName: ${self:service}-${opt:stage}-trigger<br/>  search:<br/>    handler: handler.search<br/>    events:<br/>      - http:<br/>          path: search<br/>          method: get<br/>          cors: true<br/><br/>resources:<br/>  Resources:<br/>    <strong>Bucket</strong>:<br/>      Type: AWS::S3::Bucket<br/>      ...<br/>      Properties:<br/>        NotificationConfiguration:<br/>          <strong>TopicConfigurations</strong>:<br/>            - Event: s3:ObjectCreated:Put<br/>              Topic:<br/>                Ref: BucketTopic<br/>    <strong>BucketTopic</strong>: <br/>      Type: AWS::SNS::Topic<br/>    ...<br/><br/>    <strong>Domain</strong>:<br/>      Type: AWS::Elasticsearch::Domain<br/>  ...</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(forThingCreated)<br/>    .map(toThing)<br/>    .flatMap(put)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/><br/>module.exports.<strong>trigger</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .flatMap(<strong>messagesToTriggers</strong>)<br/>    .flatMap(<strong>get</strong>)<br/>    .map(<strong>toSearchRecord</strong>)<br/>    .flatMap(index)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>const <strong>messagesToTriggers</strong> = r =&gt; _(JSON.parse(r.Sns.Message).Records);<br/><br/>const <strong>get</strong> = (trigger) =&gt; {<br/>  const params = {<br/>    Bucket: trigger.s3.bucket.name,<br/>    Key: trigger.s3.object.key,<br/>  };<br/><br/>  const s3 = new aws.S3();<br/>  return _(<br/>    s3.<strong>getObject</strong>(params).promise()<br/>      .then(data =&gt; ({<br/>        trigger: trigger,<br/>        thing: JSON.parse(Buffer.from(data.Body)),<br/>      }))<br/>  );<br/>};<br/><br/>const <strong>toSearchRecord</strong> = uow =&gt; ({<br/>  id: uow.thing.id,<br/>  name: uow.thing.name,<br/>  description: uow.thing.description,<br/>  <strong>url: `https://s3.amazonaws.com/${uow.trigger.s3.bucket.name}/${uow.trigger.s3.object.key}`</strong>,<br/>});<br/><br/>...</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-bff-rest-search@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-bff-rest-search<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>... <br/>Serverless: Stack update finished...<br/>...<br/>endpoints:<br/>  GET - <strong>https://n31t5dsei8.execute-api.us-east-1.amazonaws.com/john/search</strong><br/>functions:<br/>  listener: cncb-bff-rest-search-john-listener<br/>  trigger: cncb-bff-rest-search-john-trigger<br/>  search: cncb-bff-rest-search-john-search<br/><br/>Stack Outputs<br/>...<br/>BucketArn: arn:aws:s3:::cncb-bff-rest-search-john-bucket-1xjkvimbjtfj2<br/><strong>BucketName: cncb-bff-rest-search-john-bucket-1xjkvimbjtfj2</strong><br/>TopicArn: arn:aws:sns:us-east-1:123456789012:cncb-bff-rest-search-john-trigger<br/>DomainEndpoint: search-cncb-bf-domain-xavolfersvjd-uotz6ggdqhhwk7irxhnkjl26ay.us-east-1.es.amazonaws.com<br/>DomainName: cncb-bf-domain-xavolfersvjd<br/>...<br/>ServiceEndpoint: https://n31t5dsei8.execute-api.us-east-1.amazonaws.com/john<br/>...</pre>
<ol start="9">
<li>Review the stack and resources in the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -f publish -r us-east-1 -s $MY_STAGE -d '{"type":"thing-created","thing":{"new":{"name":"thing three","id":"33333333-2222-0000-1111-111111111111"}}}'</strong><br/><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49583553996455686705785668952918833314346020725338406914"<br/>}</pre>
<ol start="11">
<li>Invoke the following <kbd>curl</kbd> commands, after updating the <kbd>API-ID</kbd> and  <kbd>BUCKET-SUFFIX</kbd>, to search the data and retrieve the detailed data from <kbd>S3</kbd>:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl https://&lt;API-ID&gt;.execute-api.us-east-1.amazonaws.com/$MY_STAGE/search?q=three | json_pp</strong><br/><br/>[<br/>   {<br/>      "id" : "33333333-2222-0000-1111-111111111111",<br/>      "url" : "https://s3.amazonaws.com/cncb-bff-rest-search-john-bucket-1xjkvimbjtfj2/things/33333333-2222-0000-1111-111111111111",<br/>      "name" : "thing three"<br/>   }<br/>]<br/><br/><strong>$ curl https://s3.amazonaws.com/cncb-bff-rest-search-$MY_STAGE-bucket-&lt;BUCKET-SUFFIX&gt;/things/33333333-2222-0000-1111-111111111111 | json_pp</strong><br/><br/>{<br/>   "asOf" : 1526026359761,<br/>   "name" : "thing three",<br/>   "id" : "33333333-2222-0000-1111-111111111111"<br/>}</pre>
<ol start="12">
<li>Take a look at the trigger function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong></pre>
<ol start="13">
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>This recipe combines and builds on the <em>Creating a materialized view in S3</em> and the <em>Creating a materialized view in Elasticsearch</em> recipes to create a highly scalable, efficient, and cost-effective read-only view of the data in a bounded context. First, the <kbd>listener</kbd> function atomically creates the materialized view in <em>S3</em>. The S3 <kbd>Bucket</kbd> is configured to send events to a <strong>Simple Notification Service</strong> (<strong>SNS</strong>) topic called <kbd>BucketTopic</kbd>. We use SNS to deliver the S3 events because only a single observer can consume S3 events, while SNS, in turn, can deliver to any number of observers. Next, the <kbd>trigger</kbd> function atomically indexes the data in the <em>Elasticsearch</em> <kbd>Domain</kbd> and includes the <kbd>url</kbd> to the materialized view in S3.</p>
<p>The RESTful search service exposed by the API Gateway can explicitly scale to meet demand and efficiently search a large amount of indexed data. The detailed data can then be cost-effectively retrieved from S3, based on the returned URL, without the need to go through the API Gateway, a function, and the database. We create the data in S3 first and then index the data in Elasticsearch to ensure that the search results do not include data that has not been successfully stored in S3.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing an analytics BFF</h1>
                
            
            
                
<p>In the <em>Implementing a GraphQL CRUD BFF</em> recipe, we discussed how the <em>BFF</em> pattern accelerates innovation. We have also discussed how different user groups interact with data at different stages in the data life cycle, and how different persistent mechanisms are more appropriate at the different stages. In this recipe, we will create a BFF service that provides statistics about the life cycle of data. The single responsibility of this service is accumulating and aggregating metrics about data in a specific bounded context. It applies the Event Sourcing pattern to create a <em>micro event store</em> that is used to continuously calculate a <em>materialized view</em> of the metrics. The service exposes a RESTful API.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch3/bff-rest-analytics --path cncb-bff-rest-analytics</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-bff-rest-analytics</kbd> directory with <kbd>cd cncb-bff-rest-analytics</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-bff-rest-analytics<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/>  environment:<br/>    EVENTS_TABLE_NAME:<br/>      Ref: Events<br/>    VIEW_TABLE_NAME:<br/>      Ref: View<br/><br/>functions:<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>  <strong>trigger</strong>:<br/>    handler: handler.<strong>trigger</strong><br/>    events:<br/>      - stream:<br/>          type: dynamodb<br/>          arn:<br/>            Fn::GetAtt: [ Events, StreamArn ]<br/>          ...<br/>  query:<br/>    handler: handler.query<br/>    events:<br/>      - http:<br/>          ...<br/><br/>resources:<br/>  Resources:<br/>    <strong>Events</strong>:<br/>      Type: AWS::DynamoDB::Table<br/>      Properties:<br/>        ...<br/>        KeySchema:<br/>          - AttributeName: <strong>partitionKey</strong><br/>            KeyType: <strong>HASH</strong><br/>          - AttributeName: <strong>timestamp</strong><br/>            KeyType: <strong>RANGE</strong><br/>        <strong>TimeToLiveSpecification</strong>:<br/>          AttributeName: <strong>ttl</strong><br/>          Enabled: true<br/>        ...<br/>        StreamSpecification:<br/>          StreamViewType: NEW_AND_OLD_IMAGES<br/>    <strong>View</strong>:<br/>      Type: AWS::DynamoDB::Table<br/>      Properties:<br/>        ...<br/>        KeySchema:<br/>          - AttributeName: <strong>userId</strong><br/>            KeyType: <strong>HASH</strong><br/>          - AttributeName: <strong>yearmonth</strong><br/>            KeyType: <strong>RANGE</strong><br/>        ...</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>byType</strong>)<br/>    .flatMap(<strong>putEvent</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>byType</strong> = event =&gt; event.type.match(/.+/); // any<br/><br/>const putEvent = (event) =&gt; {<br/>  const params = {<br/>    TableName: process.env.<strong>EVENTS_TABLE_NAME</strong>,<br/>    Item: {<br/>      <strong>partitionKey: event.partitionKey,</strong><br/>      <strong>timestamp: event.timestamp,</strong><br/>      event: event,<br/>      <strong>ttl: moment(event.timestamp).add(1, 'h').unix()</strong><br/>    }<br/>  };<br/><br/>  const db = new aws.DynamoDB.DocumentClient();<br/>  return _(db.<strong>put</strong>(params).promise());<br/>};<br/><br/>module.exports.<strong>trigger</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .flatMap(<strong>getMicroEventStore</strong>)<br/>    <strong>.flatMap(store =&gt; _(store) // sub-stream</strong><br/><strong>      .reduce({}, count)</strong><br/><strong>      .flatMap(putCounters)</strong><br/><strong>    )</strong><br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>const <strong>getMicroEventStore</strong> = (record) =&gt; {<br/>  ...<br/>}<br/><br/><strong>const count = (counters, cur) =&gt; {</strong><br/><strong>  return Object.assign(</strong><br/><strong>    {</strong><br/><strong>      userId: cur.partitionKey,</strong><br/><strong>      yearmonth: moment(cur.timestamp).format('YYYY-MM'),</strong><br/><strong>    },</strong><br/><strong>    counters,</strong><br/><strong>    {</strong><br/><strong>      total: counters.total ? counters.total + 1 : 1,</strong><br/><strong>      [cur.event.type]: counters[cur.event.type] ? counters[cur.event.type] + 1 : 1,</strong><br/><strong>    }</strong><br/><strong>  );</strong><br/><strong>  ;</strong><br/><strong>}</strong><br/><br/>const <strong>putCounters</strong> = counters =&gt; {<br/>  ...<br/>};<br/><br/>module.exports.query = (event, context, cb) =&gt; {<br/>  ...<br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-bff-rest-analytics@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-bff-rest-analytics<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>endpoints:<br/>  GET - <strong>https://efbildhw0h.execute-api.us-east-1.amazonaws.com/john/query</strong><br/>functions:<br/>  listener: cncb-bff-rest-analytics-john-listener<br/>  trigger: cncb-bff-rest-analytics-john-trigger<br/>  query: cncb-bff-rest-analytics-john-query<br/><br/>Stack Outputs<br/>...<br/>ServiceEndpoint: https://efbildhw0h.execute-api.us-east-1.amazonaws.com/john<br/>...</pre>
<ol start="9">
<li>Review the stack and resources in the AWS Console.</li>
<li>Publish several events from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$</strong> <strong>sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"purple","partitionKey":"33333333-3333-1111-1111-111111111111"}'</strong><br/><br/><strong>$</strong> <strong>sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"orange","partitionKey":"33333333-3333-1111-1111-111111111111"}'</strong><br/><br/><strong>$</strong> <strong>sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"green","partitionKey":"33333333-3333-1111-2222-111111111111"}'</strong><br/><br/><strong>$ sls invoke -r us-east-1 -f publish -s $MY_STAGE -d '{"type":"green","partitionKey":"33333333-3333-1111-2222-111111111111"}'</strong></pre>
<ol start="11">
<li>Invoke the following <kbd>curl</kbd> command, after updating the <kbd>&lt;API-ID&gt;</kbd>, to query the analytics:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl https://&lt;API-ID&gt;.execute-api.us-east-1.amazonaws.com/$MY_STAGE/query | json_pp</strong><br/><br/>[<br/>   {<br/>      "userId" : "33333333-3333-1111-1111-111111111111",<br/>      "yearmonth" : "2018-05",<br/>      "purple" : 1,<br/>      "orange" : 1,<br/>      "total" : 2<br/>   },<br/>   {<br/>      "userId" : "33333333-3333-1111-2222-111111111111",<br/>      "yearmonth" : "2018-05",<br/>      "green" : 2,<br/>      "total" : 2<br/>   }<br/>]</pre>
<p class="mce-root"/>
<ol start="12">
<li>Take a look at the <kbd>trigger</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong></pre>
<ol start="13">
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>This recipe combines and builds on the <em>Creating a micro event store</em> and the <em>Creating a materialized view in DynamoDB</em> recipes in <a href="129afdee-aa82-4ba6-9d80-5ec70c4a766e.xhtml">Chapter 2</a>, <em>Applying the Event Sourcing and CQRS Patterns</em> to create an advanced materialized view that counts events by <kbd>type</kbd>, <kbd>user</kbd>, <kbd>month</kbd>, and <kbd>year</kbd>. The service employs two DynamoDB tables, the micro event store, and the materialized view. The <kbd>HASH</kbd> key for the event store is the <kbd>partitionKey</kbd>, which contains the <kbd>userId</kbd> so that we can correlate events by the user. The range key is the <kbd>timestamp</kbd> so that we can collate the events and query by month. The hash key for the view table is also <kbd>userId</kbd>, and the range key is <kbd>monthyear</kbd>, so that we can retrieve the statistics by user, month, and year. In this example, we are counting all events, but in a typical solution, you would be filtering <kbd>byType</kbd> for a specific set of event types.</p>
<p>The <kbd>listener</kbd> function performs the crucial job of <em>filtering</em>, <em>correlating,</em> and <em>collating</em> the events into the micro event store, but the real interesting logic in this recipe is in the <kbd>trigger</kbd> function. The logic is based on the concepts of the <em>ACID 2.0</em> transaction model. <strong>ACID</strong> 2.0 stands for <strong>Associative, Commutative, Idempotent, and Distributed</strong>. In essence, this model allows us to arrive at the same, correct answer, regardless of whether or not the events arrive in the correct order or even if we receive the same events multiple times. Our hash and range key in the micro event store handles the idempotency. For each new key, we recalculate the materialized view by querying the event store based on the context of the new event, and performing the calculation based on the latest known data. If an event arrives out of order, it simply triggers a recalculation. In this specific example, the end user would expect the statistics to eventually become consistent by the end of the month or shortly thereafter.</p>
<p>The calculations can be arbitrarily complex. The calculation is performed in memory and the results of the micro event store query can be sliced and diced in many different ways. For this recipe, the <kbd>reduce</kbd> method on the stream is perfect for counting. It is important to note that the <kbd>sub-stream</kbd> ensures that the count is performed by <kbd>userId</kbd>, because that was the hash key of the results returned from the event store. The results are stored in the materialized view as a JSON document so that they can be retrieved efficiently.</p>
<p>The <strong>TimeToLive</strong> (<strong>TTL</strong>) feature is set up on the events table. This feature can be used to keep the event store from growing unbounded, but it can also be used to trigger periodic rollup calculations. I set TTL to one hour so that you can see it execute if you wait long enough, but you would typically set this to a value suitable for your calculations, on the order of a month, quarter, or year.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing an inbound External Service Gateway</h1>
                
            
            
                
<p>The <strong>External Service Gateway</strong> (<strong>ESG</strong>) pattern provides an anti-corruption layer between a cloud-native system and any external services that it interacts with. Each gateway acts as a bridge to exchange events between the system and a specific external system. In this recipe, we will create an ESG service that allows events to flow inbound from an external service. The single responsibility of this service is to encapsulate the details of the external system. The service exposes a RESTful webhook to the external system. The external events are transformed into an internal format and published using <em>event-first</em> Event Sourcing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch3/esg-inbound --path cncb-esg-inbound</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-esg-inbound</kbd> directory with <kbd>cd cncb-esg-inbound</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-esg-inbound<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/><br/>functions:<br/>  <strong>webhook</strong>:<br/>    handler: handler.<strong>webhook</strong><br/>    events:<br/>      - http:<br/>          path: webhook<br/>          method: post<br/>    environment:<br/>      STREAM_NAME: ${cf:cncb-event-stream-${opt:stage}.streamName}</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>webhook</strong> = (request, context, callback) =&gt; {<br/>  const body = JSON.parse(request.body);<br/><br/>  const event = {<br/>    <strong>type</strong>: `issue-${body.action}`,<br/>    <strong>id</strong>: request.headers['X-GitHub-Delivery'],<br/>    partitionKey: String(body.issue.id),<br/>    timestamp: Date.parse(body.issue['updated_at']),<br/>    tags: {<br/>      region: process.env.AWS_REGION,<br/>      repository: body.repository.name,<br/>    },<br/>    issue: body, // canonical<br/>    <strong>raw</strong>: request<br/>  };<br/><br/>  ...<br/><br/>  kinesis.<strong>putRecord</strong>(params, (err, resp) =&gt; {<br/>    const response = {<br/>      statusCode: err ? 500 : 200,<br/>    };<br/>  <br/>    callback(null, response);<br/>  });  <br/>};</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
</ol>
<ol start="8">
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-esg-inbound@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/js-cloud-native-cookbook/workspace/cncb-esg-inbound<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>endpoints:<br/>  POST - <strong>https://kc880846ve.execute-api.us-east-1.amazonaws.com/john/webhook</strong><br/>functions:<br/>  webhook: cncb-esg-inbound-john-webhook<br/><br/>Stack Outputs<br/>...<br/>ServiceEndpoint: https://kc880846ve.execute-api.us-east-1.amazonaws.com/john<br/>...</pre>
<ol start="9">
<li>Review the stack and resources in the AWS Console.</li>
<li>Set up a webhook in your GitHub project:
<ol>
<li>Set the Payload URL to the endpoint of your webhook</li>
<li>Set the content type to <kbd>application/json</kbd></li>
<li>Set the secret to a random value</li>
<li>Select just the <kbd>Issues</kbd> events checkbox</li>
</ol>
</li>
</ol>
<p>For detailed instructions on creating a GitHub webhook, see <a href="https://developer.github.com/webhooks/creating">https://developer.github.com/webhooks/creating.</a></p>
<div><img src="img/5e9442a6-eee5-4020-aaaa-19afb2cccd7f.png"/></div>
<ol start="11">
<li>Create and/or update one or more issues in your GitHub project to trigger the webhook.</li>
<li>Take a look at the <kbd>webhook</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f webhook -r us-east-1 -s $MY_STAGE</strong></pre>
<ol start="13">
<li>Review the events in the data lake.</li>
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>I chose to use GitHub as the external system in this recipe because it is freely available to everyone and representative of typical requirements. In this recipe, our inbound ESG service needs to provide an API that will be invoked by the external system and conforms to the signature of the external system's webhook. We implement this webhook using the API Gateway and a <kbd>webhook</kbd> function. The single responsibility of this function is to transform the external event to an internal event and atomically publish it using event-first Event Sourcing.</p>
<p>Note that the external event ID is used as the internal event ID to provide for idempotency. The external event data is included in the internal event in its raw format so that it can be recorded as an audit in the data lake. The external format is also transformed in an internal canonical format to support the pluggability of different external systems. The logic in an inbound ESG service is intentionally kept simple to minimize the chance of errors and help ensure the atomic exchange of the events between the systems.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing an outbound External Service Gateway</h1>
                
            
            
                
<p>In the <em>Implementing an inbound External Service Gateway</em> recipe, we discussed how the ESG pattern provides an anti-corruption layer between the cloud-native system and its external dependencies. In this recipe, we will create an ESG service that allows events to flow outbound to an external service. The single responsibility of this service is to encapsulate the details of the external system. The service applies the CQRS pattern. The internal events are transformed to the external format and forwarded to the external system via its API.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>Before starting this recipe, you will need an AWS Kinesis Stream, such as the one created in the <em>Creating an event stream</em> recipe.</p>
<p>You will need a GitHub account and a repository. I recommend creating a repository named <kbd>sandbox</kbd>. Use the following command to create a GitHub personal access token, or follow the instructions in the GitHub UI:</p>
<pre style="padding-left: 30px"><strong>curl https://api.github.com/authorizations \</strong><br/><strong>--user "your-github-id" \</strong><br/><strong>--data '{"scopes":["repo"],"note":"recipe"}'</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch3/esg-outbound --path cncb-esg-outbound</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-esg-outbound</kbd> directory with <kbd>cd cncb-esg-outbound</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-esg-outbound<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  environment:<br/>    REPO: enter-your-github-project<br/>    OWNER: enter-your-github-id<br/>    TOKEN: enter-your-github-token<br/><br/>functions:<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...</pre>
<ol start="4">
<li>Update the <kbd>REPO</kbd>, <kbd>OWNER</kbd>, and <kbd>TOKEN</kbd> environment variables in the <kbd>serverless.yml</kbd> file.</li>
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>byType</strong>)<br/>    .flatMap(<strong>post</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>byType</strong> = event =&gt; event.type === '<strong>issue-created</strong>';<br/><br/>const <strong>post</strong> = event =&gt; {<br/>  // transform internal to external<br/>  const body = {<br/>    title: event.issue.new.title,<br/>    body: event.issue.new.description,<br/>  };<br/><br/>  return _(<br/>    fetch(`https://api.github.com/repos/${process.env.<strong>OWNER</strong>}/${process.env.<strong>REPO</strong>}/issues`, {<br/>      method: 'POST',<br/>      headers: {<br/>        'Content-Type': 'application/json',<br/>        'Authorization': `Bearer ${process.env.<strong>TOKEN</strong>}`<br/>      },<br/>      body: JSON.stringify(body)<br/>    })<br/>  );<br/>};</pre>
<ol start="6">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-esg-outbound@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-esg-outbound<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  listener: cncb-esg-outbound-john-listener<br/>...</pre>
<ol start="10">
<li>Review the stack and resources in the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -f publish -r us-east-1 -s $MY_STAGE -d '{"type":"issue-created","issue":{"new":{"title":"issue one","description":"this is issue one.","id":"33333333-55555-1111-1111-111111111111"}}}'</strong><br/><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49583655996852917476267887119095157508589012871374962690"<br/>}</pre>
<ol start="12">
<li>Confirm that the issue was created in your GitHub project.</li>
<li>Take a look at the <kbd>listener</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong></pre>
<ol start="14">
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>I chose to use GitHub as the external system in this recipe because it is freely available to everyone and its API is representative of typical requirements. One of the major details that are encapsulated by an ESG service is the security credentials needed to access the external API. In this recipe, we must create and secure a long-lived <em>personal access token</em> and include it as an authorization header in every API request. The details of how to secure a token are out of scope for this recipe, however, a service such as AWS Secret Manager is typically employed. For this recipe, the token is stored as an environment variable.</p>
<p>The <kbd>listener</kbd> function consumes the desired events, transforms them into the external format, and atomically invokes the external API. That is the limit of the responsibility of an ESG service. These services effectively make external services look like any other service in the system, while also encapsulating the details so that these external dependencies can be easily switched in the future. The transformation logic can become complex. The latching technique, discussed in the <em>Implementing bi-directional synchronization</em> recipe, may come into play, as well as the need to cross-reference external IDs to internal IDs. In many cases, the external data can be thought of as a materialized view, in which case the <em>micro event store</em> techniques may be useful. In a system that is offered as a service, an ESG service would provide your own outbound webhook feature.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Orchestrating collaboration between services</h1>
                
            
            
                
<p>Autonomous cloud-native services perform all inter-service communication asynchronously via streams to decouple upstream services from downstream services. Although the upstream and downstream services are not directly coupled to each other, they are coupled to the event types that they produce and consume. The <em>Event Orchestration</em> control pattern acts as a <em>mediator</em> to completely decouple event producers from event consumers by translating between event types.</p>
<p>In this recipe, we will create a control service that orchestrates the interaction between two boundary services. The single responsibility of this service is to encapsulate the details of the collaboration. The upstream events are transformed to the event types' expected downstream, and published using <em>event-first</em> Event Sourcing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch3/event-orchestration --path cncb-event-orchestration</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-event-orchestration</kbd> directory with <kbd>cd cncb-event-orchestration</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-event-orchestration<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/><br/>functions:<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>    environment:<br/>      STREAM_NAME: ${cf:cncb-event-stream-${opt:stage}.streamName}</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">const <strong>transitions</strong> = [<br/>  {<br/>    filter: '<strong>order-submitted</strong>',<br/>    emit: (uow) =&gt; ({<br/>      id: uuid.v1(),<br/>      type: '<strong>make-reservation</strong>',<br/>      timestamp: Date.now(),<br/>      partitionKey: uow.event.partitionKey,<br/>      reservation: {<br/>        sku: uow.event.order.sku,<br/>        quantity: uow.event.order.quantity,<br/>      },<br/>      context: {<br/>        order: uow.event.order,<br/>        trigger: uow.event.id<br/>      }<br/>    })<br/>  },<br/>  {<br/>    filter: '<strong>reservation-confirmed</strong>',<br/>    emit: (uow) =&gt; ({<br/>      id: uuid.v1(),<br/>      type: '<strong>update-order-status</strong>',<br/>      timestamp: Date.now(),<br/>      partitionKey: uow.event.partitionKey,<br/>      order: {<br/>        status: 'reserved',<br/>      },<br/>      context: {<br/>        reservation: uow.event.reservation,<br/>        order: uow.event.context.order,<br/>        trigger: uow.event.id<br/>      }<br/>    })<br/>  },<br/>];<br/><br/>module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToUow)<br/>    .filter(<strong>onTransitions</strong>)<br/>    .flatMap(<strong>toEvents</strong>)<br/>    .flatMap(publish)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>const recordToUow = r =&gt; ({<br/>  record: r,<br/>  event: JSON.parse(Buffer.from(r.kinesis.data, 'base64')),<br/>});<br/><br/>const <strong>onTransitions</strong> = uow =&gt; {<br/>  // find matching transitions<br/>  uow.transitions = transitions.filter(trans =&gt; trans.filter === uow.event.type);<br/><br/>  // proceed forward if there are any matches<br/>  return uow.transitions.length &gt; 0;<br/>};<br/><br/>const <strong>toEvents</strong> = uow =&gt; {<br/>  // create the event to emit for each matching transition<br/>  return _(uow.transitions.map(t =&gt; t.emit(uow)));<br/>};<br/><br/>const publish = event =&gt; {<br/>  . . .<br/>}</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-event-orchestration@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-event-orchestration<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  listener: cncb-event-orchestration-john-listener<br/>...</pre>
<ol start="9">
<li>Review the stack and resources in the AWS Console.</li>
<li>Publish these events from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -f publish -r us-east-1 -s $MY_STAGE -p ../cncb-event-orchestration/data/order.json</strong><br/><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49583655996852917476267896339723499436825420846818394114"<br/>}<br/><br/><strong>$ sls invoke -f publish -r us-east-1 -s $MY_STAGE -p ../cncb-event-orchestration/data/reservation.json</strong><br/><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49583655996852917476267896340117609254019790713686851586"<br/>}</pre>
<ol start="11">
<li>Take a look at the <kbd>listener</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong></pre>
<ol start="12">
<li>Review the events in the data lake.</li>
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>This <em>control</em> service has a single <em>stream processor</em> function that listens for specific events and reacts by emitting more events using event-first Event Sourcing. The events it listens for are described in the <kbd>transitions</kbd> metadata, which essentially defines the state machine of a long-lived business process. Each business process is implemented as an autonomous control service that orchestrates the collaboration between a set of completely decoupled boundary services. Each <em>boundary</em> service involved in the collaboration defines the set of events it produces and consumes independently of the other services. The control service provides the glue that brings these services together to deliver a higher value outcome.</p>
<p>The <em>downstream</em> services that are triggered by the emitted events do have one requirement that they must support when defining their incoming and outgoing event types. The incoming event types must accept the <kbd>context</kbd> element as an opaque set of data and pass the context data along in the outgoing event. A downstream service can leverage the context data, but should not explicitly change the context data. The context data allows the control service to correlate the events in a specific collaboration instance without needing to explicitly store and retrieve data. However, a control service could maintain its own <em>micro event store</em> to facilitate complex transition logic, such as joining multiple parallel flows back together before advancing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing a Saga</h1>
                
            
            
                
<p>The Saga pattern is a solution to long-lived transactions that is based on eventual consistency and compensating transactions. It was first discussed in a paper by Hector Garcia-Molina and Kenneth Salem (<a href="https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf">https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf</a>). Each step in a long-lived transaction is atomic. When a downstream step fails, it produces a violation event. The upstream services react to the violation event by performing a compensating action. In this recipe, we will create a service that submits data for downstream processing. The service also listens for a violation event and takes corrective action.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch3/saga --path cncb-saga</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-saga</kbd> directory with <kbd>cd cncb-sage</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-saga<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  iamRoleStatements:<br/>    ...<br/>  environment:<br/>    TABLE_NAME:<br/>      Ref: Table<br/><br/>functions:<br/>  submit:<br/>    handler: handler.submit<br/>  trigger:<br/>    handler: handler.trigger<br/>    events:<br/>      - stream:<br/>          type: dynamodb<br/>          arn:<br/>            Fn::GetAtt: [ Table, StreamArn ]<br/>          ...<br/>    environment:<br/>      STREAM_NAME: ${cf:cncb-event-stream-${opt:stage}.streamName}<br/>  <strong>listener</strong>:<br/>    handler: handler.<strong>listener</strong><br/>    events:<br/>      - stream:<br/>          type: kinesis<br/>          arn: ${cf:cncb-event-stream-${opt:stage}.streamArn}<br/>          ...<br/>  query:<br/>    handler: handler.query<br/><br/>resources:<br/>  Resources:<br/>    Table:<br/>      Type: AWS::DynamoDB::Table<br/>      Properties:<br/>        TableName: ${opt:stage}-${self:service}-orders<br/>        ...<br/>        StreamSpecification:<br/>          StreamViewType: NEW_AND_OLD_IMAGES</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">...<br/><br/>module.exports.<strong>listener</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .map(recordToEvent)<br/>    .filter(<strong>forReservationViolation</strong>)<br/>    .flatMap(<strong>compensate</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>const <strong>forReservationViolation</strong> = e =&gt; e.type === '<strong>reservation-violation</strong>';<br/><br/>const <strong>compensate</strong> = event =&gt; {<br/>  const params = {<br/>    TableName: process.env.TABLE_NAME,<br/>    Key: {<br/>      id: event.context.order.id<br/>    },<br/>    <strong>AttributeUpdates</strong>: {<br/>      status: { Action: 'PUT', Value: 'cancelled' }<br/>    },<br/>  };<br/><br/>  const db = new aws.DynamoDB.DocumentClient();<br/>  return _(db.update(params).promise());<br/>};<br/><br/>...</pre>
<ol start="5">
<li>Install the dependencies with <kbd>npm install</kbd>.</li>
<li>Run the tests with <kbd>npm test -- -s $MY_STAGE</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl -- -s $MY_STAGE</strong><br/><br/>&gt; cncb-saga@1.0.0 dp:lcl &lt;path-to-your-workspace&gt;/cncb-saga<br/>&gt; sls deploy -v -r us-east-1 "-s" "john"<br/><br/>Serverless: Packaging service...<br/>...<br/>Serverless: Stack update finished...<br/>...<br/>functions:<br/>  submit: cncb-saga-john-submit<br/>  trigger: cncb-saga-john-trigger<br/>  listener: <strong>cncb-saga-john-listener</strong><br/>  query: cncb-saga-john-query<br/>...</pre>
<ol start="9">
<li>Review the stack and resources in the AWS Console.</li>
<li>Invoke the <kbd>submit</kbd> and <kbd>query</kbd> functions with the following commands:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -f submit -r us-east-1 -s $MY_STAGE -p data/order.json</strong><br/><br/><strong>$ sls invoke -f query -r us-east-1 -s $MY_STAGE -d 33333333-7777-1111-1111-111111111111</strong><br/>{<br/>    "Item": {<br/>        "quantity": 1,<br/>        "id": "33333333-7777-1111-1111-111111111111",<br/>        "sku": "1",<br/>        <strong>"status": "submitted"</strong><br/>    }<br/>}</pre>
<ol start="11">
<li>Publish a violation event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px">$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream<br/><strong>$ sls invoke -f publish -r us-east-1 -s $MY_STAGE -p ../cncb-saga/data/reservation.json</strong><br/><br/>{<br/>    "ShardId": "shardId-000000000000",<br/>    "SequenceNumber": "49584174522005480245492626573048465901488330636951289858"<br/>}</pre>
<ol start="12">
<li>Invoke the <kbd>query</kbd> function again with the following command, and note the updated <kbd>status</kbd>:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -r us-east-1 -f query -s $MY_STAGE -d 33333333-7777-1111-1111-111111111111</strong><br/><br/>{<br/>    "Item": {<br/>        "quantity": 1,<br/>        "id": "33333333-7777-1111-1111-111111111111",<br/>        "sku": "1",<br/>        <strong>"status": "cancelled"</strong><br/>    }<br/>}</pre>
<ol start="13">
<li>Take a look at the <kbd>trigger</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong></pre>
<ol start="14">
<li>Take a look at the <kbd>listener</kbd> function logs:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong></pre>
<ol start="15">
<li>Remove the stack once you are finished with <kbd>npm run rm:lcl -- -s $MY_STAGE</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>This recipe builds on recipes that we have already covered. We submit an order domain object using <em>database-first</em> Event Sourcing, and we have a <kbd>query</kbd> function to retrieve the current status of the order. The <kbd>listener</kbd> function is the most interesting part of this recipe. It listens for the <kbd>reservation-violation</kbd> events and performs a compensating action. In this case, the <kbd>compensation</kbd> is simply to change the <kbd>status</kbd> to <kbd>cancelled</kbd>. Compensating actions can be arbitrarily complex and are specific to a given service. For example, a service may need to reverse a complex calculation, while accounting for intermediate changes and triggering cascading changes as well. The audit trail provided by Event Sourcing and a <em>micro event store</em> may be useful for recalculating the new state.</p>
<p>Another thing to note in this example is that the collaboration is implemented using <em>event</em> <em>choreography</em> instead of <em>Event Orchestration</em>. In other words, this service is explicitly coupled to the <kbd>reservation-violation</kbd> event type. Event choreography is typically used in smaller and/or younger systems, or between highly related services. As a system matures and grows, the flexibility of <em>Event Orchestration</em> becomes more valuable. We employed Event Orchestration in the <em>Orchestrating collaboration between services</em> recipe.</p>


            

            
        
    </body></html>