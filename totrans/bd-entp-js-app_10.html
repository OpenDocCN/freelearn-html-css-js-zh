<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying Our Application on a VPS</h1>
                </header>
            
            <article>
                
<p>In the last few chapters, we created a robust user directory API, which is now ready to face the outside world. Thus, in this chapter, we'll learn how to expose our API to the <strong>World Wide Web</strong> (<strong>WWW</strong>). First, we will need to set up a <strong><span>V</span></strong><span><strong>irtual Private Server</strong> (<strong>VPS</strong>)</span> to host and serve our API, and <span>associate it with </span><span>a public, </span><strong>static IP</strong><span> address</span>; we will achieve both of these goals using<span> </span><strong>DigitalOcean</strong><span> (</span><strong>DO</strong><span>), a popular cloud provider. </span>Then, to make it easier for our API consumers, we'll purchase a <strong>domain name</strong> from a <strong>domain registry</strong>, and configure its <span><strong>Domain Name System</strong> (</span><strong>DNS</strong><span>) records to resolve the domain name to the static IP.</span></p>
<p>By following this chapter, you will:</p>
<ul>
<li>Learn to set up and secure a VPS</li>
<li>Learn about <strong>privileged ports</strong></li>
<li>Keep processes alive using <strong>PM2</strong></li>
<li>Set up <strong>NGINX</strong> as a <strong>reverse proxy</strong> to our API</li>
<li>Understand the architecture of the DNS</li>
<li>Purchase and configure a domain name</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Obtaining an IP address</h1>
                </header>
            
            <article>
                
<p>The internet is a giant network of interconnected machines. For these machines to communicate with one another, each machine must have a unique identifier. The internet uses the <strong>TCP/IP protocol</strong> for its communication, which in turn uses the <strong>IP address</strong> as its unique identifier. So, the first requirement for exposing our API to the internet is to have an IP address.</p>
<p class="mce-root"/>
<p>If you are paying for internet at home, you too will have an IP address provided to you by your <strong>Internet Service Provider</strong> (<strong>ISP</strong>). You can check your IP address by using an external service such as <a href="https://ipinfo.io/" target="_blank">ipinfo.io</a>:</p>
<pre><strong>$ curl ipinfo.io/ip</strong><br/><strong>146.179.207.221</strong></pre>
<p>This means it's theoretically possible to host your API using your home PC, or even your laptop. However, doing so is problematic because of the following reasons:</p>
<ul>
<li>Most consumer-grade internet plans provide <strong>dynamic IP addresses</strong>, rather than static ones, which means your IP can change every few days</li>
<li>Many ISPs block incoming traffic to port <kbd>80</kbd>, which is the default HTTP port</li>
<li>You need to maintain your own hardware</li>
<li>Internet connection speed may be slow</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managed DNS</h1>
                </header>
            
            <article>
                
<p>The first issue can be mitigated using <strong>Managed DNS</strong> services, such as <strong>No-IP</strong> (<a href="https://www.noip.com/">noip.com</a>) and <strong>Dyn</strong> (<a href="https://dyn.com/">dyn.com</a>), which provide a <strong>dynamic DNS</strong> service. These services will provide you with a hostname (for example, <kbd>username.no-ip.info</kbd>) and update the hostname's DNS <strong>A record</strong> to point to your machine's IP address (more on DNS records later). This means any requests destined for that hostname will arrive at your associated device. To make this work, you'd also have to install a client on your device, which frequently checks its own IP, and update the Managed DNS service whenever it changes.</p>
<p>The second issue can be mitigated by using <strong>port redirect</strong>, which is a service that most Managed DNS services also provide. First, just as before, you must download the client to update the Managed DNS service with your dynamic IP. Then, bind your application to listen on a <span>port on your machine that is not blocked by your ISP. Lastly, you'd have to go to the Managed DNS service and redirect all traffic that arrives at the hostname to your device's specified port.</span></p>
<p>Dynamic DNS simply changes a DNS record; no application traffic actually arrives at the Managed DNS servers. On the other hand, with port redirect, the Managed DNS service acts as a proxy that redirects HTTP packets. If you'd like to try them out, No-IP provides a Free Dynamic DNS service, which you can sign up for at <a href="https://www.noip.com/free">noip.com/free</a>.</p>
<p class="mce-root"/>
<p>While having a dynamic IP and using a dynamic DNS is acceptable for personal use, it's nowhere near reliable enough to be used for enterprise. Your IP address can change at any time, and this can cause connections to drop and data to get lost. There will also be a bit of latency between when an IP address updates and when the Managed DNS provider is made aware of this change, and thus you can never achieve 100% uptime.</p>
<p>Businesses who host their own servers usually pay their ISP for a static IP and enhanced connection speeds. However, this can be costly. Take Comcast, the most popular and beloved broadband provider in the United States: their most basic consumer-grade offering, XFINITY Performance Internet, supports up to 60 Mbps download speed and costs $39.99 per month. However, for Comcast to assign you a static IP, you must subscribe to their business-grade plans. The most basic plan—Starter Internet—supports up to 25 Mbps speed, and costs $69.95 per month, or $89.90 if you'd want to include a static IP. <span>This is just not cost-effective.</span></p>
<p>A better alternative is to register an account with a<span> cloud</span><span> provider and </span>deploy our application on a VPS. A VPS is essentially a <strong>virtual machine</strong> (<strong>VM</strong>) that is connected to the internet and is allocated its own static IP address. In terms of costs, VPS can cost as low as $0.996 per month!</p>
<div class="packt_tip">You can find a list of cheap VPS hosting providers at <a href="https://lowendbox.com/">lowendbox.com</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a Virtual Private Server (VPS)</h1>
                </header>
            
            <article>
                
<p>There are many VPS providers, such as the following:</p>
<ul>
<li>Amazon Elastic Compute Cloud (Amazon EC2): <a href="https://aws.amazon.com/ec2/">aws.amazon.com/ec2</a></li>
<li>IBM Virtual Servers: <a href="https://www.ibm.com/cloud/virtual-servers">ibm.com/cloud/virtual-servers</a></li>
<li>Google Cloud Compute Engine: <a href="https://cloud.google.com/compute/">cloud.google.com/compute</a></li>
<li>Microsoft Azure Virtual Machines: <a href="https://azure.microsoft.com/services/virtual-machines/">azure.microsoft.com/services/virtual-machines</a></li>
<li>Rackspace Virtual Cloud Servers:<span> <a href="https://www.rackspace.com/cloud/servers">rackspace.com/cloud/servers</a></span></li>
<li>Linode: <a href="https://www.linode.com/">linode.com</a></li>
</ul>
<p>For this book, we are going to use <strong>DigitalOcean</strong> (<strong>DO</strong>, <a href="https://www.digitalocean.com/">digitalocean.com</a>). We picked DO because it has a very intuitive user interface (UI), where everything (VPS, <span>DNS, block storage, monitoring, Kubernetes) can all be managed on the same dashboard. This is unlike AWS, which has an outdated and cumbersome UI.</span></p>
<p>Now, go to the DO website (<a href="https://m.do.co/c/5cc901594b32">digitalocean.com</a>) and create an account.</p>
<div class="packt_tip">You should use this referral link: <a href="https://m.do.co/c/5cc901594b32">m.do.co/c/5cc901594b32</a>; it will give you $10 in free credits!</div>
<p>DO will ask you for your billing details, but you won't be charged until you've used their services. You should also set up <strong>Two-Factor Authentication</strong> (<strong>2FA</strong>) on your account to keep it secure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a VPS instance</h1>
                </header>
            
            <article>
                
<p>After you've successfully created your account, log in at <a href="https://cloud.digitalocean.com/">cloud.digitalocean.com</a>, click on the drop-down button that says <span class="packt_screen">Create</span>, and then select <span class="packt_screen">Droplet</span>.</p>
<div class="packt_infobox"><br/>
In DO vocabulary, a droplet is the same as a VPS.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing an image</h1>
                </header>
            
            <article>
                
<p>You'll be presented with a screen where we can configure the VPS. The first section on that screen is <span class="packt_screen">Choose an image</span>, which is where we select the Linux distribution we want our VPS to run on. We are going to select the<span> <span class="packt_screen">Ubuntu 18.04 x64</span> option for our VPS.</span></p>
<p><span>We picked 18.04 because it is a <strong>Long Term Support</strong> (<strong>LTS</strong>) version, which means it will receive hardware and maintenance updates for five years, whereas standard Ubuntu releases are only supported for nine months. This is important for enterprise-level services because it ensures any security vulnerabilities or performance updates are treated as priority over other standard releases:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/81bdd4b8-eedc-46bc-93e0-745f5c617f66.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">This diagram is reproduced from the Ubuntu lifecycle and release cadence page on the Ubuntu website (ubuntu.com/about/release-cycle<a href="https://www.ubuntu.com/about/release-cycle">)</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing a size</h1>
                </header>
            
            <article>
                
<p>Next, we must pick the size of our VPS. This determines the amount of resources (CPU, memory, storage, and bandwidth) that are available to us.</p>
<p>Elasticsearch is very memory-intensive, and their official guide suggests using machines with 16-64 GB of memory. However, that is very costly. For this book, picking a VPS with at least 4 GB of RAM should suffice.</p>
<div class="packt_infobox">We can ignore the backups and block storage options.<br/>
<br/>
Block storage is extra disk space that can be associated with our VPS. For example, if we are hosting a file server or Image API, we may want to add extra disk space to store these files/images; purchasing pure disk space is much cheaper than running a VPS with an operating system.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Picking a data center region</h1>
                </header>
            
            <article>
                
<p>Next, we must choose the data center where our VPS will reside.</p>
<p>Different machines on the internet communicate by sending messages to one another<span>. </span>A message must "hop" through a string of proxy servers before it arrives at the receiver's machine, and this takes time. Generally speaking, the more <strong>hops</strong> a message must make, the longer the latency.</p>
<p>Therefore, you should pick the data center that is closest to your target users. For example, if your target audience is largely based in the UK, then you'd pick the <span class="packt_screen">London</span> data center.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting additional options</h1>
                </header>
            
            <article>
                
<p>Next, select the following additional options:</p>
<ul>
<li><strong>Private networking</strong>: This gives each VPS instance an internal IP address, which allows services deployed in the same data center to communicate with each other. At the time of this writing, this option is free and does not count towards your monthly bandwidth quota.</li>
<li><strong>IPv6</strong>: IPv4 can support up to 4,294,967,296 unique IP addresses. The internet has grown so much that we are close to exceeding this limit. Therefore, IPv6 increases the number of bits in the IP address from 32 bits to 128 bits, yielding 340,282,366,920,938,463,463,374,607,431,768,211,456 addresses. By checking this option, we allow users to use the IPv6 address to address our server.</li>
<li><strong>Monitoring</strong>: Collects system metrics on your server, such as <span>CPU, memory, disk I/O, disk usage, public/private bandwidth, and </span>alerts you when your server is running close to the limit:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e5da637a-73d4-4426-8e4e-47820078d4d5.png" style="width:34.33em;height:5.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naming your server</h1>
                </header>
            
            <article>
                
<p>Lastly, pick a hostname for your server. This will appear in the administration panel of DigitalOcean, so pick something you can easily remember.</p>
<p class="mce-root"/>
<p>When you have many machines, it may be worth setting up a naming convention, where the name of the machine itself imparts information about how it is used. For example, your naming convention may be as follows:</p>
<pre>[environment].[feature].[function][replica]</pre>
<p>For example, if we have a machine that acts as a load balancer for an authorization service in the staging environment, its hostname may be <kbd>staging.auth.lb1</kbd>.</p>
<p>This is extremely useful when you log in to multiple servers using the terminal—they all look the same! The only way for you to figure out which machine you're working on is by looking at the hostname printed in the prompt:</p>
<pre>hobnob@staging.auth.lb1:~$</pre>
<div class="packt_tip">If you're only setting up servers for personal use, feel free to get creative with the names. Popular conventions include using names of planets, periodic elements, animals, and car models. Personally, I name my machines after different components found in a cell: nucleus, nucleolus, vesicle, cytoplasm, lysosome, and ribosomes.<br/>
<br/>
Another article worth reading is <em>Choosing a Name for Your Computer</em> (<a href="https://www.ietf.org/rfc/rfc1178">ietf.org/rfc/rfc1178</a>).</div>
<p>For now, since we only have one machine, let's specify a simple name, <kbd>hobnob</kbd>, and click <span class="packt_screen">Create</span>!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Connecting to the VPS</h1>
                </header>
            
            <article>
                
<p>After you click <span class="packt_screen">Create</span>, DigitalOcean will provision a new VPS for you. You'll also receive an email with instructions on how to log in:</p>
<pre class="raw_message_text">From: DigitalOcean &lt;support@support.digitalocean.com&gt;
Subject: Your New Droplet: hobnob<br/>Your new Droplet is all set to go! You can access it using the following credentials:

Droplet Name: hobnob
IP Address: 142.93.241.63
Username: root
Password: 58c4abae102ec3242ddbb26372

Happy Coding,
Team DigitalOcean</pre>
<p class="mce-root"/>
<p>With these credentials, connect to your server as the <kbd>root</kbd> administrative user using SSH:</p>
<pre><strong>$ ssh root@&lt;server-ip&gt;</strong></pre>
<p>Here, <kbd>&lt;server-ip&gt;</kbd> is the IP address of your server (<kbd>142.93.241.63</kbd> in our examples). This will prompt you for your password; enter the one you received in your email. After logging in, the server will ask you to change your root password:</p>
<pre><strong>$</strong> <strong>ssh root@142.93.241.63</strong><br/>The authenticity of host '142.93.241.63 (142.93.241.63)' can't be established.<br/>ECDSA key fingerprint is SHA256:AJ0iVdifdlEOQNYvvhwZc0TAsi96JtWJanaRoW29vxM.<br/>Are you sure you want to continue connecting (yes/no)? <strong>yes</strong><br/>root@142.93.241.63's password: <strong>58c4abae102ec3242ddbb26372</strong><br/>You are required to change your password immediately (root enforced)<br/>...<br/>Changing password for root.<br/>(current) UNIX password: <strong>58c4abae102ec3242ddbb26372</strong><br/>Enter new UNIX password: <strong>&lt;your-new-password&gt;</strong><br/>Retype new UNIX password: <strong>&lt;your-new-password&gt;</strong><br/>root@hobnob:# </pre>
<p>Great! You have successfully created a virtual server and logged in to it.</p>
<div class="packt_infobox"><span>For the code blocks in </span>this chapter, we will add the <kbd>&lt;user&gt;@hobnob:</kbd> prompt before any commands that are meant to run on the remote virtual server, and the normal prompt, <kbd>$</kbd>, for commands that should be run locally.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up user accounts</h1>
                </header>
            
            <article>
                
<p>At the moment, we are logging in as<span> </span><kbd>root</kbd>, which is the administrative user of the machine with all privileges. This means a <kbd>root</kbd> user can do dangerous things, such as deleting every file in the system with<span> </span><kbd>rm -rf /</kbd>. If a malicious user gains access to your <kbd>root</kbd> account, or if you accidentally issue the wrong command, then there's no turning back; most of these actions are irreversible.</p>
<p>Therefore, to protect our server from both malicious parties and human error, it's advisable to not use <kbd>root</kbd> on an everyday basis. Instead, we should set up an account with reduced privileges, and only use root privileges when we need to (for example, when installing system-wide software).</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new user</h1>
                </header>
            
            <article>
                
<p>First, we must create a new user. While still logged in as <kbd>root</kbd>, run <kbd>adduser &lt;username&gt;</kbd>, replacing <kbd>&lt;username&gt;</kbd> with your username (we will use <kbd>hobnob</kbd> as the username going forward). <span>This will initiate a wizard that asks you for details about the user, and for you to enter a password. After this, a new user with the username</span><span> </span><kbd>hobnob</kbd><span> will be created, with their own home directory located at</span><span> </span><kbd>/home/hobnob</kbd><span>:</span></p>
<pre>root@hobnob:# <strong>adduser hobnob</strong><br/>Adding user `hobnob' ...<br/>Adding new group `hobnob' (1000) ...<br/>Adding new user `hobnob' (1000) with group `hobnob' ...<br/>Creating home directory `/home/hobnob' ...<br/>Copying files from `/etc/skel' ...<br/>Enter new UNIX password: <strong>&lt;your-password&gt;</strong><br/>Retype new UNIX password: <strong>&lt;your-password&gt;</strong><br/>passwd: password updated successfully<br/>Changing the user information for hobnob<br/>Enter the new value, or press ENTER for the default<br/>  Full Name []: <strong>Daniel Li</strong><br/>  Room Number []: <br/>  Work Phone []: <br/>  Home Phone []: <br/>  Other []: <br/>Is the information correct? [Y/n] <strong>Y</strong></pre>
<p>Now that we have a user with reduced privileges, we can use it to execute everyday commands. Try logging in using a different terminal, with the username and password of your new user:</p>
<pre><strong>$ ssh hobnob@142.93.241.63</strong><br/><strong>hobnob@142.93.241.63's password: &lt;your-hobnob-user-password&gt;</strong><br/><strong>hobnob@hobnob:$</strong> </pre>
<p>Great! We've created a user account with reduced privileges and are able to access the server with this new account. But because it has limited privileges, we won't be able to perform even simple administrative tasks. Try updating the package lists by running <kbd>apt update</kbd>; it will produce an error that says <kbd>Permission denied</kbd> because this action requires root privileges:</p>
<pre><strong>hobnob@hobnob:$ apt update</strong><br/><strong>Reading package lists... Done</strong><br/><strong>E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)</strong><br/><strong>E: Unable to lock directory /var/lib/apt/lists/</strong><br/><strong>W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)</strong><br/><strong>W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)</strong></pre>
<p>However, if we run the same command with our <kbd>root</kbd> user, it executes successfully:</p>
<pre><strong>root@hobnob:# apt update</strong><br/><strong>Hit:1 https://repos.sonar.digitalocean.com/apt main InRelease</strong><br/><strong>...</strong><br/><strong>Hit:5 http://nyc2.mirrors.digitalocean.com/ubuntu bionic-backports InRelease</strong><br/><strong>Reading package lists... Done</strong><br/><strong>Building dependency tree </strong><br/><strong>Reading state information... Don</strong>e</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding a user to the sudo group</h1>
                </header>
            
            <article>
                
<p>If we are to use our <kbd>hobnob</kbd> account on a day-to-day basis, it would be annoying to have to switch to the <kbd>root</kbd> account every time we want to install something. Luckily, in Linux permissions can be assigned to each user, as well as to a named <em>group</em> of users. <span>Linux provides a </span><kbd>sudo</kbd><span> group, which allows users within that group to run commands requiring <kbd>root</kbd> privileges, simply by </span><span>prepending the command with the </span><kbd>sudo</kbd><span> keyword and providing their password. Therefore, we should add our <kbd>hobnob</kbd> user account to the <kbd>sudo</kbd> group.</span></p>
<p><span> </span>While still logged in as <kbd>root</kbd>, run the following command:</p>
<pre><strong>root@hobnob:# usermod -aG sudo hobnob</strong></pre>
<p>The<span> </span><kbd>-G</kbd><span> </span>option specifies the group we are adding the user to, and the<span> </span><kbd>-a</kbd><span> </span>flag appends the user to the group without removing them from other groups.</p>
<p>Now, try running <kbd>sudo apt update</kbd> from the <kbd>hobnob</kbd> account; it will prompt you for your password, and then it will execute the command as if you're the <kbd>root</kbd> user!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up public key authentication</h1>
                </header>
            
            <article>
                
<p>So far, we have been using password-based authentication to gain access to our server; this is cumbersome and insecure, as malicious parties can gain access to your server simply by guessing your password. It's better to use public key authentication, which has the following benefits:</p>
<ul>
<li>Infeasible to guess: Passwords tend to have a number of common patterns (for example,<span> </span><kbd>abcd1234</kbd><span> </span>or<span> </span><kbd>password</kbd>), whereas SSH keys look like gibberish and are hard to brute-force</li>
<li>Manageable: <kbd>ssh-agent</kbd><span> </span>is a program that holds private keys so that you don't have to remember your passwords</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checking for existing SSH key(s)</h1>
                </header>
            
            <article>
                
<p>Firstly, check whether you already have an SSH key pair set up on your local machine. Usually, the SSH keys are stored under a<span> </span><kbd>.ssh</kbd><span> </span>directory under your home directory:</p>
<pre><strong>$</strong> <strong>cd ~/.ssh/ &amp;&amp; ls -ahl</strong><br/><strong>total 116K</strong><br/><strong>drwx------  2 dli dli 4.0K Jul 10 10:39 .</strong><br/><strong>drwxr-xr-x 94 dli dli  16K Sep 12 18:59 ..</strong><br/><strong>-rw-r--r--  1 dli dli  151 Mar  6  2018 config</strong><br/><strong>-rw-------  1 dli dli 3.2K Oct  2  2017 id_rsa</strong><br/><strong>-rw-r--r--  1 dli dli  740 Oct  2  2017 id_rsa.pub</strong><br/><strong>-rw-r--r--  1 dli dli  80K Sep 12 19:08 known_hosts</strong></pre>
<p><span>If you see output similar to this, then you already have an SSH key and can skip ahead to the <em>Adding SSH key to remote server</em> section; otherwise, carry on with creating an SSH key.</span></p>
<div class="packt_tip">A key is basically a very long, random string that acts in place of your password. When you associate a key with a server, you're able to authenticate to that server using that key. Therefore, you may have multiple keys, each one associated with a different server.<br/>
<br/>
This also means that you can create a new key for this exercise, even if you have a key already. But generally, most developers have one key for each development machine.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an SSH key</h1>
                </header>
            
            <article>
                
<p>We will use a program called <kbd>ssh-keygen</kbd> to generate our SSH key. Run the following command:</p>
<pre><strong>$ ssh-keygen -t rsa -b 4096 -C &lt;your-email-address&gt;</strong></pre>
<p>Here, we are passing a few parameters to <kbd>ssh-keygen</kbd>, which instructs it to use the <strong>Rivest-Shamir-Adleman</strong> (<strong>RSA</strong>) cryptographic algorithm to generate key pairs of 4,096 bits in length. By default,<span> </span><kbd>ssh-keygen</kbd><span> </span>uses a key length of 2,048 bits, which should be sufficient, but since 4,096 is significantly harder to brute-force, why not enjoy that bit of extra security?</p>
<div class="packt_infobox">There are many algorithms that can be used to generate key pairs. <kbd>ssh-keygen</kbd> accepts <strong>DSA</strong>, <strong>RSA</strong>, <strong>Ed25519</strong>, and <strong>ECDSA</strong>.<br/>
<br/>
DSA is an old algorithm that is superseded by RSA, and should not be used. Ed25519 and <strong>Elliptic Curve Digital Signature Algorithm</strong> (<strong>ECDSA</strong>) are from a newer breed of cryptographic algorithms that rely on the mathematical properties of some very particular elliptical <em>curves</em>. They may potentially supersede RSA, as they can provide the same level of security but with shorter keys.<br/>
<br/>
You can use ECDSA in place of RSA by running <kbd>ssh-keygen -t ecdsa -b 521</kbd><span> </span>instead (note that<span> </span><kbd>521</kbd><span> </span>is<span> </span><em>not</em><span> </span>a typo), or Ed25519 by running<span> </span><kbd>ssh-keygen -t ed25519</kbd>.</div>
<p>After you execute the command, a wizard will ask you several questions:</p>
<ul>
<li><kbd>Enter file in which to save the key</kbd><span>:</span> By default, the keys will be saved under the <kbd>.ssh</kbd> directory in your home directory.</li>
<li><kbd>Enter passphrase</kbd>/<kbd>Enter same passphrase again</kbd><span>:</span> Anyone with access to your private key will be able to log in to your server. If you want extra security measures to protect your private key, you can set a password on it. Doing so means that only people who have your private key <em>and</em> your password are able to log in.</li>
</ul>
<div class="packt_infobox">Programs that run inside environments where user input is not possible may have to use an SSH key without a passphrase; otherwise, having a passphrase is recommended.</div>
<p class="mce-root">After you've answered those questions, <kbd>ssh-keygen</kbd> will <span>generate a private key (</span><kbd>id_rsa</kbd><span>)/public key (</span><kbd>id_rsa.pub</kbd><span>) pair and save them under the </span><kbd>~/.ssh</kbd><span> directory:</span></p>
<pre>Your identification has been saved in $HOME/.ssh/id_rsa.<br/>Your public key has been saved in $HOME/.ssh/id_rsa.pub.</pre>
<div class="packt_tip">If you do not set a passphrase on your private key, anyone with your private key is able to gain access to any servers that use the corresponding public key to authenticate you. Therefore, generally speaking,<span> </span><strong>never share your private key</strong>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding the SSH key to the remote server</h1>
                </header>
            
            <article>
                
<p>Now that we have an SSH key pair, we need to set up our virtual server to accept this key.</p>
<p>On your local machine, use the<span> </span><kbd>cat</kbd><span> </span>command to print out the content of your public key to the terminal and copy it to your clipboard (for example, using <em>Ctrl</em> + <em>Shift</em> + <em>C</em>):</p>
<pre><strong>$</strong> <strong>cat ~/.ssh/id_rsa.pub</strong><br/><strong>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC0TG9QcuUeFFtcXLqZZNO6/iggvuoLkQzlZQGbnSd39M+kLjRii+ziMBq8gL1pZUOBLWZUr6c+5DiCSOQCWtduTnHq6hR7/XkRthoS3bsdplr/6SHdxW/GTkVUjAv/DWcdJ93tx5ErkFsGsWklKM2U5wRMNA1g6k3ooc1N21zftBQKp9K+vrUW/iporjvy2Y8Dicp2VRUiOZIediDLYSZUXI/mc9eLziZivhsQtFYOZQSFMuBRBX7q4RA6XTBdnjORac1oVhVHi1NlU7ZmkWeJUECEFxncrYsp976p4tAKNOijpQMDhpKYdZT4OS83r33cIA2mdnNfK1SL1zntfoYYh+s3KODbnvoZcqCn4oar6ZPxgL9E4oqOF5Td+VQv8yRdxKstwQAgV6Yu0Ll/gJ0Z0k5xxw6SS3u/9J6Wx2q85eZLJl0o1dxHcofhQ1UZrJOZ23YnUsrDhHvqZRpHjkfXCPgDOWVNzdpTQPYbUttVuHsFw5HqjfVb5Pco4HlzhS4qCG91UkC7+tDMc6zXsaal9Sh4YIQE0RDDkRV3k3fFLYLMnxK4NCydPX9E9Fcaneopr+o1mauiNvdQLjALL4t8Bz8P0KSvfIGhu0suaQEIJamrdzPFcXigQn2IK719Ur8/0sxqbXAblzRauJ0qrYyvOXx3/1G+4VywN40MyY7xdQ== dan@danyll.com</strong></pre>
<div class="packt_tip">Alternatively, you can use <kbd>xclip</kbd> to copy the content of your public key directly to your clipboard.<br/>
<br/>
<kbd>$ xclip -selection clipboard &lt; ~/.ssh/id_rsa.pub<br/></kbd></div>
<p>Now, if you haven't done so already, log in to the remote server as <kbd>root</kbd> using your password. Next, create the <kbd>~/.ssh</kbd> directory and a <kbd>~/.ssh/authorized_keys</kbd> file, if they do not already exist. The <kbd>authorized_keys</kbd> file lists the keys that the server accepts as valid credentials:</p>
<pre><strong>root@hobnob:# mkdir ~/.ssh</strong><br/><strong>root@hobnob:#</strong> <strong>touch ~/.ssh/authorized_keys</strong></pre>
<p>Next, set the permissions on the file so that only the current user (<kbd>root</kbd>) can read the file:</p>
<pre><strong>root@hobnob:# chmod 700 ~/.ssh</strong><br/><strong>root@hobnob:#</strong> <strong>chmod 600 ~/.ssh/authorized_keys</strong></pre>
<p>Then, append the public key you just copied to the end of the <kbd>authorized_keys</kbd> file (for example, using <kbd>vim</kbd> or <kbd>nano</kbd>):</p>
<pre><strong>root@hobnob:# vim ~/.ssh/authorized_keys</strong></pre>
<p>Lastly, we need to reload the SSH daemon to ensure our changes are updated:</p>
<pre><strong>root@hobnob:#</strong> <strong>systemctl reload ssh.service</strong></pre>
<p>To test that this is working, open a new terminal window and run <kbd>ssh root@&lt;remote-ip&gt;</kbd>:</p>
<pre><strong>$ ssh root@142.93.241.63</strong><br/><strong>root@hobnob:#</strong> </pre>
<p>This time, the server doesn't ask for your password anymore, as it is using our SSH key to authenticate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using ssh-copy-id</h1>
                </header>
            
            <article>
                
<p>Next, we need to do the same for our <kbd>hobnob</kbd> user. But this time, we're going to use a handy command line tool, <kbd>ssh-copy-id</kbd>, which will do everything described previously, but with a single command:</p>
<pre><strong>$ ssh-copy-id hobnob@142.93.241.63</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Providing extra security</h1>
                </header>
            
            <article>
                
<p>Before we move on, there are a few additional measures we can take to make our setup more secure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Disable password-based authentication</h1>
                </header>
            
            <article>
                
<p><span>While we can now log in with our SSH key, we are still allowing logins via password. A chain is only as strong as its weakest link, and a system is only as secure as its least secure component. Therefore, now that we can log in using SSH, it's best to disable login via password.</span></p>
<div class="packt_tip">Double-check that you are able to log in to your server using your SSH key before disabling password-based authentication; otherwise, you'll be locked out of the server.</div>
<p>On the remote virtual server, open up the configuration file for the SSH daemon at<span> </span><kbd>/etc/ssh/sshd_config</kbd><span> (note that this is not the same as </span><kbd>/etc/ssh/ssh_config</kbd>, which is the configuration file for the <em>SSH client</em>). Search for an entry called <kbd>PasswordAuthentication</kbd> and set it to <kbd>no</kbd>:</p>
<pre>PasswordAuthentication <strong>no</strong></pre>
<p>Again, reload the SSH daemon to ensure that it is updated with our changes:</p>
<pre>root@hobnob:# <strong>systemctl reload ssh.service</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Disable root login</h1>
                </header>
            
            <article>
                
<p>We shouldn't stop there. Now that we have access to a user with <kbd>sudo</kbd> privileges, we don't need to log in as <kbd>root</kbd> anymore. Therefore, we should disable root login through another configuration entry in the <kbd>sshd_config</kbd>.</p>
<p>Find the <kbd>PermitRootLogin</kbd> entry and set that to <kbd>no</kbd>:</p>
<pre>PermitRootLogin <strong>no</strong></pre>
<p>Reload the SSH daemon to ensure that this change takes effect:</p>
<pre><strong>root@hobnob:# systemctl reload ssh.service</strong></pre>
<p>Now, from your local machine, try to log in as <kbd>root</kbd>; you should get an error:</p>
<pre><strong>$ ssh root@142.93.241.63</strong><br/><strong>Permission denied (publickey).</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Firewall</h1>
                </header>
            
            <article>
                
<p><span>The last step in securing our server is to install a firewall. The idea behind a firewall is that every exposed port is a potential security vulnerability. Therefore, we want to expose as few ports as possible.</span></p>
<p>All Linux distributions come with a firewall called <kbd>iptables</kbd>, which, <span>by default,</span> allows all traffic to pass through. Configuring <kbd>iptables</kbd> by hand can be challenging as the format is not the most intuitive. For example, an inactive <kbd>iptables</kbd> configuration looks like this:</p>
<pre><strong>$</strong> <strong>sudo iptables -L -n -v</strong><br/><strong>Chain INPUT (policy ACCEPT 0 packets, 0 bytes)</strong><br/><strong> pkts bytes target     prot opt in     out     source               destination         </strong><br/><br/><strong>Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)</strong><br/><strong> pkts bytes target     prot opt in     out     source               destination         </strong><br/><br/><strong>Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)</strong><br/><strong> pkts bytes target     prot opt in     out     source               destination         </strong></pre>
<p>To help system administrators to manage <span>the </span><kbd>iptables</kbd><span> firewall more easily, the </span>Ubuntu distribution comes with a command-line program called <kbd>ufw</kbd> (short for <strong>u</strong>ncomplicated <strong>f</strong>ire<strong>w</strong>all), which we will use here.</p>
<p><kbd>ufw</kbd> is inactive by default, but before we enable it, <span>let's add some rules for it to enforce:</span></p>
<pre><strong>hobnob@hobnob:$ sudo ufw status</strong><br/><strong>Status: inactive</strong></pre>
<p><span>The only port we need to expose right now is the one for SSH, which is port <kbd>22</kbd>. We can do this by adding individual ports directly:</span></p>
<pre><strong>hobnob@hobnob:$</strong> <strong>sudo ufw allow 22</strong></pre>
<p>However, there's an easier way: services may register their <em>profiles</em> with <kbd>ufw</kbd>, allowing <kbd>ufw</kbd> to manage their ports <em>by name</em>. You can view a list of registered applications by running <kbd>ufw app list</kbd>:</p>
<pre><strong>hobnob@hobnob:$ sudo ufw app list</strong><br/><strong>Available applications:</strong><br/><strong>  OpenSSH</strong></pre>
<p>Therefore, instead of specifying port <kbd>22</kbd>, we can specify the name of the application instead:</p>
<pre><strong>hobnob@hobnob:$ sudo ufw allow OpenSSH</strong><br/><strong>Rules updated</strong><br/><strong>Rules updated (v6)</strong></pre>
<p>Now the rules are in place, we can enable <kbd>ufw</kbd>:</p>
<pre><strong>hobnob@hobnob:$ sudo ufw enable</strong><br/><strong>Command may disrupt existing ssh connections. Proceed with operation (y|n)? y</strong><br/><strong>Firewall is active and enabled on system startup</strong></pre>
<p>Now, when we check again, only the OpenSSH port (<kbd>22</kbd>) is opened:</p>
<pre><strong>hobnob@hobnob:$ sudo ufw status</strong><br/><strong>Status: active</strong><br/><br/><strong>To            Action   From</strong><br/><strong>--            ------   ----</strong><br/><strong>OpenSSH       ALLOW    Anywhere</strong><br/><strong>OpenSSH (v6)  ALLOW    Anywhere (v6)</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the time zone</h1>
                </header>
            
            <article>
                
<p>Lastly, we should configure all our servers to use the UTC time zone. Using a single time zone prevents us from having to keep track of which server is on which time zone when accessing multiple servers at the same time:</p>
<pre><strong>hobnob@hobnob:$ sudo dpkg-reconfigure tzdata</strong></pre>
<p>After you have run the command, you'll be presented with the following screen. Use your up/down arrow keys to select <span class="packt_screen">None of the above</span>. Then, use your left/right arrow keys to select <span class="packt_screen">OK</span> and press <span class="packt_screen">Return</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/251432d7-cdcf-449c-a90e-1014c9317761.png" style="width:54.17em;height:32.75em;"/></div>
<p>On the next screen, select <span class="packt_screen">UTC</span>, which stands for <span><strong>Universal Time Coordinated</strong>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/018ffde0-6dcc-492d-a687-2fe0826387d2.png" style="width:53.33em;height:33.50em;"/></div>
<p>You should get a confirmation on your Terminal:</p>
<pre>Current default time zone: 'Etc/UTC'<br/>Local time is now:      Wed Sep 12 18:54:39 UTC 2018.<br/>Universal Time is now:  Wed Sep 12 18:54:39 UTC 2018.</pre>
<p>We have now set our time zone, but to ensure the clock is accurate, we need to perform an additional step to keep it in sync with the global NTP servers:</p>
<pre><strong>hobnob@hobnob:$ sudo apt update</strong><br/><strong>hobnob@hobnob:$ sudo apt install ntp</strong></pre>
<p>This will install and run the <kbd>ntp</kbd> daemon, which will automatically start when booting up, synchronize with these global NTP servers, and update the system's time if necessary.</p>
<p>Congratulations! You have now successfully set up and secured a VPS! We can now move on to deploying our API on it.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running our API</h1>
                </header>
            
            <article>
                
<p>Before we can run our API on the VPS, we need to install the software and libraries it depends on, which include Git, Node, yarn, the<strong> </strong><span><strong>Java </strong></span><strong><span>Development Kit</span></strong><span> (</span><strong><span>JDK</span></strong><span>),</span><span> </span>and Elasticsearch:</p>
<pre>hobnob@hobnob:$ <strong>curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -</strong><br/>hobnob@hobnob:$ <strong>echo "deb https://dl.yarnpkg.com/debian/ stable main" | sudo tee /etc/apt/sources.list.d/yarn.list</strong><br/>hobnob@hobnob:$ <strong>sudo apt update &amp;&amp; sudo apt install yarn git default-jdk</strong><br/>hobnob@hobnob:$ <strong>curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bash</strong><br/>hobnob@hobnob:$ <strong>echo 'JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"' | sudo tee --append /etc/environment &gt; /dev/null</strong><br/>hobnob@hobnob:$ <strong>cd &amp;&amp; wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.deb</strong><br/>hobnob@hobnob:$ <strong>sudo dpkg -i elasticsearch-6.3.2.deb</strong><br/>hobnob@hobnob:$ <strong>rm elasticsearch-6.3.2.deb</strong><br/>hobnob@hobnob:$ <strong>sudo systemctl start elasticsearch.service</strong><br/>hobnob@hobnob:$ <strong>sudo systemctl enable elasticsearch.service</strong></pre>
<p><span>To prevent complications with permissions, we will place our application code under the <kbd>/home/hobnob/</kbd> directory</span><span> and run it as the <kbd>hobnob</kbd> user. Therefore, create a new directory for our projects, clone our API repository from the remote repository, install the required version of Node.js, use <kbd>yarn</kbd> to install all dependencies, and serve the application:</span></p>
<pre><strong>hobnob@hobnob:$ cd &amp;&amp; mkdir projects &amp;&amp; cd projects</strong><br/><strong>hobnob@hobnob:$ git clone https://github.com/d4nyll/hobnob.git</strong><br/><strong>hobnob@hobnob:$ cd hobnob &amp;&amp; nvm install &amp;&amp; yarn</strong></pre>
<div class="packt_infobox"><span>If you want to place the API in a directory outside of the user's home directory, such as </span><kbd>/srv/</kbd><span> or </span><kbd>/var/www/</kbd><span>, then you can't use nvm, because nvm installs the Node.js binary under the installer's home directory. Instead, you'd need to install Node.js globally </span><span>using an npm package called <kbd>n</kbd> (</span><a href="https://github.com/tj/n">github.com/tj/n</a><span>).<br/>
<br/>
What you <em>absolutely must not</em> do is run the API as the <kbd>root</kbd> user, because it poses a huge security risk.</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Next, we need to set the correct environment variables. The settings in our <kbd>*.env.example</kbd> files should work out of the box, so we can just copy them:</p>
<pre><strong>hobnob@hobnob:$ cd env/</strong><br/><strong>hobnob@hobnob:$ cp .env.example .env<br/>hobnob@hobnob:$ cp test.env.example test.env</strong><br/><strong>hobnob@hobnob:$</strong> <strong>cd ../ &amp;&amp; yarn run serve</strong></pre>
<p>The site will now be running on the port we specified in our <kbd>.env</kbd> file, which is <kbd>8080</kbd>. To make it available externally, we must update our firewall to permit traffic going into port <kbd>8080</kbd>. Open up a new terminal and run the following:</p>
<pre><strong>hobnob@hobnob:$ sudo ufw allow 8080</strong></pre>
<p>In your browser, navigate to <kbd>http://&lt;vps-ip-address&gt;:8080/</kbd>, and you should see an error which says this:</p>
<pre>Cannot GET /</pre>
<p>This means that Express is working; the error response is correctly telling us that the endpoint does not exist. Feel free to play around with the deployed API. It should work the same way as it did before.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keeping our API alive with PM2</h1>
                </header>
            
            <article>
                
<p>We are running our Node.js process inside an ephemeral SSH session. When we log out, the host machine will kill any processes initiated during that session. Therefore, we need to come up with a way of keeping our process alive even after logging out.</p>
<p>Furthermore, no matter how good our code base is, or how complete our test plans are, in any application of significant size, there will be errors. Sometimes, these errors are fatal and crash the application. In these instances, we should log the error and notify the developers, but most importantly, we should restart the application as soon as it crashes.</p>
<p>Ubuntu provides the <kbd>upstart</kbd> daemon (<a href="http://upstart.ubuntu.com/">upstart.ubuntu.com</a>), which can monitor a service and respawn it if it dies unexpectedly. Likewise, there's a popular npm package called <kbd>forever</kbd> (<a href="https://github.com/foreverjs/forever">github.com/foreverjs/forever</a>), which does a similar job. However, I have found PM2 (<a href="http://pm2.keymetrics.io/">pm2.keymetrics.io</a>) to be the best process manager out there, so that's what we'll use in this book.</p>
<p class="mce-root"/>
<p>First, install PM2 as a development dependency:</p>
<pre><strong>$ yarn add pm2 --dev</strong></pre>
<p>Then, update our <kbd>serve</kbd> npm scripts to execute <kbd>pm2 start</kbd> instead of <kbd>node</kbd>:</p>
<pre>"serve": "yarn run build &amp;&amp; dotenv -e envs/.env <strong>pm2 start</strong> dist/index.js"</pre>
<p>Now, push these changes from your local machine and pull them into the virtual server. Run <kbd>yarn</kbd> again to install <kbd>pm2</kbd> and then run <kbd>yarn run serve</kbd>; now, our process is managed by PM2 and not our <kbd>hobnob</kbd> user. This means even if you log out or disconnect, our Node.js process would still continue to run:</p>
<pre><strong>hobnob@hobnob:$ yarn run serve</strong><br/><strong>...</strong><br/><strong>[PM2] Starting /home/hobnob/projects/hobnob/dist/index.js in fork_mode (1 instance)</strong><br/><strong>[PM2] Done.</strong><br/><strong>┌──────────┬────┬───────┬────────┬─────────┬────────┬─────┬─────────┐</strong><br/><strong>│ App name │ id │ pid   │ status │ restart │ uptime │ cpu │ mem     │</strong><br/><strong>├──────────┼────┼───────┼────────┼─────────┼────────┼─────┼─────────┤</strong><br/><strong>│ index    │  0 │ 15540 │ online │ 0       │ 0s     │ 1%  │ 21.9 MB │</strong><br/><strong>└──────────┴────┴───────┴────────┴─────────┴────────┴─────┴─────────┘</strong><br/><strong> Use `pm2 show &lt;id|name&gt;` to get more details about an app</strong></pre>
<p>The great thing about PM2 is that the user interface is fantastic for a CLI tool. If we run <kbd>npx pm2 monit</kbd>, you'll get a dashboard with all the running processes, and you can use the mouse to see the status, resource usage, and other statistics in real time:</p>
<pre><strong>┌─ Process list ────┐┌─ Global Logs ──────┐</strong><br/><strong>│[ 0] index         ││                    │</strong><br/><strong>└───────────────────┘└────────────────────┘</strong><br/><strong>┌─ Custom metrics  ─┐┌─ Metadata ─────────┐</strong><br/><strong>│ Loop delay      o ││ App Name     index │</strong><br/><strong>│ de-metrics)       ││ Restarts     0     │</strong><br/><strong>│                   ││ Uptime       10m   │</strong><br/><strong>└───────────────────┘└────────────────────┘</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Killing a process</h1>
                </header>
            
            <article>
                
<p>To see PM2 in action, we're going to kill our Node.js process manually, and see if PM2 will automatically restart it. We'll use the<span> </span><kbd>npx pm2 list</kbd> command, which lists all processes in a static table:</p>
<pre><strong>hobnob@hobnob:$ npx pm2 list</strong><br/><strong>┌───────┬────┬───────┬────────┬───┬────────┬─────┬─────────┐</strong><br/><strong>│ Name  │ id │ pid   │ status │ ↺ │ uptime │ cpu │ mem     │</strong><br/><strong>├───────┼────┼───────┼────────┼───┼────────┼─────┼─────────┤</strong><br/><strong>│ index │ 0  │ 15540 │ online │ 0 │ 20m    │ 0%  │ 40.8 MB │</strong><br/><strong>└───────┴────┴───────┴────────┴───┴────────┴─────┴─────────┘</strong><br/><strong>hobnob@hobnob:$ kill 15540</strong><br/><strong>hobnob@hobnob:$ npx pm2 list</strong><br/><strong>┌───────┬────┬───────┬────────┬───┬────────┬─────┬─────────┐</strong><br/><strong>│ Name  │ id │ pid   │ status │ ↺ │ uptime │ cpu │ mem     │</strong><br/><strong>├───────┼────┼───────┼────────┼───┼────────┼─────┼─────────┤</strong><br/><strong>│ index │ 0  │ 16323 │ online │ 1 │ 2s     │ 0%  │ 47.9 MB │</strong><br/>└───────┴────┴───────┴────────┴───┴────────┴─────┴─────────┘</pre>
<p>As you can see, <kbd>pm2</kbd> started a new process, with a different <strong>process ID</strong> (<strong>PID</strong>), once the old process died. The restart count has also increased to 1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keeping PM2 alive</h1>
                </header>
            
            <article>
                
<p>PM2 will keep applications running, as long as it is running itself. But if PM2 itself is terminated (for example, as a result of a reboot), then we must also configure PM2 to automatically restart. Very conveniently, PM2 provides a <kbd>startup</kbd> command, which outputs a script for you to run on your terminal:</p>
<pre><strong>hobnob@hobnob:$ npx pm2 startup</strong><br/><strong>[PM2] Init System found: systemd</strong><br/><strong>[PM2] To setup the Startup Script, copy/paste the following command:</strong><br/><strong>sudo env PATH=$PATH:/home/hobnob/.nvm/versions/node/v8.11.4/bin /home/hobnob/projects/hobnob/node_modules/pm2/bin/pm2 startup systemd -u hobnob --hp /home/hobnob</strong></pre>
<p class="mce-root"/>
<p>Run the script<span> to ensure PM2 starts on boot. Now, when you </span><span>log out of your terminal session, or when the application crashes unexpectedly, or even the whole machine restarts, you can be confident that your application will automatically restart as soon as possible.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running our API on port 80</h1>
                </header>
            
            <article>
                
<p>We are currently running our API server on port <kbd>8080</kbd>, whereas the standard port for HTTP requests is port <kbd>80</kbd>. It would be really inconvenient, and thus bad for user experience, to ask the consumers of our API to attach a port number to the URL for every request.</p>
<p>Therefore, let's change the port that Express is listening on from <kbd>8080</kbd> to <kbd>80</kbd> and see what happens. Change the <kbd>SERVER_PORT</kbd> environment variable to <kbd>80</kbd>:</p>
<pre>SERVER_PORT=<strong>80</strong></pre>
<p>Then, stop and delete the PM2 application, and run the <kbd>serve</kbd> script again. When we run it again, it will initially be successful:</p>
<pre><strong>hobnob@hobnob:$ npx pm2 delete 0; yarn run serve</strong><br/><strong>...</strong><br/><strong>[PM2] Done.</strong><br/><strong>┌───────┬──────┬────────┬───┬─────┬─────────┐</strong><br/><strong>│ Name  │ mode │ status │ ↺ │ cpu │ memory  │</strong><br/><strong>├───────┼──────┼────────┼───┼─────┼─────────┤</strong><br/><strong>│ index │ fork │ online │ 0 │ 0%  │ 16.9 MB │</strong><br/><strong>└───────┴──────┴────────┴───┴─────┴─────────┘</strong></pre>
<p>However, when we check its status again, PM2 will show you that the application has errored, and it has tried to restart it 15 times before giving up:</p>
<pre><strong>hobnob@hobnob:$ npx pm2 status</strong><br/><strong>┌───────┬──────┬─────────┬────┬─────┬────────┐</strong><br/><strong>│ Name  │ mode │ status  │ ↺  │ cpu │ memory │</strong><br/><strong>├───────┼──────┼─────────┼────┼─────┼────────┤</strong><br/><strong>│ index │ fork │ errored │ 15 │ 0%  │ 0 B    │</strong><br/><strong>└───────┴──────┴─────────┴────┴─────┴────────┘</strong></pre>
<p>We can use the <kbd>pm2 show &lt;name&gt;</kbd> <span>command </span>to get information about a particular process:</p>
<pre>hobnob@hobnob:$ <strong>npx pm2 show index</strong></pre>
<p>From the output, we can see that the errors emanating from the application are stored at <kbd>/home/hobnob/.pm2/logs/index-error.log</kbd>, so let's take a look at that to see what it says:</p>
<pre><strong>hobnob@hobnob:$ tail -n11 /home/hobnob/.pm2/logs/index-error.log</strong><br/><strong>Error: listen EACCES 0.0.0.0:80</strong><br/><strong>    at Object._errnoException (util.js:1031:13)</strong><br/>  <strong>  ...</strong></pre>
<p>The <kbd>EACCES 0.0.0.0:80</kbd> error means that our Node.js process does not have permission to access port <kbd>80</kbd>. This is because, in Linux, ports with numbers below <kbd>1024</kbd> are deemed <strong>privileged</strong>, which means they can only be bounded by processes initiated by the <kbd>root</kbd> user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Privileged ports</h1>
                </header>
            
            <article>
                
<p>Apart from being bad for developer experience, there's a more important reason why our API should be served on a <strong>privileged port</strong>: w<span>hen the consumers of our API send us their data, they need to trust that the information they sent is only handled by processes that were initiated by the server administrator (often <kbd>root</kbd>), and not by some malicious party.</span></p>
<p>Let's suppose a malicious party somehow managed to breach our server and got access to an ordinary user account. I<span>f we had set our API port to a non-privileged port, then th</span>at <span>malicious user could spawn a modified, rogue API service that binds to that port, and use it to </span><span>extract sensitive information, such as user passwords</span><span>. Now, any information sent by the client to this port would be exposed to the malicious party.</span></p>
<p>However, <span>privileged ports can only be bound by the <kbd>root</kbd> user, and so</span> the malicious user won't be able to carry out the attack anymore.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Possible solutions</h1>
                </header>
            
            <article>
                
<p>However, we do control the server, so how can we allow our API service to run on port <kbd>80</kbd>? There are a few solutions, which we will outline later, but to see them working, we should first disable port <kbd>8080</kbd> and enable port <kbd>80</kbd>:</p>
<pre><strong>hobnob@hobnob:$ sudo ufw allow 80</strong><br/><strong>hobnob@hobnob:$ sudo ufw delete allow 8080</strong></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running as root</h1>
                </header>
            
            <article>
                
<p>The most straightforward solution is to run our Node process as <kbd>root</kbd>; in other words, something akin to <kbd>sudo node src/index.js</kbd>. However, this is a very bad idea as it poses a big security risk. If someone were to find a bug or vulnerability in your application, he/she can exploit it, and because the server process is run as <kbd>root</kbd>, the hacker can potentially do everything the <kbd>root</kbd> user can do, including wiping your entire machine clean or stealing data. Running the API server as an ordinary user will limit any potential damage to what is normally permissible to that user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">De-escalating privileges</h1>
                </header>
            
            <article>
                
<p>There is a hack, however, which allows you to initiate the process as <kbd>root</kbd> using <kbd>sudo</kbd>, but de-escalate the privileges later by setting the user and group identity of the process to the user/group who issued the <kbd>sudo</kbd> command. We do this by using the environment variables <kbd>SUDO_UID</kbd> and <kbd>SUDO_GID</kbd>, and setting them using <kbd>process.setgid</kbd> and <kbd>process.setuid</kbd>:</p>
<pre>app.listen(process.env.SERVER_PORT, async () =&gt; {<br/>  <strong>const sudoGid = parseInt(process.env.SUDO_GID);</strong><br/><strong>  const sudoUid = parseInt(process.env.SUDO_UID);</strong><br/><strong>  if (sudoGid) { process.setuid(sudoGid) }</strong><br/><strong>  if (sudoUid) { process.setuid(sudoUid) }</strong><br/>  ...<br/>});</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting capabilities</h1>
                </header>
            
            <article>
                
<p>Another solution is to set <strong>capabilities</strong>.</p>
<p>On Linux, when a thread or process requires certain privilege(s) to perform an action, such as reading a file or binding to a port, it checks with a list of capabilities. If it has that capability, it'll be able to perform that function; otherwise, it can't. By default, the <kbd>root</kbd> user has all capabilities, for instance, the <kbd>CAP_CHOWN</kbd> capability, which allows it to change a file's UID and GID.</p>
<p>Therefore, rather than running the process as <kbd>root</kbd>, we can simply grant our Node process the capability of binding to privileged ports:</p>
<pre><strong>hobnob@hobnob:$ sudo setcap CAP_NET_BIND_SERVICE=+ep $(which node)</strong></pre>
<p>You can check that the capability is set for this process by running <kbd>getcap</kbd>:</p>
<pre><strong>hobnob@hobnob:$ sudo getcap $(which node)</strong><br/><strong>~/.nvm/versions/node/v8.9.0/bin/node = cap_net_bind_service+ep</strong></pre>
<p>Now, when we run <kbd>npx pm2 delete 0; yarn run serve</kbd>, it'll successfully bind to port <kbd>80</kbd>.</p>
<p>However, if we update our version of Node.js using nvm, we'd have to set the capabilities again for this new version of Node. Furthermore, this capability is not limited to binding to port <kbd>80</kbd>; it's for binding to <em>all</em> privileged ports. This is a potential security vulnerability. Therefore, it's best not to use this approach and we should unset the capabilities:</p>
<pre><strong>hobnob@hobnob:$ sudo setcap -r $(which node)</strong><br/><strong>hobnob@hobnob:$ sudo getcap $(which node)</strong><br/><strong>[No output]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using authbind</h1>
                </header>
            
            <article>
                
<p>Using <kbd>authbind</kbd> as an alternative may be preferable to setting capabilities. <kbd>authbind</kbd> is a system utility that allows users without superuser privileges to access privileged network services, including binding to privileged ports:</p>
<pre><strong>hobnob@hobnob:$ sudo apt install authbind</strong></pre>
<p>In contrast to setting capabilities, <kbd>authbind</kbd> allows more fine-grained control with regard to the port and permissions it is granting. Configuration files for <kbd>authbind</kbd> can be found at <kbd>/etc/authbind</kbd>. In short, if a user has permission to access the <kbd>/etc/authbind/byport/&lt;port&gt;</kbd><span> </span><span>file</span><span>, then that user is able to bind to that port:</span></p>
<pre><strong>hobnob@hobnob:$ sudo touch /etc/authbind/byport/80</strong><br/><strong>hobnob@hobnob:$ sudo chown hobnob /etc/authbind/byport/80</strong><br/><strong>hobnob@hobnob:$ sudo chmod 500 /etc/authbind/byport/80</strong></pre>
<p>Here, we are creating a configuration file for port <kbd>80</kbd>, changing its owner to be the user running the API server, and setting its permission so that only <kbd>hobnob</kbd> can read it. Now, we can run our start script with <kbd>authbind</kbd> and it should work:</p>
<pre><strong>hobnob@hobnob:$ npx pm2 delete 0; authbind --deep yarn run serve</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using iptables</h1>
                </header>
            
            <article>
                
<p>Another solution is to use <kbd>iptables</kbd>, which is the same firewall we used before. Apart from blocking out traffic from certain ports,<span> </span><kbd>iptables</kbd><span> </span>also allows you to redirect traffic from one port to another. Therefore, we can simply route all traffic entering port<span> </span><kbd>80</kbd><span> </span>to port<span> </span><kbd>8080</kbd>:</p>
<pre><strong>hobnob@hobnob:$</strong> <strong>sudo iptables -t nat -I PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using reverse proxy</h1>
                </header>
            
            <article>
                
<p>As you can appreciate, there are many ways of binding to port <kbd>80</kbd> as a non-root user, and our list is not even exhaustive! However, the most popular method is to use a <strong>reverse proxy</strong> server to redirect traffic from one port to another.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What's a proxy? What's a reverse proxy?</h1>
                </header>
            
            <article>
                
<p>A <strong>proxy</strong> is a server used by the client to indirectly access other servers. From the perspective of the server, it will view the proxy server as the client, and be oblivious to the original client. Proxy servers are the intermediary servers that your request passes through when it tries to get from your machine to the remote server.</p>
<p>A <strong>reverse proxy</strong> is the same, but the scheme is flipped. This is how a reverse proxy works:</p>
<ol>
<li>The reverse proxy receives a request</li>
<li>It relays the request to the proxied service (for example, an <span>application server, such as our Express application</span>)</li>
<li>It receives the response from the service</li>
<li>It sends the response back to the client(s)</li>
</ol>
<p><span>The client is oblivious to the fact that there's an internal service; in the client's view, the response came directly from the reverse proxy.</span></p>
<p>The most popular reverse proxy today is NGINX, and that's what we'll use in this book. NGINX is also a generic web server, which provides the following benefits:</p>
<ul>
<li>We can host multiple services on the same server; this provides greater <span>flexibility if we are to add extra services running on the same server later.</span></li>
<li>It can handle SSL encryption, which is required for setting up HTTPS.</li>
<li>It supports features such as caching and GZIP compression.</li>
<li>It can also act as a load balancer; this allows us to run multiple instances of our Node application, all on different ports, and have NGINX distribute the requests across these processes. It'll do so in a way that minimizes the load on any particular process, and thus maximizes the speed at which a response can be generated.</li>
<li>Configuration as code; since all HTTP traffic goes through NGINX, it's easy to see a list of all the services that we are exposing to the external world simply by reading NGINX's configurations.</li>
<li>It has an additional layer of abstraction; we can change how we structure the application internally, and all we have to do is update the NGINX settings. For example, we can have the service run on a different machine within a private network, and our external users would not know the difference.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up NGINX</h1>
                </header>
            
            <article>
                
<p><span>So let's get NGINX installed on our machine!</span></p>
<div class="packt_infobox"><span>We will outline the installation instructions for NGINX on Ubuntu. </span>Installation for other platforms can be found at <a href="https://www.nginx.com/resources/wiki/start/topics/tutorials/install/">nginx.com/resources/wiki/start/topics/tutorials/install/</a>. </div>
<p> By default, the <kbd>nginx</kbd> package should already be in Ubuntu's default repositories:</p>
<pre><strong>hobnob@hobnob:$ apt-cache show nginx</strong><br/><strong>Package: nginx</strong><br/><strong>Architecture: all</strong><br/><strong>Version: 1.14.0-0ubuntu1</strong><br/><strong>...</strong></pre>
<p>However, we should use the official NGINX repository to ensure we <em>always</em> get the most up-to-date version. To do this, we need to add NGINX's package repository to the list of repositories that Ubuntu will search for when it tries to download packages.</p>
<p>By default, there are two places that Ubuntu will search: inside the <kbd>/etc/apt/sources.list</kbd> file and inside files under the <kbd>/etc/apt/sources.list.d/</kbd> directory. We should not write directly to <span>the </span><kbd>/etc/apt/sources.list</kbd><span> file because when we upgrade our distribution, this file will be overwritten. Instead, we should create a new file with a unique name inside the <kbd>/etc/apt/sources.list.d/</kbd> directory, and add the entry for the NGINX repository:</span></p>
<pre><strong>hobnob@hobnob:$ echo "deb http://nginx.org/packages/ubuntu/ bionic nginx" | sudo tee -a /etc/apt/sources.list.d/nginx.list</strong><br/><strong>hobnob@hobnob:$ echo "deb-src http://nginx.org/packages/ubuntu/ bionic nginx" | sudo tee -a /etc/apt/sources.list.d/nginx.list</strong></pre>
<div class="packt_tip">If you ever delete your<span> </span><kbd>/etc/apt/sources.list</kbd><span> </span>file by accident, you can regenerate it using the Ubuntu Sources List Generator (<a href="https://repogen.simplylinux.ch/">repogen.simplylinux.ch</a>).</div>
<p>To ensure the integrity and authenticity of the package they download, t<span>he Ubuntu package management tools (<kbd>dpkg</kbd> and <kbd>apt</kbd>) require package distributors to sign their packages using a publicly available GPG key. Therefore, we must add this key to APT so that it knows how to check the integrity and authenticity of the packages:</span></p>
<pre><strong>hobnob@hobnob:$ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys ABF5BD827BD9BF62</strong><br/><strong>hobnob@hobnob:$ sudo apt update &amp;&amp; sudo apt install nginx</strong></pre>
<p>NGINX is now installed, but it is not yet running:</p>
<pre><strong>hobnob@hobnob:$ sudo systemctl status nginx.service</strong><br/><strong>● nginx.service - nginx - high performance web server</strong><br/><strong>   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)</strong><br/><strong>   Active: inactive (dead)</strong><br/><strong>     Docs: http://nginx.org/en/docs/</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring NGINX</h1>
                </header>
            
            <article>
                
<p>Before we start NGINX, we need to configure it. Like other system-wide services, configuration files for NGINX are stored under the <kbd>/etc/</kbd> directory. Navigate to <kbd>/etc/nginx/</kbd> and have a look at the files there:</p>
<pre><strong>hobnob@hobnob:$ cd /etc/nginx/</strong><br/><strong>hobnob@hobnob:$ ls</strong><br/><strong>conf.d fastcgi_params koi-utf koi-win mime.types modules nginx.conf scgi_params uwsgi_params win-utf</strong></pre>
<p>The main configuration is defined inside <kbd>nginx.conf</kbd>, which looks like this (once comments are removed):</p>
<pre>user nginx;<br/>worker_processes 1;<br/>error_log /var/log/nginx/error.log warn;<br/>pid /var/run/nginx.pid;<br/>events {<br/>    worker_connections 1024;<br/>}<br/>http {<br/>    include /etc/nginx/mime.types;<br/>    default_type application/octet-stream;<br/>    log_format main '$remote_addr - $remote_user [$time_local] "$request" '<br/>                      '$status $body_bytes_sent "$http_referer" '<br/>                      '"$http_user_agent" "$http_x_forwarded_for"';<br/>    access_log /var/log/nginx/access.log main;<br/>    sendfile on;<br/>    keepalive_timeout 65;<br/>    include /etc/nginx/conf.d/*.conf;<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding NGINX's configuration file</h1>
                </header>
            
            <article>
                
<p>The NGINX server is made up of <strong>modules</strong>, which are controlled by <strong>directives</strong> defined inside the <kbd>nginx.conf</kbd> configuration file. For instance, the HTTP module is configured using the <kbd>http</kbd> directive in <kbd>nginx.conf</kbd>. A directive is basically a unit of instruction/setting. There are two types of directives: <strong>simple</strong> and <strong>block</strong>.</p>
<p>A simple directive consists of a name and one or more parameters, each separated by a space and ending with a semicolon. <kbd>pid /var/run/nginx.pid;</kbd> would be an example of a simple directive. On the other hand, a block directive consists of a name followed by a pair of braces (<kbd>{}</kbd>), inside which it may contain additional directives.</p>
<p>There's also the concept of <strong>context</strong>. The top-level directives exist inside the <kbd>main</kbd> context. Each block directive envelops the contained directives in its own context. For example, in the <kbd>nginx.conf</kbd> file, the <kbd>worker_connections</kbd> directive will be within the <kbd>events</kbd> context, which is itself within the <kbd>main</kbd> context.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the HTTP module</h1>
                </header>
            
            <article>
                
<p>To allow NGINX to route requests for a given service, we must define a <kbd>server</kbd> block directive within the <kbd>http</kbd> context:</p>
<pre>http {<br/>    server {<br/>        ...<br/>    }<br/>}</pre>
<p>Within the <kbd>server</kbd> block, we can define certain directives that are only available in the <kbd>server</kbd> context. Here is a short list of the most common ones:</p>
<ul>
<li><kbd>listen</kbd>: Which port should this service be listening to. If this is not set, it'll default to port <kbd>80</kbd>.</li>
<li><kbd>server_name</kbd>: Which domain name(s) should apply to this server block.</li>
<li><kbd>location</kbd>: How it should process requests based on the URL path. The <kbd>location</kbd> directive usually has two parameters. The first parameter is the <strong>prefix</strong>, and the second is another block of directives that specify how that request should be handled. That inner block can have the following directives:
<ul>
<li><kbd>root</kbd>: Used for serving static files. It tells NGINX where it can find the requested resources on our server.</li>
<li><kbd>proxy_pass</kbd>: Used for reverse proxying. It tells NGINX the URL <span>to which </span>it should relay the request.</li>
</ul>
</li>
</ul>
<p>When NGINX receives a request that matches the server block's <kbd>listen</kbd> and <kbd>server_name</kbd> directives, it will pass it to the <kbd>server</kbd> block. Then, the path of the URL of the request would be extracted and it will try to match with the prefixes of each <kbd>location</kbd> directive. If it finds a match, the request will be processed in accordance with the directives specified within that <kbd>location</kbd> block. If there is more than one <kbd>location</kbd> prefix that matches the URL, the <kbd>location</kbd> block with the longest (and thus most specific) prefix will be used.</p>
<p>Open up <kbd>/etc/nginx/nginx.conf</kbd> and add the following server block to reverse proxy requests to our API server:</p>
<pre>...<br/>http {<br/>    ....<br/>    server {<br/>        <strong>listen 80 default_server;</strong><br/><strong>        location / {</strong><br/><strong>            proxy_pass http://localhost:8080;</strong><br/><strong>        }</strong><br/>    }<br/>}</pre>
<p>When NGINX receives a request at <kbd>http://142.93.241.63/</kbd>, the URL path (<kbd>/</kbd>) matches the prefix of the first <kbd>location</kbd> block. The <kbd>proxy_pass</kbd> directive then directs the request to our API, which would be running on port <kbd>8080</kbd>. NGINX will also relay the API's response back to the client.</p>
<p>So, let's revert our change to the <kbd>SERVER_PORT</kbd> environment variable by editing the <kbd>envs/.env</kbd><span> file:</span></p>
<pre>SERVER_PORT=<strong>8080</strong></pre>
<p><span>Then, start both our API server and the NGINX service, test our API on </span><kbd>http://142.93.241.63/</kbd><span>, and check that everything is still working:</span></p>
<pre>hobnob@hobnob:$ <strong>npx pm2 delete 0; yarn run serve</strong><br/>hobnob@hobnob:$ <strong>sudo systemctl reload nginx.service</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Splitting nginx.conf into multiple files</h1>
                </header>
            
            <article>
                
<p>However, writing directly to <kbd>/etc/nginx/nginx.conf</kbd><span> is not a good idea because if we upgrade NGINX, the <kbd>nginx.conf</kbd> file may get replaced. Also, if the server has to handle many services, the large number of </span>server blocks in the file will make it hard to read and maintain. Therefore, it's good practice to split configurations for different services into different files from the outset.</p>
<p>A common convention is to use two directories: <kbd>/etc/nginx/sites-available</kbd> and <kbd>/etc/nginx/sites-enabled</kbd>. You'd place the configuration for each service as separate files under the <kbd>sites-available</kbd> directory. Then, to enable a service, you'd create a <strong>symbolic link</strong> from the <kbd>sites-enabled</kbd> directory to a file in the <kbd>sites-available</kbd> directory. Lastly, you'd link the <kbd>/etc/nginx/sites-available</kbd> directory to the main configuration by adding an <kbd>include</kbd> entry in the configuration.</p>
<p>First, add the two directories:</p>
<pre>hobnob@hobnob:$ <strong>sudo mkdir /etc/nginx/sites-available /etc/nginx/sites-enabled</strong></pre>
<p>Then, in the <kbd>/etc/nginx/nginx.conf</kbd> file, add an <kbd>include</kbd> directive after <kbd>include /etc/nginx/conf.d/*.conf;</kbd>:</p>
<pre>...<br/>include /etc/nginx/conf.d/*.conf;<br/><strong>include /etc/nginx/sites-enabled/*;<br/></strong>...</pre>
<p>Then, pull out each <kbd>server</kbd> block from within the <kbd>http</kbd> context and place them, as separate files, inside <kbd>/etc/nginx/sites-available/</kbd>. By convention, the name of the file should correspond to the domain name, but since we don't have a domain yet, we can name it <kbd>api</kbd>.</p>
<p>Just to clarify, <kbd>/etc/nginx/sites-available/api</kbd> should be a file with the following content:</p>
<pre>server {<br/>    listen 80 default_server;<br/>    location / {<br/>        proxy_pass http://localhost:8080;<br/>    }<br/>}</pre>
<p>Now, to enable the sites, we must add to the<span> </span><kbd>/etc/nginx/sites-enabled</kbd><span> directory using a symbolic link:</span></p>
<pre><strong>hobnob@hobnob:$ sudo ln -s /etc/nginx/sites-available/api /etc/nginx/sites-enabled/</strong></pre>
<div class="packt_infobox">It's very important that you use the full, absolute path when creating symbolic links; otherwise, you may link to the wrong location.</div>
<p><span>An additional benefit to this approach is the separation of concerns:</span><span> generic configurations reside inside the </span><kbd>nginx.conf</kbd><span> file and site-specific settings (for example, SSL certificates) reside within their own files. Lastly, this is similar to how virtual hosts are set up on the Apache HTTP server; thus, adopting this approach would make it easier for administrators who are accustomed to the Apache HTTP server to migrate over.</span></p>
<p>Now, we need to reload the configuration once more:</p>
<pre><strong>hobnob@hobnob:$ sudo systemctl reload nginx.service</strong></pre>
<div class="packt_tip">If you want to learn more about NGINX, check out the NGINX documentation at <a href="http://nginx.org/en/docs/">nginx.org/en/docs/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">From IP to domain</h1>
                </header>
            
            <article>
                
<p>Right now, we can access our API using an IP address. But if we want developers to use our API, we shouldn't expect them to remember a random sequence of numbers! Instead, we want to give them an easy-to-remember domain name such as <kbd>api.hobnob.social</kbd>.</p>
<p>To do that, we must first purchase the domain name and then configure its Domain Name System (DNS) settings so that it will resolve to our server's IP address.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Buying a domain</h1>
                </header>
            
            <article>
                
<p>While the DNS is responsible for resolving domain names to IP addresses, a <strong>domain registrar</strong> is the entity/business that registers the domain(s) for you. There are many registrars available; the one we will be using is <strong>Namecheap</strong>.</p>
<p>First, we must search for the domain we want on the Namecheap website. Although a registrar is an entity that can register domain names for many TLDs, it must first check with one or more <strong>domain registries</strong> to see whether the domain name is available. Domain registries collectively hold a list of all domain names and their availability, and domain registrars are the ones who rent an available domain to you for a price.</p>
<p>Go to <a href="https://www.namecheap.com/">namecheap.com</a> and search for a domain you'd like to register (many are under US $1/year); we are going to use <kbd>hobnob.social</kbd>. Then, follow the onscreen instructions to complete the order.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding DNS</h1>
                </header>
            
            <article>
                
<p><span>We now have a domain name and a VPS, so it's time to associate them with each other. But first, we need to briefly explain how the DNS works.</span></p>
<div class="packt_infobox">The following overview is a simplification of the domain name resolution process. For brevity's sake, many details are left out. For a full interrogation of the process, please check out my blog post, <em>Resolving Domain Names</em>, which you can find at <a href="http://blog.danyll.com/resolving-domain-names/">blog.danyll.com/resolving-domain-names/</a>.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>The job of the DNS is to resolve <strong>fully qualified domain names</strong> (<strong>FQDNs</strong>) into IP addresses. When you type a URL in your browser, your computer would first look to resolve the IP locally by checking your <kbd>/etc/hosts</kbd></span><span> file. If it can't find it, it will pass the request on to a <strong>resolving nameserver</strong>, which is usually provided by your <strong>internet service provider</strong> (<strong>ISP</strong>). The resolving nameserver would first check its internal cache, and use the cached entry if available. If it cannot find an entry for your FQDN, it will query </span><span>one of the <strong>top-level domain</strong> (<strong>TLD</strong>) nameservers. </span><span>They will return the IP address of a <strong>domain-level nameserver</strong> (a.k.a. <strong>domain nameserver</strong> or <strong>authoritative nameserver</strong>), which is the nameserver that actually holds the <strong>zone file</strong> containing the DNS records (<kbd>A</kbd>, <kbd>CNAME</kbd>, <kbd>NS</kbd>, and so on) for that domain.</span></p>
<p><span>The domain nameserver for the domain is usually controlled by the registrar that registered the domain (Namecheap, in our example). </span><span>Finally, the domain nameserver will return the actual IP address of the FQDN to our resolving nameserver, which then relays that information back to us.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating the domain nameserver</h1>
                </header>
            
            <article>
                
<p>Therefore, to configure our domain name to resolve to our server's IP address, we need to update the zone file of the domain nameserver. At the moment, our domain is using Namecheap's domain nameserver, and we can update the zone file using <span>Namecheap's administrative UI.</span></p>
<p>However, this approach means we'd have to manage our servers using DigitalOcean, and our domain using Namecheap. It'd be easier if we can <span>carry out all the everyday administrative tasks using the same platform. Fortunately, DigitalOcean also has its own domain nameservers, which we can use.</span></p>
<p>Now, all we have to do is go on Namecheap's <span>administrative UI and update the TLD server to use DigitalOcean's domain nameserver, and use DO's administrative UI to update the zone file.</span></p>
<p class="mce-root CDPAlignLeft CDPAlign">Go to your Namecheap Dashboard (<a href="https://ap.www.namecheap.com/">ap.www.namecheap.com</a>) and select your domain. On the <span class="packt_screen">Domain</span> tab, there should be a section named <span class="packt_screen">Nameservers</span>. Select the <span class="packt_screen">Custom DNS</span> section and add in DigitalOcean's domain nameservers, which are <kbd>ns1.digitalocean.com</kbd>, <kbd>ns2.digitalocean.com</kbd>, and <kbd>ns3.digitalocean.com</kbd>. Then, m<span>ake sure you press the green tick to save your changes:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/6ff1b72e-62e3-4cdf-be44-340ac9a116fd.png" style="width:69.33em;height:36.17em;"/></div>
<p>Because resolving nameservers caches results, it may take up to 48 hours for our changes to be propagated to all nameservers. You can use services such as <a href="https://www.whatsmydns.net/">whatsmydns.net</a> to check the propagation progress for different nameservers around the world. Initially, you'll see that they all point to the original nameservers (<kbd>dns1.registrar-servers.com</kbd>), but after a few minutes, many of them have changed to use DigitalOcean servers (<kbd>nsx.digitalocean.com</kbd>):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/59452603-2ff7-4b48-b90f-a466692d4bee.jpg" style="width:34.00em;height:31.25em;"/></div>
<p>While we wait for our DNS changes to propagate, we can go to DigitalOcean and build our zone file using DigitalOcean's UI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building our zone file</h1>
                </header>
            
            <article>
                
<div>
<p><span>A zone file is a text file that describes a</span><span> </span><strong>DNS zone</strong><span>, which is any distinct, contiguous portion of the domain namespace that is managed by a single entity. In most cases, the boundaries of a DNS zone are confined to a single domain; thus, <em>for our purposes only</em>, a DNS zone is the same as a domain.</span></p>
</div>
<p><span>A zone file is made up of many <strong>records</strong>. Each record is a mapping between a <strong>hostname</strong> and a <strong>resource</strong>. </span><span>Let's use the DigitalOcean administrative UI to visualize these records and build our zone file.</span></p>
<div class="packt_tip"><span>We are using the administrative UI provided by DigitalOcean to manage our DNS settings. If you have chosen a different hosting provider, the UI may be different, but the principle remains the same. For example, Amazon Web Services (AWS) has an equivalent service called Route 53.</span></div>
<p><span>Make sure you're logged in to DigitalOcean's control panel, then go to the <span class="packt_screen">Networking</span> tab (<a href="https://cloud.digitalocean.com/networking/domains">cloud.digitalocean.com/networking/domains</a>). Under where it says <span class="packt_screen">Add a domain</span>, put in your domain name and click the <span class="packt_screen">Add Domain</span> button:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/db6adf3e-1d91-4ffa-ab74-8be1d6e8e06d.png" style="width:59.17em;height:10.00em;"/></div>
<p>Next, you'll be presented with a screen where we can add and update our records for the zone file of <kbd>hobnob.social</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f19f0768-bcdb-42ab-8cb9-eefcbd7fb14c.png" style="width:65.58em;height:41.67em;"/></div>
<p class="mce-root"/>
<p>The NS records have already been set for you, so let's talk about that first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NS records</h1>
                </header>
            
            <article>
                
<p>The NS records specify the domain nameservers used for resolving hostnames to IP addresses. You may ask why do zone files need an NS record at all? Because it basically references itself. This is because NS records may have changed, and other servers need to be updated with the IP/hostname of the new domain nameserver.</p>
<p>Previously, this was pointed at <kbd>dns1.registrar-servers.com</kbd>, and was cached at many resolving nameservers. When these resolving nameservers query <kbd>dns1.registrar-servers.com</kbd> for the IP of <kbd>hobnob.social</kbd>, they see that the NS record has been updated to <kbd>ns1.digitalocean.com</kbd> and send the request to DigitalOcean's domain nameservers instead.</p>
<p>We can use a program called <kbd>dig</kbd> to get the records from the zone file for our domain:</p>
<pre><strong>$ dig NS hobnob.social</strong><br/><strong>hobnob.social. 1799 IN NS ns1.digitalocean.com.</strong><br/><strong>hobnob.social. 1799 IN NS ns2.digitalocean.com.</strong><br/><strong>hobnob.social. 1799 IN NS ns3.digitalocean.com.</strong></pre>
<p>The first value is the domain; the second is the <strong>time-to-live</strong> (<strong>TTL</strong>) value, which is how long this record should be cached for in seconds. The third value, <kbd>IN</kbd>, stands for "internet," and will be present in almost all records. The fourth value, <kbd>NS</kbd>, indicates that this record should be treated as an NS record. Lastly, the last portion is the value of the record; in this case, it's the hostname of DigitalOcean's domain nameservers.</p>
<p>There are multiple NS records (and multiple domain nameservers) so that if and when one is down or overloaded, it can use the other domain nameservers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A and AAAA</h1>
                </header>
            
            <article>
                
<p>The next most important record types are the <kbd>A</kbd> and <kbd>AAAA</kbd> records, which map a hostname to an IP address. <kbd>A</kbd> maps the host to an IPv4 address, whereas an <kbd>AAAA</kbd> record maps it to an IPv6 address.</p>
<p>We want to point <kbd>api.hobnob.social</kbd> to the server that's running our server (<kbd>142.93.241.63</kbd>), so we need to create the following <kbd>A</kbd> record:</p>
<pre><strong>api    IN    A    142.93.241.63</strong></pre>
<p class="mce-root"/>
<p>We can also direct traffic going to <kbd>hobnob.social</kbd> to the same IP address. But instead of writing the full hostname (<kbd>hobnob.social</kbd>), we can replace it with the <kbd>@</kbd> symbol:</p>
<pre><strong>@    IN    A    142.93.241.63</strong></pre>
<div class="packt_infobox">There are two parameters you can set at the top of a zone file: <kbd>$ORIGIN</kbd><span> </span>and<span> </span><kbd>$TTL</kbd>. <kbd>$ORIGIN</kbd><span> </span>should be set to the DNS zone's highest level of authority, which, in most cases, is the domain name. The <kbd>$TTL</kbd> (time-to-live) parameter indicates how long this zone file should be cached for by nameservers.<br/>
<br/>
In our records, we can use the <kbd>@</kbd> symbol as a placeholder/substitute for the <kbd>$ORIGIN</kbd> parameter.<br/>
<br/>
Since these settings often don't need to be changed, DigitalOcean has set them for us, but not exposed them in the administrative UI.</div>
<p>Many domains also have a <strong>catch-all</strong> record that directs all traffic not specified with a record to an IP address:</p>
<pre><strong>*    IN    A    142.93.241.63</strong></pre>
<p>However, using a catch-all (<kbd>*</kbd>) is not a good practice because a malicious party can link to your domain using a sub-domain such as <kbd>scam.hobnob.social</kbd>. If we do not have a catch-all record, when Google crawls that link, it will receive an error saying that the host cannot be reached. However, if you have a catch-all record, the request will be directed to your server, and your web server may opt to serve the default server block. This may make <kbd>scam.hobnob.social</kbd> the top result when people search for <kbd>hobnob.social</kbd>, which is not ideal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Start of Authority (SOA)</h1>
                </header>
            
            <article>
                
<p>The last record you need to know is the SOA record, which is a mandatory record in all zone files, and is used to describe the zone and configure how often nameservers should update the zone file for this domain. It also has a version counter that ensures that only the latest version of the zone file is propagated:</p>
<pre>hobnob.social.  IN  SOA  ns1.digitalocean.com.  dan.danyll.com  ( &lt;serial&gt;, &lt;refresh&gt;, &lt;retry&gt;, &lt;expiry&gt;, &lt;negativeTTL&gt; )</pre>
<p>The first few values are similar to the ones in the NS records. The rest are as follows:</p>
<ul>
<li><kbd>ns1.digitalocean.com</kbd> is the <strong>primary master nameserver</strong>, which holds the most up-to-date zone file. There may be <strong>slave nameservers</strong> that mirror the primary nameserver to reduce its load.</li>
<li><kbd>dan.danyll.com</kbd> is the email for the administrator responsible for this DNS zone. The <kbd>@</kbd> symbol has been replaced by a period (<kbd>.</kbd>); if you have a period in your email address, it would be replaced by a backslash (<kbd>\</kbd>).</li>
<li><kbd>&lt;serial&gt;</kbd> is the serial number for the zone file, which is essentially a version counter. Every time your zone is updated, you should also increase the serial number by <kbd>1</kbd>. Slave nameservers will check this serial number to determine whether their own zone file is outdated.</li>
<li><kbd>&lt;refresh&gt;</kbd> is the amount of time a slave nameserver will wait before pinging the master server to see whether it needs to update its zone file.</li>
<li><kbd>&lt;retry&gt;</kbd> is the amount of time a slave nameserver will wait before pinging the master server again, if the previous connection attempt was unsuccessful.</li>
<li><kbd>&lt;expiry&gt;</kbd> is the amount of time that the zone file should still be deemed to be valid, even if it was no longer able to connect to the master server to update it.</li>
<li><kbd>&lt;negativeTTL&gt;</kbd> is the amount of time the nameserver will cache a lookup that failed.</li>
</ul>
<p>Again, since these values don't need to change often, and because having to manually update the serial number every time we update our zone file is tedious and error-prone, DigitalOcean has preset and hidden these values for us. DigitalOcean will update our SOA record for us when we update our records using DigitalOcean's web console.</p>
<p>Now, just make sure you have the A record set for the <kbd>api.hobnob.social</kbd> subdomain and move on to the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating NGINX</h1>
                </header>
            
            <article>
                
<p>Now that we have configured the DNS settings for our subdomain, we can update our NGINX configuration files to bear the name of our domain.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the <kbd>/etc/nginx/sites-available</kbd> and <span><kbd>/etc/nginx/sites-enabled</kbd> </span>directories, update the names of the files to the corresponding FQDN (without the trailing period):</p>
<pre><strong>hobnob@hobnob:$ cd /etc/nginx/sites-available/</strong><br/><strong>hobnob@hobnob:$ sudo mv api api.hobnob.social<br/>hobnob@hobnob:$ cd /etc/nginx/sites-enabled/</strong><br/><strong>hobnob@hobnob:$ sudo rm api</strong><br/><strong>hobnob@hobnob:$ sudo ln -s /etc/nginx/sites-available/api.hobnob.social \</strong><br/><strong> /etc/nginx/sites-enabled/</strong></pre>
<p>Lastly, update the configuration file to include a <kbd>server_name</kbd> directive. For example, the <kbd>api.hobnob.social</kbd> server block now looks like this:</p>
<pre>server {<br/>    listen 80 default_server;<br/>    <strong>server_name api.hobnob.social</strong><br/>    location / {<br/>        proxy_pass http://localhost:8080;<br/>    }<br/>}</pre>
<p>Now, reload our NGINX configuration to ensure that the changes take effect:</p>
<pre><strong>$ sudo systemctl reload nginx.service</strong></pre>
<p>Now, try sending a request to <kbd>api.hobnob.social</kbd>, and you should see the API server respond correctly!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have deployed our code to a VPS and exposed it to the external world—first through a static IP address, and later via a domain name.</p>
<p><span>In the next chapter, we are going to look into </span><strong>Continuous Integration</strong><span> (</span><strong>CI</strong><span>) and </span><strong>Continuous Deployment</strong><span> (</span><strong>CD</strong><span>) to see how we can automate the testing and deployment steps we've introduced in the last few chapters. You'll get the chance to work with <strong>Travis CI</strong> and <strong>Jenkins</strong>, a <strong>build automation</strong> tool.</span></p>
<p><span>Looking further ahead, in <a href="d336b08b-f67f-4789-8194-c65d7aa3decc.xhtml" target="_blank">Chapter 17</a>,<em> Migrating to Docker</em> and <a href="5d093a43-720f-4ea8-aac0-b64b13f96d12.xhtml" target="_blank">Chapter 18</a>,<em> Robust Infrastructure with Kubernetes</em>, </span><span>we will use <strong>Docker containers</strong> and <strong>Kubernetes</strong> to make our deployment more scalable and reliable.</span></p>


            </article>

            
        </section>
    </body></html>