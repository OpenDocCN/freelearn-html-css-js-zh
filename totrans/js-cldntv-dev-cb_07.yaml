- en: Optimizing Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring a cloud-native system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing custom metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring domain events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating alerts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating continuous synthetic transaction tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Confidence is crucial to maximize the potential of our lean and autonomous cloud-native
    services, because a crisis of confidence will stifle progress. Leveraging fully
    managed cloud services and following cloud-native design patterns to create autonomous
    services significantly increases team confidence. Decoupling deployment from release
    and shifting testing to the left, to create a streamlined continuous deployment
    pipeline, further increases team confidence. Yet, this is not enough. We need
    to shift testing to the right as well, all the way into production, so that we
    can monitor and alert the team about the status of the system. This gives teams
    confidence that they will have timely information so that they can minimize the
    mean time to recovery when errors do happen. The recipes in this chapter demonstrate
    how to optimize the observability of cloud-native services, alert about what matters,
    and continuously test in production to increase team confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring a cloud-native system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging fully managed cloud services is key to creating lean, cloud-native
    services, because embracing this disposable architecture empowers self-sufficient,
    full-stack teams to rapidly deliver with confidence based on the foundation provided
    by those cloud services. Team confidence is further increased because this foundation
    comes with good observability. This recipe demonstrates how to tap into cloud-provider
    metrics using a cloud-provider-agnostic, third-party monitoring service.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe you will need a Datadog account ([https://www.datadoghq.com](https://www.datadoghq.com)).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-datadog-account` directory with `cd cncb-datadog-account`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to **Datadog** and go to the **Integrations** page and select the AWS
    integration tile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Role Delegation, enter your AWS account ID and set the AWS role name
    to `DatadogAWSIntegrationRole`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the AWS external ID value and use it to update `sts:ExternalId` in  `serverless.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set tags to `account:cncb` and press Install Integration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the stack with `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the sample function multiple times with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Review the preset Datadog Lambda dashboard with Dashboards | Dashboard List
    | All Integrations | AWS Lambda.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you are finished with this chapter with `npm run rm:lcl
    -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud providers collect valuable metrics for their cloud services. However,
    they do not necessarily retain this data for extended periods, and the ability
    to slice and dice this data can be limited. Therefore, it is recommended to employ
    a third-party monitoring service to fill in the gaps and provide more comprehensive
    monitoring capabilities. Furthermore, in the eventuality of utilizing a polyglot
    cloud, a cloud-provider-agnostic monitoring service offers a unified monitoring
    experience. My monitoring service of choice is Datadog. This recipe shows how
    easily and quickly we can connect Datadog to an AWS account and start aggregating
    metrics to increase the observability of our cloud-native systems.
  prefs: []
  type: TYPE_NORMAL
- en: To allow Datadog to start collecting metrics from an AWS account, we must grant
    it permission to do so. As the *How to do it* section shows, this requires steps
    on the AWS side and the Datadog side. First, we deploy a stack to create `DatadogAWSIntegrationPolicy`
    with all the necessary permissions, and `DatadogAWSIntegrationRole` connects the
    AWS account with Datadog's AWS account. This last bit is important. Datadog runs
    in AWS as well. This means that we can use *Role Delegation* to connect the accounts
    instead of sharing access keys. Once `DatadogAWSIntegrationRole` is created, we
    can configure the AWS integration on the Datadog side, which has a prerequisite
    for the existence of the role. Datadog generates `ExternalId`, which we need to
    add to `DatadogAWSIntegrationRole` as a condition for assuming the role. With
    the integration in place, Datadog consumes the requested metrics from CloudWatch
    in your AWS account, so that they can be aggregated into meaningful dashboards,
    retained for historical analysis, and monitored to alert about conditions of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The metrics provided by value-added cloud services, such as *Function-as-a-service*,
    are a great starting point. Teams can put their cloud-native services into production
    with just these metrics with a reasonable level of confidence. However, more observability
    is almost always better. We need fine-grained details about the inner workings
    of our functions. This recipe demonstrates how to collect additional metrics,
    such as cold starts, memory, CPU utilization, and the latency of HTTP resources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-custom-metrics` directory with `cd cncb-custom-metrics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the endpoint shown in the stack output in the  following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Execute the service several more times and then review the Lambda dashboard
    in Datadog under Dashboards | Dashboard List | All Integrations | AWS Lambda.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explore the custom metrics in Datadog under Metrics *|* Explore with Graph:
    `hello.count` and `aws.lambda.handler.avg`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding custom metrics to a function works differently than traditional monitoring.
    The traditional approach involves adding an agent to each machine that collects
    metrics and periodically sends the data to the monitoring system. But with *Function-as-a-service,*
    there is no machine for us to deploy an agent on. An alternative is simply to send
    the collected metrics at the end of each function invocation. However, this adds
    significant latency to each function invocation. Datadog offers a unique alternative
    based on structured log statements. Counts, gauges, histograms, and checks are
    simply logged as they are collected and Datadog automatically consumes these statements
    from CloudWatch Logs.
  prefs: []
  type: TYPE_NORMAL
- en: The `serverless-datadog-metrics` library ([https://www.npmjs.com/package/serverless-datadog-metrics](https://www.npmjs.com/package/serverless-datadog-metrics)) 
    facilitates using this approach. We simply wrap the handler function with the
    `monitor` function and it will collect useful metrics, such as cold starts, errors,
    execution time, memory, and CPU utilization as well as the latency of HTTP resources.
    The HTTP metrics are very valuable. All HTTP calls to resources, such as DynamoDB,
    S3, and Kinesis, are automatically recorded so that we can see how much time a
    function spends waiting on its external resources.
  prefs: []
  type: TYPE_NORMAL
- en: This library also exports low-level functions, such as `count`, `gauge`, and
    `histogram`, to support additional custom metrics. The environment valuables,
    such as `ACCOUNT_NAME` and `SERVERLESS_PROJECT`, are used as tags for filtering
    metrics in dashboards and alerts.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring domain events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional systems, we typically focus on observing the behavior of synchronous
    requests. However, our cloud-native systems are highly asynchronous and event-driven.
    Therefore, we need to place equal or greater attention on the flow of domain events
    through the system so that we can determine when these flows deviate from the
    norm. This recipe demonstrates how to collect domain event metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need an AWS Kinesis Stream, such as the
    one created in the *Creating an event stream* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-event-metrics` directory with `cd cncb-event-metrics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the file named `serverless.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the stack: `npm run dp:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the `simulate` function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Explore the event metrics in Datadog under Metrics *|* Explore with Graph:
    `domain.event` and one graph per `type`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you are finished: `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring events works similarly to collecting events in the data lake. A single
    stream processor observes all events from all streams and simply counts the domain
    events by event `type`, along with additional tags, such as `region`, `stream`,
    and `source`. Again, these counts are recorded as structured log statements and
    Datadog consumes these statements from CloudWatch Logs. Graphing the domain event
    metrics in a dashboard can provide great insight into the behavior of a system.
    We will see how to alert about the flow of domain events in the *Creating alerts*
    recipe. We also perform special handling for `fault` events. For these events,
    we invoke the Datadog Event API, which provides for sending additional contextual
    information, such as a stack trace. We will discuss `fault` events in [Chapter
    8](5c400ff6-91da-4782-9369-549622d4a0d1.xhtml), *Designing for Failure*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating alerts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To maximize our confidence in our cloud-native services, we need to be alerted
    about issues ahead of the end users so that we can respond quickly and minimize
    the mean time to recovery. This also means that we need to eliminate alert fatigue
    and only alert on what really matters, otherwise important alerts will be lost
    in the noise. This recipe demonstrates how to create alerts on key metrics.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log in to your Datadog account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create an IAM alert with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Monitors | New Monitor | Event
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Match events from `Amazon Cloudtrail` over `aws-service:iam`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi Alert—`aws_account`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert threshold—`above 0`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Title—`aws.iam`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include trigger tags—`true`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Notify—`yourself`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create an iterator age alert with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Monitors | New Monitor | Integration | AWS Lambda
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Metric—`aws.lambda.iterator_age`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Max by—`functionname`, `region` and `account`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi Alert—`functionname`, `region` and `account`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert threshold—`7200000` (2 hrs)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning threshold—`1800000`  (0.5 hrs)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Title—`aws.lambda.iterator_age`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include trigger tags—`true`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Notify—`yourself`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a request rate alert with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Monitors | New Monitor | Anomaly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Metric—`aws.apigateway.count`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Average by—`apiname`, `region`, and `account` as `rate`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi Alert—`apiname`, `region`, and `account`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert conditions—start with the defaults and tune to your data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Title—`aws.apigateway.rate`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include trigger tags—`true`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Notify—`yourself`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a domain event rate alert with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Monitors | New Monitor | Anomaly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Metric—`domain.event`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Average by—`type`, `region`, and `account` as `rate`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi Alert—`type`, `region`, and `account`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert conditions—Start with the defaults and tune to your data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Title—`domain.event.rate`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include trigger tags—`true`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Notify—`yourself`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a Fault Event alert with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Monitors | New Monitor | Event
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Match events containing—`Fault Event` with status `error` from: `My Apps`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi Alert—`functionname`, `region`, and `account`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert threshold—`above 0`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Title—`fault.event`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include trigger tags—`true`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Notify—`yourself`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Datadog autocomplete menus are populated based on the metrics that have been
    recently collected.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that our system is observable, we need to do something proactive and useful
    with all this data. There is simply too much data to process manually and our
    confidence will only be increased if we can turn this data into valuable, timely
    information. We will certainly use this data for root-cause and post-mortem analysis,
    but our confidence is increased by our focus on mean time to recovery. Therefore,
    we need to create monitors that are constantly *testing* the data, turning it
    into information and alerting on what matters. However, we must be careful to
    avoid alert fatigue. The best practice is to alert liberally, but page judiciously
    on symptoms rather than causes. For example, we should create many monitors that
    only record that a threshold was crossed so that this additional information can
    be used in the root-cause analysis. Other monitors will email the team to warn
    of a potential problem, but a few select monitors will page the team to jump into
    immediate action.
  prefs: []
  type: TYPE_NORMAL
- en: To know the difference between a symptom and a cause, we categorize our metrics
    into work metrics and resource metrics. Work metrics represent the user-visible
    output of the system. Resource metrics represent the internal workings of the
    system. Our resource monitors will usually record and send warnings, and our work
    monitors will page the team. The RED method ([https://dzone.com/articles/red-method-for-prometheus-3-key-metrics-for-micros](https://dzone.com/articles/red-method-for-prometheus-3-key-metrics-for-micros))
    and the USE method ([https://dzone.com/articles/red-method-for-prometheus-3-key-metrics-for-micros](https://dzone.com/articles/red-method-for-prometheus-3-key-metrics-for-micros))
    break these categories down further. **RED** stands for **Rate, Error, and Duration**.
    When a critical service has a significant decrease in the rate of requests or
    events or a significant increase in errors, or latency significantly increases,
    then these may warrant paging the team. **USE** stands for **Utilization**, **Saturation**,
    and **Errors**. When the utilization of a resource, such as DynamoDB or Kinesis,
    reaches a certain level then it is probably good to warn the team. However, saturation
    and/or errors, such as throttling, may just warrant recording, because they may
    quickly subside or, if prolonged, they will trigger the work monitors.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe demonstrated a few possible monitors. The `fault` monitor represents
    work that is failing and must be addressed. The stream `iterator age` monitor
    straddles the line, because it could represent temporary resource saturation,
    or it could represent an error that is causing work to back up. Therefore, it
    has both a warning and an alert at different thresholds. The `anomaly detection`
    monitors should focus on work metrics, such as the rate of critical requests or
    domain events. It is also a good idea to monitor CloudTrail for any IAM changes,
    such as to roles and permissions.
  prefs: []
  type: TYPE_NORMAL
- en: The `notification` step is optional if you only need to record the condition.
    To warn the team, send the notification to chat and/or group email. To page the
    team, send the notification to an SNS topic. It is best to use the Multi Alert
    feature, triggered on pertinent tags, and include these in the notification title
    so that this information is available at a glance.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, to be valuable and to avoid fatigue, these monitors need care and
    feeding. These monitors are your tests in production. As your team's understanding
    of your system increases, then you will uncover better tests/monitors. When your
    monitors produce false positives, they will need to be tuned or eliminated. Your
    level of confidence is the true measure of successful monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Creating synthetic transaction tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a tree falls in a forest and no one is around to hear it, does it make a
    sound? Or more on topic, if a deployment is broken in a region and no one is awake
    to use it, does it make a difference? Of course, the answer is yes. We want to
    be alerted about the broken deployment so that it can be fixed before normal traffic
    begins. To enable this, we need to continuously pump synthetic traffic through
    the system so that there is a continuous signal to test. This recipe demonstrates
    how to generate synthetic traffic using cloud-provider-agnostic, third-party services.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need a Pingdom account ([https://www.pingdom.com](https://www.pingdom.com)).
    You will also need an AWS Cognito user pool, such as the one created in the *Creating
    a federated identity pool* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-synthetics` directory `cd cncb-synthetics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the file named `src/App.js` and update the `clientId` and `domain` fields
    with the values for the user pool stack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the app with `npm run build`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying a CloudFront distribution can take upward of 20 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Update the `callbackURLs` and `logoutURLs` of the user pool stack to include `WebsiteDistributionURL`
    and then redeploy it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Browse to `WebsiteDistributionURL` provided in the stack output to test the
    site configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to your Pingdom account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create an uptime check with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Experience Monitoring | Uptime | Add check.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name—`cncb-synthetics`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Check interval—`1 minute`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: URL—`WebsiteDistributionURL from your deploy output`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Review the Experience Monitoring *|* Uptime dashboard periodically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a real user monitor with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Experience Monitoring | Visitor Insights (RUM) | Add site.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name—`cncb-synthetics`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: URL—`WebsiteDistributionURL from your deploy output`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Review the file named `public/index.html`, uncomment the following code, and
    replace the ID with the value from the generated code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Build and redeploy the app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Review the Experience Monitoring | Visitor Insights (RUM) dashboard periodically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a Synthetic Transaction test with the following settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Experience Monitoring | Transactions | Add check
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name—`cncb-synthetics`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Test interval—`10 minutes`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Go to the URL `WebsiteDistributionURL from deploy output`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in the `username` field with `your-username`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in the `password` field with `your-password`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Click Sign In
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait for the element `.App-title` to contain `Welcome to React`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Review the Experience Monitoring *|* Transactions dashboard periodically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cancel your Pingdom free trial, under Account | Subscriptions | Manage Plan,
    before the 14 days expire, to avoid costs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing in production is different than traditional preproduction testing. As
    demonstrated in the *Creating alerts* recipe, our production tests are implemented
    as monitors that are constantly testing the signals emitted by the system. However,
    if there is no traffic, then there is no signal, and so no tests to alert us to
    problems in any deployments that happen during these periods. This, in turn, decreases
    our confidence in these deployments. The solution is to generate steady synthetic
    traffic to fill in the gaps when there is no natural traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Uptime checks are the simplest to put in place because they only make a single
    request. These should be included at a minimum, because they can be put in place
    quickly with little effort and because they have the highest frequency. **Real
    User Monitoring** (**RUM**) should be included because it only requires a simple
    code modification and because a significant amount of the user performance experience
    in cloud-native systems is executed in the browser by single-page applications.
    Finally, a small but strategic set of synthetic transaction scripts needs to be
    implemented to smoke test crucial features continuously. These scripts resemble
    traditional test scripts, but their focus is on continuously exercising these
    critical happy paths to ensure that the crucial features are unaffected by the
    continuous deployment of new features.
  prefs: []
  type: TYPE_NORMAL
