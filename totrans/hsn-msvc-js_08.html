<html><head></head><body>
		<div id="_idContainer085">
			<h1 class="chapter-number" id="_idParaDest-134"><a id="_idTextAnchor135"/>8</h1>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor136"/>Real-Time Data Streaming Using Microservices</h1>
			<p>Certain microservice applications, such as financial trading platforms and ride-hailing services, demand events to be produced and consumed with minimal latency. Real-time data streaming has become increasingly crucial in modern software development due to its ability to provide immediate, continuous insights and responses based on the most current data. This type of real-time data usage is particularly important in industries such as finance, healthcare, and logistics, where delays in data processing can lead to significant losses or even <span class="No-Break">life-threatening situations.</span></p>
			<p>Applications that rely on real-time data can offer a more responsive and interactive user experience. For example, social media platforms, online gaming, and live sports streaming rely on real-time data to keep users engaged and provide a <span class="No-Break">seamless experience.</span></p>
			<p>This chapter is all about real-time streaming with microservices. Our purpose is to understand when and how to establish such a type of communication when dealing <span class="No-Break">with microservices.</span></p>
			<p>This chapter covers <span class="No-Break">the following:</span></p>
			<ul>
				<li>What is <span class="No-Break">real-time streaming?</span></li>
				<li>Getting started with the earthquake <span class="No-Break">streaming API</span></li>
				<li>Implementing the earthquake <span class="No-Break">stream consumer</span></li>
			</ul>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor137"/>Technical requirements</h1>
			<p>To follow along with the chapter, you will need an IDE (we prefer Visual Studio Code), Postman, Docker, and a browser of <span class="No-Break">your choice.</span></p>
			<p>It is preferable to download our repository from <a href="https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript">https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript</a> and open the <strong class="source-inline">Ch08</strong> folder to easily follow along with the <span class="No-Break">code snippets.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor138"/>What is real-time streaming?</h1>
			<p><strong class="bold">Real-time streaming</strong> is a data-processing paradigm where data is continuously generated, transmitted, and processed as it is created, with minimal delay. Unlike batch processing, which collects and processes data in large groups or batches at regular intervals, real-time streaming focuses on the immediate and continuous flow of data, enabling instant analysis and<a id="_idIndexMarker609"/> response. It’s like watching a live stream instead of waiting for a video to <span class="No-Break">download entirely.</span></p>
			<p>Before we continue any further, let us look at some of the key characteristics of <span class="No-Break">real-time streaming:</span></p>
			<ul>
				<li><strong class="bold">Continuous data flow</strong>: Real-time streaming is like a never-ending flow of information coming in all at once from different places. This information can be from sensors, people <a id="_idIndexMarker610"/>using things online, money being bought and sold, such as Bitcoin, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">Low latency</strong>: The main goal of real-time streaming is to make the delay between information being created and it being used as short <span class="No-Break">as possible.</span></li>
				<li><strong class="bold">Event-driven processing</strong>: Real-time streaming works by following events as they happen, such as things being created or changing. Each event is dealt with on its own or in small batches, so the system can react right away to <span class="No-Break">new situations.</span></li>
				<li><strong class="bold">Scalability</strong>: Real-time streaming systems can handle different amounts and speeds of information, growing bigger or smaller depending on how much information is <span class="No-Break">coming in</span></li>
				<li><strong class="bold">Fault tolerance</strong>: To ensure continuous operation, real-time streaming systems incorporate fault tolerance mechanisms, such as data replication and automatic recovery from failures. As we mentioned in previous chapters, this is one of the important attributes of Apache Kafka, which we plan to also use for <span class="No-Break">this chapter.</span></li>
				<li><strong class="bold">Data consistency</strong>: Maintaining data consistency is important in real-time streaming, especially when processing brings multiple distributed components into the table. Techniques such as <strong class="bold">exactly-once processing</strong> and <strong class="bold">idempotency</strong> are employed to ensure accuracy. Exactly-once processing ensures that each message is processed only once, even in the case of failures or retries, preventing <a id="_idIndexMarker611"/>duplicates. As we use Apache Kafka for most chapters, you can easily configure idempotency and exactly-once behavior <span class="No-Break">in it.</span></li>
			</ul>
			<p>The importance of real-time data in modern applications cannot be overstated. In today’s data-driven world, the ability to process and act on data as it is generated provides a significant <span class="No-Break">competitive edge.</span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor139"/>Why real-time data is essential</h2>
			<p>Here are some key reasons<a id="_idIndexMarker612"/> why real-time data is a must-have for most <span class="No-Break">modern applications:</span></p>
			<ul>
				<li><strong class="bold">Enhanced decision-making</strong>: Real-time data can enhance an application’s decision-making abilities because of <span class="No-Break">the following:</span><ul><li><strong class="bold">Immediate insights</strong>: Real-time data provides immediate insights, allowing businesses to make informed decisions quickly. This is important in dynamic environments such as stock trading, where market conditions can <span class="No-Break">change rapidly.</span></li><li><strong class="bold">Proactive problem-solving</strong>: By continuously monitoring data, organizations can identify and address issues before they escalate, reducing downtime and enhancing <span class="No-Break">operational efficiency.</span></li></ul></li>
				<li><strong class="bold">Improved user experience</strong>: Real-time data empowers applications to provide a more dynamic and personalized user experience by enhancing interactivity and responsiveness while tailoring content to <span class="No-Break">individual preferences.</span></li>
				<li><strong class="bold">Operational efficiency</strong>: Organizations can significantly boost their efficiency by using real-time data, which enables both real-time monitoring and automation of processes, helping to streamline operations and <span class="No-Break">reduce costs.</span></li>
				<li><strong class="bold">Competitive advantage</strong>: Leveraging real-time data gives businesses a distinct edge by enhancing their agility and fostering innovation, allowing them to swiftly respond to market changes and create cutting-edge products <span class="No-Break">and services.</span></li>
				<li><strong class="bold">Increased revenue</strong>: Utilizing real-time data enables businesses to enhance their revenue streams through optimized marketing strategies and dynamic fraud detection, ensuring <a id="_idIndexMarker613"/>more effective customer engagement and <span class="No-Break">financial security.</span></li>
				<li><strong class="bold">Enhanced security</strong>: Real-time data strengthens security by enabling continuous monitoring and anomaly detection, allowing organizations to quickly identify and respond to potential threats and <span class="No-Break">system irregularities.</span></li>
				<li><strong class="bold">Scalability and flexibility</strong>: Real-time data systems provide the ability to efficiently handle large volumes of data while maintaining adaptability, ensuring optimal performance even as data loads and <span class="No-Break">requirements fluctuate.</span></li>
				<li><strong class="bold">Customer satisfaction</strong>: Real-time data enhances customer satisfaction by enabling instant support and immediate feedback, allowing businesses to quickly address concerns and continuously improve their products <span class="No-Break">and services.</span></li>
			</ul>
			<p>Real-time streaming allows data to be processed as it’s generated, offering immediate insights and responses. This continuous flow of data, coupled with low latency and event-driven processing, is crucial in industries such as finance, healthcare, and logistics. The ability to make real-time decisions, enhance user experiences, and improve operational efficiency provides businesses with a competitive edge, fostering innovation and increasing revenue while ensuring system scalability, fault tolerance, and <span class="No-Break">enhanced security.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor140"/>Understanding use cases</h2>
			<p>At the beginning of the <a id="_idIndexMarker614"/>chapter, I briefly mentioned that the use of real-time data can have incredible impacts on some industries. Therefore, it is crucial to understand the use cases of real-time data when you design your microservices. Let’s look at these use <span class="No-Break">cases here:</span></p>
			<ul>
				<li><strong class="bold">Financial services</strong>: Real-time data plays a pivotal role in this industry, enabling algorithmic stock trading with split-second decisions and supporting continuous risk management to ensure compliance and mitigate potential <span class="No-Break">financial threats.</span></li>
				<li><strong class="bold">Healthcare</strong>: Real-time data is transforming healthcare by enabling continuous patient monitoring for timely interventions and enhancing telemedicine through real-time video consultations <a id="_idIndexMarker615"/>and data sharing, improving patient care <span class="No-Break">and accessibility.</span></li>
				<li><strong class="bold">Retail and e-commerce</strong>: Real-time data enhances retail and e-commerce operations by optimizing inventory management to prevent shortages and enabling dynamic pricing strategies that adjust to demand and <span class="No-Break">competitor activity.</span></li>
				<li><strong class="bold">Transportation and logistics</strong>: Real-time data optimizes fleet management by improving route planning and delivery times, while real-time traffic data enhances traffic management, reducing congestion and improving <span class="No-Break">overall mobility.</span></li>
				<li><strong class="bold">Telecommunications</strong>: Real-time data enhances network management by ensuring continuous performance monitoring for optimal service quality, while also improving customer experience through the rapid resolution of <span class="No-Break">network issues</span></li>
			</ul>
			<p>In the end, what we can say for sure is that real-time data is a cornerstone of modern applications, driving enhanced decision-making, improved user experiences, operational efficiency, and competitive advantage. By leveraging real-time data, organizations can innovate, adapt, and thrive in a rapidly changing <span class="No-Break">digital landscape.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor141"/>Relationship between real-time streaming and microservices</h2>
			<p>Now that we have<a id="_idIndexMarker616"/> understood what real-time data is and exactly why it’s necessary, it’s time we understood the relationship between real-time streaming and microservices. The union of real-time streaming with microservices is a symbiotic one, which extends the power, productivity, and scalability of modern software architectures. Systems now, as a service, are more reactive, more adaptable, and faster as a result of this integration. Let’s try to understand how real-time <a id="_idIndexMarker617"/>streaming and microservices play well with <span class="No-Break">each other:</span></p>
			<ul>
				<li><strong class="bold">Decoupling and scalability</strong>: Real-time streaming complements microservices by promoting loose coupling and independent scaling, allowing services to communicate asynchronously and scale efficiently based <span class="No-Break">on demand.</span></li>
				<li><strong class="bold">Flexibility and agility</strong>: The combination of real-time streaming with microservices enhances flexibility and agility, enabling continuous service evolution and real-time data processing for applications requiring immediate insights and <span class="No-Break">rapid iteration.</span></li>
				<li><strong class="bold">Resilience and fault tolerance</strong>: Integrating real-time streaming with microservices enhances resilience and fault tolerance by isolating failures to individual services and ensuring data durability, allowing for seamless recovery and continuous operation even in the event of <span class="No-Break">service disruptions.</span></li>
				<li><strong class="bold">Real-time communication</strong>: Real-time streaming enhances communication within microservices by enabling event-driven architecture and immediate data propagation, allowing services to interact asynchronously and respond quickly to events, leading to more responsive and <span class="No-Break">synchronized systems.</span></li>
				<li><strong class="bold">Operational efficiency</strong>: Combining real-time streaming with microservices enhances operational efficiency by optimizing resource utilization and simplifying data pipelines, allowing continuous data flow and reducing the complexity of traditional <span class="No-Break">batch-processing methods.</span></li>
				<li><strong class="bold">Enhanced monitoring and analytics</strong>: Integrating real-time streaming with microservices enables real-time monitoring and analytics, offering immediate visibility into service performance and providing actionable insights that allow for the proactive management and dynamic optimization <span class="No-Break">of services.</span></li>
			</ul>
			<p>The synergy between real-time streaming and microservices offers a robust framework for building responsive, scalable, and efficient systems. By leveraging the strengths of both paradigms, organizations can create applications that are capable of handling dynamic workloads, providing real-time insights, and delivering superior user experiences. This <a id="_idIndexMarker618"/>combination is particularly powerful in environments where rapid data processing and immediate reactions are critical <span class="No-Break">to success.</span></p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor142"/>Microservices we will develop</h2>
			<p>To make our learning process more interactive and more understandable, we will develop two simple microservices. The first microservice will act as a producer of stream and the domain of this<a id="_idIndexMarker619"/> microservices will be an earthquake. An API that streams real-time information about earthquakes can be valuable for <span class="No-Break">several reasons:</span></p>
			<ul>
				<li><strong class="bold">Emergency response</strong>: Real-time data can be crucial for emergency responders who need to assess damage and deploy resources quickly after an earthquake. The API will provide information on the location, magnitude, and depth of the earthquake, which can help responders prioritize areas that may be <span class="No-Break">most affected.</span></li>
				<li><strong class="bold">Public awareness</strong>: The API could be used for public awareness to create applications that send alerts to people in affected areas. This could help people take shelter or evacuate <span class="No-Break">if necessary.</span></li>
				<li><strong class="bold">Research</strong>: Researchers can use the API to track earthquake activity and improve their understanding of earthquake patterns. This data can be used to develop better earthquake prediction models and improve <span class="No-Break">building codes.</span></li>
				<li><strong class="bold">News and media</strong>: News organizations can use the API to get real-time updates on earthquake activity, which can help them report on the <span class="No-Break">latest developments.</span></li>
			</ul>
			<p>In addition to these, there are commercial applications for such an API as well. For instance, insurance companies could use it to assess potential risks and losses, or engineering firms could use it to design <span class="No-Break">earthquake-resistant structures.</span></p>
			<p>Of course, when building such type of APIs for production, we will need to choose a reliable source of earthquake data; but to demonstrate the purpose and implementation of real-time data streaming, our API will act as a source <span class="No-Break">of truth.</span></p>
			<p>From a data format perspective, we should select a format for the data that is easy to use and integrate with other applications. Common formats include JSON and XML. Our choice <span class="No-Break">is JSON.</span></p>
			<p>By providing valuable<a id="_idIndexMarker620"/> and timely data, your earthquake API can be a useful tool for a variety <span class="No-Break">of users.</span></p>
			<p>The second microservice is going to be a consumer of the data. Throughout our learning process, we have implemented our microservices using different packages and frameworks with nearly full skeletons. For the current chapter, our focus is streaming rather than building an application skeleton from scratch. Our focus is not on implementing any architecture. You can refer to previous chapters if you want to add additional functionalities and make it a fully self-contained <span class="No-Break">architectural application.</span></p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor143"/>Getting started with an earthquake streaming API</h1>
			<p>In our GitHub repository, in the <strong class="source-inline">Ch08</strong> folder, we have two subfolders: <strong class="source-inline">earthquakeService</strong>, the earthquake streaming API, and <strong class="source-inline">earthquakeConsumer</strong>, the consumer API. As we mentioned before, our main focus is on implementing streaming. To make this <a id="_idIndexMarker621"/>chapter more focused on the topic, we haven’t implemented a proper detailed design for this API. This is also the case with the <span class="No-Break">consumer API.</span></p>
			<p>It is best to follow along by creating everything with us <span class="No-Break">from scratch.</span></p>
			<p><strong class="source-inline">earthquakeService</strong> has the <span class="No-Break">following dependencies:</span></p>
			<pre class="source-code">
"dependencies": {
    "dotenv": "^16.4.5",
    "express": "^4.19.2",
    "joi": "^17.13.1",
    "node-rdkafka": "^3.0.1"
  }</pre>			<p>First, you need to generate a <strong class="source-inline">package.json</strong> file that contains all dependencies. To create the file, run <strong class="source-inline">npm init</strong> and follow the prompts from the terminal. After <strong class="source-inline">package.json</strong> is created, run the <strong class="source-inline">npm install 'your_required_package_names'</strong> template command to install packages one by one. For example, to install the <strong class="source-inline">express</strong> package, just run <strong class="source-inline">npm install express</strong>, and hit <em class="italic">Enter</em>. We have already talked about <strong class="source-inline">package.json</strong> and the package installation process. You can check the previous chapters for more information. While we have reused some of the microservices from our previous chapter in our current chapter, we’re also going to use  <strong class="source-inline">node-rdkafka</strong> package which is new <span class="No-Break">for us.</span></p>
			<p><strong class="source-inline">node-rdkafka</strong> is a Node.js library that provides a wrapper around the native librdkafka library, enabling efficient communication with Apache Kafka for high-performance data streaming. It leverages the power of <strong class="source-inline">librdkafka</strong> for efficient communication with Kafka and handles complexities such as balancing writes and managing brokers, making Kafka interaction easier <span class="No-Break">for developers.</span></p>
			<p>You can install <strong class="source-inline">node-rdkafka</strong> <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">npm</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
npm install node-rdkafka</pre>			<p>It is not the only package to<a id="_idIndexMarker622"/> use for streaming, and depending on your personal preference, you can select any other one. The <strong class="source-inline">node-rdkafka</strong> package supports a really easy stream writing and reading process, which is why we prefer to use it in this chapter for <span class="No-Break">learning purposes.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should always try to use official packages for production apps. Using official packages helps keep your app safe because trusted developers manage them, and they are checked often. They are also more reliable, as they are tested, updated, and have good support, which is important for apps <span class="No-Break">in production.</span></p>
			<p>We use Apache Kafka as a streaming platform. So, you need Apache Kafka to be running. As before, we plan to use the <strong class="source-inline">docker-compose.yml</strong> file, which should be up and running with Apache Kafka. Our <strong class="source-inline">docker-compose.yml</strong> file for this example will only contain the services needed for Kafka, excluding unnecessary components like PostgreSQL to reduce resource usage. Of course, you can run the <strong class="source-inline">docker-compose.yml</strong> file from the previous chapters that use Apache Kafka, but having additional services will use up more resources on <span class="No-Break">your PC.</span></p>
			<p>Here is our <span class="No-Break"><strong class="source-inline">docker-compose.yml</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
services:
  zookeeper:
    image: bitnami/zookeeper:3.8
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/bitnami
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
  kafka1:
    image: bitnami/kafka:3.6
    volumes:
      - kafka_data1:/bitnami
    environment:
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_LISTENERS: INTERNAL://:9092,EXTERNAL://0.0.0.0:29092
      KAFKA_CFG_ADVERTISED_LISTENERS: INTERNAL://kafka1:9092,EXTERNAL://localhost:29092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: 'true'
      ALLOW_PLAINTEXT_LISTENER: 'yes'
    ports:
      - "9092:9092"
      - "29092:29092"
    depends_on:
      - zookeeper
      
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - 9100:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      - kafka1
volumes:
  zookeeper_data:
    driver: local
  kafka_data1:
    driver: local</pre>			<p>In this configuration, we define <strong class="source-inline">INTERNAL</strong> and <strong class="source-inline">EXTERNAL</strong> listeners to differentiate between connections within the Docker network (<strong class="source-inline">INTERNAL://kafka1:9092</strong>) and connections from outside the Docker network, such as your local machine (<strong class="source-inline">EXTERNAL://localhost:29092</strong>). This separation ensures that services within the Docker network can use the internal address, while external clients (like a Node.js app running on your host) can connect using the external port. By doing so, Kafka can properly advertise the correct addresses to different clients, avoiding connection issues caused by mismatched <span class="No-Break">listener configurations.</span></p>
			<p>This file contains Apache Kafka, the Kafka UI, and ZooKeeper. Just check our root folder (<strong class="source-inline">Ch08/earthquakeService</strong>) to find and run it. To run the <strong class="source-inline">docker-compose.yml</strong> file, first launch Docker <a id="_idIndexMarker623"/>Desktop, ensure it’s running, and then follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Pull and open <strong class="source-inline">Ch08</strong> from <span class="No-Break">the repository.</span></li>
				<li>Open the project from Visual Studio Code (or any text editor you prefer) and navigate <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">Ch08</strong></span><span class="No-Break">.</span></li>
				<li>If you use Visual Studio Code, then go to <strong class="bold">Terminal</strong> | <strong class="bold">New Terminal</strong> from the <strong class="bold">Menu</strong>; otherwise, use the command line to navigate to the <span class="No-Break">root folder.</span></li>
				<li>Run the <strong class="source-inline">docker-compose up -d</strong> command from the terminal (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer080">
					<img alt="Figure 8.1: Docker Desktop after running the docker-compose.yml file" src="image/B09148_08_001.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1: Docker Desktop after running the docker-compose.yml file</p>
			<p>To connect to Apache Kafka, we need to store the required configuration in a separate file. That is why we use the <strong class="source-inline">dotenv</strong> package to read configuration information. Create a <strong class="source-inline">configs</strong> folder under the<a id="_idIndexMarker624"/> root folder (<strong class="source-inline">Ch08/earthquake</strong>) and add a <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">env</strong></span><span class="No-Break"> file.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout"> The <strong class="source-inline">config</strong> and <strong class="source-inline">configs</strong> folders are separate and serve different purposes. Be sure to use the correct folder to avoid confusion. We store the <strong class="source-inline">.env</strong> file under the <strong class="source-inline">configs</strong> folder. On the other hand, we store the <strong class="source-inline">config.js</strong> file under the <strong class="source-inline">config</strong> folder, which loads environment variables using the <strong class="source-inline">dotenv</strong> package, validates them with <strong class="source-inline">Joi</strong>, and returns a configuration object for a Kafka-based microservice, throwing an error if <span class="No-Break">validation fails.</span></p>
			<p>Here is what the <strong class="source-inline">configs/.env</strong> file should <span class="No-Break">look like:</span></p>
			<pre class="source-code">
PORT=3001
#KAFKA Configuration
KAFKA_CLIENT_ID=earthquake-service
KAFKA_BROKERS=localhost:29092
KAFKA_TOPIC=earthquake-service-topic</pre>			<p>We have Kafka configuration such as client ID, brokers, and topic name with port information. As we learned before, all application source code lives under the <strong class="source-inline">src</strong> folder. Create the <strong class="source-inline">src</strong> folder <a id="_idIndexMarker625"/>on the same level as your <strong class="source-inline">configs</strong> folder (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer081">
					<img alt="Figure 8.2: General structure of earthquakeService" src="image/B09148_08_002.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2: General structure of earthquakeService</p>
			<p>We store configuration information in the <strong class="source-inline">.env</strong> file, but we need to add a reading and validating mechanism over our <strong class="source-inline">config</strong>. To implement proper reading and validating, we need to create a <strong class="source-inline">configs.js</strong> file under the <strong class="source-inline">src/configc</strong> folder. Here is what it <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
const dotenv = require('dotenv');
const Joi = require('joi');
const envVarsSchema = Joi.object()
    .keys({
        PORT: Joi.number().default(3000),
        KAFKA_CLIENT_ID: Joi.string().required(),
        KAFKA_BROKERS: Joi.string().required(),
        KAFKA_TOPIC: Joi.string().required()
    })
    .unknown();
function createConfig(configPath) {
    dotenv.config({ path: configPath });
    const { value: envVars, error } = envVarsSchema
        .prefs({ errors: { label: 'key' } })
        .validate(process.env);
    if (error) {
        throw new Error(`Config validation error:
          ${error.message}`);
    }
    return {
        port: envVars.PORT,
        kafka: {
            clientID: envVars.KAFKA_CLIENT_ID,
            brokers: envVars.KAFKA_BROKERS,
            topic: envVars.KAFKA_TOPIC
        }
    };
}
module.exports = {
    createConfig,
};</pre>			<p>We are using the same <strong class="source-inline">config</strong> read and validation mechanism as the account microservice. We have already explained this file in the <a href="B09148_07.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p>Our <strong class="source-inline">services</strong> folder is responsible for storing service files. To implement real-time streaming functionality, we need to <a id="_idIndexMarker626"/>create a new file called <strong class="source-inline">earthquake.js</strong> under the <strong class="source-inline">services</strong> folder. Here is what it <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
const Kafka = require('node-rdkafka');
const { createConfig } = require('../config/config');
const path = require('path');
class EarthquakeEventProducer {
constructor() {
        this.intervalId = null;
    }
    #generateEarthquakeEvent() {
        return {
            id: Math.random().toString(36).substring(2,
              15),
            magnitude: Math.random() * 9, // Random magnitude between 0 and 9
            location: {
                latitude: Math.random() * 180 - 90, // Random latitude between -90 and 90
                longitude: Math.random() * 360 - 180, // Random longitude between -180 and 180
            },
            timestamp: Date.now(),
        };
    }
……..</pre>			<p>This code defines a class called <strong class="source-inline">EarthquakeEventProducer</strong> that simulates generating and publishing <a id="_idIndexMarker627"/>earthquake event data to a Kafka topic. Let’s walk through the code’s <span class="No-Break">elements here:</span></p>
			<ul>
				<li><strong class="source-inline">require('node-rdkafka')</strong>: Imports the <strong class="source-inline">node-rdkafka</strong> library for interacting with a <span class="No-Break">Kafka cluster.</span></li>
				<li><strong class="source-inline">require('../config/config')</strong>: Imports a function (likely from <strong class="source-inline">../config/config.js</strong>) that reads configuration settings from <span class="No-Break">a file.</span></li>
				<li><strong class="source-inline">require('path'):</strong> Imports the <strong class="source-inline">path</strong> module for file <span class="No-Break">path manipulation.</span></li>
				<li><strong class="source-inline">The EarthquakeEventProducer class</strong>: This class handles earthquake event generation <span class="No-Break">and publishing.</span></li>
				<li><strong class="source-inline">#generateEarthquakeEvent()</strong>: This private method generates a simulated earthquake event object with the <span class="No-Break">following properties:</span><ul><li><strong class="source-inline">id</strong>: A random unique <span class="No-Break">identifier string.</span></li><li><strong class="source-inline">magnitude</strong>: A random floating-point number between <strong class="source-inline">0</strong> and <strong class="source-inline">9</strong> representing the <span class="No-Break">earthquake’s magnitude</span></li><li><strong class="source-inline">location</strong>: An object containing <span class="No-Break">the following:</span></li><li><strong class="source-inline">latitude</strong>: A random floating-point number between <strong class="source-inline">-90</strong> and <strong class="source-inline">90</strong> representing <span class="No-Break">the latitude.</span></li><li><strong class="source-inline">longitude</strong>: A random floating-point number between <strong class="source-inline">-180</strong> and <strong class="source-inline">180</strong> representing <span class="No-Break">the longitude.</span></li><li><strong class="source-inline">timestamp</strong>: The current timestamp <span class="No-Break">in milliseconds.</span></li></ul></li>
			</ul>
			<p>Here is how we specify our <a id="_idIndexMarker628"/>main method <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">runEarthquake</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
    async runEarthquake() {
        const configPath = path.join(__dirname,
          '../../configs/.env');
        const appConfig = createConfig(configPath);
        // Returns a new writable stream
        const stream = Kafka.Producer.createWriteStream({
            'metadata.broker.list':
              appConfig.kafka.brokers,
            'client.id': appConfig.kafka.clientID
        }, {}, {
            topic: appConfig.kafka.topic
        });
        // To make our stream durable we listen to this event
        stream.on('error', (err) =&gt; {
            console.error('Error in our kafka stream');
            console.error(err);
        });
       this.intervalId  = setInterval(async () =&gt; {
            const event =
              await this.#generateEarthquakeEvent();
            // Writes a message to the stream
            const queuedSuccess = stream.write(Buffer.from(
              JSON.stringify(event)));
            if (queuedSuccess) {
                console.log('The message has been queued!');
            } else {
                // If the stream's queue is full
                console.log('Too many messages in queue already');
            }
        }, 100);
    }</pre>			<p>Let’s break this code <span class="No-Break">down here:</span></p>
			<ul>
				<li><strong class="source-inline">runEarthquake()</strong>: This async method is responsible for setting up the Kafka producer and publishing <a id="_idIndexMarker629"/><span class="No-Break">earthquake events.</span></li>
				<li><strong class="source-inline">configPath</strong>: This constructs the path to the configuration file <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">path.join</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">appConfig</strong>: This reads configuration from the file using the imported <span class="No-Break"><strong class="source-inline">createConfig</strong></span><span class="No-Break"> function.</span></li>
				<li><strong class="source-inline">stream</strong>: This creates a Kafka producer write stream using <strong class="source-inline">Kafka.Producer.createWriteStream</strong>. The configuration includes <span class="No-Break">the following:</span><ul><li><strong class="source-inline">'metadata.broker.list'</strong>: A comma-separated list of Kafka broker addresses from <span class="No-Break">the configuration</span></li><li><strong class="source-inline">'client.id'</strong>: A unique identifier for this producer client from <span class="No-Break">the configuration</span></li><li><strong class="source-inline">Topic</strong>: The exact topic that should get the <span class="No-Break">streamed data</span></li></ul></li>
				<li><strong class="source-inline">stream.on('error')</strong>: This attaches an event listener for errors in the Kafka stream. It logs the error message to <span class="No-Break">the console.</span></li>
				<li><strong class="source-inline">setInterval</strong>: This sets up an interval timer to generate and publish events every 100 milliseconds (adjustable). Inside the interval callback is <span class="No-Break">the following:</span><ul><li><strong class="source-inline">event</strong>: Generates a new earthquake event object <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">#generateEarthquakeEvent</strong></span></li><li><strong class="source-inline">stream.write</strong>: Attempts to write the event data (converted to a buffer using <strong class="source-inline">JSON.stringify</strong>) to the <span class="No-Break">Kafka stream</span></li><li><strong class="source-inline">queuedSuccess</strong>: Checks the return value <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">stream.write</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">true</strong>: Indicates successful queuing of the message. A success message is logged to <span class="No-Break">the console.</span></li><li><strong class="source-inline">false</strong>: Indicates the stream’s queue is full. A message about exceeding the queue capacity is logged to <span class="No-Break">the console.</span></li></ul></li></ul></li>
			</ul>
			<p>In order to stop our earthquake service, we need to clear <span class="No-Break">the interval:</span></p>
			<pre class="source-code">
stopEarthquake() {
        if (this.intervalId) {
            clearInterval(this.intervalId);
            this.intervalId = null;
            console.log('Earthquake event stream stopped.');
        } else {
            console.log('No running earthquake event stream to stop.');
        }
    }
module.exports = EarthquakeEventProducer;</pre>			<p>The <strong class="source-inline">stopEarthquake()</strong> method stops the ongoing earthquake event stream by checking whether there is an active interval running, indicated by the presence of <strong class="source-inline">this.intervalId</strong>. If the interval exists, it uses <strong class="source-inline">clearInterval()</strong> to stop the event generation and resets <strong class="source-inline">this.intervalId</strong> to <strong class="source-inline">null</strong> to indicate that the stream has stopped. A success message is logged when the stream is stopped. If no interval is running (i.e., <strong class="source-inline">this.intervalId</strong> is <strong class="source-inline">null</strong>), it logs a message saying there’s no active stream to stop. This ensures that the function can only stop an existing stream and won’t attempt to stop a <span class="No-Break">non-existent one.</span></p>
			<p>In the end, this code simulates <a id="_idIndexMarker630"/>earthquake event generation and publishes these events to a Kafka topic at regular intervals, demonstrating basic Kafka producer usage with error handling <span class="No-Break">and logging.</span></p>
			<p>We plan to launch streaming using an API, but to make things as simple as possible, we use a minimal API approach that doesn’t require us to create controllers. This behavior is implemented in the <strong class="source-inline">app.js</strong> file. Here is <span class="No-Break">the file:</span></p>
			<pre class="source-code">
const express = require('express');
const EarthquakeEventProducer = require('./services/earthquake');
const app = express();
const earthquakeProducer = new EarthquakeEventProducer();
// Function to run streaming
app.post('/earthquake-events/start', async (req, res) =&gt; {
    earthquakeProducer.runEarthquake();
    res.status(200).send('Earthquake event stream started');
});
// Stop the earthquake event stream
app.post('/earthquake-events/stop', (req, res) =&gt; {
    earthquakeProducer.stopEarthquake();
    res.status(200).send('Earthquake event stream stopped');
});
module.exports = app;</pre>			<p>The code defines two API <a id="_idIndexMarker631"/>endpoints using Express.js to start and stop an earthquake event stream. The <strong class="source-inline">/earthquake-events/start </strong>endpoint triggers the <strong class="source-inline">runEarthquake()</strong> function from the <strong class="source-inline">EarthquakeEventProducer</strong> class, starting the event stream, and responds with a success message. The <strong class="source-inline">/earthquake-events/stop</strong> endpoint calls the <strong class="source-inline">stopEarthquake()</strong> function to stop the event stream and also responds with a success message. The <strong class="source-inline">earthquakeProducer</strong> object is an instance of the <strong class="source-inline">EarthquakeEventProducer</strong> class, which manages the event stream operations. Finally, the Express app is exported to be used in other parts of the application. This setup allows external clients, such as Postman, to control the Kafka event stream through <span class="No-Break">API calls.</span></p>
			<p>In an Express.js application, the <strong class="source-inline">index.js</strong> file in the root directory typically serves as the entry point for your server. It acts as the central hub where you configure and launch your Express app. Here is our <span class="No-Break"><strong class="source-inline">index.js</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
const path = require('path');
const app = require('./app');
const { createConfig } = require('./config/config');
async function execute() {
    const configPath = path.join(__dirname, '../configs/.env');
    const appConfig = createConfig(configPath);
    const server = app.listen(appConfig.port, () =&gt; {
        console.log('earthquake service started',
          { port: appConfig.port });
    });
    const closeServer = () =&gt; {
        if (server) {
            server.close(() =&gt; {
                console.log('server closed');
                process.exit(1);
            });
        } else {
            process.exit(1);
        }
    };
    const unexpectedError = (error) =&gt; {
        console.log('unhandled error', { error });
        closeServer();
    };
    process.on('uncaughtException', unexpectedError);
    process.on('unhandledRejection', unexpectedError);
}
execute();</pre>			<p>We have the following functionalities in the <span class="No-Break"><strong class="source-inline">index.js</strong></span><span class="No-Break"> file:</span></p>
			<ul>
				<li>Imports the Express app (<strong class="source-inline">app.js</strong>) and configuration <span class="No-Break">function (</span><span class="No-Break"><strong class="source-inline">config.js</strong></span><span class="No-Break">).</span></li>
				<li>Reads configuration from a file <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">createConfig</strong></span><span class="No-Break">.</span></li>
				<li>Starts the server using <strong class="source-inline">app.listen</strong> on the configured port and logs <span class="No-Break">a message.</span></li>
				<li>Defines functions to<a id="_idIndexMarker632"/> gracefully close the server and handle <span class="No-Break">unexpected errors.</span></li>
				<li>Attaches event listeners for uncaught exceptions and unhandled promise rejections, calling the <span class="No-Break">error-handler function.</span></li>
				<li>Finally, calls the <strong class="source-inline">execute</strong> function to <span class="No-Break">start everything.</span></li>
			</ul>
			<p>We have implemented our <strong class="source-inline">earthquakeService</strong>; now it is time to test it. Here’s how you can <span class="No-Break">do that:</span></p>
			<ol>
				<li>Open <strong class="bold">Terminal</strong> | <strong class="bold">New Terminal</strong> from the menu if you’re using Visual <span class="No-Break">Studio Code.</span></li>
				<li>Navigate to the <span class="No-Break"><strong class="source-inline">src</strong></span><span class="No-Break"> folder.</span></li>
				<li>Run the <strong class="source-inline">node </strong><span class="No-Break"><strong class="source-inline">index.js</strong></span><span class="No-Break"> command:</span><pre class="source-code">
<strong class="bold">PS C:\packtGit\Hands-on-Microservices-with-JavaScript\Ch08\earthquakeService\src&gt; node index.js</strong>
<strong class="bold">Debugger listening on ws://127.0.0.1:61042/876d7d9e-3292-482a-b011-e6c2d66e7615</strong>
<strong class="bold">For help, see: https://nodejs.org/en/docs/inspector</strong>
<strong class="bold">Debugger attached.</strong>
<strong class="bold">earthquake service started { port: 3001 }</strong></pre></li>				<li>Open Postman and send a POST request <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">http://localhost:3001/earthquake-events/start</strong></span><span class="No-Break">.</span></li>
				<li>To stop streaming, open Postman and send a POST request to <strong class="source-inline">http://localhost:3001/earthquake-events/stop</strong> (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer082">
					<img alt="Figure 8.3: Stopping event streaming" src="image/B09148_08_003.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3: Stopping event streaming</p>
			<p class="list-inset">The topic should <a id="_idIndexMarker633"/>automatically be created with some events (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer083">
					<img alt="Figure 8.4: Apache Kafka event after streaming" src="image/B09148_08_004.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4: Apache Kafka event after streaming</p>
			<p>We have implemented the streaming API. Now it is time to <span class="No-Break">consume data.</span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor144"/>Implementing the earthquake stream consumer</h1>
			<p>Producing is not very<a id="_idIndexMarker634"/> valuable if you don’t have a consumer to consume data. Our second microservice, called <strong class="source-inline">earthquakeConsumer</strong>, is going to consume data from Apache Kafka. It has a similar code structure to our streaming API (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer084">
					<img alt="Figure 8.5: Earthquake consumer API structure" src="image/B09148_08_005.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5: Earthquake consumer API structure</p>
			<p>Let’s start from the <strong class="source-inline">configs</strong> folder. As in our first microservice in <a href="B09148_05.xhtml#_idTextAnchor074"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we have a <strong class="source-inline">.env</strong> file inside the folder. The<a id="_idIndexMarker635"/> responsibility of this folder is to store relevant configurations. Here is what it <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
PORT=3002
#KAFKA Configuration
KAFKA_CLIENT_ID=earthquake-consumer-service
KAFKA_BROKERS=localhost:29092
KAFKA_TOPIC=earthquake-service-topic
KAFKA_GROUP_ID=earthquake-consumer-group</pre>			<p>We introduced an additional configuration, <strong class="source-inline">KAFKA_GROUP_ID</strong>, which identifies the consumer group, allowing Kafka to balance partition assignments among consumers. It is a string property used to identify a collection of consumer instances and acts as the glue that binds consumers together for <span class="No-Break">collaborative consumption.</span></p>
			<p>Kafka automatically distributes topic partitions among consumers in the same group, allowing parallel processing while ensuring that each partition is consumed by only one consumer at a time within the group. If a consumer in the group fails, Kafka reassigns its partitions to remaining active consumers, ensuring uninterrupted message processing. With proper configuration, consumer groups can achieve exactly-once delivery semantics, guaranteeing each message is processed by only one consumer exactly once. When working with Kafka consumer groups, it’s essential to understand how they manage message consumption and workload distribution across multiple consumers. The following are key points to keep in mind when configuring and utilizing consumer groups for efficient <span class="No-Break">message processing:</span></p>
			<ul>
				<li>Only one consumer can process a partition at a time within <span class="No-Break">a group.</span></li>
				<li>Consumers with different group IDs treat topics as independent streams and don’t share <span class="No-Break">the workload.</span></li>
				<li>Always consider using a meaningful group ID to improve cluster management <span class="No-Break">and monitoring.</span></li>
			</ul>
			<p>To read and validate this <a id="_idIndexMarker636"/>config, we use the same mechanism as we did for the streaming API. We have <strong class="source-inline">src/config/config.js</strong>. It reads and validates our configuration with the <span class="No-Break">additional </span><span class="No-Break"><strong class="source-inline">KAFKA_GROUP_ID</strong></span><span class="No-Break">.</span></p>
			<p>The main functionality has been implemented inside the <strong class="source-inline">src/service/earthquake.js</strong> file. Here is our <span class="No-Break">stream-consuming process:</span></p>
			<pre class="source-code">
const Kafka = require('node-rdkafka');
const { createConfig } = require('../config/config');
const path = require('path');
class EarthquakeEventConsumer {
    constructor() {
        const configPath = path.join(__dirname,
          '../../configs/.env');
        this.appConfig = createConfig(configPath);
        // Create the Kafka consumer stream here (once)
        this.stream =
          Kafka.KafkaConsumer.createReadStream({
            'metadata.broker.list':
              this.appConfig.kafka.brokers,
            'group.id': this.appConfig.kafka.groupID,
            'socket.keepalive.enable': true,
            'enable.auto.commit': false
        }, {}, {
            topics: this.appConfig.kafka.topic,
            waitInterval: 0,
            objectMode: false
        });
    }
    async consumeData() {
        // Now use the pre-created stream for data consumption
        this.stream.on('data', (message) =&gt; {
            console.log('Got message');
            console.log(JSON.parse(message));
        });
    }
}
module.exports = EarthquakeEventConsumer;</pre>			<p>This code defines a <a id="_idIndexMarker637"/>class named <strong class="source-inline">EarthquakeEventConsumer</strong>, which acts as a consumer for messages from a Kafka topic containing earthquake event data. Here’s a breakdown of <span class="No-Break">the code:</span></p>
			<ul>
				<li><strong class="source-inline">Kafka</strong> from <strong class="source-inline">node-rdkafka</strong>: This library provides functionalities to interact with Kafka as a consumer <span class="No-Break">or producer.</span></li>
				<li><strong class="source-inline">createConfig</strong> from <strong class="source-inline">../config/config</strong>: This imports a function from another file (<strong class="source-inline">config/config.js</strong>) that reads <span class="No-Break">configuration details.</span></li>
				<li><strong class="source-inline">path</strong>: This is a<a id="_idIndexMarker638"/> built-in Node.js module for manipulating <span class="No-Break">file paths.</span></li>
				<li><strong class="source-inline">EarthquakeEventConsumer</strong>: This class is responsible for consuming earthquake <span class="No-Break">event data.</span></li>
				<li><strong class="source-inline">constructor()</strong>: This special method is called when you create a new instance <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">EarthquakeEventConsumer</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">configPath</strong>: This constructs the path to a configuration file (such as a <strong class="source-inline">.env</strong> file) containing Kafka connection details such as brokers and <span class="No-Break">group ID.</span></li>
				<li><strong class="source-inline">appConfig</strong>: This calls the <strong class="source-inline">createConfig</strong> function (imported from another file) to read the configuration details from the <strong class="source-inline">.env</strong> file and stores it in <strong class="source-inline">this.appConfig</strong>. This makes the configuration accessible throughout the <span class="No-Break">object’s lifetime.</span></li>
				<li><strong class="source-inline">this.stream</strong>: This line is the key part. It uses <strong class="source-inline">Kafka.KafkaConsumer.createReadStream</strong> to create a stream for reading messages from Kafka. Here’s what the options passed to <span class="No-Break"><strong class="source-inline">createReadStream</strong></span><span class="No-Break"> do:</span><ul><li><strong class="source-inline">'metadata.broker.list'</strong>: This specifies the list of Kafka brokers to connect to, obtained from the configuration stored <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">this.appConfig</strong></span><span class="No-Break">.</span></li><li><strong class="source-inline">'group.id'</strong>: This sets the consumer group ID, also obtained from the configuration. Consumers in the same group will share the messages from a topic <span class="No-Break">among themselves.</span></li><li><strong class="source-inline">'socket.keepalive.enable'</strong>: This enables a mechanism to keep the connection alive with <span class="No-Break">the broker.</span></li><li><strong class="source-inline">'enable.auto.commit'</strong>: This is set to <strong class="source-inline">true</strong> to enable the automatic committing <span class="No-Break">of offsets.</span></li><li><strong class="source-inline">topics</strong>: This specifies the Kafka topic name to consume from, obtained from the configuration (likely <strong class="source-inline">librdtesting-01</strong> in <span class="No-Break">this case).</span></li><li><strong class="source-inline">waitInterval</strong>: This is set to <strong class="source-inline">0</strong>, indicating no waiting between attempts to receive messages if none <span class="No-Break">are available.</span></li><li><strong class="source-inline">objectMode</strong>: This is set to <strong class="source-inline">false</strong>, meaning the messages received from the stream will be<a id="_idIndexMarker639"/> raw buffers, not <span class="No-Break">JavaScript objects.</span></li></ul><p class="list-inset">Crucially, this stream creation happens only once in the constructor, <span class="No-Break">ensuring efficiency.</span></p></li>
				<li><strong class="source-inline">async consumeData()</strong>: This is an asynchronous method that initiates the data <span class="No-Break">consumption process.</span></li>
				<li><strong class="source-inline">.on('data', ...)</strong>: This sets up a listener for the data event emitted by the pre-created stream (<strong class="source-inline">this.stream</strong>). The callback function executes each time a new message arrives, logging that a message was received and parsing the JSON-encoded data for further handling.The callback function logs a message indicating a new message was received. It then parses the raw message buffer (assuming it’s JSON-encoded data) using <strong class="source-inline">JSON.parse</strong> and logs the <span class="No-Break">parsed data.</span></li>
				<li><strong class="source-inline">module.exports = EarthquakeEventConsumer</strong>: This line exports the <strong class="source-inline">EarthquakeEventConsumer</strong> class so it can be used in other parts of <span class="No-Break">your application.</span></li>
			</ul>
			<p>To summarize, the code defines a consumer that connects to Kafka, subscribes to a specific topic, and listens for incoming earthquake event data. It then parses the JSON-encoded messages and logs them to the console. The key improvement here is creating the Kafka consumer stream only once in the constructor, making the code <span class="No-Break">more efficient.</span></p>
			<p>To run the service, we have <strong class="source-inline">app.js</strong> and <strong class="source-inline">index.js</strong>, which follow the same structure as our <span class="No-Break">streaming API.</span></p>
			<p>We have now implemented our <strong class="source-inline">earthquakeConsumer</strong> and it is time to <span class="No-Break">test it:</span></p>
			<ol>
				<li>Open <strong class="bold">Terminal</strong> | <strong class="bold">New Terminal</strong> from the menu if you use Visual <span class="No-Break">Studio Code.</span></li>
				<li>Navigate to<a id="_idIndexMarker640"/> the <strong class="source-inline">src</strong> <span class="No-Break">folder (</span><span class="No-Break"><strong class="source-inline">Ch08/earthquakeConsumer/src</strong></span><span class="No-Break">).</span></li>
				<li>Run the <strong class="source-inline">node </strong><span class="No-Break"><strong class="source-inline">index.js</strong></span><span class="No-Break"> command.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout"> You don’t need to manually navigate to the <strong class="source-inline">src</strong> folder and run <strong class="source-inline">node index.js</strong> every time you want to start the application. Instead, you can streamline this process by configuring a script in your <strong class="source-inline">package.json</strong> file. Simply add the following to the <strong class="source-inline">scripts</strong> section <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">package.json</strong></span><span class="No-Break">:</span></p>
			<p class="callout"><strong class="source-inline">{</strong></p>
			<p class="callout"><strong class="source-inline"> “</strong><span class="No-Break"><strong class="source-inline">scripts”: {</strong></span></p>
			<p class="callout"><strong class="source-inline">  “start”: “</strong><span class="No-Break"><strong class="source-inline">node src/index.js”</strong></span></p>
			<p class="callout"><strong class="source-inline">  }</strong></p>
			<p class="callout"><strong class="source-inline">}</strong></p>
			<ol>
				<li value="4">Once this is set up, you can start your application from the root of your project by simply running <span class="No-Break">the following:</span><pre class="source-code">
<strong class="bold">npm start</strong></pre><p class="list-inset">This will automatically launch the application, saving you time and effort each time you run the code. When running the earthquake consumer service using Node.js, the following output confirms that the service has started successfully and is ready <span class="No-Break">for operation:</span></p><pre class="source-code"><strong class="bold">PS C:\packtGit\Hands-on-Microservices-with-JavaScript\Ch08\earthquakeConsumer&gt; npm start</strong>
<strong class="bold">Debugger listening on ws://127.0.0.1:62120/3f477ceb-6d5a-4d84-a98a-8f6185f8f11d</strong>
<strong class="bold">For help, see: https://nodejs.org/en/docs/inspector</strong>
<strong class="bold">Debugger attached.</strong>
<strong class="bold">&gt; earthquakeconsumer@1.0.0 start</strong>
<strong class="bold">&gt; node src/index.js</strong>
<strong class="bold">Debugger listening on ws://127.0.0.1:62125/d84e3d2b-6be1-4a3f-8ba3-2bca4d1fe710</strong>
<strong class="bold">For help, see: https://nodejs.org/en/docs/inspector</strong>
<strong class="bold">Debugger attached.</strong>
<strong class="bold">earthquake Consumer started { port: 3002 }</strong></pre></li>				<li>Run the <strong class="source-inline">earthquakeService</strong> streaming API to start the <span class="No-Break">streaming process.</span></li>
				<li>Go to Postman<a id="_idIndexMarker641"/> and <span class="No-Break">hit </span><span class="No-Break"><strong class="bold">Send</strong></span><span class="No-Break">.</span></li>
				<li>While the <strong class="source-inline">earthquakeService</strong> streaming API prints <strong class="bold">The message has been queued!</strong>, our consumer API will print consumed data such as that <span class="No-Break">shown here:</span><pre class="source-code">
<strong class="bold">Got message</strong>
<strong class="bold">{</strong>
<strong class="bold">  id: 's0iwb737f2',</strong>
<strong class="bold">  magnitude: 6.473388041641288,</strong>
<strong class="bold">  location: { latitude: -26.569165455403734, longitude: -167.263244317978 },</strong>
<strong class="bold">  timestamp: 1725611270994</strong>
<strong class="bold">}</strong>
<strong class="bold">Got message</strong>
<strong class="bold">{</strong>
<strong class="bold">  id: 'agmk58tick6',</strong>
<strong class="bold">  magnitude: 1.9469044303512526,</strong>
<strong class="bold">  location: { latitude: -19.102647524780792, longitude: 58.15282259841075 },</strong>
<strong class="bold">  timestamp: 1725611271106</strong>
<strong class="bold">}</strong></pre></li>			</ol>
			<p>You can add some more<a id="_idIndexMarker642"/> logic to these services, but this should be enough to demonstrate streaming as simply <span class="No-Break">as possible.</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor145"/>Summary</h1>
			<p>This chapter explored the concept of real-time data streaming in microservices architecture. We used the example of an earthquake data-streaming service to illustrate how microservices can efficiently handle continuous flows <span class="No-Break">of information.</span></p>
			<p>Rather than storing data in bulk, the producer service publishes data as a continuous stream, allowing immediate processing and analysis as each new data point arrives. This approach is beneficial for real-time scenarios where immediate processing and analysis <span class="No-Break">are crucial.</span></p>
			<p>Another microservice acts as the consumer in this scenario. It subscribes to the earthquake data stream produced by the first service. As new data arrives, the consumer microservice receives and processes it in <span class="No-Break">real time.</span></p>
			<p>The consumer microservice can perform various actions based on the earthquake data. It might trigger alerts, update dashboards, or integrate with other services for further analysis <span class="No-Break">and response.</span></p>
			<p>Real-time data streaming with microservices offers a powerful approach to handling continuous information flows. In <a href="B09148_09.xhtml#_idTextAnchor147"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, you’ll learn how to secure microservices through authentication, authorization, and API protection, while also implementing logging and monitoring tools to proactively detect and address <span class="No-Break">potential issues.</span></p>
		</div>
	

		<div class="Content" id="_idContainer086">
			<h1 id="_idParaDest-145" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor146"/>Part 3:Securing, Testing, and Deploying Microservices</h1>
			<p>In this final part, we will focus on the crucial aspects of securing, testing, and deploying microservices. We’ll learn about implementing authentication, authorization, and monitoring tools to ensure that your microservices are secure and reliable. This section also covers the process of building a CI/CD pipeline, which is vital for automating the deployment of your microservices, and concludes with strategies to deploy our microservices <span class="No-Break">to production.</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B09148_09.xhtml#_idTextAnchor147"><em class="italic">Chapter 9</em></a>, <em class="italic">Securing Microservices</em></li>
				<li><a href="B09148_10.xhtml#_idTextAnchor160"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring Microservices</em></li>
				<li><a href="B09148_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a>, <em class="italic">Microservices Architecture</em></li>
				<li><a href="B09148_12.xhtml#_idTextAnchor196"><em class="italic">Chapter 12</em></a>, <em class="italic">Testing Microservices</em></li>
				<li><a href="B09148_13.xhtml#_idTextAnchor211"><em class="italic">Chapter 13</em></a>, <em class="italic">A CI/CD Pipeline for Your Microservices</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer087">
			</div>
		</div>
		<div>
			<div class="Basic-Graphics-Frame" id="_idContainer088">
			</div>
		</div>
	</body></html>