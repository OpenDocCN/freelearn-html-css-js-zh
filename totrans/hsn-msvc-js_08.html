<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-134"><a id="_idTextAnchor135"/>8</h1>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor136"/>Real-Time Data Streaming Using Microservices</h1>
			<p>Certain microservice applications, such as financial trading platforms and ride-hailing services, demand events to be produced and consumed with minimal latency. Real-time data streaming has become increasingly crucial in modern software development due to its ability to provide immediate, continuous insights and responses based on the most current data. This type of real-time data usage is particularly important in industries such as finance, healthcare, and logistics, where delays in data processing can lead to significant losses or even life-threatening situations.</p>
			<p>Applications that rely on real-time data can offer a more responsive and interactive user experience. For example, social media platforms, online gaming, and live sports streaming rely on real-time data to keep users engaged and provide a seamless experience.</p>
			<p>This chapter is all about real-time streaming with microservices. Our purpose is to understand when and how to establish such a type of communication when dealing with microservices.</p>
			<p>This chapter covers the following:</p>
			<ul>
				<li>What is real-time streaming?</li>
				<li>Getting started with the earthquake streaming API</li>
				<li>Implementing the earthquake stream consumer</li>
			</ul>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor137"/>Technical requirements</h1>
			<p>To follow along with the chapter, you will need an IDE (we prefer Visual Studio Code), Postman, Docker, and a browser of your choice.</p>
			<p>It is preferable to download our repository from <a href="https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript">https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript</a> and open the <code>Ch08</code> folder to easily follow along with the code snippets.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor138"/>What is real-time streaming?</h1>
			<p><strong class="bold">Real-time streaming</strong> is a data-processing paradigm where data is continuously generated, transmitted, and processed as it is created, with minimal delay. Unlike batch processing, which collects and processes data in large groups or batches at regular intervals, real-time streaming focuses on the immediate and continuous flow of data, enabling instant analysis and<a id="_idIndexMarker609"/> response. It’s like watching a live stream instead of waiting for a video to download entirely.</p>
			<p>Before we continue any further, let us look at some of the key characteristics of real-time streaming:</p>
			<ul>
				<li><strong class="bold">Continuous data flow</strong>: Real-time streaming is like a never-ending flow of information coming in all at once from different places. This information can be from sensors, people <a id="_idIndexMarker610"/>using things online, money being bought and sold, such as Bitcoin, and so on.</li>
				<li><strong class="bold">Low latency</strong>: The main goal of real-time streaming is to make the delay between information being created and it being used as short as possible.</li>
				<li><strong class="bold">Event-driven processing</strong>: Real-time streaming works by following events as they happen, such as things being created or changing. Each event is dealt with on its own or in small batches, so the system can react right away to new situations.</li>
				<li><strong class="bold">Scalability</strong>: Real-time streaming systems can handle different amounts and speeds of information, growing bigger or smaller depending on how much information is coming in</li>
				<li><strong class="bold">Fault tolerance</strong>: To ensure continuous operation, real-time streaming systems incorporate fault tolerance mechanisms, such as data replication and automatic recovery from failures. As we mentioned in previous chapters, this is one of the important attributes of Apache Kafka, which we plan to also use for this chapter.</li>
				<li><strong class="bold">Data consistency</strong>: Maintaining data consistency is important in real-time streaming, especially when processing brings multiple distributed components into the table. Techniques such as <strong class="bold">exactly-once processing</strong> and <strong class="bold">idempotency</strong> are employed to ensure accuracy. Exactly-once processing ensures that each message is processed only once, even in the case of failures or retries, preventing <a id="_idIndexMarker611"/>duplicates. As we use Apache Kafka for most chapters, you can easily configure idempotency and exactly-once behavior in it.</li>
			</ul>
			<p>The importance of real-time data in modern applications cannot be overstated. In today’s data-driven world, the ability to process and act on data as it is generated provides a significant competitive edge.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor139"/>Why real-time data is essential</h2>
			<p>Here are some key reasons<a id="_idIndexMarker612"/> why real-time data is a must-have for most modern applications:</p>
			<ul>
				<li><strong class="bold">Enhanced decision-making</strong>: Real-time data can enhance an application’s decision-making abilities because of the following:<ul><li><strong class="bold">Immediate insights</strong>: Real-time data provides immediate insights, allowing businesses to make informed decisions quickly. This is important in dynamic environments such as stock trading, where market conditions can change rapidly.</li><li><strong class="bold">Proactive problem-solving</strong>: By continuously monitoring data, organizations can identify and address issues before they escalate, reducing downtime and enhancing operational efficiency.</li></ul></li>
				<li><strong class="bold">Improved user experience</strong>: Real-time data empowers applications to provide a more dynamic and personalized user experience by enhancing interactivity and responsiveness while tailoring content to individual preferences.</li>
				<li><strong class="bold">Operational efficiency</strong>: Organizations can significantly boost their efficiency by using real-time data, which enables both real-time monitoring and automation of processes, helping to streamline operations and reduce costs.</li>
				<li><strong class="bold">Competitive advantage</strong>: Leveraging real-time data gives businesses a distinct edge by enhancing their agility and fostering innovation, allowing them to swiftly respond to market changes and create cutting-edge products and services.</li>
				<li><strong class="bold">Increased revenue</strong>: Utilizing real-time data enables businesses to enhance their revenue streams through optimized marketing strategies and dynamic fraud detection, ensuring <a id="_idIndexMarker613"/>more effective customer engagement and financial security.</li>
				<li><strong class="bold">Enhanced security</strong>: Real-time data strengthens security by enabling continuous monitoring and anomaly detection, allowing organizations to quickly identify and respond to potential threats and system irregularities.</li>
				<li><strong class="bold">Scalability and flexibility</strong>: Real-time data systems provide the ability to efficiently handle large volumes of data while maintaining adaptability, ensuring optimal performance even as data loads and requirements fluctuate.</li>
				<li><strong class="bold">Customer satisfaction</strong>: Real-time data enhances customer satisfaction by enabling instant support and immediate feedback, allowing businesses to quickly address concerns and continuously improve their products and services.</li>
			</ul>
			<p>Real-time streaming allows data to be processed as it’s generated, offering immediate insights and responses. This continuous flow of data, coupled with low latency and event-driven processing, is crucial in industries such as finance, healthcare, and logistics. The ability to make real-time decisions, enhance user experiences, and improve operational efficiency provides businesses with a competitive edge, fostering innovation and increasing revenue while ensuring system scalability, fault tolerance, and enhanced security.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor140"/>Understanding use cases</h2>
			<p>At the beginning of the <a id="_idIndexMarker614"/>chapter, I briefly mentioned that the use of real-time data can have incredible impacts on some industries. Therefore, it is crucial to understand the use cases of real-time data when you design your microservices. Let’s look at these use cases here:</p>
			<ul>
				<li><strong class="bold">Financial services</strong>: Real-time data plays a pivotal role in this industry, enabling algorithmic stock trading with split-second decisions and supporting continuous risk management to ensure compliance and mitigate potential financial threats.</li>
				<li><strong class="bold">Healthcare</strong>: Real-time data is transforming healthcare by enabling continuous patient monitoring for timely interventions and enhancing telemedicine through real-time video consultations <a id="_idIndexMarker615"/>and data sharing, improving patient care and accessibility.</li>
				<li><strong class="bold">Retail and e-commerce</strong>: Real-time data enhances retail and e-commerce operations by optimizing inventory management to prevent shortages and enabling dynamic pricing strategies that adjust to demand and competitor activity.</li>
				<li><strong class="bold">Transportation and logistics</strong>: Real-time data optimizes fleet management by improving route planning and delivery times, while real-time traffic data enhances traffic management, reducing congestion and improving overall mobility.</li>
				<li><strong class="bold">Telecommunications</strong>: Real-time data enhances network management by ensuring continuous performance monitoring for optimal service quality, while also improving customer experience through the rapid resolution of network issues</li>
			</ul>
			<p>In the end, what we can say for sure is that real-time data is a cornerstone of modern applications, driving enhanced decision-making, improved user experiences, operational efficiency, and competitive advantage. By leveraging real-time data, organizations can innovate, adapt, and thrive in a rapidly changing digital landscape.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor141"/>Relationship between real-time streaming and microservices</h2>
			<p>Now that we have<a id="_idIndexMarker616"/> understood what real-time data is and exactly why it’s necessary, it’s time we understood the relationship between real-time streaming and microservices. The union of real-time streaming with microservices is a symbiotic one, which extends the power, productivity, and scalability of modern software architectures. Systems now, as a service, are more reactive, more adaptable, and faster as a result of this integration. Let’s try to understand how real-time <a id="_idIndexMarker617"/>streaming and microservices play well with each other:</p>
			<ul>
				<li><strong class="bold">Decoupling and scalability</strong>: Real-time streaming complements microservices by promoting loose coupling and independent scaling, allowing services to communicate asynchronously and scale efficiently based on demand.</li>
				<li><strong class="bold">Flexibility and agility</strong>: The combination of real-time streaming with microservices enhances flexibility and agility, enabling continuous service evolution and real-time data processing for applications requiring immediate insights and rapid iteration.</li>
				<li><strong class="bold">Resilience and fault tolerance</strong>: Integrating real-time streaming with microservices enhances resilience and fault tolerance by isolating failures to individual services and ensuring data durability, allowing for seamless recovery and continuous operation even in the event of service disruptions.</li>
				<li><strong class="bold">Real-time communication</strong>: Real-time streaming enhances communication within microservices by enabling event-driven architecture and immediate data propagation, allowing services to interact asynchronously and respond quickly to events, leading to more responsive and synchronized systems.</li>
				<li><strong class="bold">Operational efficiency</strong>: Combining real-time streaming with microservices enhances operational efficiency by optimizing resource utilization and simplifying data pipelines, allowing continuous data flow and reducing the complexity of traditional batch-processing methods.</li>
				<li><strong class="bold">Enhanced monitoring and analytics</strong>: Integrating real-time streaming with microservices enables real-time monitoring and analytics, offering immediate visibility into service performance and providing actionable insights that allow for the proactive management and dynamic optimization of services.</li>
			</ul>
			<p>The synergy between real-time streaming and microservices offers a robust framework for building responsive, scalable, and efficient systems. By leveraging the strengths of both paradigms, organizations can create applications that are capable of handling dynamic workloads, providing real-time insights, and delivering superior user experiences. This <a id="_idIndexMarker618"/>combination is particularly powerful in environments where rapid data processing and immediate reactions are critical to success.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor142"/>Microservices we will develop</h2>
			<p>To make our learning process more interactive and more understandable, we will develop two simple microservices. The first microservice will act as a producer of stream and the domain of this<a id="_idIndexMarker619"/> microservices will be an earthquake. An API that streams real-time information about earthquakes can be valuable for several reasons:</p>
			<ul>
				<li><strong class="bold">Emergency response</strong>: Real-time data can be crucial for emergency responders who need to assess damage and deploy resources quickly after an earthquake. The API will provide information on the location, magnitude, and depth of the earthquake, which can help responders prioritize areas that may be most affected.</li>
				<li><strong class="bold">Public awareness</strong>: The API could be used for public awareness to create applications that send alerts to people in affected areas. This could help people take shelter or evacuate if necessary.</li>
				<li><strong class="bold">Research</strong>: Researchers can use the API to track earthquake activity and improve their understanding of earthquake patterns. This data can be used to develop better earthquake prediction models and improve building codes.</li>
				<li><strong class="bold">News and media</strong>: News organizations can use the API to get real-time updates on earthquake activity, which can help them report on the latest developments.</li>
			</ul>
			<p>In addition to these, there are commercial applications for such an API as well. For instance, insurance companies could use it to assess potential risks and losses, or engineering firms could use it to design earthquake-resistant structures.</p>
			<p>Of course, when building such type of APIs for production, we will need to choose a reliable source of earthquake data; but to demonstrate the purpose and implementation of real-time data streaming, our API will act as a source of truth.</p>
			<p>From a data format perspective, we should select a format for the data that is easy to use and integrate with other applications. Common formats include JSON and XML. Our choice is JSON.</p>
			<p>By providing valuable<a id="_idIndexMarker620"/> and timely data, your earthquake API can be a useful tool for a variety of users.</p>
			<p>The second microservice is going to be a consumer of the data. Throughout our learning process, we have implemented our microservices using different packages and frameworks with nearly full skeletons. For the current chapter, our focus is streaming rather than building an application skeleton from scratch. Our focus is not on implementing any architecture. You can refer to previous chapters if you want to add additional functionalities and make it a fully self-contained architectural application.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor143"/>Getting started with an earthquake streaming API</h1>
			<p>In our GitHub repository, in the <code>Ch08</code> folder, we have two subfolders: <code>earthquakeService</code>, the earthquake streaming API, and <code>earthquakeConsumer</code>, the consumer API. As we mentioned before, our main focus is on implementing streaming. To make this <a id="_idIndexMarker621"/>chapter more focused on the topic, we haven’t implemented a proper detailed design for this API. This is also the case with the consumer API.</p>
			<p>It is best to follow along by creating everything with us from scratch.</p>
			<p><code>earthquakeService</code> has the following dependencies:</p>
			<pre class="source-code">
"dependencies": {
    "dotenv": "^16.4.5",
    "express": "^4.19.2",
    "joi": "^17.13.1",
    "node-rdkafka": "^3.0.1"
  }</pre>			<p>First, you need to generate a <code>package.json</code> file that contains all dependencies. To create the file, run <code>npm init</code> and follow the prompts from the terminal. After <code>package.json</code> is created, run the <code>npm install 'your_required_package_names'</code> template command to install packages one by one. For example, to install the <code>express</code> package, just run <code>npm install express</code>, and hit <em class="italic">Enter</em>. We have already talked about <code>package.json</code> and the package installation process. You can check the previous chapters for more information. While we have reused some of the microservices from our previous chapter in our current chapter, we’re also going to use  <code>node-rdkafka</code> package which is new for us.</p>
			<p><code>node-rdkafka</code> is a Node.js library that provides a wrapper around the native librdkafka library, enabling efficient communication with Apache Kafka for high-performance data streaming. It leverages the power of <code>librdkafka</code> for efficient communication with Kafka and handles complexities such as balancing writes and managing brokers, making Kafka interaction easier for developers.</p>
			<p>You can install <code>node-rdkafka</code> using <code>npm</code>:</p>
			<pre class="console">
npm install node-rdkafka</pre>			<p>It is not the only package to<a id="_idIndexMarker622"/> use for streaming, and depending on your personal preference, you can select any other one. The <code>node-rdkafka</code> package supports a really easy stream writing and reading process, which is why we prefer to use it in this chapter for learning purposes.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should always try to use official packages for production apps. Using official packages helps keep your app safe because trusted developers manage them, and they are checked often. They are also more reliable, as they are tested, updated, and have good support, which is important for apps in production.</p>
			<p>We use Apache Kafka as a streaming platform. So, you need Apache Kafka to be running. As before, we plan to use the <code>docker-compose.yml</code> file, which should be up and running with Apache Kafka. Our <code>docker-compose.yml</code> file for this example will only contain the services needed for Kafka, excluding unnecessary components like PostgreSQL to reduce resource usage. Of course, you can run the <code>docker-compose.yml</code> file from the previous chapters that use Apache Kafka, but having additional services will use up more resources on your PC.</p>
			<p>Here is our <code>docker-compose.yml</code> file:</p>
			<pre class="source-code">
services:
  zookeeper:
    image: bitnami/zookeeper:3.8
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/bitnami
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
  kafka1:
    image: bitnami/kafka:3.6
    volumes:
      - kafka_data1:/bitnami
    environment:
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_LISTENERS: INTERNAL://:9092,EXTERNAL://0.0.0.0:29092
      KAFKA_CFG_ADVERTISED_LISTENERS: INTERNAL://kafka1:9092,EXTERNAL://localhost:29092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: 'true'
      ALLOW_PLAINTEXT_LISTENER: 'yes'
    ports:
      - "9092:9092"
      - "29092:29092"
    depends_on:
      - zookeeper
      
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - 9100:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      - kafka1
volumes:
  zookeeper_data:
    driver: local
  kafka_data1:
    driver: local</pre>			<p>In this configuration, we define <code>INTERNAL</code> and <code>EXTERNAL</code> listeners to differentiate between connections within the Docker network (<code>INTERNAL://kafka1:9092</code>) and connections from outside the Docker network, such as your local machine (<code>EXTERNAL://localhost:29092</code>). This separation ensures that services within the Docker network can use the internal address, while external clients (like a Node.js app running on your host) can connect using the external port. By doing so, Kafka can properly advertise the correct addresses to different clients, avoiding connection issues caused by mismatched listener configurations.</p>
			<p>This file contains Apache Kafka, the Kafka UI, and ZooKeeper. Just check our root folder (<code>Ch08/earthquakeService</code>) to find and run it. To run the <code>docker-compose.yml</code> file, first launch Docker <a id="_idIndexMarker623"/>Desktop, ensure it’s running, and then follow these steps:</p>
			<ol>
				<li>Pull and open <code>Ch08</code> from the repository.</li>
				<li>Open the project from Visual Studio Code (or any text editor you prefer) and navigate to <code>Ch08</code>.</li>
				<li>If you use Visual Studio Code, then go to <strong class="bold">Terminal</strong> | <strong class="bold">New Terminal</strong> from the <strong class="bold">Menu</strong>; otherwise, use the command line to navigate to the root folder.</li>
				<li>Run the <code>docker-compose up -d</code> command from the terminal (<em class="italic">Figure 8</em><em class="italic">.1</em>).</li>
			</ol>
			<div><div><img alt="Figure 8.1: Docker Desktop after running the docker-compose.yml file" src="img/B09148_08_001.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1: Docker Desktop after running the docker-compose.yml file</p>
			<p>To connect to Apache Kafka, we need to store the required configuration in a separate file. That is why we use the <code>dotenv</code> package to read configuration information. Create a <code>configs</code> folder under the<a id="_idIndexMarker624"/> root folder (<code>Ch08/earthquake</code>) and add a <code>.</code><code>env</code> file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"> The <code>config</code> and <code>configs</code> folders are separate and serve different purposes. Be sure to use the correct folder to avoid confusion. We store the <code>.env</code> file under the <code>configs</code> folder. On the other hand, we store the <code>config.js</code> file under the <code>config</code> folder, which loads environment variables using the <code>dotenv</code> package, validates them with <code>Joi</code>, and returns a configuration object for a Kafka-based microservice, throwing an error if validation fails.</p>
			<p>Here is what the <code>configs/.env</code> file should look like:</p>
			<pre class="source-code">
PORT=3001
#KAFKA Configuration
KAFKA_CLIENT_ID=earthquake-service
KAFKA_BROKERS=localhost:29092
KAFKA_TOPIC=earthquake-service-topic</pre>			<p>We have Kafka configuration such as client ID, brokers, and topic name with port information. As we learned before, all application source code lives under the <code>src</code> folder. Create the <code>src</code> folder <a id="_idIndexMarker625"/>on the same level as your <code>configs</code> folder (<em class="italic">Figure 8</em><em class="italic">.2</em>).</p>
			<div><div><img alt="Figure 8.2: General structure of earthquakeService" src="img/B09148_08_002.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2: General structure of earthquakeService</p>
			<p>We store configuration information in the <code>.env</code> file, but we need to add a reading and validating mechanism over our <code>config</code>. To implement proper reading and validating, we need to create a <code>configs.js</code> file under the <code>src/configc</code> folder. Here is what it looks like:</p>
			<pre class="source-code">
const dotenv = require('dotenv');
const Joi = require('joi');
const envVarsSchema = Joi.object()
    .keys({
        PORT: Joi.number().default(3000),
        KAFKA_CLIENT_ID: Joi.string().required(),
        KAFKA_BROKERS: Joi.string().required(),
        KAFKA_TOPIC: Joi.string().required()
    })
    .unknown();
function createConfig(configPath) {
    dotenv.config({ path: configPath });
    const { value: envVars, error } = envVarsSchema
        .prefs({ errors: { label: 'key' } })
        .validate(process.env);
    if (error) {
        throw new Error(`Config validation error:
          ${error.message}`);
    }
    return {
        port: envVars.PORT,
        kafka: {
            clientID: envVars.KAFKA_CLIENT_ID,
            brokers: envVars.KAFKA_BROKERS,
            topic: envVars.KAFKA_TOPIC
        }
    };
}
module.exports = {
    createConfig,
};</pre>			<p>We are using the same <code>config</code> read and validation mechanism as the account microservice. We have already explained this file in the <a href="B09148_07.xhtml#_idTextAnchor121"><em class="italic">Chapter 7</em></a>.</p>
			<p>Our <code>services</code> folder is responsible for storing service files. To implement real-time streaming functionality, we need to <a id="_idIndexMarker626"/>create a new file called <code>earthquake.js</code> under the <code>services</code> folder. Here is what it looks like:</p>
			<pre class="source-code">
const Kafka = require('node-rdkafka');
const { createConfig } = require('../config/config');
const path = require('path');
class EarthquakeEventProducer {
constructor() {
        this.intervalId = null;
    }
    #generateEarthquakeEvent() {
        return {
            id: Math.random().toString(36).substring(2,
              15),
            magnitude: Math.random() * 9, // Random magnitude between 0 and 9
            location: {
                latitude: Math.random() * 180 - 90, // Random latitude between -90 and 90
                longitude: Math.random() * 360 - 180, // Random longitude between -180 and 180
            },
            timestamp: Date.now(),
        };
    }
……..</pre>			<p>This code defines a class called <code>EarthquakeEventProducer</code> that simulates generating and publishing <a id="_idIndexMarker627"/>earthquake event data to a Kafka topic. Let’s walk through the code’s elements here:</p>
			<ul>
				<li><code>require('node-rdkafka')</code>: Imports the <code>node-rdkafka</code> library for interacting with a Kafka cluster.</li>
				<li><code>require('../config/config')</code>: Imports a function (likely from <code>../config/config.js</code>) that reads configuration settings from a file.</li>
				<li><code>require('path'):</code> Imports the <code>path</code> module for file path manipulation.</li>
				<li><code>The EarthquakeEventProducer class</code>: This class handles earthquake event generation and publishing.</li>
				<li><code>#generateEarthquakeEvent()</code>: This private method generates a simulated earthquake event object with the following properties:<ul><li><code>id</code>: A random unique identifier string.</li><li><code>magnitude</code>: A random floating-point number between <code>0</code> and <code>9</code> representing the earthquake’s magnitude</li><li><code>location</code>: An object containing the following:</li><li><code>latitude</code>: A random floating-point number between <code>-90</code> and <code>90</code> representing the latitude.</li><li><code>longitude</code>: A random floating-point number between <code>-180</code> and <code>180</code> representing the longitude.</li><li><code>timestamp</code>: The current timestamp in milliseconds.</li></ul></li>
			</ul>
			<p>Here is how we specify our <a id="_idIndexMarker628"/>main method called <code>runEarthquake</code>:</p>
			<pre class="source-code">
    async runEarthquake() {
        const configPath = path.join(__dirname,
          '../../configs/.env');
        const appConfig = createConfig(configPath);
        // Returns a new writable stream
        const stream = Kafka.Producer.createWriteStream({
            'metadata.broker.list':
              appConfig.kafka.brokers,
            'client.id': appConfig.kafka.clientID
        }, {}, {
            topic: appConfig.kafka.topic
        });
        // To make our stream durable we listen to this event
        stream.on('error', (err) =&gt; {
            console.error('Error in our kafka stream');
            console.error(err);
        });
       this.intervalId  = setInterval(async () =&gt; {
            const event =
              await this.#generateEarthquakeEvent();
            // Writes a message to the stream
            const queuedSuccess = stream.write(Buffer.from(
              JSON.stringify(event)));
            if (queuedSuccess) {
                console.log('The message has been queued!');
            } else {
                // If the stream's queue is full
                console.log('Too many messages in queue already');
            }
        }, 100);
    }</pre>			<p>Let’s break this code down here:</p>
			<ul>
				<li><code>runEarthquake()</code>: This async method is responsible for setting up the Kafka producer and publishing <a id="_idIndexMarker629"/>earthquake events.</li>
				<li><code>configPath</code>: This constructs the path to the configuration file using <code>path.join</code>.</li>
				<li><code>appConfig</code>: This reads configuration from the file using the imported <code>createConfig</code> function.</li>
				<li><code>stream</code>: This creates a Kafka producer write stream using <code>Kafka.Producer.createWriteStream</code>. The configuration includes the following:<ul><li><code>'metadata.broker.list'</code>: A comma-separated list of Kafka broker addresses from the configuration</li><li><code>'client.id'</code>: A unique identifier for this producer client from the configuration</li><li><code>Topic</code>: The exact topic that should get the streamed data</li></ul></li>
				<li><code>stream.on('error')</code>: This attaches an event listener for errors in the Kafka stream. It logs the error message to the console.</li>
				<li><code>setInterval</code>: This sets up an interval timer to generate and publish events every 100 milliseconds (adjustable). Inside the interval callback is the following:<ul><li><code>event</code>: Generates a new earthquake event object using <code>#generateEarthquakeEvent</code></li><li><code>stream.write</code>: Attempts to write the event data (converted to a buffer using <code>JSON.stringify</code>) to the Kafka stream</li><li><code>queuedSuccess</code>: Checks the return value from <code>stream.write</code>:<ul><li><code>true</code>: Indicates successful queuing of the message. A success message is logged to the console.</li><li><code>false</code>: Indicates the stream’s queue is full. A message about exceeding the queue capacity is logged to the console.</li></ul></li></ul></li>
			</ul>
			<p>In order to stop our earthquake service, we need to clear the interval:</p>
			<pre class="source-code">
stopEarthquake() {
        if (this.intervalId) {
            clearInterval(this.intervalId);
            this.intervalId = null;
            console.log('Earthquake event stream stopped.');
        } else {
            console.log('No running earthquake event stream to stop.');
        }
    }
module.exports = EarthquakeEventProducer;</pre>			<p>The <code>stopEarthquake()</code> method stops the ongoing earthquake event stream by checking whether there is an active interval running, indicated by the presence of <code>this.intervalId</code>. If the interval exists, it uses <code>clearInterval()</code> to stop the event generation and resets <code>this.intervalId</code> to <code>null</code> to indicate that the stream has stopped. A success message is logged when the stream is stopped. If no interval is running (i.e., <code>this.intervalId</code> is <code>null</code>), it logs a message saying there’s no active stream to stop. This ensures that the function can only stop an existing stream and won’t attempt to stop a non-existent one.</p>
			<p>In the end, this code simulates <a id="_idIndexMarker630"/>earthquake event generation and publishes these events to a Kafka topic at regular intervals, demonstrating basic Kafka producer usage with error handling and logging.</p>
			<p>We plan to launch streaming using an API, but to make things as simple as possible, we use a minimal API approach that doesn’t require us to create controllers. This behavior is implemented in the <code>app.js</code> file. Here is the file:</p>
			<pre class="source-code">
const express = require('express');
const EarthquakeEventProducer = require('./services/earthquake');
const app = express();
const earthquakeProducer = new EarthquakeEventProducer();
// Function to run streaming
app.post('/earthquake-events/start', async (req, res) =&gt; {
    earthquakeProducer.runEarthquake();
    res.status(200).send('Earthquake event stream started');
});
// Stop the earthquake event stream
app.post('/earthquake-events/stop', (req, res) =&gt; {
    earthquakeProducer.stopEarthquake();
    res.status(200).send('Earthquake event stream stopped');
});
module.exports = app;</pre>			<p>The code defines two API <a id="_idIndexMarker631"/>endpoints using Express.js to start and stop an earthquake event stream. The <code>/earthquake-events/start </code>endpoint triggers the <code>runEarthquake()</code> function from the <code>EarthquakeEventProducer</code> class, starting the event stream, and responds with a success message. The <code>/earthquake-events/stop</code> endpoint calls the <code>stopEarthquake()</code> function to stop the event stream and also responds with a success message. The <code>earthquakeProducer</code> object is an instance of the <code>EarthquakeEventProducer</code> class, which manages the event stream operations. Finally, the Express app is exported to be used in other parts of the application. This setup allows external clients, such as Postman, to control the Kafka event stream through API calls.</p>
			<p>In an Express.js application, the <code>index.js</code> file in the root directory typically serves as the entry point for your server. It acts as the central hub where you configure and launch your Express app. Here is our <code>index.js</code> file:</p>
			<pre class="source-code">
const path = require('path');
const app = require('./app');
const { createConfig } = require('./config/config');
async function execute() {
    const configPath = path.join(__dirname, '../configs/.env');
    const appConfig = createConfig(configPath);
    const server = app.listen(appConfig.port, () =&gt; {
        console.log('earthquake service started',
          { port: appConfig.port });
    });
    const closeServer = () =&gt; {
        if (server) {
            server.close(() =&gt; {
                console.log('server closed');
                process.exit(1);
            });
        } else {
            process.exit(1);
        }
    };
    const unexpectedError = (error) =&gt; {
        console.log('unhandled error', { error });
        closeServer();
    };
    process.on('uncaughtException', unexpectedError);
    process.on('unhandledRejection', unexpectedError);
}
execute();</pre>			<p>We have the following functionalities in the <code>index.js</code> file:</p>
			<ul>
				<li>Imports the Express app (<code>app.js</code>) and configuration function (<code>config.js</code>).</li>
				<li>Reads configuration from a file using <code>createConfig</code>.</li>
				<li>Starts the server using <code>app.listen</code> on the configured port and logs a message.</li>
				<li>Defines functions to<a id="_idIndexMarker632"/> gracefully close the server and handle unexpected errors.</li>
				<li>Attaches event listeners for uncaught exceptions and unhandled promise rejections, calling the error-handler function.</li>
				<li>Finally, calls the <code>execute</code> function to start everything.</li>
			</ul>
			<p>We have implemented our <code>earthquakeService</code>; now it is time to test it. Here’s how you can do that:</p>
			<ol>
				<li>Open <strong class="bold">Terminal</strong> | <strong class="bold">New Terminal</strong> from the menu if you’re using Visual Studio Code.</li>
				<li>Navigate to the <code>src</code> folder.</li>
				<li>Run the <code>node </code><code>index.js</code> command:<pre class="source-code">
<strong class="bold">PS C:\packtGit\Hands-on-Microservices-with-JavaScript\Ch08\earthquakeService\src&gt; node index.js</strong>
<strong class="bold">Debugger listening on ws://127.0.0.1:61042/876d7d9e-3292-482a-b011-e6c2d66e7615</strong>
<strong class="bold">For help, see: https://nodejs.org/en/docs/inspector</strong>
<strong class="bold">Debugger attached.</strong>
<code>http://localhost:3001/earthquake-events/start</code>.</li>
				<li>To stop streaming, open Postman and send a POST request to <code>http://localhost:3001/earthquake-events/stop</code> (<em class="italic">Figure 8</em><em class="italic">.3</em>).</li>
			</ol>
			<div><div><img alt="Figure 8.3: Stopping event streaming" src="img/B09148_08_003.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3: Stopping event streaming</p>
			<p class="list-inset">The topic should <a id="_idIndexMarker633"/>automatically be created with some events (<em class="italic">Figure 8</em><em class="italic">.4</em>).</p>
			<div><div><img alt="Figure 8.4: Apache Kafka event after streaming" src="img/B09148_08_004.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4: Apache Kafka event after streaming</p>
			<p>We have implemented the streaming API. Now it is time to consume data.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor144"/>Implementing the earthquake stream consumer</h1>
			<p>Producing is not very<a id="_idIndexMarker634"/> valuable if you don’t have a consumer to consume data. Our second microservice, called <code>earthquakeConsumer</code>, is going to consume data from Apache Kafka. It has a similar code structure to our streaming API (<em class="italic">Figure 8</em><em class="italic">.5</em>).</p>
			<div><div><img alt="Figure 8.5: Earthquake consumer API structure" src="img/B09148_08_005.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5: Earthquake consumer API structure</p>
			<p>Let’s start from the <code>configs</code> folder. As in our first microservice in <a href="B09148_05.xhtml#_idTextAnchor074"><em class="italic">Chapter 5</em></a>, we have a <code>.env</code> file inside the folder. The<a id="_idIndexMarker635"/> responsibility of this folder is to store relevant configurations. Here is what it looks like:</p>
			<pre class="source-code">
PORT=3002
#KAFKA Configuration
KAFKA_CLIENT_ID=earthquake-consumer-service
KAFKA_BROKERS=localhost:29092
KAFKA_TOPIC=earthquake-service-topic
KAFKA_GROUP_ID=earthquake-consumer-group</pre>			<p>We introduced an additional configuration, <code>KAFKA_GROUP_ID</code>, which identifies the consumer group, allowing Kafka to balance partition assignments among consumers. It is a string property used to identify a collection of consumer instances and acts as the glue that binds consumers together for collaborative consumption.</p>
			<p>Kafka automatically distributes topic partitions among consumers in the same group, allowing parallel processing while ensuring that each partition is consumed by only one consumer at a time within the group. If a consumer in the group fails, Kafka reassigns its partitions to remaining active consumers, ensuring uninterrupted message processing. With proper configuration, consumer groups can achieve exactly-once delivery semantics, guaranteeing each message is processed by only one consumer exactly once. When working with Kafka consumer groups, it’s essential to understand how they manage message consumption and workload distribution across multiple consumers. The following are key points to keep in mind when configuring and utilizing consumer groups for efficient message processing:</p>
			<ul>
				<li>Only one consumer can process a partition at a time within a group.</li>
				<li>Consumers with different group IDs treat topics as independent streams and don’t share the workload.</li>
				<li>Always consider using a meaningful group ID to improve cluster management and monitoring.</li>
			</ul>
			<p>To read and validate this <a id="_idIndexMarker636"/>config, we use the same mechanism as we did for the streaming API. We have <code>src/config/config.js</code>. It reads and validates our configuration with the additional <code>KAFKA_GROUP_ID</code>.</p>
			<p>The main functionality has been implemented inside the <code>src/service/earthquake.js</code> file. Here is our stream-consuming process:</p>
			<pre class="source-code">
const Kafka = require('node-rdkafka');
const { createConfig } = require('../config/config');
const path = require('path');
class EarthquakeEventConsumer {
    constructor() {
        const configPath = path.join(__dirname,
          '../../configs/.env');
        this.appConfig = createConfig(configPath);
        // Create the Kafka consumer stream here (once)
        this.stream =
          Kafka.KafkaConsumer.createReadStream({
            'metadata.broker.list':
              this.appConfig.kafka.brokers,
            'group.id': this.appConfig.kafka.groupID,
            'socket.keepalive.enable': true,
            'enable.auto.commit': false
        }, {}, {
            topics: this.appConfig.kafka.topic,
            waitInterval: 0,
            objectMode: false
        });
    }
    async consumeData() {
        // Now use the pre-created stream for data consumption
        this.stream.on('data', (message) =&gt; {
            console.log('Got message');
            console.log(JSON.parse(message));
        });
    }
}
module.exports = EarthquakeEventConsumer;</pre>			<p>This code defines a <a id="_idIndexMarker637"/>class named <code>EarthquakeEventConsumer</code>, which acts as a consumer for messages from a Kafka topic containing earthquake event data. Here’s a breakdown of the code:</p>
			<ul>
				<li><code>Kafka</code> from <code>node-rdkafka</code>: This library provides functionalities to interact with Kafka as a consumer or producer.</li>
				<li><code>createConfig</code> from <code>../config/config</code>: This imports a function from another file (<code>config/config.js</code>) that reads configuration details.</li>
				<li><code>path</code>: This is a<a id="_idIndexMarker638"/> built-in Node.js module for manipulating file paths.</li>
				<li><code>EarthquakeEventConsumer</code>: This class is responsible for consuming earthquake event data.</li>
				<li><code>constructor()</code>: This special method is called when you create a new instance of <code>EarthquakeEventConsumer</code>.</li>
				<li><code>configPath</code>: This constructs the path to a configuration file (such as a <code>.env</code> file) containing Kafka connection details such as brokers and group ID.</li>
				<li><code>appConfig</code>: This calls the <code>createConfig</code> function (imported from another file) to read the configuration details from the <code>.env</code> file and stores it in <code>this.appConfig</code>. This makes the configuration accessible throughout the object’s lifetime.</li>
				<li><code>this.stream</code>: This line is the key part. It uses <code>Kafka.KafkaConsumer.createReadStream</code> to create a stream for reading messages from Kafka. Here’s what the options passed to <code>createReadStream</code> do:<ul><li><code>'metadata.broker.list'</code>: This specifies the list of Kafka brokers to connect to, obtained from the configuration stored in <code>this.appConfig</code>.</li><li><code>'group.id'</code>: This sets the consumer group ID, also obtained from the configuration. Consumers in the same group will share the messages from a topic among themselves.</li><li><code>'socket.keepalive.enable'</code>: This enables a mechanism to keep the connection alive with the broker.</li><li><code>'enable.auto.commit'</code>: This is set to <code>true</code> to enable the automatic committing of offsets.</li><li><code>topics</code>: This specifies the Kafka topic name to consume from, obtained from the configuration (likely <code>librdtesting-01</code> in this case).</li><li><code>waitInterval</code>: This is set to <code>0</code>, indicating no waiting between attempts to receive messages if none are available.</li><li><code>objectMode</code>: This is set to <code>false</code>, meaning the messages received from the stream will be<a id="_idIndexMarker639"/> raw buffers, not JavaScript objects.</li></ul><p class="list-inset">Crucially, this stream creation happens only once in the constructor, ensuring efficiency.</p></li>
				<li><code>async consumeData()</code>: This is an asynchronous method that initiates the data consumption process.</li>
				<li><code>.on('data', ...)</code>: This sets up a listener for the data event emitted by the pre-created stream (<code>this.stream</code>). The callback function executes each time a new message arrives, logging that a message was received and parsing the JSON-encoded data for further handling.The callback function logs a message indicating a new message was received. It then parses the raw message buffer (assuming it’s JSON-encoded data) using <code>JSON.parse</code> and logs the parsed data.</li>
				<li><code>module.exports = EarthquakeEventConsumer</code>: This line exports the <code>EarthquakeEventConsumer</code> class so it can be used in other parts of your application.</li>
			</ul>
			<p>To summarize, the code defines a consumer that connects to Kafka, subscribes to a specific topic, and listens for incoming earthquake event data. It then parses the JSON-encoded messages and logs them to the console. The key improvement here is creating the Kafka consumer stream only once in the constructor, making the code more efficient.</p>
			<p>To run the service, we have <code>app.js</code> and <code>index.js</code>, which follow the same structure as our streaming API.</p>
			<p>We have now implemented our <code>earthquakeConsumer</code> and it is time to test it:</p>
			<ol>
				<li>Open <strong class="bold">Terminal</strong> | <strong class="bold">New Terminal</strong> from the menu if you use Visual Studio Code.</li>
				<li>Navigate to<a id="_idIndexMarker640"/> the <code>src</code> folder (<code>Ch08/earthquakeConsumer/src</code>).</li>
				<li>Run the <code>node </code><code>index.js</code> command.</li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout"> You don’t need to manually navigate to the <code>src</code> folder and run <code>node index.js</code> every time you want to start the application. Instead, you can streamline this process by configuring a script in your <code>package.json</code> file. Simply add the following to the <code>scripts</code> section of <code>package.json</code>:</p>
			<p class="callout"><code>{</code></p>
			<p class="callout"><code> “</code><code>scripts”: {</code></p>
			<p class="callout"><code>  “start”: “</code><code>node src/index.js”</code></p>
			<p class="callout"><code>  }</code></p>
			<p class="callout"><code>}</code></p>
			<ol>
				<li value="4">Once this is set up, you can start your application from the root of your project by simply running the following:<pre class="source-code">
<strong class="bold">npm start</strong></pre><p class="list-inset">This will automatically launch the application, saving you time and effort each time you run the code. When running the earthquake consumer service using Node.js, the following output confirms that the service has started successfully and is ready for operation:</p><pre class="source-code"><strong class="bold">PS C:\packtGit\Hands-on-Microservices-with-JavaScript\Ch08\earthquakeConsumer&gt; npm start</strong>
<strong class="bold">Debugger listening on ws://127.0.0.1:62120/3f477ceb-6d5a-4d84-a98a-8f6185f8f11d</strong>
<strong class="bold">For help, see: https://nodejs.org/en/docs/inspector</strong>
<strong class="bold">Debugger attached.</strong>
<strong class="bold">&gt; earthquakeconsumer@1.0.0 start</strong>
<strong class="bold">&gt; node src/index.js</strong>
<strong class="bold">Debugger listening on ws://127.0.0.1:62125/d84e3d2b-6be1-4a3f-8ba3-2bca4d1fe710</strong>
<strong class="bold">For help, see: https://nodejs.org/en/docs/inspector</strong>
<strong class="bold">Debugger attached.</strong>
<code>earthquakeService</code> streaming API to start the streaming process.</li>
				<li>Go to Postman<a id="_idIndexMarker641"/> and hit <strong class="bold">Send</strong>.</li>
				<li>While the <code>earthquakeService</code> streaming API prints <strong class="bold">The message has been queued!</strong>, our consumer API will print consumed data such as that shown here:<pre class="source-code">
<strong class="bold">Got message</strong>
<strong class="bold">{</strong>
<strong class="bold">  id: 's0iwb737f2',</strong>
<strong class="bold">  magnitude: 6.473388041641288,</strong>
<strong class="bold">  location: { latitude: -26.569165455403734, longitude: -167.263244317978 },</strong>
<strong class="bold">  timestamp: 1725611270994</strong>
<strong class="bold">}</strong>
<strong class="bold">Got message</strong>
<strong class="bold">{</strong>
<strong class="bold">  id: 'agmk58tick6',</strong>
<strong class="bold">  magnitude: 1.9469044303512526,</strong>
<strong class="bold">  location: { latitude: -19.102647524780792, longitude: 58.15282259841075 },</strong>
<strong class="bold">  timestamp: 1725611271106</strong>
<strong class="bold">}</strong></pre></li>			</ol>
			<p>You can add some more<a id="_idIndexMarker642"/> logic to these services, but this should be enough to demonstrate streaming as simply as possible.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor145"/>Summary</h1>
			<p>This chapter explored the concept of real-time data streaming in microservices architecture. We used the example of an earthquake data-streaming service to illustrate how microservices can efficiently handle continuous flows of information.</p>
			<p>Rather than storing data in bulk, the producer service publishes data as a continuous stream, allowing immediate processing and analysis as each new data point arrives. This approach is beneficial for real-time scenarios where immediate processing and analysis are crucial.</p>
			<p>Another microservice acts as the consumer in this scenario. It subscribes to the earthquake data stream produced by the first service. As new data arrives, the consumer microservice receives and processes it in real time.</p>
			<p>The consumer microservice can perform various actions based on the earthquake data. It might trigger alerts, update dashboards, or integrate with other services for further analysis and response.</p>
			<p>Real-time data streaming with microservices offers a powerful approach to handling continuous information flows. In <a href="B09148_09.xhtml#_idTextAnchor147"><em class="italic">Chapter 9</em></a>, you’ll learn how to secure microservices through authentication, authorization, and API protection, while also implementing logging and monitoring tools to proactively detect and address potential issues.</p>
		</div>
	

		<div><h1 id="_idParaDest-145" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor146"/>Part 3:Securing, Testing, and Deploying Microservices</h1>
			<p>In this final part, we will focus on the crucial aspects of securing, testing, and deploying microservices. We’ll learn about implementing authentication, authorization, and monitoring tools to ensure that your microservices are secure and reliable. This section also covers the process of building a CI/CD pipeline, which is vital for automating the deployment of your microservices, and concludes with strategies to deploy our microservices to production.</p>
			<p>This part contains the following chapters:</p>
			<ul>
				<li><a href="B09148_09.xhtml#_idTextAnchor147"><em class="italic">Chapter 9</em></a>, <em class="italic">Securing Microservices</em></li>
				<li><a href="B09148_10.xhtml#_idTextAnchor160"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring Microservices</em></li>
				<li><a href="B09148_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a>, <em class="italic">Microservices Architecture</em></li>
				<li><a href="B09148_12.xhtml#_idTextAnchor196"><em class="italic">Chapter 12</em></a>, <em class="italic">Testing Microservices</em></li>
				<li><a href="B09148_13.xhtml#_idTextAnchor211"><em class="italic">Chapter 13</em></a>, <em class="italic">A CI/CD Pipeline for Your Microservices</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>