- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-Time Data Streaming Using Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Certain microservice applications, such as financial trading platforms and ride-hailing
    services, demand events to be produced and consumed with minimal latency. Real-time
    data streaming has become increasingly crucial in modern software development
    due to its ability to provide immediate, continuous insights and responses based
    on the most current data. This type of real-time data usage is particularly important
    in industries such as finance, healthcare, and logistics, where delays in data
    processing can lead to significant losses or even life-threatening situations.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that rely on real-time data can offer a more responsive and interactive
    user experience. For example, social media platforms, online gaming, and live
    sports streaming rely on real-time data to keep users engaged and provide a seamless
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is all about real-time streaming with microservices. Our purpose
    is to understand when and how to establish such a type of communication when dealing
    with microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is real-time streaming?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with the earthquake streaming API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the earthquake stream consumer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with the chapter, you will need an IDE (we prefer Visual Studio
    Code), Postman, Docker, and a browser of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: It is preferable to download our repository from [https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript](https://github.com/PacktPublishing/Hands-on-Microservices-with-JavaScript)
    and open the `Ch08` folder to easily follow along with the code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: What is real-time streaming?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Real-time streaming** is a data-processing paradigm where data is continuously
    generated, transmitted, and processed as it is created, with minimal delay. Unlike
    batch processing, which collects and processes data in large groups or batches
    at regular intervals, real-time streaming focuses on the immediate and continuous
    flow of data, enabling instant analysis and response. It’s like watching a live
    stream instead of waiting for a video to download entirely.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue any further, let us look at some of the key characteristics
    of real-time streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous data flow**: Real-time streaming is like a never-ending flow of
    information coming in all at once from different places. This information can
    be from sensors, people using things online, money being bought and sold, such
    as Bitcoin, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low latency**: The main goal of real-time streaming is to make the delay
    between information being created and it being used as short as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event-driven processing**: Real-time streaming works by following events
    as they happen, such as things being created or changing. Each event is dealt
    with on its own or in small batches, so the system can react right away to new
    situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Real-time streaming systems can handle different amounts and
    speeds of information, growing bigger or smaller depending on how much information
    is coming in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: To ensure continuous operation, real-time streaming systems
    incorporate fault tolerance mechanisms, such as data replication and automatic
    recovery from failures. As we mentioned in previous chapters, this is one of the
    important attributes of Apache Kafka, which we plan to also use for this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency**: Maintaining data consistency is important in real-time
    streaming, especially when processing brings multiple distributed components into
    the table. Techniques such as **exactly-once processing** and **idempotency**
    are employed to ensure accuracy. Exactly-once processing ensures that each message
    is processed only once, even in the case of failures or retries, preventing duplicates.
    As we use Apache Kafka for most chapters, you can easily configure idempotency
    and exactly-once behavior in it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of real-time data in modern applications cannot be overstated.
    In today’s data-driven world, the ability to process and act on data as it is
    generated provides a significant competitive edge.
  prefs: []
  type: TYPE_NORMAL
- en: Why real-time data is essential
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some key reasons why real-time data is a must-have for most modern
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhanced decision-making**: Real-time data can enhance an application’s decision-making
    abilities because of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Immediate insights**: Real-time data provides immediate insights, allowing
    businesses to make informed decisions quickly. This is important in dynamic environments
    such as stock trading, where market conditions can change rapidly.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proactive problem-solving**: By continuously monitoring data, organizations
    can identify and address issues before they escalate, reducing downtime and enhancing
    operational efficiency.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved user experience**: Real-time data empowers applications to provide
    a more dynamic and personalized user experience by enhancing interactivity and
    responsiveness while tailoring content to individual preferences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operational efficiency**: Organizations can significantly boost their efficiency
    by using real-time data, which enables both real-time monitoring and automation
    of processes, helping to streamline operations and reduce costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Competitive advantage**: Leveraging real-time data gives businesses a distinct
    edge by enhancing their agility and fostering innovation, allowing them to swiftly
    respond to market changes and create cutting-edge products and services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased revenue**: Utilizing real-time data enables businesses to enhance
    their revenue streams through optimized marketing strategies and dynamic fraud
    detection, ensuring more effective customer engagement and financial security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced security**: Real-time data strengthens security by enabling continuous
    monitoring and anomaly detection, allowing organizations to quickly identify and
    respond to potential threats and system irregularities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and flexibility**: Real-time data systems provide the ability
    to efficiently handle large volumes of data while maintaining adaptability, ensuring
    optimal performance even as data loads and requirements fluctuate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer satisfaction**: Real-time data enhances customer satisfaction by
    enabling instant support and immediate feedback, allowing businesses to quickly
    address concerns and continuously improve their products and services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time streaming allows data to be processed as it’s generated, offering
    immediate insights and responses. This continuous flow of data, coupled with low
    latency and event-driven processing, is crucial in industries such as finance,
    healthcare, and logistics. The ability to make real-time decisions, enhance user
    experiences, and improve operational efficiency provides businesses with a competitive
    edge, fostering innovation and increasing revenue while ensuring system scalability,
    fault tolerance, and enhanced security.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the beginning of the chapter, I briefly mentioned that the use of real-time
    data can have incredible impacts on some industries. Therefore, it is crucial
    to understand the use cases of real-time data when you design your microservices.
    Let’s look at these use cases here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Financial services**: Real-time data plays a pivotal role in this industry,
    enabling algorithmic stock trading with split-second decisions and supporting
    continuous risk management to ensure compliance and mitigate potential financial
    threats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**: Real-time data is transforming healthcare by enabling continuous
    patient monitoring for timely interventions and enhancing telemedicine through
    real-time video consultations and data sharing, improving patient care and accessibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retail and e-commerce**: Real-time data enhances retail and e-commerce operations
    by optimizing inventory management to prevent shortages and enabling dynamic pricing
    strategies that adjust to demand and competitor activity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transportation and logistics**: Real-time data optimizes fleet management
    by improving route planning and delivery times, while real-time traffic data enhances
    traffic management, reducing congestion and improving overall mobility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Telecommunications**: Real-time data enhances network management by ensuring
    continuous performance monitoring for optimal service quality, while also improving
    customer experience through the rapid resolution of network issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, what we can say for sure is that real-time data is a cornerstone
    of modern applications, driving enhanced decision-making, improved user experiences,
    operational efficiency, and competitive advantage. By leveraging real-time data,
    organizations can innovate, adapt, and thrive in a rapidly changing digital landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Relationship between real-time streaming and microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have understood what real-time data is and exactly why it’s necessary,
    it’s time we understood the relationship between real-time streaming and microservices.
    The union of real-time streaming with microservices is a symbiotic one, which
    extends the power, productivity, and scalability of modern software architectures.
    Systems now, as a service, are more reactive, more adaptable, and faster as a
    result of this integration. Let’s try to understand how real-time streaming and
    microservices play well with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decoupling and scalability**: Real-time streaming complements microservices
    by promoting loose coupling and independent scaling, allowing services to communicate
    asynchronously and scale efficiently based on demand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility and agility**: The combination of real-time streaming with microservices
    enhances flexibility and agility, enabling continuous service evolution and real-time
    data processing for applications requiring immediate insights and rapid iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilience and fault tolerance**: Integrating real-time streaming with microservices
    enhances resilience and fault tolerance by isolating failures to individual services
    and ensuring data durability, allowing for seamless recovery and continuous operation
    even in the event of service disruptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time communication**: Real-time streaming enhances communication within
    microservices by enabling event-driven architecture and immediate data propagation,
    allowing services to interact asynchronously and respond quickly to events, leading
    to more responsive and synchronized systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operational efficiency**: Combining real-time streaming with microservices
    enhances operational efficiency by optimizing resource utilization and simplifying
    data pipelines, allowing continuous data flow and reducing the complexity of traditional
    batch-processing methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced monitoring and analytics**: Integrating real-time streaming with
    microservices enables real-time monitoring and analytics, offering immediate visibility
    into service performance and providing actionable insights that allow for the
    proactive management and dynamic optimization of services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synergy between real-time streaming and microservices offers a robust framework
    for building responsive, scalable, and efficient systems. By leveraging the strengths
    of both paradigms, organizations can create applications that are capable of handling
    dynamic workloads, providing real-time insights, and delivering superior user
    experiences. This combination is particularly powerful in environments where rapid
    data processing and immediate reactions are critical to success.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices we will develop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make our learning process more interactive and more understandable, we will
    develop two simple microservices. The first microservice will act as a producer
    of stream and the domain of this microservices will be an earthquake. An API that
    streams real-time information about earthquakes can be valuable for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Emergency response**: Real-time data can be crucial for emergency responders
    who need to assess damage and deploy resources quickly after an earthquake. The
    API will provide information on the location, magnitude, and depth of the earthquake,
    which can help responders prioritize areas that may be most affected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Public awareness**: The API could be used for public awareness to create
    applications that send alerts to people in affected areas. This could help people
    take shelter or evacuate if necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Research**: Researchers can use the API to track earthquake activity and
    improve their understanding of earthquake patterns. This data can be used to develop
    better earthquake prediction models and improve building codes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**News and media**: News organizations can use the API to get real-time updates
    on earthquake activity, which can help them report on the latest developments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these, there are commercial applications for such an API as well.
    For instance, insurance companies could use it to assess potential risks and losses,
    or engineering firms could use it to design earthquake-resistant structures.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, when building such type of APIs for production, we will need to choose
    a reliable source of earthquake data; but to demonstrate the purpose and implementation
    of real-time data streaming, our API will act as a source of truth.
  prefs: []
  type: TYPE_NORMAL
- en: From a data format perspective, we should select a format for the data that
    is easy to use and integrate with other applications. Common formats include JSON
    and XML. Our choice is JSON.
  prefs: []
  type: TYPE_NORMAL
- en: By providing valuable and timely data, your earthquake API can be a useful tool
    for a variety of users.
  prefs: []
  type: TYPE_NORMAL
- en: The second microservice is going to be a consumer of the data. Throughout our
    learning process, we have implemented our microservices using different packages
    and frameworks with nearly full skeletons. For the current chapter, our focus
    is streaming rather than building an application skeleton from scratch. Our focus
    is not on implementing any architecture. You can refer to previous chapters if
    you want to add additional functionalities and make it a fully self-contained
    architectural application.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with an earthquake streaming API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our GitHub repository, in the `Ch08` folder, we have two subfolders: `earthquakeService`,
    the earthquake streaming API, and `earthquakeConsumer`, the consumer API. As we
    mentioned before, our main focus is on implementing streaming. To make this chapter
    more focused on the topic, we haven’t implemented a proper detailed design for
    this API. This is also the case with the consumer API.'
  prefs: []
  type: TYPE_NORMAL
- en: It is best to follow along by creating everything with us from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '`earthquakeService` has the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First, you need to generate a `package.json` file that contains all dependencies.
    To create the file, run `npm init` and follow the prompts from the terminal. After
    `package.json` is created, run the `npm install 'your_required_package_names'`
    template command to install packages one by one. For example, to install the `express`
    package, just run `npm install express`, and hit *Enter*. We have already talked
    about `package.json` and the package installation process. You can check the previous
    chapters for more information. While we have reused some of the microservices
    from our previous chapter in our current chapter, we’re also going to use `node-rdkafka`
    package which is new for us.
  prefs: []
  type: TYPE_NORMAL
- en: '`node-rdkafka` is a Node.js library that provides a wrapper around the native
    librdkafka library, enabling efficient communication with Apache Kafka for high-performance
    data streaming. It leverages the power of `librdkafka` for efficient communication
    with Kafka and handles complexities such as balancing writes and managing brokers,
    making Kafka interaction easier for developers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install `node-rdkafka` using `npm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It is not the only package to use for streaming, and depending on your personal
    preference, you can select any other one. The `node-rdkafka` package supports
    a really easy stream writing and reading process, which is why we prefer to use
    it in this chapter for learning purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should always try to use official packages for production apps. Using official
    packages helps keep your app safe because trusted developers manage them, and
    they are checked often. They are also more reliable, as they are tested, updated,
    and have good support, which is important for apps in production.
  prefs: []
  type: TYPE_NORMAL
- en: We use Apache Kafka as a streaming platform. So, you need Apache Kafka to be
    running. As before, we plan to use the `docker-compose.yml` file, which should
    be up and running with Apache Kafka. Our `docker-compose.yml` file for this example
    will only contain the services needed for Kafka, excluding unnecessary components
    like PostgreSQL to reduce resource usage. Of course, you can run the `docker-compose.yml`
    file from the previous chapters that use Apache Kafka, but having additional services
    will use up more resources on your PC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is our `docker-compose.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this configuration, we define `INTERNAL` and `EXTERNAL` listeners to differentiate
    between connections within the Docker network (`INTERNAL://kafka1:9092`) and connections
    from outside the Docker network, such as your local machine (`EXTERNAL://localhost:29092`).
    This separation ensures that services within the Docker network can use the internal
    address, while external clients (like a Node.js app running on your host) can
    connect using the external port. By doing so, Kafka can properly advertise the
    correct addresses to different clients, avoiding connection issues caused by mismatched
    listener configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This file contains Apache Kafka, the Kafka UI, and ZooKeeper. Just check our
    root folder (`Ch08/earthquakeService`) to find and run it. To run the `docker-compose.yml`
    file, first launch Docker Desktop, ensure it’s running, and then follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Pull and open `Ch08` from the repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the project from Visual Studio Code (or any text editor you prefer) and
    navigate to `Ch08`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you use Visual Studio Code, then go to **Terminal** | **New Terminal** from
    the **Menu**; otherwise, use the command line to navigate to the root folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `docker-compose up -d` command from the terminal (*Figure 8**.1*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1: Docker Desktop after running the docker-compose.yml file](img/B09148_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Docker Desktop after running the docker-compose.yml file'
  prefs: []
  type: TYPE_NORMAL
- en: To connect to Apache Kafka, we need to store the required configuration in a
    separate file. That is why we use the `dotenv` package to read configuration information.
    Create a `configs` folder under the root folder (`Ch08/earthquake`) and add a
    `.``env` file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `config` and `configs` folders are separate and serve different purposes.
    Be sure to use the correct folder to avoid confusion. We store the `.env` file
    under the `configs` folder. On the other hand, we store the `config.js` file under
    the `config` folder, which loads environment variables using the `dotenv` package,
    validates them with `Joi`, and returns a configuration object for a Kafka-based
    microservice, throwing an error if validation fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what the `configs/.env` file should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We have Kafka configuration such as client ID, brokers, and topic name with
    port information. As we learned before, all application source code lives under
    the `src` folder. Create the `src` folder on the same level as your `configs`
    folder (*Figure 8**.2*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: General structure of earthquakeService](img/B09148_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: General structure of earthquakeService'
  prefs: []
  type: TYPE_NORMAL
- en: 'We store configuration information in the `.env` file, but we need to add a
    reading and validating mechanism over our `config`. To implement proper reading
    and validating, we need to create a `configs.js` file under the `src/configc`
    folder. Here is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We are using the same `config` read and validation mechanism as the account
    microservice. We have already explained this file in the [*Chapter 7*](B09148_07.xhtml#_idTextAnchor121).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `services` folder is responsible for storing service files. To implement
    real-time streaming functionality, we need to create a new file called `earthquake.js`
    under the `services` folder. Here is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This code defines a class called `EarthquakeEventProducer` that simulates generating
    and publishing earthquake event data to a Kafka topic. Let’s walk through the
    code’s elements here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`require(''node-rdkafka'')`: Imports the `node-rdkafka` library for interacting
    with a Kafka cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`require(''../config/config'')`: Imports a function (likely from `../config/config.js`)
    that reads configuration settings from a file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`require(''path''):` Imports the `path` module for file path manipulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`The EarthquakeEventProducer class`: This class handles earthquake event generation
    and publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#generateEarthquakeEvent()`: This private method generates a simulated earthquake
    event object with the following properties:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id`: A random unique identifier string.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`magnitude`: A random floating-point number between `0` and `9` representing
    the earthquake’s magnitude'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`location`: An object containing the following:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latitude`: A random floating-point number between `-90` and `90` representing
    the latitude.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`longitude`: A random floating-point number between `-180` and `180` representing
    the longitude.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timestamp`: The current timestamp in milliseconds.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is how we specify our main method called `runEarthquake`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break this code down here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`runEarthquake()`: This async method is responsible for setting up the Kafka
    producer and publishing earthquake events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`configPath`: This constructs the path to the configuration file using `path.join`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`appConfig`: This reads configuration from the file using the imported `createConfig`
    function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream`: This creates a Kafka producer write stream using `Kafka.Producer.createWriteStream`.
    The configuration includes the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''metadata.broker.list''`: A comma-separated list of Kafka broker addresses
    from the configuration'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''client.id''`: A unique identifier for this producer client from the configuration'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Topic`: The exact topic that should get the streamed data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream.on(''error'')`: This attaches an event listener for errors in the Kafka
    stream. It logs the error message to the console.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setInterval`: This sets up an interval timer to generate and publish events
    every 100 milliseconds (adjustable). Inside the interval callback is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`event`: Generates a new earthquake event object using `#generateEarthquakeEvent`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream.write`: Attempts to write the event data (converted to a buffer using
    `JSON.stringify`) to the Kafka stream'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`queuedSuccess`: Checks the return value from `stream.write`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`true`: Indicates successful queuing of the message. A success message is logged
    to the console.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`false`: Indicates the stream’s queue is full. A message about exceeding the
    queue capacity is logged to the console.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to stop our earthquake service, we need to clear the interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `stopEarthquake()` method stops the ongoing earthquake event stream by checking
    whether there is an active interval running, indicated by the presence of `this.intervalId`.
    If the interval exists, it uses `clearInterval()` to stop the event generation
    and resets `this.intervalId` to `null` to indicate that the stream has stopped.
    A success message is logged when the stream is stopped. If no interval is running
    (i.e., `this.intervalId` is `null`), it logs a message saying there’s no active
    stream to stop. This ensures that the function can only stop an existing stream
    and won’t attempt to stop a non-existent one.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, this code simulates earthquake event generation and publishes these
    events to a Kafka topic at regular intervals, demonstrating basic Kafka producer
    usage with error handling and logging.
  prefs: []
  type: TYPE_NORMAL
- en: 'We plan to launch streaming using an API, but to make things as simple as possible,
    we use a minimal API approach that doesn’t require us to create controllers. This
    behavior is implemented in the `app.js` file. Here is the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The code defines two API endpoints using Express.js to start and stop an earthquake
    event stream. The `/earthquake-events/start` endpoint triggers the `runEarthquake()`
    function from the `EarthquakeEventProducer` class, starting the event stream,
    and responds with a success message. The `/earthquake-events/stop` endpoint calls
    the `stopEarthquake()` function to stop the event stream and also responds with
    a success message. The `earthquakeProducer` object is an instance of the `EarthquakeEventProducer`
    class, which manages the event stream operations. Finally, the Express app is
    exported to be used in other parts of the application. This setup allows external
    clients, such as Postman, to control the Kafka event stream through API calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an Express.js application, the `index.js` file in the root directory typically
    serves as the entry point for your server. It acts as the central hub where you
    configure and launch your Express app. Here is our `index.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the following functionalities in the `index.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: Imports the Express app (`app.js`) and configuration function (`config.js`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reads configuration from a file using `createConfig`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starts the server using `app.listen` on the configured port and logs a message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defines functions to gracefully close the server and handle unexpected errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attaches event listeners for uncaught exceptions and unhandled promise rejections,
    calling the error-handler function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, calls the `execute` function to start everything.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have implemented our `earthquakeService`; now it is time to test it. Here’s
    how you can do that:'
  prefs: []
  type: TYPE_NORMAL
- en: Open **Terminal** | **New Terminal** from the menu if you’re using Visual Studio
    Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the `src` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the `node` `index.js` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To stop streaming, open Postman and send a POST request to `http://localhost:3001/earthquake-events/stop`
    (*Figure 8**.3*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.3: Stopping event streaming](img/B09148_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Stopping event streaming'
  prefs: []
  type: TYPE_NORMAL
- en: The topic should automatically be created with some events (*Figure 8**.4*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Apache Kafka event after streaming](img/B09148_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Apache Kafka event after streaming'
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented the streaming API. Now it is time to consume data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the earthquake stream consumer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Producing is not very valuable if you don’t have a consumer to consume data.
    Our second microservice, called `earthquakeConsumer`, is going to consume data
    from Apache Kafka. It has a similar code structure to our streaming API (*Figure
    8**.5*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Earthquake consumer API structure](img/B09148_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Earthquake consumer API structure'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start from the `configs` folder. As in our first microservice in [*Chapter
    5*](B09148_05.xhtml#_idTextAnchor074), we have a `.env` file inside the folder.
    The responsibility of this folder is to store relevant configurations. Here is
    what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We introduced an additional configuration, `KAFKA_GROUP_ID`, which identifies
    the consumer group, allowing Kafka to balance partition assignments among consumers.
    It is a string property used to identify a collection of consumer instances and
    acts as the glue that binds consumers together for collaborative consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka automatically distributes topic partitions among consumers in the same
    group, allowing parallel processing while ensuring that each partition is consumed
    by only one consumer at a time within the group. If a consumer in the group fails,
    Kafka reassigns its partitions to remaining active consumers, ensuring uninterrupted
    message processing. With proper configuration, consumer groups can achieve exactly-once
    delivery semantics, guaranteeing each message is processed by only one consumer
    exactly once. When working with Kafka consumer groups, it’s essential to understand
    how they manage message consumption and workload distribution across multiple
    consumers. The following are key points to keep in mind when configuring and utilizing
    consumer groups for efficient message processing:'
  prefs: []
  type: TYPE_NORMAL
- en: Only one consumer can process a partition at a time within a group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consumers with different group IDs treat topics as independent streams and don’t
    share the workload.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always consider using a meaningful group ID to improve cluster management and
    monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To read and validate this config, we use the same mechanism as we did for the
    streaming API. We have `src/config/config.js`. It reads and validates our configuration
    with the additional `KAFKA_GROUP_ID`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main functionality has been implemented inside the `src/service/earthquake.js`
    file. Here is our stream-consuming process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This code defines a class named `EarthquakeEventConsumer`, which acts as a
    consumer for messages from a Kafka topic containing earthquake event data. Here’s
    a breakdown of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Kafka` from `node-rdkafka`: This library provides functionalities to interact
    with Kafka as a consumer or producer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`createConfig` from `../config/config`: This imports a function from another
    file (`config/config.js`) that reads configuration details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path`: This is a built-in Node.js module for manipulating file paths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EarthquakeEventConsumer`: This class is responsible for consuming earthquake
    event data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`constructor()`: This special method is called when you create a new instance
    of `EarthquakeEventConsumer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`configPath`: This constructs the path to a configuration file (such as a `.env`
    file) containing Kafka connection details such as brokers and group ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`appConfig`: This calls the `createConfig` function (imported from another
    file) to read the configuration details from the `.env` file and stores it in
    `this.appConfig`. This makes the configuration accessible throughout the object’s
    lifetime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`this.stream`: This line is the key part. It uses `Kafka.KafkaConsumer.createReadStream`
    to create a stream for reading messages from Kafka. Here’s what the options passed
    to `createReadStream` do:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''metadata.broker.list''`: This specifies the list of Kafka brokers to connect
    to, obtained from the configuration stored in `this.appConfig`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''group.id''`: This sets the consumer group ID, also obtained from the configuration.
    Consumers in the same group will share the messages from a topic among themselves.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''socket.keepalive.enable''`: This enables a mechanism to keep the connection
    alive with the broker.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''enable.auto.commit''`: This is set to `true` to enable the automatic committing
    of offsets.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topics`: This specifies the Kafka topic name to consume from, obtained from
    the configuration (likely `librdtesting-01` in this case).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`waitInterval`: This is set to `0`, indicating no waiting between attempts
    to receive messages if none are available.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`objectMode`: This is set to `false`, meaning the messages received from the
    stream will be raw buffers, not JavaScript objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Crucially, this stream creation happens only once in the constructor, ensuring
    efficiency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`async consumeData()`: This is an asynchronous method that initiates the data
    consumption process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.on(''data'', ...)`: This sets up a listener for the data event emitted by
    the pre-created stream (`this.stream`). The callback function executes each time
    a new message arrives, logging that a message was received and parsing the JSON-encoded
    data for further handling.The callback function logs a message indicating a new
    message was received. It then parses the raw message buffer (assuming it’s JSON-encoded
    data) using `JSON.parse` and logs the parsed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`module.exports = EarthquakeEventConsumer`: This line exports the `EarthquakeEventConsumer`
    class so it can be used in other parts of your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize, the code defines a consumer that connects to Kafka, subscribes
    to a specific topic, and listens for incoming earthquake event data. It then parses
    the JSON-encoded messages and logs them to the console. The key improvement here
    is creating the Kafka consumer stream only once in the constructor, making the
    code more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: To run the service, we have `app.js` and `index.js`, which follow the same structure
    as our streaming API.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now implemented our `earthquakeConsumer` and it is time to test it:'
  prefs: []
  type: TYPE_NORMAL
- en: Open **Terminal** | **New Terminal** from the menu if you use Visual Studio
    Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the `src` folder (`Ch08/earthquakeConsumer/src`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `node` `index.js` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You don’t need to manually navigate to the `src` folder and run `node index.js`
    every time you want to start the application. Instead, you can streamline this
    process by configuring a script in your `package.json` file. Simply add the following
    to the `scripts` section of `package.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{`'
  prefs: []
  type: TYPE_NORMAL
- en: '`“``scripts”: {`'
  prefs: []
  type: TYPE_NORMAL
- en: '`“start”: “``node src/index.js”`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is set up, you can start your application from the root of your project
    by simply running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will automatically launch the application, saving you time and effort
    each time you run the code. When running the earthquake consumer service using
    Node.js, the following output confirms that the service has started successfully
    and is ready for operation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Go to Postman and hit **Send**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While the `earthquakeService` streaming API prints **The message has been queued!**,
    our consumer API will print consumed data such as that shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can add some more logic to these services, but this should be enough to
    demonstrate streaming as simply as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the concept of real-time data streaming in microservices
    architecture. We used the example of an earthquake data-streaming service to illustrate
    how microservices can efficiently handle continuous flows of information.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than storing data in bulk, the producer service publishes data as a continuous
    stream, allowing immediate processing and analysis as each new data point arrives.
    This approach is beneficial for real-time scenarios where immediate processing
    and analysis are crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Another microservice acts as the consumer in this scenario. It subscribes to
    the earthquake data stream produced by the first service. As new data arrives,
    the consumer microservice receives and processes it in real time.
  prefs: []
  type: TYPE_NORMAL
- en: The consumer microservice can perform various actions based on the earthquake
    data. It might trigger alerts, update dashboards, or integrate with other services
    for further analysis and response.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data streaming with microservices offers a powerful approach to handling
    continuous information flows. In [*Chapter 9*](B09148_09.xhtml#_idTextAnchor147),
    you’ll learn how to secure microservices through authentication, authorization,
    and API protection, while also implementing logging and monitoring tools to proactively
    detect and address potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: Part 3:Securing, Testing, and Deploying Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final part, we will focus on the crucial aspects of securing, testing,
    and deploying microservices. We’ll learn about implementing authentication, authorization,
    and monitoring tools to ensure that your microservices are secure and reliable.
    This section also covers the process of building a CI/CD pipeline, which is vital
    for automating the deployment of your microservices, and concludes with strategies
    to deploy our microservices to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B09148_09.xhtml#_idTextAnchor147), *Securing Microservices*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B09148_10.xhtml#_idTextAnchor160), *Monitoring Microservices*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B09148_11.xhtml#_idTextAnchor174), *Microservices Architecture*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B09148_12.xhtml#_idTextAnchor196), *Testing Microservices*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B09148_13.xhtml#_idTextAnchor211), *A CI/CD Pipeline for Your
    Microservices*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
