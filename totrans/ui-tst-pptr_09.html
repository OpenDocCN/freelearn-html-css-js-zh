<html><head></head><body>
		<div><h1 id="_idParaDest-144"><em class="italic"><a id="_idTextAnchor156"/>Chapter 9</em>: Scraping tools</h1>
			<p>Back in <a href="B16113_01_Final_SK_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting started with Puppeteer</em>, we talked about the different uses for web automation. Of all those use cases, web scraping is the one that excites developers the most. When I give talks about automation, I know that I will get the crowd's full attention when I start talking about task automation, but even more when I get into the topic of <strong class="bold">web scraping</strong>.</p>
			<p>Don't get me wrong, I think that UI testing is important. As we saw in the previous chapters, it's not just about running automation tests but also about taking care of your customers. But web scraping has that fun spark, a hacker feeling, and I didn't want to leave this topic out of the book.</p>
			<p>A few months ago, I read a book about web scraping that included a chapter about UI testing at the end of the book. We are going to do the same, but the other way around. This is a UI testing book with a scraping chapter at the end.</p>
			<p>We will begin this chapter by defining and demystifying web scraping. Is it just for hackers? Is it even legal? We will also talk about scraping ethics, such as when it is OK to scrape and when it is not.</p>
			<p>The second part of the chapter will discuss the different tools available to scrape with Puppeteer.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Introduction to web scraping</li>
				<li>Creating scrapers</li>
				<li>Running scrapers in parallel</li>
				<li>How to avoid being detected as a bot</li>
				<li>Dealing with authentication and authorization</li>
			</ul>
			<p>By the end of this chapter, you will be able to apply all the concepts you have learned during the course of this book in a brand-new field of web automation.</p>
			<p>Let's get started.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor157"/>Technical requirements</h1>
			<p>You will find all the code of this chapter on the GitHub repository (<a href="https://github.com/PacktPublishing/UI-Testing-with-Puppeteer">https://github.com/PacktPublishing/UI-Testing-with-Puppeteer</a>) under the <code>Chapter9</code> directory. Remember to run <code>npm install</code> on that directory.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor158"/>Introduction to web scraping</h1>
			<p>The best way to <a id="_idIndexMarker465"/>introduce a new concept is by giving a few concrete and straightforward definitions. Let's begin by defining <strong class="bold">data scraping</strong>. According <a id="_idIndexMarker466"/>to Wikipedia (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/data-scraping">https://www.hardkoded.com/ui-testing-with-puppeteer/data-scraping</a>), "<em class="italic">Data scraping is a technique in which a computer program extracts data from human-readable output coming from another program</em>." Any information coming out from a computer can be extracted and processed. The first scrapers were called "screen scrapers." A <a id="_idIndexMarker467"/>screen scraper is something as simple as an application that can capture the screen. Then, by <a id="_idIndexMarker468"/>running <strong class="bold">Optical Character Recognition</strong> (<strong class="bold">OCR</strong>), it extracts the text from that image for further processing.</p>
			<p>Web scraping takes this idea to the next level. <em class="italic">Web scraping is a technique used to extract data from one or multiple websites using a piece of software.</em></p>
			<p>You might be wondering: Is that even legal? Amazon is a public site. I can freely navigate through the site; why wouldn't I be able to run a script to extract data that is already public? Well, it depends. Let me share with you some scenarios from the real world that have similar ethical dilemmas to web scraping.</p>
			<p><strong class="bold">First scenario</strong></p>
			<p>A small grocery store owner <a id="_idIndexMarker469"/>goes to a big mall to compare their products and prices. She can't go and get a box of milk and leave without paying, but she can walk around and take notes of the product prices, take that list to her shop, and compare their prices. The price is not a product. She's not stealing anything. Also, the mall is too big, and they can't control every person walking around taking notes. But what if the same person goes to a small grocery store on the next block and starts taking notes? I bet the owner already knows her, and it's too evident that she's taking notes, and that will probably threaten their business. Is it illegal? No. But she might get into a fight.</p>
			<p><strong class="bold">Second scenario</strong></p>
			<p>Some exclusive furniture stores won't let you take photos inside the store.</p>
			<p><strong class="bold">Last scenario</strong></p>
			<p>You can't count cards in the casino! They will kick you out and ban you for life.</p>
			<p>These are real-life "scrapers." People are trying to extract information in the real world. Scraping the web is quite <a id="_idIndexMarker470"/>similar. You will be able to scrape a site as long as a) the site welcomes (implicitly or explicitly) scrapers, b) your attitude is considerate while scraping, and c) what you are scraping is allowed. Let's unpack these concepts.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor159"/>Does the site allow scrapers?</h2>
			<p>The first reaction to this question would be: "No! why would the owner of a website allow scrapers?" But that's <a id="_idIndexMarker471"/>not necessarily true. What if you own a hotel? If an aggregator website scrapes your booking page, then it shows those results on their website with a link back to your site, they will make some profit from that, and you will get more customers: win-win. Or, if you own a non-profit site such as Wikipedia or a government website, scraping might not be an issue. As it's a non-profit website, you shouldn't care much about bots coming to your site to extract data unless they affect your website's performance. But if your site is about song lyrics, you won't want anybody to come to your site and extract the lyrics. Lyrics are your product, your assets.</p>
			<p>In this chapter, we will see many techniques to bypass some validations, but my personal rule is: If the site doesn't want to be scraped, I won't scrape it. No means no.</p>
			<p>So, how do we know whether we can scrape a website? The owner of a website can express that in at least four different ways.</p>
			<h3>Terms and conditions</h3>
			<p>The first thing you should <a id="_idIndexMarker472"/>check before scraping a website is the terms and conditions. The website owners can make it very clear that they don't want it to be scraped.</p>
			<p>The terms and conditions is that huge block of text we often ignore when installing an app. I bet you've also had emails telling you that a website has changed its terms and conditions, and you said "yeah, whatever" and archived the email.</p>
			<p>But we shouldn't do that. According to <a id="_idIndexMarker473"/>iubenda (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/iubenda-terms">https://www.hardkoded.com/ui-testing-with-puppeteer/iubenda-terms</a>), "'<em class="italic">Terms and Conditions' is the document governing the contractual relationship between the provider of a service and its user. The Terms and Conditions are nothing other than a contract in which the owner clarifies the conditions of use of its service</em>." Sometimes we might think that when we buy <a id="_idIndexMarker474"/>digital content (software, music, e-books) on a website, we then own that product, when, in fact, if you read the terms and conditions, you have bought the right to use the product, but you don't own it.</p>
			<p>The terms and conditions also states what they allow you to do on a website. Many sites make that very clear. Let's take the <a id="_idIndexMarker475"/>example of the terms and conditions of www.ebay.com:</p>
			<p><em class="italic">3. Using eBay</em></p>
			<p><em class="italic">In connection with using or accessing the Services you will not:</em></p>
			<p><em class="italic">use any robot, spider, scraper or other automated means to access our Services for any purpose;</em></p>
			<p><em class="italic">bypass our robot exclusion headers, interfere with the working of our Services, or impose an unreasonable or disproportionately large load on our infrastructure.</em></p>
			<p>As we can see, eBay makes it very clear. You can't scrape their site. eBay <a id="_idIndexMarker476"/>v. Bidder's Edge's (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/ebay-vs-edge">https://www.hardkoded.com/ui-testing-with-puppeteer/ebay-vs-edge</a>) was a well-known case back in the 2000s. eBay alleged that Bidder's Edge activities constituted a <a id="_idIndexMarker477"/>trespass of eBay's chattels (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/Trespass-to-chattels">https://www.hardkoded.com/ui-testing-with-puppeteer/Trespass-to-chattels</a>). In other words, Bidder's Edge's scraping affected the servers that are eBay's property. I bet you don't want to go to court against eBay.</p>
			<p>Now, let's <a id="_idIndexMarker478"/>take a look at Ryanair's terms and conditions (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/ryanair-terms">https://www.hardkoded.com/ui-testing-with-puppeteer/ryanair-terms</a>):</p>
			<p><em class="italic">Use of any automated system or software, whether operated by a third party or otherwise, to extract any data from this website for commercial purposes ("screen scraping") is strictly prohibited.</em></p>
			<p>Ryanair doesn't like scrapers either, but it says "for commercial purposes," which would mean that you could code your scraper to look for the best price for your next vacation.</p>
			<p>If the terms and conditions is not clear regarding scrapers, the second way a site owner can express their relationship with scrapers is through the <code>robots.txt</code> file.</p>
			<h3>robots.txt file</h3>
			<p>Wikipedia, again, has a great <a id="_idIndexMarker479"/>definition of the robots file. According to Wikipedia (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/Robots-exclusion-standard">https://www.hardkoded.com/ui-testing-with-puppeteer/Robots-exclusion-standard</a>), the robots exclusion protocol "<em class="italic">is a standard used by websites to communicate with web crawlers and other web robots. The standard specifies how to inform the web robot about which areas of the website should not be processed or scanned. Robots are often used by search engines to categorize websites</em>."</p>
			<p>The keyword in that definition is "inform." The website owner can express in the robots file which parts of the site can be scraped. Most websites only use the <code>robots.txt</code> file to tell search engines where they can find the sitemap to scrape:</p>
			<pre>User-agent: *
Sitemap: https://www.yoursite.com/sitemap.xml</pre>
			<p>With these two simple lines of code, they tell search engines, such as Google, to get that Sitemap file and scrape those pages. But you can find more complex definitions in that file. For instance, the <code>robots.txt</code> file on Wikipedia has over 700 lines! That tells us that the site is being scraped quite a lot. Let's see some examples of what we can find in that file:</p>
			<pre># Please note: There are a lot of pages on this site, and there are
# some misbehaved spiders out there that go _way_ too fast. If you're
# irresponsible, your access to the site may be blocked.</pre>
			<p>I love that the file begins with a message to us! They expect us to come to this page to read it. The next section is interesting:</p>
			<pre># Crawlers that are kind enough to obey, but which we'd rather not have
# unless they're feeding search engines.
User-agent: UbiCrawler
Disallow: /
User-agent: DOC
Disallow: /
User-agent: Zao
Disallow: /
# Some bots are known to be trouble, particularly those designed to copy
# entire sites. Please obey robots.txt.
User-agent: sitecheck.internetseer.com
Disallow: /</pre>
			<p>Here, Wikipedia informs <a id="_idIndexMarker480"/>that they don't want scrapers identified with the user-agent <strong class="bold">UbiCrawler</strong>, <strong class="bold">DOC</strong>, <strong class="bold">Zao</strong>, and <strong class="bold">sitecheck.internetseer.com</strong> to scrape the site. And the file closes with general rules for all user-agents:</p>
			<pre>User-agent: *
Allow: /w/api.php?action=mobileview&amp;
Allow: /w/load.php?
Allow: /api/rest_v1/?doc
Disallow: /w/
Disallow: /api/
Disallow: /trap/
Disallow: /wiki/Special:
…</pre>
			<p>They basically say that all the rest (<code>User-agent: *</code>) can scrape the whole site except some URLs such as <code>/w/</code>, <code>/api/</code>, and so on.</p>
			<p>If we don't find anything <a id="_idIndexMarker481"/>useful in the terms and conditions or in the <code>robots.txt</code> file, we might find some hints in th<a id="_idTextAnchor160"/>e page response.</p>
			<h3>Are you a human?</h3>
			<p>I bet no one in real life has asked you whether you are a human, but many websites ask us that question all the <a id="_idIndexMarker482"/>time:</p>
			<div><div><img src="img/Figure_9.01_B16113.jpg" alt="Bot check by reCAPTCHA&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Bot check by reCAPTCHA</p>
			<p>What started as a simple "type the word you see" turned into more and more complicated <a id="_idIndexMarker483"/>challenges:</p>
			<div><div><img src="img/Figure_9.02_B16113.jpg" alt="Complex CAPTCHAs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Complex CAPTCHAs</p>
			<p>Should we check the third square of the first row? Who knows… But their goal is clear. In a friendly way, using the UI, they want to kick scrapers out.</p>
			<p>When a site puts a CAPTCHA, like the one in the previous screenshot, it's not always because they don't want to be scraped. Maybe they want to protect their users. They don't want malicious bots to test users and passwords or stolen credit card numbers. But the intention is clear: that page is for humans.</p>
			<p>I call these validations a "friendly" way to kick bots out. But you can also get some unfriendly responses <a id="_idIndexMarker484"/>from the server:</p>
			<div><div><img src="img/Figure_9.03_B16113.jpg" alt="Page returning a 403 HTTP error&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Page returning a 403 HTTP error</p>
			<p>If a page you would typically be able to access now returns a 403 when you scrape it, it means that the server considered your actions as an attack on the server and banned your IP.</p>
			<p>That's terrible news. It means that you won't be able to access that site using your current IP. The ban could be for a few hours or forever. That would also mean that if you share your public IP with other computers, for instance, inside an organization, no one will be able to access that site. There are two ways to fix this issue. One is by reaching out to the site owner, apologizing, and asking them to remove your IP from the forbidden list. Or you could keep playing badly, trying to change your public IP or using proxies. But if you got caught once, you will get caught twice.</p>
			<p>The last way a site can tell you not to scrape is by giving you an API.</p>
			<h3>Using the Website API</h3>
			<p>In this context, an API is <a id="_idIndexMarker485"/>a set of URLs that the website exposes that instead of returning a page, return some data, generally in JSON format:</p>
			<div><div><img src="img/Figure_9.04_B16113.jpg" alt="Wikipedia API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Wikipedia API</p>
			<p>The question here is, why would you waste time parsing HTML elements, wait for network calls, and so on, if you can hit a URL to get all the information you need? </p>
			<p>I think this is an excellent way for a site to tell you: <strong class="bold">hey, don't scrape me, here is the data you might need. You are welcome to come here and fetch the information you need</strong>.</p>
			<p>APIs also give the website's owner the chance to set the rules, such as the rate limit (how many requests you can make per second or minute), the data the API exposes, and what will remain hidden for consumers.</p>
			<p>These are some ways the site <a id="_idIndexMarker486"/>can communicate what we can and can't do. But our attitude toward the site being scraped is also important.</p>
			<h3>Our attitude</h3>
			<p>This part is quite simple, and it can be reduced to two words: be nice. You have to think that on the other side of the page you are trying to scrape, there are people like you whose goal is to keep the site up and running. They have to pay for a server and network bandwidth. Remember, they are not your enemy. They are your friends. They have the data you need, so be nice.</p>
			<p>You should look at available APIs first. If a website exposes an API with the data you want, there is no need to scrape the site.</p>
			<p>Next, you should consider your scraping rate/speed. Although you might want to scrape a site as fast as possible, I would try to make the scrape process close to real user interaction. Later in this chapter, we will see tools to scrape pages in parallel, but I would use those tools very carefully.</p>
			<p>There is one thing that many scrapers using Puppeteer forget. You should always identify yourself as a bot. In the next section, we will see that the idea is telling the server that you are a bot and not a real user.</p>
			<p>The last thing we need to consider is to evaluate what data we are extracting.</p>
			<h3>What's the data we are scraping?</h3>
			<p>I think this is common <a id="_idIndexMarker487"/>sense. Are we extracting copyrighted data? Are we extracting the assets of the site? For instance, if we go to a lyrics website and extract the lyrics, we are taking the very purpose of the website. But if we go to an airline's website and check for flight prices, the price is not the company's asset. They sell flights, not prices.</p>
			<p>The other thing to consider is what the data we extracted will be used for. We should consider whether our actions will empower the scraped site, for instance, in the case of the hotel booking website, they might get more customers, or threaten it, for instance, if we scrape lyrics to create our own lyrics site.</p>
			<p>Truth be told, lots of the sites we know today used scraping techniques to seed their websites. You might see that on real-estate websites. Who would want to go to an empty real-estate website? No one. So, these websites would be seeded with postings from other websites to make them more appealing to new customers.</p>
			<p>Enough theory. Let's create some scrapers.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor161"/>Creating scrapers</h1>
			<p>Let's try to scrape <a id="_idIndexMarker488"/>book prices from the Packt site. The terms and conditions said nothing about scrapers (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/packtpub-terms">https://www.hardkoded.com/ui-testing-with-puppeteer/packtpub-terms</a>). But the <code>robots.txt</code> file has some clear rules:</p>
			<pre>User-agent: *
Disallow: /index.php/
Disallow: /*?
Disallow: /checkout/
Disallow: /app/
Disallow: /lib/
Disallow: /*.php$
Disallow: /pkginfo/
Disallow: /report/
Disallow: /var/
Disallow: /catalog/
Disallow: /customer/
Disallow: /sendfriend/
Disallow: /review/
Disallow: /*SID=</pre>
			<p>They don't want us to go to those pages. But the site has a pretty massive <code>sitemap.xml</code>, with over 9,000 lines. If <code>robots.txt</code> is the "don't go here" sign for scrapers, <code>sitemap.xml</code> is the "please, check this out" sign. These are the first items on the <code>sitemap.xml</code> file:</p>
			<pre>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
    xmlns:image="http://www.google.com/schemas/sitemap-image/1.1"&gt;
    &lt;url&gt;
        &lt;loc&gt;https://www.packtpub.com/web-development&lt;/loc&gt;
        &lt;lastmod&gt;2020-12-15T12:22:50+00:00&lt;/lastmod&gt;
        &lt;changefreq&gt;daily&lt;/changefreq&gt;
        &lt;priority&gt;0.5&lt;/priority&gt;
    &lt;/url&gt;
    &lt;url&gt;
        &lt;loc&gt;https://www.packtpub.com/web-development/ecommerce&lt;/loc&gt;
        &lt;lastmod&gt;2019-09-13T07:29:53+00:00&lt;/lastmod&gt;
        &lt;changefreq&gt;daily&lt;/changefreq&gt;
        &lt;priority&gt;0.5&lt;/priority&gt;
    &lt;/url&gt;
    ...
&lt;/urlset&gt;</pre>
			<p>Based on this XML, we <a id="_idIndexMarker489"/>are going to build a <strong class="bold">crawler</strong>. A crawler is a program that will navigate through the website scraping all the pages. This will be our plan:</p>
			<ul>
				<li>We are going to build an array of book category URLs. To make the run shorter, we will scrape only category pages, such as <a href="https://www.packtpub.com/web-development">https://www.packtpub.com/web-development</a>. We will also limit the list to 10 categories, so we are nice with the server.</li>
				<li>Once we get that list, we will navigate each page, getting book links. We don't want to get duplicates in there, so we need to be careful.</li>
				<li>Once we get the list of books, we navigate each book page and get the price for the print book plus the e-book and the print book alone: </li>
			</ul>
			<div><div><img src="img/Figure_9.05_B16113.jpg" alt="Book details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Book details</p>
			<ul>
				<li>Those prices will be <a id="_idIndexMarker490"/>collected in a JSON array and sent to disk at the end of the process.</li>
			</ul>
			<p>Let's get started! You can get the code of this section in the <code>crawler.js</code> file. We will get the <code>sitemap.xml</code> file and get the first 10 categories:</p>
			<pre>const https = require('https');
(async function() {
    const sitemapxml = await getSitemap();
})();
function getSitemap() {
    let resolve;
    const promise = new Promise(r =&gt; resolve = r);
    https.get('https://www.packtpub.com/sitemap.xml', function(res) {
        let body = '';
        res.on('data', (chunk)  =&gt; body += chunk);
        res.on('end', () =&gt; resolve(body));
    });
    return promise;
};</pre>
			<p>There we are using the native <code>https</code> package to download the <code>sitemap.xml</code> file. We create a Promise that will be resolved when we call the <code>resolve</code> function. Then we call the <code>get</code> function to fetch the file. We collect the information being downloaded in the <code>data</code> event, and when we receive an <code>end</code> event, we resolve the <code>Promise</code> by returning the <code>body</code> string we were <a id="_idIndexMarker491"/>collecting. It might take a while to get the code's flow, but it's very straightforward once you get used to it.</p>
			<p>That <code>sitemapxml</code> variable is a string. We need first to parse the XML and then get a JavaScript model from there. <code>xml2js</code> will do most of the job for us. Let's install that module using <code>npm install</code> in our terminal: </p>
			<pre>npm install xml2js   </pre>
			<p>Once we have that module, we can start using it in our code:</p>
			<pre>const xmlParser = require('xml2js').parseString;</pre>
			<p>We will be able to parse the sitemap by calling the <code>xmlParser</code> function:</p>
			<pre>(async function() {
    const sitemapxml = await getSitemap();
    const categories = await getCategories(sitemapxml);
})();
function getCategories(sitemapxml) {
    let resolve;
    const promise = new Promise(r =&gt; resolve = r);
    xmlParser(sitemapxml, function (err, result) {
        const output = result.urlset.url
          .filter(url =&gt; url.loc[0].match(/\//g).length === 3)
          .slice(0, 10)
          .map(url =&gt; url.loc[0]);
        resolve(output);
    });
    return promise;
};</pre>
			<p>As you can see, <a id="_idTextAnchor162"/>we are using the same <code>Promise</code> pattern we used before. When we call <code>xmlParser</code>, we will get the parsed <code>result</code> in the result argument of the callback we just passed in. Once we get the result, we prepare the output. It might be helpful to read the code while looking at the <code>sitemap.xml</code> file to get more context. We get URL elements from the <code>result.urlset.url</code> array. Then we <code>filter</code> URL elements with a <code>loc</code> with three slashes (such as <a href="https://www.packtpub.com/web-development">https://www.packtpub.com/web-development</a>). Then, we grab only the <a id="_idIndexMarker492"/>first 10 elements using the <code>slice</code> function. Lastly, we use the <code>map</code> function to return only the resulting URL, returning an array of strings containing category URLs.</p>
			<p>Now it's time to use Puppeteer. We will navigate each of the categories we grabbed from the <code>sitemap.xml</code> and return book URLs. We are not going to scrape only the first page of the category page. I'll leave that feature to you as homework.</p>
			<p>Let's begin by creating a browser that we are going to use across the program:</p>
			<pre>(async function() {
    /*Previous code*/
    const books = [];
    const page = await getPuppeteerPage();
})();
async function getPuppeteerPage() {
    const browser = await puppeteer.launch({
        headless: false, 
        slowMo: 500
    });
    const userAgent = await browser.userAgent();
    const page = await browser.newPage();
    await page.setUserAgent(userAgent + ' UITestingWithPuppeteerDemoBot');
    return page;
}</pre>
			<p>I bet you are pretty familiar with this piece of code. The first new thing we can see here is that we are passing the <code>slowMo</code> option. That means that we are going to wait 500 milliseconds after each action <a id="_idIndexMarker493"/>that Puppeteer will perform. We will also use the <code>userAgent</code> function to get the user-agent from the browser. Then, we grab that string, and we append <code>' UITestingWithPuppeteerDemoBot'</code>, so the server admins in the publisher will know that's us.</p>
			<p>Let the scraping begin!</p>
			<pre>(async function() {
    /*Previous code*/
    for(const categoryURL of categories) {
        const newBooks = await getBooks(categoryURL, page);
        if(newBooks) {
            books.push(...newBooks);
        }
    }
    page.browser().close();
})();
async function getBooks(categoryURL, page) {
    try {
        await page.goto(categoryURL);
        await page.waitForSelector('a.card-body');
        return await page.evaluate(() =&gt; {
            const links = document.querySelectorAll('a.card-body');
            return Array.from(links).map(l =&gt; l.getAttribute('href')).slice(0, 10);
        });
    }
    catch {
        console.log(`Unable to get books from ${categoryURL}`);
    }
}</pre>
			<p>We will iterate through the <code>categories</code> list in the main function, and we will call <code>getBooks</code>, passing <code>categoryURL</code> and a Puppeteer page. That function will return a list of book URLs that <a id="_idIndexMarker494"/>we will append to our <code>books</code> array using the <code>push</code> function. We are using <code>slice(0, 10)</code>, so we return only the first 10 items. </p>
			<p>We wrapped all the code into a <code>try/catch</code> block because we don't want the code to fail if one category has failed.</p>
			<p>Let's now take a look at the <code>getBooks</code> function. It looks pretty straightforward. We go to <code>categoryURL</code> and wait for an element using the <code>a.card-body</code> CSS selector. That selector will give us book URLs. Once the books are loaded, we will call <code>evaluate</code> so we can get all the links with <code>a.card-body</code>, and then, using the <code>map</code> function, we will return the <code>href</code> attribute of the link, which will give us the URL.</p>
			<p>Scraping books won't be that different:</p>
			<pre>(async function() {
    /*Previous code*/
    const prices = [];
    for(const bookURL of books) {
        const price = await getPrice(bookURL, page);
        if(price) {
            prices.push(price);
        }
    }
    fs.writeFile('./prices.json', prices);
    page.browser().close();
})();
async function getPrice(bookURL, page) {
    try
    {
        await page.goto(bookURL);
        await page.waitForSelector('.price-list__item .price-list__price');
        return await page.evaluate(() =&gt; {
            const prices = document.querySelectorAll('.price-list__item .price-list__price');
            if(document.querySelectorAll('.price-list__name')[1].innerText.trim() == 'Print + eBook') {
                return {
                    book: document.querySelector('.product-info__title').innerText,
                    print: prices[1].innerText,
                    ebook: prices[2].innerText,
                }
            }
        });
    }
    catch {
        console.log(`Unable to get price from ${bookURL}`);
    }
}</pre>
			<p>Here we are applying everything we have already learned in this book. We go to a page, wait for a selector, and then we will call the <code>evaluate</code> function, which will return an object. We haven't used the <code>evaluate</code> function in this way yet. </p>
			<p>Inside <code>evaluate</code> we get the prices using the<code>.price-list__item .price-list__price</code> CSS selector, and <a id="_idIndexMarker495"/>we get the book title using the<code>.product-info__title</code> CSS selector. Then, if the product name is <code>"Print + eBook"</code>, because the site also offers videos, we return an object with three properties: <code>book</code>, <code>print</code>, and <code>ebook</code>.</p>
			<p>The last thing to highlight is that we are wrapping the code in a <code>try/catch</code> block. If we fail in fetching one book, we don't want the entire program to fail.</p>
			<p>The main function will collect those results and then save them to the file using <code>fs.writeFile</code>. In order to use that function, you will need to import <code>fs</code> by adding <code>const fs = require('fs');</code> in the first line of the program.</p>
			<p>If everything goes as expected, we will get a <code>prices.json</code> file with something like this:</p>
			<pre>[
  {
    "book": "Kubernetes and Docker - An Enterprise Guide",
    "print": "$39.99",
    "ebook": "$5.00 Was $27.99"
  },
  {
    "book": "The Docker Workshop",
    "print": "$39.99",
    "ebook": "$5.00 Was $27.99"
  },
]</pre>
			<p>And we have our first scraper. From <a id="_idIndexMarker496"/>there, you have the data in the filesystem ready to be analyzed by other tools.</p>
			<p>Can this be made better? Yes, it can. We could try to see whether we can do some parallel scraping.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor163"/>Running scrapers in parallel</h1>
			<p>I'm not saying this just <a id="_idIndexMarker497"/>because I coded it, but our scraper has a pretty good structure. Every piece is separated into different functions, making it easy to identify which parts can run in parallel.</p>
			<p>I don't want to sound repetitive, but remember, the site being scraped, in this case, Packt, is our friend and even my publisher. We don't affect the site; we want to look like normal users. We don't want to run 1,000 calls in parallel. We don't need to do that. So, we will try to run our scraper in parallel but with caution.</p>
			<p>The good news is that we don't have to code a parallel architecture to solve this. We will use a <a id="_idIndexMarker498"/>package called <strong class="bold">puppeteer-cluster</strong> (<a href="https://www.npmjs.com/package/puppeteer-cluster">https://www.npmjs.com/package/puppeteer-cluster</a>). This is what this library does according to the description at npmjs:</p>
			<ul>
				<li>Handles crawling errors</li>
				<li>Auto restarts the browser in case of a crash</li>
				<li>Can automatically retry if a job fails</li>
				<li>Offers different concurrency models to choose from (pages, contexts, browsers)</li>
				<li>Is simple to use, small boilerplate</li>
				<li>Offers progress view and monitoring statistics (see the following code snippet)</li>
			</ul>
			<p>Sounds pretty promising. Let's see how we can implement it. First, we need to install the package:</p>
			<pre>npm install puppeteer-cluster</pre>
			<p>That will get us the package ready to be used. You can get the code of this section in the <code>crawler-with-cluster.js</code> file. Let's import the cluster in our scraper by calling <code>require</code> in the first line <a id="_idIndexMarker499"/>of our code:</p>
			<pre>const { Cluster } = require("puppeteer-cluster");</pre>
			<p>Now that we have imported the <code>Cluster</code> class, we can create a new <strong class="bold">cluster</strong> in the main function:</p>
			<pre>const cluster = await Cluster.launch({
    concurrency: Cluster.CONCURRENCY_PAGE,
    maxConcurrency: 2,
    retryLimit: 1,
    monitor: true,
    puppeteerOptions: {
        headless : false, 
        slowMo: 500
    }
});</pre>
			<p>The <code>Cluster.launch</code> function has many options, but I think that, for now, we only need to know about these options:</p>
			<ul>
				<li><code>concurrency</code> will tell the cluster the level of isolation we want to use. The default is <code>Cluster.CONCURRENCY_CONTEXT</code>. These are all the options available:<p>a) Using <code>Cluster.CONCURRENCY_CONTEXT</code>, each job will have its own context.</p><p>b) Using <code>Cluster.CONCURRENCY_PAGE</code>, each job will have its own page, but the same context will be shared across all jobs. </p><p>c) Using <code>Cluster.CONCURRENCY_BROWSER</code>, each job will have its own browser.</p></li>
				<li><code>maxConcurrency</code> will help <a id="_idIndexMarker500"/>us set how many tasks we want to run simultaneously.</li>
				<li>With <code>retryLimit</code>, we can set how many times the cluster will run a task if it fails. The default is 0, but we will give it one more chance to do the task, setting this to 1.</li>
				<li>If we set the <code>monitor</code> option to <code>true</code>, we will get a nice console output, showing the current process.</li>
				<li>The last option we will cover here is <code>puppeteerOptions</code>. The cluster will pass this object to the <code>puppeteer.launch</code> function.</li>
			</ul>
			<p>One thing that the package description mentions is that it supports error handling. Let's add the error handling they have in the example:</p>
			<pre>cluster.on('taskerror', (err, data, willRetry) =&gt; {
    if (willRetry) {
      console.warn(`Encountered an error while crawling ${data}. ${err.message}\nThis job will be retried`);
    } else {
      console.error(`Failed to crawl ${data}: ${err.message}`);
    }
});</pre>
			<p>That looks pretty solid. When a task fails, the cluster will fire a <code>taskerror</code> event. There we can see the error, the data, and whether the action will be retried.</p>
			<p>We don't need to change how we download and process <code>sitemap.xml</code>. There is nothing to change there. But once we have the category, instead of calling the <code>getBooks</code> function, we will use <code>queue</code> for that task:</p>
			<pre>for(const categoryURL of categories) {
    cluster.queue(categoryURL, getBooks);
}</pre>
			<p>We are telling the cluster that <a id="_idIndexMarker501"/>we need to run <code>getBooks</code> by passing that <code>categoryURL</code>.</p>
			<p>I have more good news. Our scraping functions are almost ready to be used in a cluster – <strong class="bold">almost ready</strong>. We need to change four things:</p>
			<pre>async function getBooks({page, data}) {
    const userAgent = await page.browser().userAgent();
    await page.setUserAgent(userAgent + ' UITestingWithPuppeteerDemoBot');
    await page.goto(data);
    await page.waitForSelector('a.card-body');
    
    const newBooks = await page.evaluate(() =&gt; {
        const links = document.querySelectorAll('a.card-body');
        return Array.from(links).map(l =&gt; l.getAttribute('href')).slice(0, 10);
    });
    for(const book of newBooks) {
        cluster.queue(book, getPrice);
    }
}</pre>
			<p>First, we changed the signature a little bit. Instead of expecting <code>(page, categoryURL)</code>, it will expect an <a id="_idIndexMarker502"/>object with a <code>page</code> property and a <code>data</code> property, where <code>page</code> will be the page created and managed by the cluster, and the <code>data</code> property will be the <code>categoryURL</code> instance we passed in when we queued the task.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The first argument to pass to the <code>queue</code> function doesn't need to be a URL. It doesn't even need to be a string. You can pass in any object, and the function will get that object in the <code>data</code> property.</p>
			<p>The second thing we had to do was to add the call to <code>setUserAgent</code> as the page is created by the cluster itself.</p>
			<p>Then, instead of returning the list of books, we added more tasks to the queue, but in this case, we enqueued the <code>getPrice</code> function, passing the <code>book</code> URL.</p>
			<p>The last thing we had to do is remove the <code>try/catch</code> block because the cluster will handle that for us.</p>
			<p>Now it's time to update the <code>getPrice</code> function:</p>
			<pre>async function getPrice({ page, data}) {
    const userAgent = await page.browser().userAgent();
    await page.setUserAgent(userAgent + ' UITestingWithPuppeteerDemoBot');
    await page.goto(data);
    await page.waitForSelector('.price-list__item .price-list__price');
    prices.push(await page.evaluate(() =&gt; {
        const prices = document.querySelectorAll('.price-list__item .price-list__price');
        if(document.querySelectorAll('.price-list__name')[1].innerText.trim() == 'Print + eBook') {
            return {
                book: document.querySelector('.product-info__title').innerText,
                print: prices[1].innerText,
                ebook: prices[2].innerText,
            }
        }
    }));
}</pre>
			<p>We did pretty much the same. We <a id="_idIndexMarker503"/>changed the signature, added the call to <code>setUserAgent</code>, removed <code>try/catch</code>, and instead of returning the price, we are pushing to the <code>prices</code> array inside the function. </p>
			<p>Finally, we need to wait for the cluster to finish its work:</p>
			<pre>await cluster.idle();
await cluster.close();</pre>
			<p>The call to <code>idle</code> will wait for all the tasks to complete, and then the <code>close</code> function will close the browser and the cluster. Let's see if all this works!</p>
			<pre>&gt; node crawler-with-cluster.js
== Start:     2020-12-29 08:50:14.475
== Now:       2020-12-29 08:51:28.078 (running for 1.2 minutes)
== Progress:  6 / 70 (8.57%), errors: 0 (0.00%)
== Remaining: 13.1 minutes (@ 0.08 pages/second)
== Sys. load: 6.1% CPU / 95.3% memory
== Workers:   2
   #0 WORK https://www.packtpub.com/security
   #1 WORK https://www.packtpub.com/all-products</pre>
			<p>The output of <code>puppeteer-cluster</code> is amazing. We can see the elapsed time, the progress, and what the <a id="_idIndexMarker504"/>workers are processing.</p>
			<p>Until now, we played by the rules. But what if we want to avoid being detected as scrapers? Let's find that out.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor164"/>How to avoid being detected as a bot</h1>
			<p>I hesitated about adding this section after everything I mentioned about scraping ethics. I think I made my point clear <a id="_idIndexMarker505"/>when I said that when the owner says no, it means no. But if I'm writing a chapter about scraping, I think I need to show you these tools. It's then up to you what to do with the information you have learned so far.</p>
			<p>Websites that don't want to be scraped, and are being actively scraped, will invest a good amount of time and money in trying not to be scraped. The effort would become even more important if the scrapers damage not only the site's performance but also the business.</p>
			<p>Developers in charge of dealing with bots won't rely only on the user agent because, as we saw, that could be easily manipulated. They should rely only on evaluating the number of requests from an IP because, as we also saw, scrapers can slow down their scripts, simulating an interested user.</p>
			<p>If the site can't stop scrapers by checking the user agent and monitoring traffic spikes, they would try to catch scrapers using different techniques.</p>
			<p>They would begin by introducing CAPTCHAs. But, as we will see in this section, scrapers can solve some of them.</p>
			<p>Then, they would try to evaluate the time between requests. Did you click a link after 500 milliseconds? Did you fill a form in less than 1 second? You might be a bot.</p>
			<p>They could also add JavaScript code to check your browser capabilities. Don't you have plugins? Not even the ones that come with Chrome by default? Don't you have a language set? You might be a bot.</p>
			<p>Finally, they will try to set traps to catch you. For instance, Packt knows that you might be scraping links using the <code>a.card-body</code> CSS selector. They could add a hidden link with that selector, but in that case, the link's URL could be https://www.packtpub.com/bot-detected. If you got to the bot-detected URL, you would get caught. In the case of <a id="_idIndexMarker506"/>forms, they could add hidden inputs that a typical user wouldn't complete because it is hidden. If the server gets a value in that hidden input, sorry—you were caught again.</p>
			<p>This is a cat-and-mouse game. The mouse will always try to find new ways to sneak in, and the cat will work hard to cover the holes in the wall.</p>
			<p>That being said, let's see what tools we have if we are the mouse in this game.</p>
			<p>Antoine Vastel has a great bot detection demo page (<a href="https://arh.antoinevastel.com/bots/areyouheadless">https://arh.antoinevastel.com/bots/areyouheadless</a>). You can get the code of this section in the <code>bot.js</code> file. Let's try to take a screenshot of that page using Puppeteer:</p>
			<pre>const puppeteer = require('puppeteer');
(async function() {
    const browser = await puppeteer.launch({});
    const page = await browser.newPage();
    await page.goto('https://arh.antoinevastel.com/bots/areyouheadless');
    await page.screenshot({ path : './bot.png'});
    browser.close()
})();</pre>
			<p>Simple Puppeteer code. We open the browser in headless mode, navigate to the page, and take a screenshot. Let's see what the screenshot looks like:</p>
			<div><div><img src="img/Figure_9.06_B16113.jpg" alt="Antoine Vastel’s bot detection&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Antoine Vastel's bot detection</p>
			<p>Antoine got us. We were <a id="_idIndexMarker507"/>detected as a bot, but that's not the end of the game. There are a few things we can do. Let's start by incorporating <code>puppeteer-extra</code> (<a href="https://github.com/berstend/puppeteer-extra">https://github.com/berstend/puppeteer-extra</a>). The <code>puppeteer-extra</code> package allows us to add plugins to Puppeteer. This package will allow us to use the <a id="_idIndexMarker508"/><code>puppeteer-extra-plugin-stealth</code> plugin (<a href="https://www.npmjs.com/package/puppeteer-extra-plugin-stealth">https://www.npmjs.com/package/puppeteer-extra-plugin-stealth</a>). This package is like a mouse master in this game. It will add all the tricks (or many tricks), so our code is not detected as a bot.</p>
			<p>The first thing we need to do is install those two packages from the terminal:</p>
			<pre>npm install puppeteer-extra
npm install puppeteer-extra-plugin-stealth</pre>
			<p>Now we can replace this line:</p>
			<pre>const puppeteer = require('puppeteer');</pre>
			<p>We replace it with these three lines:</p>
			<pre>const puppeteer = require('puppeteer-extra');
const StealthPlugin = require('puppeteer-extra-plugin-stealth');
puppeteer.use(StealthPlugin());</pre>
			<p>There, we import <code>puppeteer</code>, but from <code>puppeteer-extra</code>. Then, we import the stealth plugin and install it <a id="_idIndexMarker509"/>using the <code>use</code> function. And that's it!</p>
			<div><div><img src="img/Figure_9.07_B16113.jpg" alt="Antoine Vastel’s bot detection bypassed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Antoine Vastel's bot detection bypassed</p>
			<p>The <code>puppeteer-extra-plugin-stealth</code> package is not bulletproof. As I mentioned before, it's a cat-and-mouse game. There are many other extras you can use. You can see the full list in the package's repository (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/puppeteer-extra-packages">https://www.hardkoded.com/ui-testing-with-puppeteer/puppeteer-extra-packages</a>). There, you can find <code>puppeteer-extra-plugin-anonymize-ua</code>, which will change the user agent in all the pages, or <code>puppeteer-extra-plugin-recaptcha</code>, which will <a id="_idIndexMarker510"/>try to solve reCAPTCHA (<a href="https://www.google.com/recaptcha/about/">https://www.google.com/recaptcha/about/</a>) challenges.</p>
			<p>A scraping chapter wouldn't be complete if we didn't talk, at least a little, about how to deal with authorization.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor165"/>Dealing with authorization</h1>
			<p>Authentication and <a id="_idIndexMarker511"/>authorization is a vast topic in web development. Authentication is how a website can identify you. To make it simple, it's the login. On the other hand, authorization is what you can do on the site once you are authenticated, for instance, checking whether you have access to a specific page.</p>
			<p>There are many types of authentication modes. We covered the simplest one in this book: a user and password login page. But things can get more complicated. Testing integration with Facebook or single sign-on logins could be quite challenging, but they would be about automating user interaction.</p>
			<p>There is one authentication method that you won't be able to perform by <a id="_idIndexMarker512"/>automating the DOM—<strong class="bold">HTTP basic authentication</strong>:</p>
			<div><div><img src="img/Figure_9.08_B16113.jpg" alt="HTTP basic authentication&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">HTTP basic authentication</p>
			<p>That login popup is not popular these days. In fact, I don't think they ever were popular. But you might have seen them if you have set up a router. That modal is like the dialogs we saw in <a href="B16113_05_Final_SK_ePub.xhtml#_idTextAnchor087"><em class="italic">Chapter 5</em></a>, <em class="italic">Waiting for elements and network calls</em>. Puppeteer won't help us out with this authentication because there is no HTML to automate there. Luckily for us, automating this is easy. You can get the <a id="_idIndexMarker513"/>code of this section in the <code>authentication.js</code> file:</p>
			<pre>(async function() {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();
    await page.authenticate({username: 'user', password: 'password'});
    await page.goto('https://ptsv2.com/t/ui-testing-puppeteer/post');
    await page.screenshot({ path : './authentication.png'});
    browser.close()
})();</pre>
			<p>The only thing we need to do to authenticate into https://ptsv2.com/t/ui-testing-puppeteer/post is to call the <code>authenticate</code> function before calling <code>goto</code>. The <code>authenticate</code> function expects an object with two properties: <code>username</code> and <code>password</code>.</p>
			<p>Once we are authenticated, we need to tell the server who we are on every request, so they can authorize us (or not) to perform certain tasks. The web server is, in theory, stateless. It doesn't have a way to know who we are unless 1) they inject some information in their responses, using cookies, or 2) we tell them. The most common way is with HTTP Headers. But that can be solved by passing a key as a Query String argument or as part of the HTTP post data. </p>
			<p>When you want to alter the authentication data, you need to get that information elsewhere. You might need to open your browser, log in to the site you want to scrape, and extract the authentication data from there, so you can then use that data in your scraper. </p>
			<p>Let's say that you want to scrape the Packt website, but this time you want to scrape it being logged in. So, you open your browser, log in and then you can use a tool such as the <em class="italic">Export cookie JSON file for Puppeteer</em> extension (you can find it with that name in the Chrome web store) to <a id="_idIndexMarker514"/>export all the cookies generated by the site. Once we have the JSON file named <code>account.packtpub.com.cookies.json</code> with all the cookies, you can copy that file into your workspace and do something like this:</p>
			<pre>const puppeteer = require('puppeteer');
const cookies = require('./account.packtpub.com.cookies.json');
(async function() {
    const browser = await puppeteer.launch({defaultViewport : { width: 1024, height: 1024}});
    const page = await browser.newPage();
    await page.setCookie(...cookies);
    await page.goto('https://account.packtpub.com/account/details');
    await page.waitForSelector('[autocomplete="given-name"]');
    await page.screenshot({ path : './cookies.png'});
    browser.close()
})();</pre>
			<p>The new element in this code is a call to the <code>setCookie</code> function. That function expects a list of cookies. As we have all the cookies in a JSON file, we load that JSON file and pass the content to the <code>setCookie</code> function. Let's take a look at what a cookie looks like inside that file:</p>
			<pre>{
  "name": "packt_privacy",
  "value": "true",
  "domain": ".packtpub.com",
  "path": "/",
  "expires": 1611427077,
  "httpOnly": false,
  "secure": false
}</pre>
			<p>The structure is quite simple and straightforward. You don't need to use an extension and load a cookie from a JSON file. You can call the <code>setCookie</code> function passing an object with the <code>name</code>, <code>value</code>, <code>domain</code>, <code>path</code>, and <code>expires</code> properties (the latter is a Unix time in seconds), whether it's <code>httpOnly</code>, and whether it should be marked as <code>secure</code>.</p>
			<p>Now it's time to see how we <a id="_idIndexMarker515"/>can handle authorizations implemented using HTTP headers. You might find sites using the <code>authorization</code> HTTP header to pass some kind of user identifier. The <code>authorization</code> header would look something like this: </p>
			<pre>Authorization: &lt;type&gt; &lt;credentials&gt;</pre>
			<p>According to MDN (<a href="https://www.hardkoded.com/ui-testing-with-puppeteer/authentication-schemes">https://www.hardkoded.com/ui-testing-with-puppeteer/authentication-schemes</a>), you can find the following types: <code>Basic</code>, <code>Bearer</code>, <code>Digest</code>, <code>HOBA</code>, <code>Mutual</code>, and <code>AWS4-HMAC-SHA256</code>. If those names sound scary, don't worry about that. There is a high chance that you will only see the <code>Bearer</code> type. What would be the credentials? Well, that's what you will need to find out while coding your scraper. You would need to see what information is being sent when you use a website for real and try to mimic that.</p>
			<p>For our example, we will use <code>Basic</code> because that's the same HTTP basic authentication we saw before. When you log in using the authentication popup, the browser will send the authorization header passing <code>basic</code> and <code>username:password</code> in Base64. In our example, the username was <code>user</code>, and the password was <code>password</code>. So, we can use any Base64 encoder <a id="_idIndexMarker516"/>available, for instance, <a href="https://www.base64encode.net/">https://www.base64encode.net/</a>, and get <code>user:password</code> in base64: <code>dXNlcjpwYXNzd29yZA==</code>.</p>
			<p>We can inject this header in two ways. The first one is using the <code>setExtraHTTPHeaders</code> function. You can see this code in the <code>header-inject.js</code> file:</p>
			<pre>(async function() {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();
    await page.setExtraHTTPHeaders({
        authorization: 'basic dXNlcjpwYXNzd29yZA=='
    });
    await page.goto('https://ptsv2.com/t/ui-testing-puppeteer/post');
    await page.screenshot({ path : './authentication-header.png'});
    browser.close()
})();</pre>
			<p><code>setExtraHTTPHeaders</code> expects <a id="_idIndexMarker517"/>an object where the property name is the header name, and the value is the header value. Here we are adding the <code>authorization</code> header with the value <code>'basic dXNlcjpwYXNzd29yZA=='</code>. And that's it. Puppeteer will add that header to every request that the page will make.</p>
			<p>But what if the site we are trying to scrape needs an authorization header not in every request, but only in some of them? Well, it will be quite tricky, but not that hard. You can follow this code in the <code>header-inject2.js</code> file:</p>
			<pre>const puppeteer = require('puppeteer');
(<strong class="bold">async function</strong>() {
    <strong class="bold">const</strong> browser = await puppeteer.launch();
    <strong class="bold">const</strong> page = await browser.newPage();
    await page.setRequestInterception(true);
    page.on('request', r =&gt; {
        <strong class="bold">const</strong> overrides = {
            headers: r.headers()
        };
        if(r.url() == 'https://ptsv2.com/t/ui-testing-puppeteer/post')
            overrides.headers.authorization = 'basic dXNlcjpwYXNzd29yZA==';
        
        r.continue(overrides);
    });
    await page.goto('https://ptsv2.com/t/ui-testing-puppeteer/post');
    await page.screenshot({ path : './authentication-header.png'});
    browser.close()
})();</pre>
			<p>We are first telling Puppeteer we <a id="_idIndexMarker518"/>want to intercept every request the page will make. We do that by calling the <code>setRequestInterception</code> function with the first argument as <code>true</code>. Then we start listening to the <code>request</code> event. If the request meets the condition we need, in this case, if it matches our URL, we create an <code>overrides</code> object with a <code>headers</code> property and then call the <code>continue</code> function of the request object. We cannot override the headers. The <code>overrides</code> object can also have the <code>url</code>, <code>method</code> (the HTTP method), and <code>postData</code> properties.</p>
			<p>The request object also has a function called <code>abort</code>. With this function, you can cancel that request. For instance, you could check whether the request is an image and <code>abort</code> it. The result will be a website with no images.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you call <code>setRequestInterception</code>, you need to implement a <code>request</code> event listener. And you need to <code>continue</code> or <code>abort</code> every request you listen to.</p>
			<p>As I mentioned when I <a id="_idIndexMarker519"/>opened this section, this doesn't cover all the different authentication and authorization schemes, but it will have you covered in more than 90% of cases. Now it's time for a wrap-up.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor166"/>Summary</h1>
			<p>Although this is not a scraping book, we covered a lot of ground here. I hope the first section gave you a good idea of what scraping is, as well as covering what you can do and what you shouldn't do. We also learned how to create our own scrapers. We created a crawler in less than 100 lines. We added two new tools to our toolbox: <code>puppeteer-cluster</code> and <code>puppeteer-extra</code>. We closed this chapter learning a little bit about authentication and authorization, giving you almost everything you need to get started in the scraping world.</p>
			<p>If you weren't that excited about scraping before this chapter, I hope it is the spark that will make you start creating your own scrapers. If you knew about scraping, I hope this chapter gave you more tools to scrape as a professional.</p>
			<p>Our next and final chapter will be about performance and how we can measure it using Puppeteer.</p>
		</div>
	</body></html>