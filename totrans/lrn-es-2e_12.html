<html><head></head><body>
        

                            
                    <h1 class="header-title">Shared Memory and Atomics</h1>
                
            
            
                
<p class="mce-root">Let's go to low-level memory stuff! This chapter is going to be a bit advanced, but interesting. I'll try to make it as simple and understandable as possible.</p>
<p>With that out of the way, let's get to what we've FINALLY in JavaScript! Low-level memory access, multi-threading, atomics, shared memory, and all that cool and powerful stuff. But, as someone said, with great power comes great responsibility. Let's go!</p>
<p>We'll cover the following things in this chapter:</p>
<ul>
<li>Basics of memory management in computers</li>
<li>What is shared memory?</li>
<li>Using <kbd>SharedArrayBuffer</kbd></li>
<li>Introduction to parallel programming</li>
<li>Problems when multiple threads access one memory location</li>
<li>What are atomics?</li>
<li>Performing atomic operations</li>
<li>Atomic APIs in JavaScript</li>
<li>Using parallel programming the right way</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Basics of memory</h1>
                
            
            
                
<p>We have to understand a little about how memory works in order to appreciate the significance of <kbd>SharedArrayBuffer</kbd> in JavaScript. </p>
<p>Think of memory as a collection of a lot of drawers in a big almirah kind of structure, where you can open a drawer and put something in it. Every drawer has its own maximum capacity.</p>
<p>Every drawer also has a sticker associated with it, which has a unique number on it that helps you to note down which drawer has data and which doesn't. When the time comes to access that data, you are supplied with the numbered drawer and you can take out data accordingly.</p>
<p>Now, let us start by understanding the basics of memory storage. Suppose I want to store a number, say, 100, in memory. First of all, we need to convert this number into binary, because that is what computers understand, and it is easy for them to store:</p>
<div><img height="47" src="img/64948eed-49b0-4261-805f-6f030bb34c48.png" width="225"/></div>
<p>The preceding figure is a binary representation of the number 100 and is how it is stored in memory.</p>
<p>Easy! In a similar manner, we can store more complicated data, such as letters, by converting them to numbers (called ASCII values), and then storing those numbers directly, instead of letters. Similarly, an image (assumed to be black and white) can be stored by, say, storing the brightness levels of each pixel floating point number.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Abstraction of memory management</h1>
                
            
            
                
<p>Memory management means that you're actually interacting directly with the hardware to store/update/free blocks of memory yourself from your code. Most higher level programming languages take away the memory management from the developers.</p>
<p>This is because managing memory is hard. It really is! In complicated programs, humans are bound to make mistakes and cause a ton of problems, not limited to memory leaks (which is the easiest mistake someone can make).</p>
<p>Of course, this abstraction comes at a performance cost. But compared with the security, readability, and convenience advantages, this is a fair deal.</p>
<p>JavaScript also manages memory automatically. The JavaScript engine is responsible for registering memory whenever a new variable is created, freeing the memory when it is no longer needed, and so on. Imagine managing memory for a <strong>closure</strong> program yourself! Even if the program is a bit complicated, it is very easy to lose track of which variables to keep in memory and which ones to discard, even after function execution ends. JavaScript to the rescue!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Garbage collection</h1>
                
            
            
                
<p>JavaScript is a garbage collected language. What that means is that a JavaScript engine will occasionally fire something called a garbage collector, which looks for unused and inaccessible references in memory for the program and clears them, making the memory available for storing other data.</p>
<p>Garbage collectors make life a lot easier, but add a bit of overhead in performance-critical applications. Say you are coding a 3D game where you want very high <strong>Frames Per Second</strong> (<strong>FPS</strong>) on not so good hardware.</p>
<p>You might see that the results are extremely good for a game coded in C/C++, as compared to a garbage collected language like Java. This is because when you're playing the game, garbage collectors might fire off even when it is unnecessary, which wastes some resources that could've been used by the rendering thread.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Manually managing memory</h1>
                
            
            
                
<p>Languages such as C/C++ are on their own in terms of memory management. In such languages, you have to allocate the memory and de-allocate it all by yourself. This is the reason why C/C++ are so fast--because they're very close to the hardware, and almost no abstraction is there. But that makes it painful to write complex applications because things can slip out of hand real quick.</p>
<p>There is something called as <strong>WebAssembly</strong>, which is the compiled form of a JavaScript alternative on the web. C/C++ code can be compiled down to WebAssembly, which is in some cases 100-200% faster than native JavaScript!</p>
<p>WebAssembly is going to be the future of the web because of its speed and multiple types of language support. However, it'll again require you to manage memory yourself, as, at the end of the day, C/C++ is what you'll need to write your code in.</p>
<p>Manually managing memory is hard. It is hard to know when to clear off the part of memory you don't require in bigger programs. Do it early, and you break the application. Do it late, and you are out of memory. This is the reason abstraction is good in a lot of cases.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What is shared memory?</h1>
                
            
            
                
<p>Let's suppose we are working on a real-time performance-critical application, which is the reason we're so concerned about this interesting topic. Suppose I have two web workers running in the background, and I want to share some data from one worker to another. Web workers run independently on separate OS-level threads and have no idea about each other.</p>
<p>One way is to make use of <kbd>postMessage</kbd> to transfer messages between web workers, as we saw in the last chapter. However, this is slow.</p>
<p>Another way is to transfer the object to another worker completely; however, if you remember, that makes the object which is transferred inaccessible from the worker which sent it.</p>
<p>The solution to this problem is <kbd>SharedArrayBuffer</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction to SharedArrayBuffer</h1>
                
            
            
                
<p>The <kbd>SharedArrayBuffer</kbd> is the way to create a memory store which is accessible to all workers simultaneously. Now, if you've been reading keenly, you will have understood something mischievous that can happen once something like a shared memory store is allowed to exist.</p>
<p>If you remember, the only reason workers didn't have direct access to <kbd>DOM</kbd> was because the DOM API is not thread-safe, and could cause problems like deadlocks and race conditions. And if you were able to judge that the same thing might happen here, you're right! But that's a topic for a later section (<em>The race condition</em>).</p>
<p>Let's get back to <kbd>SharedArrayBuffer</kbd>. So what's different from <kbd>ArrayBuffer</kbd>?</p>
<p>Well, <kbd>SharedArrayBuffer</kbd> is pretty much the <kbd>ArrayBuffer</kbd> which is available to a lot of scripts. You just have to create <kbd>SharedArrayBuffer</kbd> in one place and use <kbd>postMessage</kbd> to post it to other workers (and not transfer it!).</p>
<p>You should not transfer it because you'll then lose ownership of the <kbd>SharedArrayBuffer</kbd>. When you post it, only the reference of the buffer is automatically passed and becomes available to all the other scripts:</p>
<pre>const sab = new SharedArrayBuffer(1024);<br/>worker.postMessage(sab); // DO NOT TRANSFER: worker.postMessage(sab, [sab]);</pre>
<p>Once you do that, all the workers will be able to access, read, and write to <kbd>SharedArrayBuffer</kbd>. Take a look at the representation as follows:</p>
<div><img height="98" src="img/b91c486c-0f13-4cc3-86c3-dc675edf6464.png" width="154"/></div>
<p>This is a rough representation of how you might imagine <kbd>SharedArrayBuffer</kbd> is connected to the memory under the hood. Let's assume each thread is spawned on a different CPU, for now.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding parallel programming</h1>
                
            
            
                
<p>Parallel programming, as the name suggests, is just a program running in such a way that instances of that program are running simultaneously multiple times.</p>
<p>Concurrent programming, on the other hand, is very similar to parallel programming, but with the difference that tasks never happen together.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Parallel versus concurrent programming</h1>
                
            
            
                
<p>To understand the difference between parallel and concurrent programming, let us consider an example.</p>
<p>Suppose there's a competition to eat candies put on two plates. Plates are at a distance of five meters from each other. Let's say you're the only player for now, and the constraint is that you have to keep the number of differences in candies on both plates to less than two.</p>
<p>What will you do here? You have to eat from plate one, run five meters to plate two, eat from plate two, run five meters again to plate one, and so on.</p>
<p>Now, let's assume you have got a friend. Now, both of you can choose a plate and start eating your own candies.</p>
<p>Try to relate it to concurrent programming and parallel programming, respectively. In the first example, you are the CPU's core, which is running here and there, again and again, between two threads (plates). You are running fast, but, no matter how hard you try, you cannot eat from both plates at the same time due to your physical limits. Similarly, the CPU in concurrent programming is doing both of the tasks, but instead of doing them simultaneously, it is doing both in chunks.</p>
<p>In the next example, for parallel programming, your friend acts like another CPU, which is handling the other thread completely. This way, each of you only have to execute your own thread. This is parallelism.</p>
<p>If that makes sense, then let us get into parallel programming, the thing which web workers give us, and how to make use of shared memory with parallel programming to actually make things faster and not slow them down (because that happens a lot of times, when you do it incorrectly).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Myth-busting--Parallel computation is always faster</h1>
                
            
            
                
<p>It seems so intuitive to say that parallel computation should always be faster than computing on a single thread. Like spawning two threads should, intuitively, almost halve the computation time. Not only is this numerically wrong, but parallel computing creates garbage results if not done properly.</p>
<p>To understand this, consider a man who is given a task to transfer a pile of blocks from one place to another:</p>
<div><img height="85" src="img/637443e1-2bc6-427d-a689-57b7177e6cd1.png" width="274"/></div>
<p>He does this work at some speed. Putting another man with him might sound like doubling the speed of the work, but the two might actually crash into each other on their way and make things slower instead of faster.</p>
<p>This actually happens a lot of times when parallelism is implemented incorrectly, as we shall see now.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Let's count one billion!</h1>
                
            
            
                
<p>To verify that parallel computing, if set up wrongly, is actually garbage, let us count to one billion using a single-threaded and multi-threaded environment in JavaScript.</p>
<p>Let us first try single-threaded counting:</p>
<pre>// Main thread<br/><br/>const sharedMem = new SharedArrayBuffer(4);<br/><br/>function countSingleThread(limit) {<br/>    const arr = new Uint32Array(sharedMem);<br/>    for(let i=0; i&lt;limit; i++) {<br/>        arr[0] = arr[0] + 1;<br/>    }<br/>}<br/><br/>const now = performance.now();<br/>countSingleThread(1000000000);<br/>console.log(`Time Taken: ${performance.now() - now}`);</pre>
<p>On my MacBook Air, it takes <em>∼2606</em> milliseconds for this program to run. That is roughly <em>2.6</em> seconds:</p>
<div><img height="119" src="img/8229befe-f419-425d-9598-42992ac49e2d.png" width="289"/></div>
<p>Let us try to split the code among two workers now, and see what happens:</p>
<pre>// Main thread<br/><br/>const sharedMem = new SharedArrayBuffer(4);<br/>const workers = [new Worker('worker.js'), new Worker('worker.js')];<br/>let oneWorkerDone = false;<br/>const now = performance.now();<br/><br/>for(let i=0;i&lt;2;i++) {<br/>  workers[i].postMessage({message: 'sab', memory: sharedMem});<br/><br/>  workers[i].addEventListener('message', data =&gt; {<br/>      if(!oneWorkerDone) {<br/>        oneWorkerDone = true;<br/>      } else {<br/>        console.log("Both workers done. The memory is: ", new<br/>        Uint32Array(sharedMem))<br/>        console.log(`Time taken: ${performance.now()-now}`)<br/>      }<br/>  });<br/><br/>  workers[i].postMessage({cmd: 'start', iterations: 500000000});<br/>}</pre>
<p>Alright! So what the heck is going on here? The following is an explanation: </p>
<ol>
<li>We created a <kbd>SharedArrayBuffer</kbd> in order to create a memory storage area which can be accessed simultaneously by both of the spawned web workers.</li>
<li>The size of <kbd>SharedArrayBuffer</kbd> is <kbd>4</kbd> because, to add numbers to the integer array, we'll cast it to <kbd>Uint32Array</kbd>, which has a size in multiples of <kbd>4</kbd>.</li>
<li>We started two web workers from the same file.</li>
</ol>
<ol start="4">
<li>We gave them access to <kbd>SharedArrayBuffer</kbd>.</li>
<li>We're listening in the main script when both workers say they're done.</li>
<li>We are sending 500 million iterations to each worker, thus splitting the work among these two threads.</li>
</ol>
<p>Let us now look at what <kbd>worker.js</kbd> looks like:</p>
<pre>// worker.js<br/>let sharedMem;<br/><br/>addEventListener('message', ({data}) =&gt; {<br/>  //console.log(data);<br/>    if(data.message == 'sab') {<br/>        sharedMem = data.memory;<br/>        console.log('Memory ready');<br/>    }<br/>    if(data.cmd == 'start') {<br/>      console.log('Iterations ready');<br/>        startCounting(data.iterations);<br/>    }<br/>});<br/><br/>function startCounting(limit) {<br/>    const arr = new Uint32Array(sharedMem);<br/>    for(let i=0;i&lt;limit;i++) {<br/>        arr[0] += 1;<br/>    }<br/>    postMessage('done')<br/>}</pre>
<p>In <kbd>worker.js</kbd>, we do the following:</p>
<ol>
<li>Listen for messages from the main script.</li>
<li>Check if the message says to store <kbd>SharedArrayBuffer</kbd>; if it does, we store it.</li>
<li>If the message says to start the iterations, we start by first converting it to <kbd>Uint32Array</kbd>.</li>
<li>After iterations, we send a nice 'done' message to the main script to inform it that we're done.</li>
</ol>
<p><strong>Expectation:</strong> The program will have a speed-up of around 2x because each thread has to do half of the work. Also, we expect the final value to be one billion.</p>
<p class="CDPAlignLeft CDPAlign"><strong>Reality: </strong>Test #1 is as follows.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Running the preceding code for the first time produces the following result:</p>
<div><img height="175" src="img/002fe4d7-06af-4992-89b2-3165678b15ea.png" width="437"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Test #2 is as follows.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Running the preceding code for the second time produces the following result:</p>
<div><img height="163" src="img/3015f7ac-dab7-487e-b41b-b7adb94db3b8.png" width="404"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Test #3 is as follows.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Running the preceding code for the third time produces the following result:</p>
<div><img height="165" src="img/01bf5e2b-11bc-4dd0-be61-c6e62a2fd17d.png" width="408"/></div>
<p>I've got garbage values! Every time I run the program, I get different values, near 500 million. Why is this so?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The race condition</h1>
                
            
            
                
<p>The garbage values which were there in the immediately previous two screenshots represent a classic race condition example. Do you remember the first image I showed you in the <em>Introduction to SharedArrayBuffer</em> section? Remember the <kbd>SharedArrayBuffer</kbd> linking to <strong>CPU 1</strong> and <strong>CPU 2,</strong> which links to <strong>Worker 1</strong> and <strong>Worker 2</strong>? Well, it turns out it's not completely correct.</p>
<p>Here's how the actual setup is in your machine:</p>
<div><img height="156" src="img/f2018f69-4963-4618-8ce8-7d02fdf9b183.png" width="153"/></div>
<p>The problem arises here. Race condition means that <strong>CPU 1</strong> fetches the shared memory and sends it to <strong>Worker 1</strong>. Meanwhile, <strong>CPU 2</strong> also fetches it, but doesn't know that <strong>CPU 1</strong> is already working on it. So, by the time <strong>Worker 1</strong> has changed the value from <em>0</em> to <em>1</em>, <strong>CPU 2</strong>, that is, <strong>Worker 2</strong>, is still fetching the value 0.</p>
<p><strong>Worker 1</strong> then updates the shared memory to a value of 1, and then <strong>Worker 2</strong> updates its own copy to a value of 1 (because it doesn't know that <strong>CPU 1</strong> has already updated it to 1), and then writes it again to the shared memory.</p>
<p>Here, we've successfully wasted two computations, which required only one. That was a quick example of how not to do parallelism:</p>
<div><img height="331" src="img/7079a465-8da9-49d9-a90d-99abe42774cf.png" width="441"/></div>
<p>How do we fix this? Atomics (we will come back to this problem later in the chapter, in the section <em>Fixing one billion count with atomics</em>).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What are atomics?</h1>
                
            
            
                
<p>What are atomics? Atomics, or, more precisely, an atomic operation, is an operation which happens in one go, not in steps. It is like an atom --indivisible (although an atom is technically divisible, let's not destroy the analogy).</p>
<p>An atomic operation is a single operation as seen by all other working threads. It just happens immediately. It is like the execution of one machine code, which is either not done yet or is completed. There is no in-between.</p>
<p>In a nutshell, something being atomic means that only one operation can be done on it at a time. For example, updating a variable can be made atomic. This can be used to avoid a race condition.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Information about lock and mutex</h1>
                
            
            
                
<p>When I said updating a variable can be made atomic, it means that during the time a thread is accessing that memory, no other thread should be allowed to access it. This is only possible when you introduce a lock or a mutex (mutual exclusion) on the variable being accessed. This way, the other thread knows that the variable is in use and it should wait for the lock to be released.</p>
<p>This is how you make an operation atomic. But this sense of security also comes at a cost. Atomic locking is not an operation which will take negligible time, so it definitely involves some overhead.</p>
<p>Do it a billion times, and you're probably screwed (we'll see that soon in <em>Fixing one billion count with atomics</em>).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Atomics in JavaScript</h1>
                
            
            
                
<p>JavaScript has an <kbd>Atomics</kbd> object, which provides us with exactly the functionality we discussed previously. However, it is quite limited, in the sense that you can only do addition, subtraction, bitwise AND, bitwise OR, bitwise XOR, and storing.</p>
<p>Other features can be built on top of these, and, in future, there will be libraries providing that. For now, let's learn about the natively available methods.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the Atomics.load(typedArray, index) method</h1>
                
            
            
                
<p>The <kbd>Atomics.load</kbd> method returns the value inside a typed array at a particular index value. Here's how to use it:</p>
<pre>const sab = new SharedArrayBuffer(1);<br/>const arr = new Uint8Array(sab);<br/>arr[0] = 5;<br/><br/>console.log(Atomics.load(arr, 0));</pre>
<p>The preceding code is just a thread-safe way to access <kbd>arr[0]</kbd>.</p>
<p>This outputs:</p>
<pre><strong>5</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the Atomics.add(typedArray, index, value) method</h1>
                
            
            
                
<p><kbd>Atomics.add</kbd> is a way to add a particular value to a particular index in a typed array. It is fairly simple to understand and write:</p>
<pre>const sab = new SharedArrayBuffer(1);<br/>const arr = new Uint8Array(sab);<br/>arr[0] = 5;<br/><br/>console.log(Atomics.add(arr, 0, 10));<br/>console.log(Atomics.load(arr, 0));</pre>
<p><kbd>Atomics.add</kbd> is, again, a thread-safe way of performing <kbd>arr[0] += 10</kbd>.</p>
<p>This outputs:</p>
<pre><strong>5</strong><br/><strong>15</strong></pre>
<div><kbd>Atomics.add</kbd> returns the old value at that index. The value is updated at that index after the command is run.</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the Atomics.sub(typedArray, index, value) method</h1>
                
            
            
                
<p><kbd>Atomics.sub</kbd> is a way to subtract a particular value from a particular index in a typed array. It is also fairly simple to use:</p>
<pre>const sab = new SharedArrayBuffer(1);<br/>const arr = new Uint8Array(sab);<br/>arr[0] = 5;<br/><br/>console.log(Atomics.sub(arr, 0, 2));<br/>console.log(Atomics.load(arr, 0));</pre>
<p><kbd>Atomics.sub</kbd> is, again, a thread-safe way of doing <kbd>arr[0] -= 2</kbd>.</p>
<p>This outputs:</p>
<pre><strong>5</strong><br/><strong>3</strong></pre>
<div><kbd>Atomics.sub</kbd> returns the old value at that index. The value is updated at that index after the command is run.</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the Atomics.and(typedArray, index, value) method</h1>
                
            
            
                
<p><kbd>Atomics.and</kbd> performs a bitwise AND between the value at that particular index in the typed array and the value you supplied:</p>
<pre>const sab = new SharedArrayBuffer(1);<br/>const arr = new Uint8Array(sab);<br/>arr[0] = 5; // 5 is 0101 in binary.<br/><br/>Atomics.and(arr, 0, 12); // 12 is 1100 in binary<br/>console.log(Atomics.load(arr, 0));</pre>
<p><kbd>Atomics.and</kbd> here performs a bitwise AND between <kbd>arr[0]</kbd> and the number <kbd>12</kbd>.</p>
<p>This outputs:</p>
<pre><strong>4</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How bitwise AND works</h1>
                
            
            
                
<p class="mce-root">Suppose we want to take a bitwise AND of 5 and 12:</p>
<ol>
<li>Covert both numbers to binary; 5 is 0101 and 12 is 1100.</li>
<li>Bitwise AND performs the <kbd>AND</kbd> operation bit by bit, starting from the first bit:<br/>
<em>5  &amp;  12</em><br/>
<em>0 &amp;&amp; 1 = 0</em><br/>
<em>1 &amp;&amp; 1 = 1</em><br/>
<em>0 &amp;&amp; 0 = 0</em><br/>
<p><em>1 &amp;&amp; 0 = 0</em></p>
</li>
<li>Thus, <em>5 &amp;&amp; 12 = 0100</em>, which is 4.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the Atomics.or(typedArray, index, value) method</h1>
                
            
            
                
<p>Similar to bitwise AND, <kbd>Atomics.or</kbd> method performs a bitwise OR:</p>
<pre>const sab = new SharedArrayBuffer(1);<br/>const arr = new Uint8Array(sab);<br/>arr[0] = 5; // 5 is 0101 in binary.<br/><br/>Atomics.or(arr, 0, 12); // 12 is 1100 in binary<br/>console.log(Atomics.load(arr, 0));</pre>
<p>Here, <kbd>Atomics.or</kbd> method performed a bitwise OR between <kbd>arr[0]</kbd> and the number <kbd>12</kbd>.</p>
<p>The output is:</p>
<pre><strong>13</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How bitwise OR works</h1>
                
            
            
                
<p>Suppose we want to take a bitwise OR of 5 and 12:</p>
<ol>
<li>Covert both numbers to binary; 5 is 0101 and 12 is 1100</li>
<li>Bitwise OR performs an OR operation bit by bit, starting from the first bit:<br/>
<em>5  |  12</em><br/>
<em>0 || 1 = 1</em><br/>
<em>1 || 1 = 1</em><br/>
<em>0 || 0 = 0</em><br/>
<em>1 || 0 = 1</em></li>
<li>Thus, <em>5 | 12 = 1101</em> which is 13.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Using the Atomics.xor(typedArray, index, value) method</h1>
                
            
            
                
<p>Again, <kbd>Atomics.xor</kbd> method performs a bitwise XOR operation, which is an exclusive OR (that is, it is an OR gate which gives <kbd>0</kbd> when both inputs are <kbd>1</kbd>)</p>
<pre>const sab = new SharedArrayBuffer(1);<br/>const arr = new Uint8Array(sab);<br/>arr[0] = 5; // 5 is 0101 in binary.<br/><br/>Atomics.xor(arr, 0, 12); // 10 is 1100 in binary<br/>console.log(Atomics.load(arr, 0));</pre>
<p><kbd>Atomics.xor</kbd> here performed a XOR operation between <kbd>arr[0]</kbd> and the number <kbd>10</kbd>.</p>
<p>This outputs:</p>
<pre><strong>9</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How bitwise XOR works</h1>
                
            
            
                
<p class="mce-root">Suppose we want to take a bitwise XOR of 5 and 12:</p>
<ol>
<li>Covert both numbers to binary; 5 is 0101 and 12 is 1100.</li>
<li>Bitwise XOR performs an XOR operation bit by bit, starting from the first bit:<br/>
<em>5  ^  12</em><br/>
<em>0 ^ 1 = 1</em><br/>
<em>1 ^ 1 = 0</em><br/>
<em>0 ^ 0 = 0</em><br/>
<em>1 ^ 0 = 1</em></li>
<li>Thus, <em>5 ^ 12 = 1001</em>, which is 9.</li>
</ol>
<p>With all the knowledge we required about atomics, let's hop back to our one billion count problem and see a possible fix.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Fixing one billion count with atomics</h1>
                
            
            
                
<p>Now we know the reason for our garbage value, and, with atomics, it should be easy to fix the problem. Right? Wrong.</p>
<p>There is a massive performance penalty on using atomic locking a billion times. Let's look at our updated <kbd>worker.js</kbd> code now:</p>
<pre>// worker.js<br/>let sharedMem;<br/><br/>addEventListener('message', ({data}) =&gt; {<br/>  //console.log(data);<br/>    if(data.message == 'sab') {<br/>        sharedMem = data.memory;<br/>        console.log('Memory ready');<br/>    }<br/>    if(data.cmd == 'start') {<br/>      console.log('Iterations ready');<br/>        startCounting(data.iterations);<br/>    }<br/>});<br/><br/>function startCounting(limit) {<br/>    const arr = new Uint32Array(sharedMem);<br/>    for(let i=0;i&lt;limit;i++) {<br/>        Atomics.add(arr, 0, 1);<br/>    }<br/>    postMessage('done')<br/>}</pre>
<p>This is similar to our previous implementation of the problem, with the change being that instead of adding it directly to the array, we are performing an atomic operation so that the value isn't changed by the other thread while one thread is adding value to it.</p>
<p>Sure, it is a beautiful fix. And it works, as well:</p>
<div><img height="163" src="img/895b6207-da11-4939-bdb9-0af21a5604c8.png" width="410"/></div>
<p>But look at that time: 80 seconds! That is the penalty you get when you lock and unlock over a memory one billion times.</p>
<p>Single threaded is faster, because it can access the local variable values from the register really quickly and use them. Our performance is slow because we're getting the reference to shared memory down, locking it, incrementing it, putting it up, and unlocking it.</p>
<p>Let's read that again. Single threaded is faster because it can access the local variable values from the register really quickly and use them. Can we do something about this? Let's see!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The optimized fix</h1>
                
            
            
                
<p>Why not combine the good parts of atomics and the good parts of the speed of local variables sitting in the CPU register? Here we are:</p>
<pre>// worker.js<br/>let sharedMem;<br/><br/>addEventListener('message', ({data}) =&gt; {<br/>  //console.log(data);<br/>    if(data.message == 'sab') {<br/>        sharedMem = data.memory;<br/>        console.log('Memory ready');<br/>    }<br/>    if(data.cmd == 'start') {<br/>      console.log('Iterations ready');<br/>        startCounting(data.iterations);<br/>    }<br/>});<br/><br/>function startCounting(limit) {<br/>    const arr = new Uint32Array(sharedMem);<br/>    let count = 0;<br/>    for(let i=0;i&lt;limit;i++) {<br/>        count += 1;<br/>    }<br/>    Atomics.add(arr, 0, count);<br/>    postMessage('done')<br/>}</pre>
<p>Here, from our last implementation, we took <kbd>Atomics.add</kbd> out of the loop to avoid calling it a billion times. Instead, we performed the work assigned to this web worker inside a local variable, and updated the memory with atomics only when we were done. This ensures no overwrite by two threads, in case they finish at the same moment. It's time to see the output.</p>
<p>Look at these awesome results:</p>
<div><img height="170" src="img/5019587e-0546-40c9-89a0-a186e48262a3.png" width="413"/></div>
<p class="mce-root">It is less than a second! That is about a 2.5 times gain by using just two workers! When we implemented parallel programming correctly, we were able to defeat our projected speed-up of 2x and shoot it to about 2.5x!</p>
<p>Hang on. The story doesn't end here. Let's add <kbd>4</kbd> workers and see what happens:</p>
<pre>// Main Script<br/><br/>const sharedMem = new SharedArrayBuffer(4);<br/>const workers = [new Worker('worker.js'), new Worker('worker.js'), new Worker('worker.js'), new Worker('worker.js')];<br/>let workersDone = 0;<br/><br/>const now = performance.now();<br/><br/>for(let i=0;i&lt;2;i++) {<br/>  workers[i].postMessage({message: 'sab', memory: sharedMem});<br/><br/>  workers[i].addEventListener('message', data =&gt; {<br/>      if(++workersDone == 4) { // don't worry. this is thread-safe ;)<br/>        console.log("All workers done. The memory is: ", new Uint32Array(sharedMem))<br/>        console.log(`Time taken: ${performance.now()-now}`)<br/>      }<br/>  });<br/><br/>  workers[i].postMessage({cmd: 'start', iterations: 1000000000/workers.length});<br/>}</pre>
<p>Excited to see the results? So am I! Let's see:</p>
<div><img height="229" src="img/f31bc77f-259d-4c90-bcc6-0710118f5c42.png" width="394"/></div>
<p>Uh oh. Hmm, that doesn't seem like a very impressive performance gain. Going from one thread to two threads was a huge boost. Why is going from two to four, not one?</p>
<p>Let's take a look at the Network tab:</p>
<div><img height="77" src="img/8dcf2e50-e45e-4065-ae61-1bdb346507d7.png" width="364"/></div>
<p>Eureka moment! It looks like we're spending 462 ms out of 911 ms of total program execution on just downloading the <kbd>worker.js</kbd> file! This even excludes the time for compilation of every individual script to machine code, which the JavaScript engine performs once it downloads.</p>
<p>Unfortunately, that is the end of what we can do from our end. Now it's browser's task to optimize the thing that if a single file is called again and again in the web worker, it should pickup compiled file from cache so it can actually use an already compiled instance of one file instead of downloading it three more times and then compiling them again.</p>
<p>In future, if Chrome optimizes according to the preceding suggestion, we can say it takes around <em>~120</em> ms instead of 462 ms of downloading and compiling.</p>
<p>Therefore, our script, in the near future, will take around ~570 ms to count to one billion. That is a performance gain of 500% over a single thread. That is multi-threading in JavaScript for you, folks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">A peek into Spectre</h1>
                
            
            
                
<p>On January 3, 2018, there was a fundamental flaw discovered with the CPU architecture we've been using for the past 20 years. This has shaken modern security to its roots. While the workings of Spectre and Meltdown are highly complicated (and deeply interesting, if you like the security field), what you have to know right now is that because of Spectre, all major browser vendors have disabled <kbd>SharedArrayBuffer</kbd> in browsers by default.</p>
<p>You can enable <kbd>SharedArrayBuffer</kbd> by going to <kbd>chrome://flags</kbd> and searching for <kbd>SharedArrayBuffer</kbd> and enabling it.</p>
<p>The reason for disabling <kbd>SharedArrayBuffer</kbd> is to mitigate Spectre, which is a dangerous but beautifully crafted exploit which requires a very precise measurement of time to attack. <kbd>SharedArrayBuffer</kbd> provides a way for multiple threads to be accessible to every thread, and atomics add more precision over the data available. This can be used to create highly precise clocks using <kbd>SharedArrayBuffer</kbd>, which can be used to carry out a Spectre attack.</p>
<p>Spectre basically exploits the fact that modern CPUs precompute a lot of things and put them in the cache. So, if you get ACCESS DENIED for a part of memory your program is not supposed to access way quicker than it's supposed to be, chances are, that particular block of memory is in the cache. Using beautifully crafted scripts, it is even possible to know which value is in the cache because your program is the one which put it there!</p>
<p>Ahh! It'll take a long chapter to actually have a little fun with Spectre and Meltdown. But that is for some other day, some other book. The takeaway from here is that at the time of writing this chapter, <kbd>SharedArrayBuffer</kbd> is not enabled in browsers by default. It will be enabled in the future, when proper patches are put in place by all browser vendors.</p>
<p>Here are some articles for people who find such stuff cool:</p>
<ul>
<li>How Spectre works: <a href="http://www.i-programmer.info/news/149-security/11449-how-spectre-works.html">http://www.i-programmer.info/news/149-security/11449-how-spectre-works.html</a></li>
<li>Spectre and Meltdown explained: <a href="https://www.csoonline.com/article/3247868/vulnerabilities/spectre-and-meltdown-explained-what-they-are-how-they-work-whats-at-risk.html">https://www.csoonline.com/article/3247868/vulnerabilities/spectre-and-meltdown-explained-what-they-are-how-they-work-whats-at-risk.html</a></li>
</ul>
<p>Till then, stay safe!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>It wouldn't be wrong to say that this is now my favorite chapter, with <em>Chapter 4, Asynchronous Programming,</em> sliding down to number two. This technology is raw, fresh, and waiting to be explored.</p>
<p>We learned a lot of new stuff about ES2017 in this chapter, which, in the near future, will be the base of multi-threaded programs written in JavaScript. Well, that is it! You're now a good developer who knows a lot about ES2017 (that is, ES8) and a lot more about future tech, as well. Use your powers to make this world a good place! </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>


            

            
        
    </body></html>