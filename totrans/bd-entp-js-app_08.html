<html><head></head><body>
        

                            
                    <h1 class="header-title">Writing Unit/Integration Tests</h1>
                
            
            
                
<p>We have now done as much as we can to modularize our code base, but how much confidence do we have in each of the modules? If one of the E2E tests fails, how would we pinpoint the source of the error? How do we know which module is faulty?</p>
<p>We need a lower level of testing that works at the module level to ensure they work as distinct, standalone units—we need <strong>unit</strong><strong> tests</strong>. Likewise, we should test that multiple units can work well together as a larger logical unit; to do that, we need to also implement some <strong>integration tests</strong>.</p>
<p>By following this chapter, you will be able to do the following:</p>
<ul>
<li>Write unit and integration tests using <strong>Mocha</strong></li>
<li>Record function calls with <strong>spies</strong>, and simulate behavior with <strong>stubs</strong>, both provided by the <strong>Sinon</strong> library</li>
<li>Stub out dependencies in unit tests using <strong>dependency injection</strong> (<strong>DI</strong>) or <strong>monkey patching</strong></li>
<li>Measuring <strong>test coverage</strong> with <strong>Istanbul</strong>/<strong>nyc</strong></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Picking a testing framework</h1>
                
            
            
                
<p>While there's only one <em>de facto</em> testing framework for E2E tests for JavaScript (Cucumber), there are several popular testing frameworks for unit and integration tests, namely Jasmine (<a href="https://jasmine.github.io/">jasmine.github.io</a>), Mocha (<a href="https://mochajs.org/">mochajs.org</a>), Jest (<a href="https://jestjs.io/">jestjs.io</a>), and AVA (<a href="https://github.com/avajs/ava">github.com/avajs/ava</a>).</p>
<p>We will be using Mocha for this book, but let's understand the rationale behind that decision. As always, there are pros and cons for each choice:</p>
<ul>
<li><strong>Maturity</strong>: Jasmine and Mocha have been around for the longest, and for many years were the only two viable testing frameworks for JavaScript and Node. Jest and AVA are the new kids on the block. Generally, the maturity of a library correlates with the number of features and the level of support.</li>
<li><strong>Popularity</strong>: Generally, the more popular a library is, the larger the community, and the higher likelihood of receiving support when things go awry. In terms of popularity, let's examine several metrics (correct as of September 7, 2018):
<ul>
<li>GitHub stars@ Jest (20,187), Mocha (16,165), AVA (14,633), Jasmine (13,816)</li>
<li>Exposure (percentage of developers who have heard of it): Mocha (90.5%), Jasmine (87.2%), Jest (62.0%), AVA (23.9%)</li>
<li>Developer satisfaction (percentage of developers who have used the tool <em>and would use it again</em>): Jest (93.7%), Mocha (87.3%), Jasmine (79.6%), AVA (75.0%).</li>
</ul>
</li>
<li><strong>Parallelism</strong>: Mocha and Jasmine both run tests serially (meaning one after the other), which means they can be quite slow. Instead, AVA and Jest, by default, run unrelated tests in parallel, as separate processes, making tests run faster because one test suite doesn't have to wait for the preceding one to finish in order to start.<br/></li>
<li><strong>Backing</strong>: Jasmine is maintained by developers at Pivotal Labs, a software consultancy from San Francisco. Mocha was created by TJ Holowaychuk and is maintained by several developers; although it is not maintained by a single company, it is backed by larger companies such as Sauce Labs, Segment, and Yahoo!. AVA was started in 2015 by Sindre Sorhus and is maintained by several developers. Jest is developed by Facebook, and so has the best backing of all the frameworks.</li>
<li><strong>Composability</strong>: Jasmine and Jest have different tools bundled into one framework, which is great to get started quickly, but it means we can't see how everything fits together. Mocha and AVA, on the other hand, simply run the tests, and you can use other libraries such as <kbd>Chai</kbd>, <kbd>Sinon</kbd>, and <kbd>nyc</kbd> for assertions, mocking, and coverage reports, respectively.</li>
</ul>
<p>The exposure and developer satisfaction figures are derived from The State of JavaScript survey, 2017 (<a href="https://2017.stateofjs.com/2017/testing/results/">2017.stateofjs.com/2017/testing/results</a>).</p>
<p>We have chosen to use Mocha for this book, as it allows us to compose a custom testing stack. By doing this, it allows us to examine each testing tool individually, which is beneficial for your understanding. However, once you understand the intricacies of each testing tool,  I do encourage you to try Jest, as it is easier to set up and use.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Mocha</h1>
                
            
            
                
<p class="mce-root">First, let's install Mocha as a development dependency:</p>
<pre class="mce-root"><strong>$ yarn add mocha --dev</strong></pre>
<p class="mce-root">This will install an executable, <kbd>mocha</kbd>, at <kbd>node_modules/mocha/bin/mocha</kbd>, which we can execute later to run our tests.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Structuring our test files</h1>
                
            
            
                
<p class="mce-root">Next, we are going to write our unit tests, but where should we put them? There are generally two approaches:</p>
<ul>
<li class="mce-root">Placing all tests for the application in a top-level <kbd>test/</kbd> directory</li>
<li class="mce-root">Placing the unit tests for a module of code next to the module itself, and using a generic <kbd>test</kbd> directory only for application-level integration tests (for example, testing integration with external resources such as databases)</li>
</ul>
<p>The second approach (as shown in the following example) is better as it keeps each module <em>truly</em> separated in the filesystem:</p>
<pre style="padding-left: 60px"><strong>$ tree</strong><br/><strong>.</strong><br/><strong>├── src</strong><br/><strong>│   └── feature</strong><br/><strong>│       ├── index.js</strong><br/><strong>│       └── index.unit.test.js</strong><br/><strong>└── test</strong><br/><strong>    ├── db.integration.test.js</strong><br/><strong>    └── app.integration.test.js</strong></pre>
<p class="mce-root">Furthermore, we're going to use the <kbd>.test.js</kbd> extension to indicate that a file contains tests (although using <kbd>.spec.js</kbd> is also a common convention). We will be even more explicit and specify the <em>type</em> of test in the extension itself; that is, using <kbd>unit.test.js</kbd> for unit test, and <kbd>integration.test.js</kbd> for integration tests.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Writing our first unit test</h1>
                
            
            
                
<p>Let's write unit tests for the <kbd>generateValidationErrorMessage</kbd> function. But first, let's convert our <kbd>src/validators/errors/messages.js</kbd> file into its own directory so that we can group the implementation and test code together in the same directory:</p>
<pre><strong>$ cd src/validators/errors</strong><br/><strong>$ mkdir messages</strong><br/><strong>$ mv messages.js messages/index.js</strong><br/><strong>$ touch messages/index.unit.test.js</strong></pre>
<p>Next, in <kbd>index.unit.test.js</kbd>, import the <kbd>assert</kbd> library and our <kbd>index.js</kbd> file:</p>
<pre>import assert from 'assert';<br/>import generateValidationErrorMessage from '.';</pre>
<p>Now, we are ready to write our tests.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Describing the expected behavior</h1>
                
            
            
                
<p>When we installed the <kbd>mocha</kbd> npm package, it provided us with the <kbd>mocha</kbd> command to execute our tests. When we run <kbd>mocha</kbd>, it will inject several functions, including <kbd>describe</kbd> and <kbd>it</kbd>, as global variables into the test environment. The <kbd>describe</kbd> function allows us to group relevant test cases together, and the <kbd>it</kbd> function defines the actual test case.</p>
<p>Inside <kbd>index.unit.tests.js</kbd>, let's define our first <kbd>describe</kbd> block:</p>
<pre>import assert from 'assert';<br/>import generateValidationErrorMessage from '.';<br/><br/>describe('generateValidationErrorMessage', function () {<br/>  it('should return the correct string when error.keyword is "required"', function () {<br/>    const errors = [{<br/>      keyword: 'required',<br/>      dataPath: '.test.path',<br/>      params: {<br/>        missingProperty: 'property',<br/>      },<br/>    }];<br/>    const actualErrorMessage = generateValidationErrorMessage(errors);<br/>    const expectedErrorMessage = "The '.test.path.property' field is missing";<br/>    assert.equal(actualErrorMessage, expectedErrorMessage);<br/>  });<br/>});</pre>
<p>Both the <kbd>describe</kbd> and <kbd>it</kbd> functions accept a string as their first argument, which is used to describe the group/test. The description has no influence on the outcome of the test, and is simply there to provide context for someone reading the tests.</p>
<p>The second argument of the <kbd>it</kbd> function is another function where you'd define the assertions for your tests. The function should throw an <kbd>AssertionError</kbd> if the test fails; otherwise, Mocha will assume that the test should pass.</p>
<p>In our test, we have created a dummy <kbd>errors</kbd> array that mimics the <kbd>errors</kbd> array, which is typically generated by Ajv. We then passed the array into the <kbd>generateValidationErrorMessage</kbd> function and capture its returned value. Lastly, we compare the actual output with our expected output; if they match, the test should pass; otherwise, it should fail.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Overriding ESLint for test files</h1>
                
            
            
                
<p>The preceding test code should have caused some ESLint errors. This is because we violated three rules:</p>
<ul>
<li><kbd>func-names</kbd>: Unexpected unnamed function</li>
<li><kbd>prefer-arrow-callback</kbd>: Unexpected function expression</li>
<li><kbd>no-undef</kbd>: <kbd>describe</kbd> is not defined</li>
</ul>
<p>Let's fix them before we continue.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding arrow functions in Mocha</h1>
                
            
            
                
<p>We have already encountered the <kbd>func-names</kbd> and <kbd>prefer-arrow-callback</kbd> rules before when we wrote our E2E tests with <kbd>cucumber-js</kbd>. Back then, we needed to keep using function expressions instead of arrow functions because <kbd>cucumber-js</kbd> uses <kbd>this</kbd> inside each function to maintain context between different steps of the same scenario. If we'd used arrow functions, <kbd>this</kbd> would be bound, in our case, to the global context, and we'd have to go back to using file-scope variables to maintain state between steps.</p>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">As it turns out, Mocha also uses <kbd>this</kbd> to maintain a "context". However, in Mocha's vocabulary, a "context" is not used to persist state between steps; rather, a Mocha context provides the following methods, which you can use to control the flow of your tests:</p>
<ul>
<li class="mce-root"><kbd>this.timeout()</kbd>: To specify how long, in milliseconds, to wait for a test to complete before marking it as failed</li>
<li class="mce-root"><kbd>this.slow()</kbd>: To specify how long, in milliseconds, a test should run for before it is considered "slow"</li>
<li class="mce-root"><kbd>this.skip()</kbd> : To skip/abort a test</li>
<li class="mce-root"><kbd>this.retries()</kbd>: To retry a test a specified number of times</li>
</ul>
<p>It is also impractical to give names to every test function; therefore, we should disable both the <kbd>func-names</kbd> and <kbd>prefer-arrow-callback</kbd> rules.</p>
<p>So, how do we disable these rules for our test files? For our E2E tests, we created a new <kbd>.eslintrc.json</kbd> and placed it inside the <kbd>spec/</kbd> directory. This would apply those configurations to all files under the <kbd>spec/</kbd> directory. However, our test files are not separated into their own directory, but interspersed between all our application code. Therefore, creating a new <kbd>.eslintrc.json</kbd> won't work.</p>
<p>Instead, we can add an <kbd>overrides</kbd> property to our top-level <kbd>.eslintrc.json</kbd>, which allows us to override rules for files that match the specified file glob(s). Update <kbd>.eslintrc.json</kbd> to the following:</p>
<pre>{<br/>    "extends": "airbnb-base",<br/>    "rules": {<br/>        "no-underscore-dangle": "off"<br/>    },<br/>    "overrides": [<br/>        {<br/>            "files": ["*.test.js"],<br/>            "rules": {<br/>                "func-names": "off",<br/>                "prefer-arrow-callback": "off"<br/>            }<br/>        }<br/>    ]<br/>}</pre>
<p>Here, we are indicating that files with the extension <kbd>.test.js</kbd> should have the <kbd>func-names</kbd> and <kbd>prefer-arrow-callback</kbd> rules turned off.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Specifying ESLint environments</h1>
                
            
            
                
<p>However, ESLint will still complain that we are violating the <kbd>no-undef</kbd> rule. This is because when we invoke the <kbd>mocha</kbd> command, it will inject the <kbd>describe</kbd> and <kbd>it</kbd> functions as global variables. However, ESLint doesn't know this is happening and warns us against using variables that are not defined inside the module.</p>
<p>We can instruct ESLint to ignore these undefined globals by specifying an <strong>environment</strong>. An environment defines global variables that are predefined. Update our overrides array entry to the following:</p>
<pre>{<br/> "files": ["*.test.js"],<br/> <strong>"env": {</strong><br/><strong> "mocha": true</strong><br/><strong> },</strong><br/> "rules": {<br/> "func-names": "off",<br/> "prefer-arrow-callback": "off"<br/> }<br/>}</pre>
<p>Now, ESLint should not complain anymore!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running our unit tests</h1>
                
            
            
                
<p>To run our test, we'd normally just run <kbd>npx mocha</kbd>. However, when we try that here, we get a warning:</p>
<pre><strong>$ npx mocha</strong><br/><strong>Warning: Could not find any test files matching pattern: test</strong><br/><strong>No test files found</strong></pre>
<p>This is because, by default, Mocha will try to find a directory named <kbd>test</kbd> at the root of the project and run the tests contained inside it. Since we placed our test code next to their corresponding module code, we must inform Mocha of the location of these test files. We can do this by passing a <strong>glob</strong> matching our test files as the second argument to <kbd>mocha</kbd>. Try running the following: </p>
<pre><strong>$ npx mocha "src/**/*.test.js"</strong><br/><strong>src/validators/users/errors/index.unit.test.js:1</strong><br/><strong>(function (exports, require, module, __filename, __dirname) { import assert from 'assert';</strong><br/><strong>                                                              ^^^^^^</strong><br/><strong>SyntaxError: Unexpected token import</strong><br/><strong>  ....</strong></pre>
<p>We got another error. We had already encountered this when we worked with <kbd>cucumber-js</kbd>. This error occurs because Mocha is not using Babel to transpile our test code before running it. With <kbd>cucumber-js</kbd>, we used the <kbd>--require-module</kbd> flag to require the <kbd>@babel/register</kbd> package, which ??. We can do the same with Mocha using its <kbd>--require</kbd> flag:</p>
<pre><strong> $ npx mocha "src/**/*.test.js" --require @babel/register</strong><br/><br/><strong>  generateValidationErrorMessage</strong><br/><strong>    <img src="img/20da118f-b69a-4d2c-acca-6fe1aaed606f.png" style="width:1.50em;height:1.58em;"/> should return the correct string when error.keyword is "required"</strong><br/><br/><strong>  1 passing (32ms)</strong></pre>
<p>If you've forgotten about the different Babel packages (for example, <kbd>@babel/node</kbd>, <kbd>@babel/register</kbd>, <kbd>@babel/polyfill</kbd>, and so on), refer back to <a href="76e42f28-9731-49ca-9e87-fab7b2b6a7e8.xhtml" target="_blank">Chapter 6</a>, <em>Setting Up Development Tools</em>, under the <em>Different faces of Babel</em> section.</p>
<p>Note that the test description we passed into <kbd>describe</kbd> and <kbd>it</kbd> is displayed in the test output.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running unit tests as an npm script</h1>
                
            
            
                
<p>Typing out the full <kbd>mocha</kbd> command each time can be tiresome. Therefore, we should create an npm script just like we did with the E2E tests. Add the following to the scripts object inside our <kbd>package.json</kbd> file:</p>
<pre>"test:unit": "mocha 'src/**/*.test.js' --require @babel/register",</pre>
<p>Furthermore, let's also update our existing <kbd>test</kbd> npm script to run all our tests (both unit and E2E):</p>
<pre>"test": "yarn run test:unit &amp;&amp; yarn run test:e2e",</pre>
<p>Now, we can run our unit tests by running <kbd>yarn run test:unit</kbd>, and run all our tests with <kbd>yarn run test</kbd>. We have now completed our first unit test, so let's commit the changes and move on to writing even more tests:</p>
<pre><strong>$ git add -A &amp;&amp; \</strong><br/><strong>  git commit -m "Implement first unit test for generateValidationErrorMessage"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Completing our first unit test suite</h1>
                
            
            
                
<p>We have only covered a single scenario with our first unit test. Therefore, we should write more tests to cover every scenario. Try completing the unit test suite for <kbd>generateValidationErrorMessage</kbd> yourself; once you are ready, compare your solution with the following one:</p>
<pre>import assert from 'assert';<br/>import generateValidationErrorMessage from '.';<br/><br/>describe('generateValidationErrorMessage', function () {<br/>  it('should return the correct string when error.keyword is "required"', function () {<br/>    const errors = [{<br/>      keyword: 'required',<br/>      dataPath: '.test.path',<br/>      params: {<br/>        missingProperty: 'property',<br/>      },<br/>    }];<br/>    const actualErrorMessage = generateValidationErrorMessage(errors);<br/>    const expectedErrorMessage = "The '.test.path.property' field is missing";<br/>    assert.equal(actualErrorMessage, expectedErrorMessage);<br/>  });<br/>  it('should return the correct string when error.keyword is "type"', function () {<br/>    const errors = [{<br/>      keyword: 'type',<br/>      dataPath: '.test.path',<br/>      params: {<br/>        type: 'string',<br/>      },<br/>    }];<br/>    const actualErrorMessage = generateValidationErrorMessage(errors);<br/>    const expectedErrorMessage = "The '.test.path' field must be of type string";<br/>    assert.equal(actualErrorMessage, expectedErrorMessage);<br/>  });<br/>  it('should return the correct string when error.keyword is "format"', function () {<br/>    const errors = [{<br/>      keyword: 'format',<br/>      dataPath: '.test.path',<br/>      params: {<br/>        format: 'email',<br/>      },<br/>    }];<br/>    const actualErrorMessage = generateValidationErrorMessage(errors);<br/>    const expectedErrorMessage = "The '.test.path' field must be a valid email";<br/>    assert.equal(actualErrorMessage, expectedErrorMessage);<br/>  });<br/>  it('should return the correct string when error.keyword is "additionalProperties"', function () {<br/>    const errors = [{<br/>      keyword: 'additionalProperties',<br/>      dataPath: '.test.path',<br/>      params: {<br/>        additionalProperty: 'email',<br/>      },<br/>    }];<br/>    const actualErrorMessage = generateValidationErrorMessage(errors);<br/>    const expectedErrorMessage = "The '.test.path' object does not support the field 'email'";<br/>    assert.equal(actualErrorMessage, expectedErrorMessage);<br/>  });<br/>});<br/><br/></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Run the tests again, and note how the tests are grouped under the <kbd>describe</kbd> block:</p>
<pre><strong>$ yarn run test:unit</strong><br/><br/><strong>  generateValidationErrorMessage</strong><br/><strong>    <img src="img/6f12f9ec-a57d-48cb-a5f0-7f20588ce6e0.png"/> should return the correct string when error.keyword is "required"</strong><br/><strong>    <img src="img/6f12f9ec-a57d-48cb-a5f0-7f20588ce6e0.png"/> should return the correct string when error.keyword is "type"</strong><br/><strong>    <img src="img/6f12f9ec-a57d-48cb-a5f0-7f20588ce6e0.png"/> should return the correct string when error.keyword is "format"</strong><br/><strong>    <img src="img/6f12f9ec-a57d-48cb-a5f0-7f20588ce6e0.png"/> should return the correct string when error.keyword is "additionalProperties"</strong><br/><strong>    <img src="img/6f12f9ec-a57d-48cb-a5f0-7f20588ce6e0.png"/> should return the correct string when error.keyword is not recognized</strong><br/><br/><strong>  5 passing (20ms)</strong></pre>
<p class="mce-root">We have now completed the unit tests for <kbd>generateValidationErrorMessage</kbd>, so let's commit it:</p>
<pre><strong>$ git add -A &amp;&amp; \</strong><br/><strong>  git commit -m "Complete unit tests for generateValidationErrorMessage"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Unit testing ValidationError</h1>
                
            
            
                
<p>Next, let's focus on testing the <kbd>ValidationError</kbd> class. Once again, we will move the <kbd>validation.js</kbd> file into its own director:</p>
<pre><strong>$ cd src/validators/errors/ &amp;&amp; \</strong><br/><strong>  mkdir validation-error &amp;&amp; \</strong><br/><strong>  mv validation-error.js validation-error/index.js &amp;&amp; \</strong><br/><strong>  cd ../../../</strong></pre>
<p>Now, create a new file at <kbd>src/validators/errors/validation-error/index.unit.test.js</kbd> to house our unit tests:</p>
<pre>import assert from 'assert';<br/>import ValidationError from '.';<br/><br/>describe('ValidationError', function () {<br/>  it('should be a subclass of Error', function () {<br/>    const validationError = new ValidationError();<br/>    assert.equal(validationError instanceof Error, true);<br/>  });<br/>  describe('constructor', function () {<br/>    it('should make the constructor parameter accessible via the `message` property of the instance', function () {<br/>      const TEST_ERROR = 'TEST_ERROR';<br/>      const validationError = new ValidationError(TEST_ERROR);<br/>      assert.equal(validationError.message, TEST_ERROR);<br/>    });<br/>  });<br/>});</pre>
<p>Run the tests and make sure they pass. Then, commit it into the repository:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Add unit tests for ValidationError"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Unit testing middleware</h1>
                
            
            
                
<p>Next, we are going to test our middleware functions, starting with the <kbd>checkEmptyPayload</kbd> middleware. Like we did previously, move the middleware module into its own directory:</p>
<pre><strong>$ cd src/middlewares/ &amp;&amp; \</strong><br/><strong>  mkdir check-empty-payload &amp;&amp; \</strong><br/><strong>  mv check-empty-payload.js check-empty-payload/index.js &amp;&amp; \</strong><br/><strong>  touch check-empty-payload/index.unit.test.js &amp;&amp; \</strong><br/><strong>  cd ../../</strong></pre>
<p>Then, inside <kbd>src/middlewares/check-content-type.js/index.unit.test.js</kbd>, lay out the skeleton of our first test:</p>
<pre>import assert from 'assert';<br/>import checkEmptyPayload from '.';<br/><br/>describe('checkEmptyPayload', function () {<br/>  describe('When req.method is not one of POST, PATCH or PUT', function () {<br/>    it('should not modify res', function () {<br/>      // Assert that `res` has not been modified<br/>    });<br/><br/>    it('should call next() once', function () {<br/>      // Assert that `next` has been called once<br/>    });<br/>  });});</pre>
<p>The purpose of the <kbd>checkEmptyPayload</kbd> middleware is to ensure that the <kbd>POST</kbd>, <kbd>PATCH</kbd>, and <kbd>PUT</kbd> requests always carry a non-empty payload. Therefore, if we pass in a request with a different method, say <kbd>GET</kbd>, we should be able to assert the following:</p>
<ul>
<li>That the <kbd>res</kbd> object is not modified</li>
<li>That the <kbd>next</kbd> function is invoked once</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Asserting deep equality</h1>
                
            
            
                
<p>To assert that the <kbd>res</kbd> object has not been modified, we need to perform a deep comparison of the <kbd>res</kbd> object before and after <kbd>checkEmptyPayload</kbd> has been called.</p>
<p>Instead of implementing this function ourselves, we can save time by using existing utility libraries. For instance, Lodash provides the <kbd>cloneDeep</kbd> method (<a href="https://lodash.com/docs/#cloneDeep">lodash.com/docs/#cloneDeep</a>) for deep cloning, and the <kbd>isEqual</kbd> method (<a href="https://lodash.com/docs/#isEqual">lodash.com/docs/#isEqual</a>) for deep object comparison.</p>
<p>To use these methods in our code, we can install the <kbd>lodash</kbd> package from npm, which contains hundreds of utility methods. However, we won't be using most of these methods in our project; if we install the entire utility library, most of the code would be unused. We should always try to be as lean as possible, minimizing the number, and size, of our project's dependencies.</p>
<p>Fortunately, Lodash provides a separate npm package for each method, so let's add them to our project:</p>
<pre><strong>$ yarn add lodash.isequal lodash.clonedeep --dev</strong></pre>
<p>You can use an online tool called Bundlephobia (<a href="https://bundlephobia.com/">bundlephobia.com</a>) to find out the file size of an npm package, without downloading it.<br/>
<br/>
For example, we can see from <a href="https://bundlephobia.com/result?p=lodash@4.17.10">bundlephobia.com/result?p=lodash@4.17.10</a> that the <kbd>lodash</kbd> package is 24.1 KB in size after it's been minified and gzipped. Similarly, the <kbd>lodash.isequal</kbd> and <kbd>lodash.clonedeep</kbd> packages have a size of 3.7 KB and 3.3 KB, respectively. Therefore, by installing the more specific packages, we have reduced the amount of unused code in our project by 17.1 KB.</p>
<p>Now, let's use the <kbd>deepClone</kbd> method to clone the <kbd>res</kbd> object before passing it to <kbd>checkEmptyPayload</kbd>. Then, after <kbd>checkEmptyPayload</kbd> has been called, use <kbd>deepEqual</kbd> to compare the <kbd>res</kbd> object and its clone, and assert whether the <kbd>res</kbd> object has been modified or not.</p>
<p>Have a go at implementing it yourself, and compare your solution with ours, as follows:</p>
<pre>import assert from 'assert';<br/><strong>import deepClone from 'lodash.clonedeep';</strong><br/><strong>import deepEqual from 'lodash.isequal';</strong><br/>import checkEmptyPayload from '.';<br/><br/>describe('checkEmptyPayload', function () {<br/><strong>  let req;</strong><br/><strong>  let res;</strong><br/><strong>  let next;</strong><br/>  describe('When req.method is not one of POST, PATCH or PUT', function    <br/>  () {<br/><strong>    let clonedRes;</strong><br/><br/><strong>    beforeEach(function () {</strong><br/><strong>      req = { method: 'GET' };</strong><br/><strong>      res = {};</strong><br/><strong>      next = spy();</strong><br/><strong>      clonedRes = deepClone(res);</strong><br/><strong>      checkEmptyPayload(req, res, next);</strong><br/><strong>    });</strong><br/><br/>    it('should not modify res', function () {<br/>      <strong>assert(deepEqual(res, clonedRes));</strong><br/>    });<br/><br/>    it('should call next() once', function () {<br/>      // Assert that `next` has been called<br/>    });<br/>  });<br/>});</pre>
<p>Next, we need a way to assert that the <kbd>next</kbd> function has been called once. We can do that by using test <strong>spies</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Asserting function calls with spies</h1>
                
            
            
                
<p>A spy is a function that records information about every call made to it. For example, instead of assigning an empty function to <kbd>next</kbd>, we can assign a spy to it. Whenever <kbd>next</kbd> is invoked, information about each invocation is stored inside the spy object. We can then use this information to determine the number of times the spy has been called.</p>
<p>The <em>de facto</em> spy library in the ecosystem is Sinon (<a href="http://sinonjs.org/">sinonjs.org</a>), so let's install it:</p>
<pre><strong>$ yarn add sinon --dev</strong></pre>
<p>Then, in our unit test, import the <kbd>spy</kbd> <strong>named export</strong> from the <kbd>sinon</kbd> package:</p>
<pre>import { spy } from 'sinon';</pre>
<p>Now, in our test function, instead of assigning an empty function to <kbd>next</kbd>, assign it a new spy:</p>
<pre>const next = spy();</pre>
<p>When the spy function is called, the spy will update some of its properties to reflect the state of the spy. For example, when it's been called once, the spy's <kbd>calledOnce</kbd> property will be set to <kbd>true</kbd>; if the spy function is invoked again, the <kbd>calledOnce</kbd> property will be set to <kbd>false</kbd> and the <kbd>calledTwice</kbd> property will be set to <kbd>true</kbd>. There are many other useful properties such as <kbd>calledWith</kbd>, but let's update our <kbd>it</kbd> block by checking the <kbd>calledOnce</kbd> property of our spy:</p>
<pre>it('should call next() once', function () {<br/>  <strong>assert(next.calledOnce);</strong><br/>});</pre>
<p>Next, we'll define more tests to examine what happens when <kbd>req.method</kbd> is one of <kbd>POST</kbd>, <kbd>PATCH</kbd>, or <kbd>PUT</kbd>. Implement the following tests, which test what happens when the <kbd>content-length</kbd> header is not <kbd>0</kbd>:</p>
<pre>describe('checkEmptyPayload', function () {<br/>  let req;<br/>  let res;<br/>  let next;<br/>  ...<br/>  <strong>(['POST', 'PATCH', 'PUT']).forEach((method) =&gt; {</strong><br/><strong>    describe(`When req.method is ${method}`, function () {</strong><br/><strong>      describe('and the content-length header is not "0"', function () {</strong><br/><strong>        let clonedRes;</strong><br/><br/><strong>        beforeEach(function () {</strong><br/><strong>          req = {</strong><br/><strong>            method,</strong><br/><strong>            headers: {</strong><br/><strong>              'content-length': '1',</strong><br/><strong>            },</strong><br/><strong>          };</strong><br/><strong>          res = {};</strong><br/><strong>          next = spy();</strong><br/><strong>          clonedRes = deepClone(res);</strong><br/><strong>          checkEmptyPayload(req, res, next);</strong><br/><strong>        });</strong><br/><br/><strong>        it('should not modify res', function () {</strong><br/><strong>          assert(deepEqual(res, clonedRes));</strong><br/><strong>        });</strong><br/><br/><strong>        it('should call next()', function () {</strong><br/><strong>          assert(next.calledOnce);</strong><br/><strong>        });</strong><br/><strong>      });</strong><br/><strong>    });</strong><br/><strong>  });</strong><br/>});</pre>
<p><kbd>beforeEach</kbd> is another function that is injected into the global scope by Mocha. <kbd>beforeEach</kbd> will run the function passed into it, prior to running each <kbd>it</kbd> block that resides on the same or lower level as the <kbd>beforeEach</kbd> block. Here, we are using it to invoke <kbd>checkEmptyPayload</kbd> before each assertion.</p>
<div><kbd>beforeEach</kbd> is a type of <strong>hook</strong> function. There are also <kbd>afterEach</kbd>, <kbd>before</kbd>, and <kbd>after</kbd>. See how you can use them by referring to the documentation at <a href="https://mochajs.org/#hooks">mochajs.org/#hooks</a>.</div>
<p>Next, when the <kbd>content-type</kbd> header is <kbd>0</kbd>, we want to assert that the <kbd>res.status</kbd>, <kbd>res.set</kbd>, and <kbd>res.json</kbd> methods are called correctly:</p>
<pre>describe('and the content-length header is "0"', function () {<br/>  let resJsonReturnValue;<br/><br/>  beforeEach(function () {<br/>    req = {<br/>      method,<br/>      headers: {<br/>        <strong>'content-length': '0',</strong><br/>      },<br/>    };<br/>    resJsonReturnValue = {};<br/>    res = {<br/>      status: spy(),<br/>      set: spy(),<br/>      json: spy(),<br/>    };<br/>    next = spy();<br/>    checkEmptyPayload(req, res, next);<br/>  });<br/><br/>  describe('should call res.status()', function () {<br/>    it('once', function () {<br/>      assert(res.status.calledOnce);<br/>    });<br/>    it('with the argument 400', function () {<br/>      assert(res.status.calledWithExactly(400));<br/>    });<br/>  });<br/><br/>  describe('should call res.set()', function () {<br/>    it('once', function () {<br/>      assert(res.set.calledOnce);<br/>    });<br/>    it('with the arguments "Content-Type" and "application/json"', function () {<br/>      assert(res.set.calledWithExactly('Content-Type', 'application/json'));<br/>    });<br/>  });<br/><br/>  describe('should call res.json()', function () {<br/>    it('once', function () {<br/>      assert(res.json.calledOnce);<br/>    });<br/>    it('with the correct error object', function () {<br/>      assert(res.json.calledWithExactly({ message: 'Payload should not be empty' }));<br/>    });<br/>  });<br/><br/>  it('should not call next()', function () {<br/>    assert(next.notCalled);<br/>  });<br/>});</pre>
<p>Lastly, we need to test that <kbd>checkEmptyPayload</kbd> will return the output of <kbd>res.json()</kbd>. To do that, we need to use another test construct called <strong>stubs</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Simulating behavior with stubs</h1>
                
            
            
                
<p>Stubs are functions that simulate the behavior of another component.</p>
<p>In Sinon, stubs are an extension to spies; this means that all the methods that are available to spies are also available to stubs.</p>
<p>In the context of our tests, we don't really care about the returned value of <kbd>res.json()</kbd> – we only care that our <kbd>checkEmptyPayload</kbd> middleware function relays this value back faithfully. Therefore, we can turn our <kbd>res.json</kbd> spy into a stub, and make it return a reference to an object:</p>
<pre><strong>resJsonReturnValue = {};</strong><br/>res = {<br/>  status: spy(),<br/>  set: spy(),<br/>  <strong>json: stub().returns(resJsonReturnValue),</strong><br/>};</pre>
<p>We can then add another assertion step to compare the value returned by the <kbd>checkEmptyPayload</kbd> function, and the value returned by our <kbd>res.json</kbd> stub; they should be strictly identical:</p>
<pre>describe('and the content-length header is "0"', function () {<br/>  let resJsonReturnValue;<br/>  <strong>let returnedValue;</strong><br/><br/>  beforeEach(function () {<br/>    ...<br/>    <strong>returnedValue =</strong> checkEmptyPayload(req, res, next);<br/>  });<br/><br/>  ...<br/><br/>  <strong>it('should return whatever res.json() returns', function () {</strong><br/><strong>    assert.strictEqual(returnedValue, resJsonReturnValue);</strong><br/><strong>  });</strong><br/><br/>  ...<br/>});</pre>
<p>Run the unit tests by executing <kbd>yarn run test:unit</kbd>, fix any errors that cause the tests to fail, and then commit the unit tests to the repository:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Add unit tests for checkEmptyPayload middleware"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Testing all middleware functions</h1>
                
            
            
                
<p>Now, it's time for you to write some unit tests yourself. Try following the same approach to test the <kbd>checkContentTypeIsJson</kbd>, <kbd>checkContentTypeIsSet</kbd>, and <kbd>errorHandler</kbd> middleware functions. Refer to the code bundle for help if needed. As always, run the tests and commit your code!</p>
<p>Once all of our middleware functions have been unit tested, we will move on to testing the request handlers and the engine.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Unit testing the request handler</h1>
                
            
            
                
<p>First, we'll move the <kbd>src/handlers/users/create.js</kbd> module into its own directory. Then, we will correct the file paths specified in the <kbd>import</kbd> statements to point to the correct file. Lastly, we will create an <kbd>index.unit.test.js</kbd> file next to our module to house the unit tests.</p>
<p>Let's take a look at the <kbd>createUser</kbd> function inside our request handler module. It has the following structure:</p>
<pre>import create from '../../../engines/users/create';<br/>function createUser(req, res, db) {<br/>  create(req, db)<br/> .then(onFulfilled, onRejected)<br/> .catch(...)<br/>}</pre>
<p>First, it will call the <kbd>create</kbd> function that was imported from <kbd>src/engines/users/create/index.js</kbd>. Based on the result, it will invoke either the <kbd>onFulfilled</kbd> or <kbd>onRejected</kbd> callbacks inside the <kbd>then</kbd> block.</p>
<p>Although our <kbd>createUser</kbd> function depends on the <kbd>create</kbd> function, when writing a unit test, our test should test only the relevant unit, not its dependencies. Therefore, if the result of our tests relies on the <kbd>create</kbd> function, we should use a stub to control its behavior. Otherwise, our test would, in fact, be an <em>integration test</em>.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Stubbing create</h1>
                
            
            
                
<p>We can create different stubs that return different results, each mimicking the possible return values of the <kbd>create</kbd> function:</p>
<pre>import { stub } from 'sinon';<br/>import ValidationError from '../../../validators/errors/validation-error';<br/><br/>const createStubs = {<br/>  success: stub().resolves({ _id: 'foo'}),<br/>  validationError: stub().rejects(new ValidationError()),<br/>  otherError: stub().rejects(new Error()),<br/>}</pre>
<p>Now, if we invoke <kbd>createStubs.success()</kbd>, it will always resolve to the <kbd>{ _id: 'foo'}</kbd> object; therefore, we can use this stub to test for scenarios where the <kbd>req</kbd> object we pass into the <kbd>createUser</kbd> function is valid. Likewise, we can use <kbd>createStubs.validationError()</kbd> to mimic a situation where the <kbd>req</kbd> object causes <kbd>createUser</kbd> to reject with <kbd>ValidationError</kbd>.</p>
<p>Now, we know how to stub out the <kbd>create</kbd> function, but how do we actually replace it inside the <kbd>createUser</kbd> function? When testing the <kbd>createUser</kbd> function, the only variables we can change in our test are the parameters we pass into the function, and the <kbd>createUser</kbd> method accepts only three parameters: <kbd>req</kbd>, <kbd>res</kbd>, and <kbd>db</kbd>.</p>
<p>There are two approaches to this: <strong>dependency injection</strong> and <strong>monkey patching</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Dependency injection</h1>
                
            
            
                
<p>The idea of dependency injection is to make every dependency a parameter of the function.</p>
<p>At the moment, our <kbd>createUser</kbd> function relies on entities outside of its parameters; this includes the <kbd>create</kbd> function and the <kbd>ValidationError</kbd> class. If we were to use dependency injection, we'd modify our <kbd>createUser</kbd> function to have the following structure:</p>
<pre>function createUser(req, res, db, <strong>create, ValidationError</strong>) {<br/>  create(req)<br/>    .then(onFulfilled, onRejected)<br/>    .catch(...)<br/>}</pre>
<p class="mce-root"/>
<p>Then, we would be able to inject the following dependencies from our tests:</p>
<pre>...<br/>import ValidationError from '../../../validators/errors/validation-error';<br/>import createUser from '.';<br/><br/>const generateCreateStubs = {<br/>  success: () =&gt; stub().resolves({ _id: 'foo'})<br/>}<br/><br/>describe('create', function () {<br/>  describe('When called with valid request object', function (done) {<br/>    ...<br/>    createUser(req, res, db, <strong>generateCreateStubs.success(), ValidationError</strong>)<br/>      .then((result) =&gt; {<br/>        // Assertions here<br/>      })<br/>  })<br/>})</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Monkey patching</h1>
                
            
            
                
<p>An alternative approach to dependency injection is monkey patching, where we dynamically modify the system at runtime. In our example, we might want to replace the <kbd>create</kbd> function with our stub functions, but <em>only</em> when we are running our tests.</p>
<p>Implementations of monkey patching libraries tend to be hacky and usually involves reading the module code into a string, injecting custom code into the string, and then loading it. Thus, the entities being monkey patched would be modified in some way.</p>
<p>There are several libraries that allow us to apply monkey patches when running tests; the most popular library is <kbd>rewire</kbd> (<a href="https://www.npmjs.com/package/rewire">npmjs.com/package/rewire</a>). It also has a Babel plugin equivalent called <kbd>babel-plugin-rewire</kbd> (<a href="https://github.com/speedskater/babel-plugin-rewire">github.com/speedskater/babel-plugin-rewire</a>).</p>
<p>This plugin will add the <kbd>__set__</kbd>, <kbd>__get__</kbd>, and <kbd>__with__</kbd> methods to every top-level file-scoped entity in the module being "rewired". Now, we can use the <kbd>__set__</kbd> method of our <kbd>createUser</kbd> module to monkey patch our <kbd>create</kbd> function, like so:</p>
<pre>createUser.__set__('create', createUserStubs.success)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The <kbd>__set__</kbd> method returns a function that we can use to revert the <kbd>create</kbd> function back to its original state. This is useful when you want to run tests using different variants of <kbd>create</kbd>. In that case, you'd simply <kbd>revert</kbd> the create function after each test run, and patch it again at the beginning of the next run.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Dependency injection versus monkey patching</h1>
                
            
            
                
<p>Both approaches have their pros and cons, so let's compare their differences and see which one is the most appropriate for our use case.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Modularity</h1>
                
            
            
                
<p>Dependency injection has the benefit of keeping every module as decoupled as possible, as modules do not have predefined dependencies; every dependency is passed in (injected) at runtime. This makes unit testing a lot easier, as we can replace any dependencies with stubs, keeping our unit tests truly unit tests.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Readability</h1>
                
            
            
                
<p>With dependency injection, every dependency must be a parameter of the function. Thus, if the module has 20 dependencies, it'll need to have 20 parameters. This can make the module hard to read.</p>
<p>Often, you'll have a single root file where every dependency is imported, instantiated, and injected; these dependencies would then be passed down to child functions, and their child functions, and so on. This means for a developer to find the source of the dependency, he/she would have to follow the trail of function calls leading up to the root where the dependency is originally injected. This could be three or four function calls, or it might be a dozen.</p>
<p>Generally speaking, the more abstraction layers there are in a project, the harder it is for developers to read the code, but this is especially true when using the dependency injection approach.</p>
<p>With monkey patching, the signature of the module functions can be much leaner. Only dynamic dependencies would be included in the function parameters list; utility functions and static dependencies can be imported at the top of the file.</p>
<p class="mce-root"/>
<p>For instance, the <kbd>req</kbd>, <kbd>res</kbd>, and <kbd>db</kbd> parameters of the <kbd>createUser</kbd> function are dynamic – <kbd>req</kbd> and <kbd>res</kbd> would be different for each request, and <kbd>db</kbd> is only instantiated at startup. On the other hand, the <kbd>create</kbd> function and <kbd>ValidationError</kbd> class are static – you know their exact value before you run the code.</p>
<p>Therefore, using monkey patching can improve the readability of our application code, at the expense of making our test code a bit more complicated.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reliance on third-party tools</h1>
                
            
            
                
<p>Dependency injection is a simple concept to implement and does not require any third-party tools. On the other hand, monkey patching is hard to implement and you'd normally use <kbd>babel-plugin-rewire</kbd> or a similar library. This means that our test would now have to depend on the <kbd>babel-plugin-rewire</kbd> package.</p>
<p>This can become an issue if <kbd>babel-plugin-rewire</kbd> becomes unmaintained, or if maintenance is slow. At the time of writing this book, the <kbd>babel-plugin-rewire</kbd> plugin still lacks support for Babel 7. If a developer is using the <kbd>babel-plugin-rewire</kbd> plugin, he/she won't be able to upgrade their Babel version, and for developers who are already using Babel 7, they won't be able to monkey patch until support is implemented.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Following the dependency injection pattern</h1>
                
            
            
                
<p>From the preceding discussion, it seems like dependency injection is the better choice. Readability should not be too much of an issue, as we only have two layers of abstraction – handlers and engines. Therefore, let's migrate our code to use the dependency injection pattern.</p>
<p>First, remove the <kbd>import</kbd> statements from <kbd>src/handlers/users/create/index.js</kbd> and change the signature of the <kbd>createUser</kbd> function to include the <kbd>create</kbd> engine function and the <kbd>ValidationError</kbd> class:</p>
<pre>function createUser(req, res, db<strong>, create, ValidationError</strong>) { ... }</pre>
<p>Now, we need to inject these dependencies into the handler. In <kbd>src/index.js</kbd>, we are already using the <kbd>injectHandlerDependencies</kbd> function to inject the database client into the handler, so let's modify it to also inject the corresponding engine function and <kbd>ValidationError</kbd> class.</p>
<p>First, let's import all the dependencies inside <kbd>src/index.js</kbd>:</p>
<pre>import ValidationError from './validators/errors/validation-error';<br/>import createUserHandler from './handlers/users/create';<br/>import createUserEngine from './engines/users/create';</pre>
<p>Next, let's create a mapping of handler functions to engine functions, and call it <kbd>handlerToEngineMap</kbd>. We will pass this <kbd>handlerToEngineMap</kbd> function into the <kbd>injectHandlerDependencies</kbd> function, so that it knows which engine to inject:</p>
<pre>const handlerToEngineMap = new Map([<br/>  [createUserHandler, createUserEngine],<br/>]);</pre>
<p>We are using the <kbd>Map</kbd> object, which was introduced in ECMAScript 2015 (ES6). A <kbd>Map</kbd> is a key-value store, where the keys and values can be of any type – primitives, objects, arrays, or functions (the last two are just special types of object). This is unlike an object literal, where the keys must be either a string or a Symbol. Here, we are storing the handler function as the key, and the engine function as the value.</p>
<p>All that's left to do in <kbd>src/index.js</kbd> is to add <kbd>handlerToEngineMap</kbd> and <kbd>ValidationError</kbd> into <kbd>injectHandlerDependencies</kbd>:</p>
<pre>app.post('/users', injectHandlerDependencies(createUserHandler, client<strong>, handlerToEngineMap, ValidationError</strong>));</pre>
<p>Finally, update the <kbd>injectHandlerDependencies</kbd> function to relay these dependencies into the handler:</p>
<pre>function injectHandlerDependencies(handler, db<strong>, handlerToEngineMap, ValidationError</strong>) {<br/>  <strong>const engine = handlerToEngineMap.get(handler);</strong><br/>  return (req, res) =&gt; { handler(req, res, db<strong>, engine, ValidationError</strong>); };<br/>}</pre>
<p>We've made a lot of changes in many files, so you should run all of our existing tests again to make sure that we didn't break anything. You may also want to commit these changes to the Git repository:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Implement dependency injection pattern"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Promises and Mocha</h1>
                
            
            
                
<p>We're now ready to get back to our original task – writing unit tests for our Create User request handler! You should have enough understanding to implement the unit tests for the handler yourself, but we'd like to first give you some hints with regards to promises.</p>
<p>If the function we are testing perform asynchronous operations, there's no guarantee that the asynchronous operations would complete before our assertion code is run. For instance, if our <kbd>create</kbd> engine function is actually very slow to resolve, like so:</p>
<pre>function createUser() {<br/>  aVerySlowCreate()<br/>    .then((result) =&gt; {<br/>      res.status(201);<br/>    });<br/>}</pre>
<p>Then the following test would fail:</p>
<pre>describe("When create resolves with the new user's ID", function () {<br/>  beforeEach(function () {<br/>    createUser(req, res, db, create, ValidationError);<br/>  });<br/>  it('should call res.status() once', function () {<br/>    assert(res.status.calledOnce);<br/>  });<br/>});</pre>
<p>Mocha can deal with asynchronous code in two ways – using callbacks or promises. Since we'd generally avoid using callbacks, let's focus on working with promises. In Mocha, if we return a promise in the preceding <kbd>beforeEach</kbd> block, Mocha will wait for the promise to settle before running the relevant <kbd>describe</kbd> and <kbd>it</kbd> blocks. Therefore, when writing functions that involve asynchronous operations, we should <em>always return a promise</em>. Not only does it make the function easier to test, but it also allows you to chain multiple promises together should you have that need in the future.</p>
<p>Therefore, we must update our <kbd>createUser</kbd> function to a promise:</p>
<pre>function createUser(req, res, db, create, ValidationError) {<br/>  <strong>return</strong> create(req, db)<br/>    ...<br/>}</pre>
<p class="mce-root"/>
<p>Then, make sure that all of our <kbd>beforeEach</kbd> blocks also return a promise:</p>
<pre>beforeEach(function () {<br/>  create = generateCreateStubs.success();<br/>  <strong>return</strong> createUser(req, res, db, create, ValidationError);<br/>});</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Dealing with rejected promises</h1>
                
            
            
                
<p>However, another limitation of Mocha is that you cannot return a rejected promise inside the hook functions. If you do, Mocha will think the test has failed. In those cases, you should move the function that you expect to fail inside the <kbd>it</kbd> block, and make any assertions inside a <kbd>catch</kbd> block:</p>
<pre>it('should fail', function() {<br/>  createUser(...)<br/>    .catch(actualError =&gt; assert(actualError, expectedError))<br/>});</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Completing the unit tests</h1>
                
            
            
                
<p>You now have enough understanding of unit tests, Mocha, and working with promises to complete the unit tests for the Create User handler. Have a go at implementing this yourself, referring back to the reference code sample only if you need to.</p>
<p>As always, don't forget to run the unit and E2E tests to make sure you haven't introduced any regression, and then commit the changes to our repository:</p>
<pre>$ git add -A &amp;&amp; git commit -m "Add unit tests for Create User request handler"</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Unit testing our engine</h1>
                
            
            
                
<p>Next, let's test our <kbd>create</kbd> engine function. Like our previous <kbd>createUser</kbd> request handler, the <kbd>src/engines/users/create/index.js</kbd> module contains two <kbd>import</kbd> statements, which makes it difficult to test. Therefore, just like before, we must pull these dependencies out, and import them back into <kbd>src/index.js</kbd>:</p>
<pre><strong>import createUserValidator from './validators/users/create';</strong><br/>...<br/><strong>const handlerToValidatorMap = new Map([</strong><br/><strong>  [createUserHandler, createUserValidator],</strong><br/><strong>]);</strong><br/>...<br/>app.post('/users', injectHandlerDependencies(createUserHandler, client, handlerToEngineMap, <strong>handlerToValidatorMap</strong>, ValidationError));<br/><br/></pre>
<p>Then, update the <kbd>injectHandlerDependencies</kbd> function to inject the validator function into the handler:</p>
<pre>function injectHandlerDependencies(<br/>  handler, db, handlerToEngineMap, <strong>handlerToValidatorMap</strong>, ValidationError,<br/>) {<br/>  const engine = handlerToEngineMap.get(handler);<br/>  <strong>const validator = handlerToValidatorMap.get(handler);</strong><br/>  return (req, res) =&gt; { handler(req, res, db, engine, <strong>validator</strong>, ValidationError); };<br/>}</pre>
<p>Then, inside the handler, relay the validator function and <kbd>ValidationError</kbd> class into the engine function:</p>
<pre>function createUser(req, res, db, create<strong>, validator, ValidationError</strong>) {<br/>  return create(req, db<strong>, validator, ValidationError</strong>)<br/>    ...<br/>}</pre>
<p>Finally, update the unit tests to cater for this change. Once all tests pass, commit this change to Git:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Implement dependency injection for engine"</strong></pre>
<p>Once that's committed, let's move on to writing the unit tests themselves. There are only two cases to test for – when the validator returns with a <kbd>ValidationError</kbd>, or when it returns with <kbd>undefined</kbd>. Again, because we don't want our unit tests to depend on the validator, and so we will use stubs to simulate its functionality. Attempt to implement it yourself and compare it with our implementation, as follows:</p>
<pre>import assert from 'assert';<br/>import { stub } from 'sinon';<br/>import ValidationError from '../../../validators/errors/validation-error';<br/>import create from '.';<br/><br/>describe('User Create Engine', function () {<br/>  let req;<br/>  let db;<br/>  let validator;<br/>  const dbIndexResult = {};<br/>  beforeEach(function () {<br/>    req = {};<br/>    db = {<br/>      index: stub().resolves(dbIndexResult),<br/>    };<br/>  });<br/>  describe('When invoked and validator returns with undefined', function () {<br/>    let promise;<br/>    beforeEach(function () {<br/>      validator = stub().returns(undefined);<br/>      promise = create(req, db, validator, ValidationError);<br/>      return promise;<br/>    });<br/>    describe('should call the validator', function () {<br/>      it('once', function () {<br/>        assert(validator.calledOnce);<br/>      });<br/>      it('with req as the only argument', function () {<br/>        assert(validator.calledWithExactly(req));<br/>      });<br/>    });<br/>    it('should relay the promise returned by db.index()', function () {<br/>      promise.then(res =&gt; assert.strictEqual(res, dbIndexResult));<br/>    });<br/>  });<br/><br/>  describe('When validator returns with an instance of ValidationError', function () {<br/>    it('should reject with the ValidationError returned from validator', function () {<br/>      const validationError = new ValidationError();<br/>      validator = stub().returns(validationError);<br/>      return create(req, db, validator, ValidationError)<br/>        .catch(err =&gt; assert.strictEqual(err, validationError));<br/>    });<br/>  });<br/>});</pre>
<p>As always, run the tests and commit the code:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Implement unit tests for Create User engine"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Integration testing our engine</h1>
                
            
            
                
<p>So far, we have been retrofitting our code with unit tests, which test each unit individually, independent of external dependencies. However, it's also important to have confidence that different units are compatible with each other. This is where integration tests are useful. So, let's add some integration tests to our User Create engine that'll test its interaction with the database.</p>
<p>First, let's update our npm scripts to include a <kbd>test:integration</kbd> script. We'll also update the glob file in our <kbd>test:unit</kbd> npm to be more specific and select only unit tests. Lastly, update the <kbd>test</kbd> script to run the integration tests after the unit tests:</p>
<pre>"test": "yarn run test:unit <strong>&amp;&amp; yarn run test:integration</strong> &amp;&amp; yarn run test:e2e",<br/>"test:unit": "mocha 'src/**/*<strong>.unit</strong>.test.js' --require @babel/register",<br/><strong>"test:integration": "dotenv -e envs/test.env -e envs/.env mocha -- src/**/*.integration.test.js' --require @babel/register",</strong></pre>
<p>The <kbd>dotenv mocha</kbd> part will run Mocha after loading all the environment variables. We are then using a double dash (<kbd>--</kbd>) to signify to our <em>bash</em> shell that this is the end of the options for the <kbd>dotenv</kbd> command; anything after the double dash is passed into the <kbd>mocha</kbd> command, like it did previously.</p>
<p>You write your integration tests in the same way as your unit tests, the only difference being instead of stubbing everything, you supply the unit you're testing with genuine parameters. Let's take a look at the signature of our create function once again:</p>
<pre>create(req, db, createUserValidator, ValidationError)</pre>
<p>Previously, we used stubs to simulate the real <kbd>db</kbd> object and <kbd>createUserValidator</kbd> function. For an integration test, you'd actually import the real validator function and instantiate a real Elasticsearch JavaScript client. Once again, try to implement the integration tests yourself, and check back here for our solution:</p>
<pre>import assert from 'assert';<br/>import elasticsearch from 'elasticsearch';<br/>import ValidationError from '../../../validators/errors/validation-error';<br/>import createUserValidator from '../../../validators/users/create';<br/>import create from '.';<br/><br/>const db = new elasticsearch.Client({<br/>  host: `${process.env.ELASTICSEARCH_PROTOCOL}://${process.env.ELASTICSEARCH_HOSTNAME}:${process.env.ELASTICSEARCH_PORT}`,<br/>});<br/><br/>describe('User Create Engine', function () {<br/>  describe('When invoked with invalid req', function () {<br/>    it('should return promise that rejects with an instance of ValidationError', function () {<br/>      const req = {};<br/>      create(req, db, createUserValidator, ValidationError)<br/>        .catch(err =&gt; assert(err instanceof ValidationError));<br/>    });<br/>  });<br/>  describe('When invoked with valid req', function () {<br/>    it('should return a success object containing the user ID', function () {<br/>      const req = {<br/>        body: {<br/>          email: 'e@ma.il',<br/>          password: 'password',<br/>          profile: {},<br/>        },<br/>      };<br/>      create(req, db, createUserValidator, ValidationError)<br/>        .then((result) =&gt; {<br/>          assert.equal(result.result, 'created');<br/>          assert.equal(typeof result._id, 'string');<br/>        });<br/>    });<br/>  });<br/>});<br/><br/></pre>
<p>Again, run all the tests to make sure they all pass, then commit these changes to the repository:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Add integration tests for Create User engine"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding test coverage</h1>
                
            
            
                
<p>At the beginning of our TDD process, we wrote E2E tests first and used them to drive development. However, for unit and integration tests, we actually retrofitted them back into our implementation. Therefore, it's very likely that we missed some scenarios that we should have tested for.</p>
<p>To remedy this practical problem, we can summon the help of <strong>test coverage</strong> tools. A test coverage tool will run your tests and record all the lines of code that were executed; it will then compare this with the total number of lines in your source file to return a percentage coverage. For example, if my module contains 100 lines of code, and my tests only ran 85 lines of my module code, then my test coverage is 85%. This may mean that I have dead code or that I missed certain use cases. Once I know that some of my tests are not covering all of my code, I can then go back and add more test cases.</p>
<p>The <em>de facto</em> test coverage framework for JavaScript is <kbd>istanbul</kbd> (<a href="https://github.com/gotwarlost/istanbul">github.com/gotwarlost/istanbul</a>). We will be using istanbul via its command line interface, <kbd>nyc</kbd> (<a href="https://github.com/istanbuljs/nyc">github.com/istanbuljs/nyc</a>). So, let's install the <kbd>nyc</kbd> package:</p>
<pre><strong>$ yarn add nyc --dev</strong></pre>
<p>Now, add the following npm script to <kbd>package.json</kbd>:</p>
<pre>"test:unit:coverage": "nyc --reporter=html --reporter=text yarn run test:unit",</pre>
<p>Now, we can run <kbd>yarn run test:unit:coverage</kbd> to get a report of our code coverage. Because we specified the <kbd>--reporter=text</kbd> option, <kbd>nyc</kbd> which will print the results to stdout in a text table format:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8b292ab9-3c72-40fc-82e7-f2de0962bfd6.png" style="width:39.58em;height:16.83em;"/></p>
<p>The <kbd>--reporter=html</kbd> flag will also instruct <kbd>nyc</kbd> to create an HTML report, which is stored at a new <kbd>coverage</kbd> directory at the root of the project.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reading a test coverage report</h1>
                
            
            
                
<p>Inside the <kbd>coverage</kbd> directory, you should find an <kbd>index.html</kbd> file; open it up in a web browser to continue:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/4850beca-4bde-4ce5-b632-41320b7ac81c.png"/></p>
<p>At the top, you can see different percentages of test coverage. Here's what they mean:</p>
<ul>
<li><strong>Lines</strong>: Percentage of the total lines of code (LoC) that were run.</li>
<li><strong>Statements</strong>: Percentage of total statements that were executed. If you always use a separate line for each statement (as is the case in our project), then Statements and Lines would have the same value. If you have multiple statements per line (for example, <kbd>if (condition) { bar = 1; }</kbd>), then there'll be more statements than lines, and the Statements coverage may be lower. The Statements coverage is more useful than Lines coverage; the Lines coverage exists for interoperability with line-oriented coverage tools like <kbd>lcov</kbd>. Note that you can use ESLint to enforce having one statement per line by enabling the <kbd>max-statements-per-line</kbd> rule.</li>
<li><strong>Branches</strong>: Imagine our code as a set of paths – if certain conditions are met, the execution of our program will follow a certain path; when a different set of conditions is employed, the execution will follow a different path. These paths diverge at conditional statements into <em>branches</em>. The branch coverage indicates how many of these branches are covered.</li>
<li><strong>Functions</strong> : The percentage of total functions that were called.</li>
</ul>
<p>We can see that our overall Statements coverage is 91.84%, which is pretty good already. However, our <kbd>handlers/users/create/index.js</kbd> file seems to have only 66.67% coverage. Let's investigate why!</p>
<p>Click on the handlers/users/create link until you arrive at the screen showing the source code of the file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/90679309-0891-4780-847b-ec29341915c0.png" style="width:44.17em;height:31.00em;"/></p>
<p>A green bar on the left-hand side indicates that the line is covered. Furthermore, <kbd>nyc</kbd> will give you a count for how many times that line was executed over the entire run of our unit test suite. For example, the preceding <kbd>res.status(201)</kbd> line has been executed 8 times.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>A red bar indicates that the line has not been executed. This can mean one of a few things:</p>
<ul>
<li>Our tests are insufficient and do not test all possible scenarios</li>
<li>There's unreachable code in our project</li>
</ul>
<p>Any other gaps in the coverage are indicated in the code itself as a letter enclosed inside a black box; when you hover over it, it will provide a more descriptive reason. In our case, there's a letter E, which stands for "else path not taken", meaning there's no test that covers what happens when the <kbd>create</kbd> function rejects with an error that is <em>not</em> an instance of <kbd>ValidationError</kbd>.</p>
<p>In our case, it actually highlights an error in our code. Inside the <kbd>onRejected</kbd> function of our <kbd>then</kbd> block, we are returning <kbd>undefined</kbd> if the error is not an instance of <kbd>ValidationError</kbd>. This will, in effect, return a resolved promise, and thus the <kbd>catch</kbd> block will never catch the error. Furthermore, we are also not testing for the case where the <kbd>create</kbd> function returns a generic error. Therefore, let's increase the test coverage for this module by fixing these two issues.</p>
<p>Before we do, let's commit our existing changes:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Implement test coverage for unit tests"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Improving test coverage</h1>
                
            
            
                
<p>First, inside the <kbd>/home/dli/.d4nyll/.beja/final/code/9/src/handlers/users/create/index.js</kbd> file, change the <kbd>return undefined;</kbd> statement to propagate the error down the promise chain:</p>
<pre>      return res.json({ message: err.message });<br/>    }<br/>    <strong>throw err;</strong><br/>  }).catch(() =&gt; {<br/>    res.status(500);</pre>
<p>Then, add unit tests to <kbd>src/handlers/users/create/index.unit.test.js</kbd> to cover this missed scenario:</p>
<pre>const generateCreateStubs = {<br/>  success: () =&gt; stub().resolves({ _id: USER_ID }),<br/>  <strong>genericError: () =&gt; stub().rejects(new Error()),</strong><br/>  validationError: () =&gt; stub().rejects(new ValidationError(VALIDATION_ERROR_MESSAGE)),<br/>};<br/>...<br/>describe('createUser', function () {<br/>  ...<br/>  describe('<strong>When create rejects with an instance of Error</strong>', function () {<br/>    beforeEach(function () {<br/>      create = generateCreateStubs.genericError();<br/>      return createUser(req, res, db, create, validator, ValidationError);<br/>    });<br/>    describe('should call res.status()', function () {<br/>      it('once', function () {<br/>        assert(res.status.calledOnce);<br/>      });<br/>      it('with the argument 500', function () {<br/>        assert(res.status.calledWithExactly(<strong>500</strong>));<br/>      });<br/>    });<br/><br/>    describe('should call res.set()', function () {<br/>      it('once', function () {<br/>        assert(res.set.calledOnce);<br/>      });<br/>      it('with the arguments "Content-Type" and "application/json"', function () {<br/>        assert(res.set.calledWithExactly('Content-Type', 'application/json'));<br/>      });<br/>    });<br/><br/>    describe('should call res.json()', function () {<br/>      it('once', function () {<br/>        assert(res.json.calledOnce);<br/>      });<br/>      it('with a validation error object', function () {<br/>        assert(res.json.calledWithExactly({ message: <strong>'Internal Server Error'</strong> }));<br/>      });<br/>    });<br/>  });<br/>});<br/><br/></pre>
<p>Now, when we run our <kbd>test:unit:coverage</kbd> script and look at the report again, you will be glad to see that coverage is now 100%!</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3e3f73fe-4f42-4530-b43a-095629bbb786.png" style="width:44.00em;height:30.58em;"/></p>
<p>Now, commit this refactoring step into your repository:</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Test catch block in createUser"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Code coverage versus test quality</h1>
                
            
            
                
<p>As illustrated in the preceding section, code coverage tools can help you uncover mistakes in your code. However, they should be used as a diagnostic tool only; you shouldn't be chasing after 100% code coverage as a goal in itself.</p>
<p class="mce-root"/>
<p>This is because code coverage has no relation to the quality of your tests. You can define test cases that cover 100% of your code, but if the assertions are wrong, or if the tests have errors in it, then the perfect coverage means nothing. For instance, the following test block will always pass, even though one of the assertions suggests it would fail:</p>
<pre>it('This will always pass', function () {<br/>  it('Even though you may expect it to fail', function () {<br/>    assert(true, false);<br/>  });<br/>});</pre>
<p>This highlights the point that <em>code coverage cannot detect bad tests</em>. Instead, you should focus on writing meaningful tests that will actually show bugs when they arise; if you do that, the test coverage will naturally remain high, and you can use the reports to improve whatever you've missed in your tests.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">You don't have to test everything, all the time</h1>
                
            
            
                
<p>After we updated our unit tests to cover the missed <kbd>catch</kbd> block, our Statements coverage is now 100%. However, if we examine our code, we'll find two modules that still lack unit tests:</p>
<ul>
<li><kbd>validate</kbd>: User validation function at <kbd>src/validators/users/create.js</kbd></li>
<li><kbd>injectHandlerDependencies</kbd>: Utility function at <kbd>src/utils/inject-handler-dependencies.js<br/></kbd></li>
</ul>
<p>They did not show up in the coverage report because the unit tests never imported those files. But do we need to write unit tests for every unit? To answer this question, you should ask yourself – "Do I have confidence that this block of code works?" If the answer is "yes", then writing additional tests may be unnecessary.</p>
<p>Code coverage for a unit should not be analyzed based on unit tests alone, since there may be integration and E2E tests that use that unit. If these other tests cover what the unit tests don't, and the tests are passing, then that should give you confidence that your unit is working as intended.</p>
<p>Therefore, a more useful metric is to analyze the code coverage of <em>all tests</em>, not just unit tests.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Unifying test coverage</h1>
                
            
            
                
<p>Therefore, let's add coverage scripts for integration and E2E tests:</p>
<pre>"test:coverage": "nyc --reporter=html --reporter=text yarn run test",<br/>"test:integration:coverage": "nyc --reporter=html --reporter=text yarn run test:integration",<br/>"test:e2e:coverage": "nyc --reporter=html --reporter=text yarn run test:e2e",</pre>
<p>However, when we run the <kbd>test:e2e:coverage</kbd> script, the coverage report shows results for compiled files in the <kbd>dist/</kbd> directory, rather than the source files from <kbd>src/</kbd>. This is because our E2E test script (<kbd>scripts/e2e.test.sh</kbd>) is running the <kbd>serve</kbd> npm script, which transpiles our code before running it. To fix this, let's add a new <kbd>test:serve</kbd> script, which uses <kbd>babel-node</kbd> to directly run our code:</p>
<pre>"test:serve": "dotenv -e envs/test.env -e envs/.env babel-node src/index.js",</pre>
<p>Then, update <kbd>scripts/e2e.test.sh</kbd> to use this modified script instead of <kbd>serve</kbd>:</p>
<pre>yarn run <strong>test:</strong>serve &amp;</pre>
<p>Now, when we run the <kbd>test:coverage</kbd> or <kbd>test:e2e:coverage</kbd> again, it will show coverage for files under <kbd>src/</kbd> instead of <kbd>dist/</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Ignoring files</h1>
                
            
            
                
<p>However, you may have also noticed that our step definitions are showing up in our coverage report. Istanbul is not smart enough to figure out that our step definition files are part of the tests, and not the code; therefore, we need to manually instruct Istanbul to ignore them. We can do with by adding a <kbd>.nycrc</kbd> file and specifying the <kbd>exclude</kbd> option:</p>
<pre>{<br/>  "exclude": [<br/>    "coverage/**",<br/>    "packages/*/test/**",<br/>    "test/**",<br/>    "test{,-*}.js",<br/>    "**/*{.,-}test.js"<br/>    ,"**/__tests__/**",<br/>    "**/node_modules/**",<br/>    "dist/",<br/>    "spec/",<br/>    "src/**/*.test.js"<br/>  ]<br/>}</pre>
<p>Now, when we run the <kbd>test:coverage</kbd> script, the step definition files are excluded from the results. All that's left to do is commit our code!</p>
<pre><strong>$ git add -A &amp;&amp; git commit -m "Implement coverage for all tests"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Finishing up</h1>
                
            
            
                
<p>We have now modularized and tested the code for the Create User feature. Therefore, now is a good time to merge our current <kbd>create-user/refactor-modules</kbd> branch into the <kbd>create-user/main</kbd> branch. Since this also completes the Create User feature, we should merge the <kbd>create-user/main</kbd> feature branch back into the <kbd>dev</kbd> branch:</p>
<pre><strong>$ git checkout create-user/main</strong><br/><strong>$ git merge --no-ff create-user/refactor-modules</strong><br/><strong>$ git checkout dev</strong><br/><strong>$ git merge --no-ff create-user/main</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Over the course of the preceding three chapters, we have shown you how to write E2E tests, use them to drive the development of your feature, modularize your code wherever possible, and then increase confidence in your code by covering modules with unit and integration tests.</p>
<p>In the next chapter, you will be tasked with implementing the rest of the features by yourself. We will outline some principles of API design that you should follow, and you can always reference our sample code bundle, but the next chapter is where you truly get to practice this process independently.</p>
<p>"Learning is an active process. We learn by doing. Only knowledge that is used sticks in your mind."</p>
<p>- Dale Carnegie, author of the book How to Win Friends and Influence People</p>


            

            
        
    </body></html>