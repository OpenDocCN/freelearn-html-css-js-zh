<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying to Multiple Regions</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, the following recipes will be covered:</p>
<ul>
<li>Implementing latency-based routing</li>
<li>Creating a regional health check</li>
<li>Triggering regional failover</li>
<li>Implementing regional replication with DynamoDB</li>
<li>Implementing round-robin replication</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>It is not a matter of if but when will a given cloud provider experience a news-worthy regional disruption. It is inevitable. In my experience, this happens approximately every two years or so. When such an event does occur, many systems have no recourse and become unavailable during the disruption because they are only designed to work across multiple availability zones within a single region. Meanwhile, other systems barely experience a blip in availability because they have been designed to run across multiple regions. The bottom line is that truly cloud-native systems capitalize on regional bulkheads and run in multiple regions. Fortunately, we leverage fully managed, value-added cloud services that already run across availability zones. This empowers teams to refocus that effort on creating an active-active, multi-regional system. The recipes in this chapter cover multi-regional topics from three interrelated perspectives—synchronous requests, database replication, and asynchronous event streams.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing latency-based routing</h1>
                </header>
            
            <article>
                
<p>Many systems make the conscious decision not to run across multiple regions because it is simply not worth the additional effort and cost. This is completely understandable when running in an active-passive mode because the additional effort does not produce an easily visible benefit until there is a regional disruption. It is also understandable when running in active-active mode doubles the monthly runtime cost. Conversely, serverless cloud-native systems are easily deployed to multiple regions and the increase in cost is nominal since the cost of a given transaction volume is spread across the regions. This recipe demonstrates how to run an AWS API Gateway and Lambda-based service in multiple regions and leverage Route53 to route traffic across these active-active regions to minimize latency.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>You will need a registered domain name and a <strong>Route53 Hosted Zone</strong> that you can use in this recipe to create a subdomain for the service that will be deployed, such as we discussed in the <em>Associating a custom domain name with a CDN</em> recipe. You will also need a wildcard certificate for your domain name in the <kbd>us-east-1</kbd> and <kbd>us-west-2</kbd> regions, such as we discussed in the <em>Creating an SSL certificate for encryption in transit</em> recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch10/latency-based-routing --path cncb-latency-based-routing</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-latency-based-routing</kbd> directory with <kbd>cd cncb-latency-based-routing</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-latency-based-routing<br/><br/>plugins:<br/>  - <strong>serverless-multi-regional-plugin</strong><br/><br/>provider:<br/>  ...<br/>  endpointType: <strong>REGIONAL</strong><br/><br/>custom:<br/>  dns:<br/>    hostedZoneId: <strong>ZXXXXXXXXXXXXX</strong><br/>    domainName: ${self:service}.<strong>example</strong>.com<br/>    regionalDomainName: ${opt:stage}-${self:custom.dns.domainName}<br/>    us-east-1:<br/>      <strong>acmCertificateArn</strong>: arn:aws:acm:us-east-1:xxxxxxxxxxxx:certificate/...<br/>    us-west-2:<br/>      <strong>acmCertificateArn</strong>: arn:aws:acm:us-west-2:xxxxxxxxxxxx:certificate/...<br/>  cdn:<br/>    region: <strong>us-east-1</strong><br/>    <strong>aliases</strong>:<br/>      - ${self:custom.dns.domainName}<br/>    acmCertificateArn: ${self:custom.dns.us-east-1.acmCertificateArn}<br/><br/>functions:<br/>  hello:<br/>    ...</pre>
<ol start="4">
<li>Update the following fields in the <kbd>serverless.yml</kbd> file:
<ul>
<li><kbd>custom.dns.hostedZoneId</kbd></li>
<li><kbd>custom.dns.domainName</kbd></li>
<li><kbd>custom.dns.us-east-1.acmCertificateArn</kbd></li>
<li><kbd>custom.dns.us-west-2.acmCertificateArn</kbd></li>
</ul>
</li>
<li>Review the file named <kbd>handler.js</kbd>.</li>
<li>Install the dependencies<span> with </span><kbd>npm install</kbd>.</li>
<li>Run the tests<span> with </span><kbd>npm test</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack in the <kbd>us-west-2</kbd> region<span> with </span><kbd>npm run dp:lcl:<strong>w</strong> -- -s $MY_STAGE</kbd>.</li>
<li>Review the stack and resources in the AWS Console for the <kbd>us-west-2</kbd> region.</li>
<li>Test the <kbd>regional</kbd> endpoint of the service and note the region returned in the payload:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl -v https://$MY_STAGE-cncb-latency-based-routing.example.com/$MY_STAGE/hello</strong><br/><br/>{"message":"Your function executed successfully in <strong>us-west-2</strong>!"}</pre>
<ol start="12">
<li>Deploy the stack in the <kbd>us-east-1</kbd> region<span> with </span><kbd>npm run dp:lcl:<strong>e</strong> -- -s $MY_STAGE</kbd>.</li>
</ol>
<div class="packt_infobox">Deploying a CloudFront distribution can take 20 minutes or longer.</div>
<ol start="13">
<li>Review the stack and resources in the AWS console for the <kbd>us-east-1</kbd> region.</li>
<li>Test the <kbd>global</kbd> endpoint of the service and note the region returned in the payload, which should be different from above if you are not closest to the <kbd>us-west-2</kbd> region:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl -v https://cncb-regional-failover-service.example.com/hello</strong><br/><br/>{"message":"Your function executed successfully in <strong>us-east-1</strong>!"}</pre>
<ol start="15"/>
<ol start="15"/>
<ol start="15">
<li>Remove the stacks once you have finished:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run rm:lcl:e -- -s $MY_STAGE</strong><br/><strong>$ npm run rm:lcl:w -- -s $MY_STAG</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we are taking a single service and deploying it to two regions—<kbd>us-east-1</kbd> and <kbd>us-west-2</kbd>. From the perspective of the API Gateway and the Lambda function, we are simply just creating two different CloudFormation stacks, one in each region. We have two scripts—<kbd>dp:lcl:<strong>e</strong></kbd> and <kbd>dp:lcl:<strong>w</strong></kbd>, and the only difference between the two is that they specify different regions. As a result, the effort to deploy to two regions is marginal, and there is no additional cost because we only pay per transaction. One thing of note in the <kbd>serverless.yml</kbd> file is that we are defining the <kbd>endpointType</kbd> for the API Gateway as <kbd>REGIONAL</kbd>, which will allow us to leverage the Route53 regional routing capabilities:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/fb2eaaad-8517-4439-8413-3d2ff7999a62.png" style="width:41.83em;height:22.42em;"/></p>
<p>As shown in the preceding diagram, we need to configure Route53 to perform <em>latency-</em>based routing between the two regions. This means that Route53 will route requests to the region that is closest to the requester. The <kbd>serverless-multi-regional-plugin</kbd> encapsulates the majority of these configuration details, so we only need to specify the variables under <kbd>custom.dns</kbd>. First, we provide the <kbd>hostedZoneId</kbd> for the zone that hosts the top-level domain name, such as <kbd>example.com</kbd>. Next, we define the <kbd>domainName</kbd> that will be used as the <kbd>alias</kbd> to access the service globally via <strong>CloudFront</strong>. For this, we use the service name (that is, <kbd>${self:service}</kbd>) as a subdomain of the top-level domain to uniquely identify a service.</p>
<p>We also need to define a <kbd>regionalDomainName</kbd> to provide a common name across all the regions so that CloudFront can rely on Route53 to pick the best region to access. For this, we are using the stage (that is, <kbd>${opt:stage}-${self:custom.dns.domainName}</kbd>) as a prefix, and note that we are concatenating this with a dash so that it works with a simple wildcard certificate, such as <kbd>*.example.com</kbd>. The regional <kbd>acmCertificateArn</kbd> variables point to copies of your wildcard certificate in each region, as mentioned in the <em>Getting ready</em> section. API Gateway requires that the certificates live in the same region as the service. CloudFront requires that the certificate lives in the <kbd>us-east-1</kbd> region. CloudFront is a global service, so we only need to deploy the CloudFront distribution from the <kbd>us-east-1</kbd> region.</p>
<p>All requests to the global endpoint (<kbd>service.example.com</kbd>) will be routed to the closest CloudFront edge location. CloudFront then forwards the requests to the regional endpoint (<kbd>stage-service.example.com</kbd>), and Route53 will route the requests to the closest region. Once a request is in a region, all requests to services, such as Lambda, DynamoDB and Kinesis, will stay within the region to minimize latency. All changes to state will be replicated to the other regions, as we discuss in the <em>Implementing regional replication with DynamoDB</em> and <em>Implementing round-robin replication</em> recipes.</p>
<div class="packt_tip">I highly recommend looking at the generated <em>CloudFormation</em> template to see the details of all the resources that are created.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a regional health check</h1>
                </header>
            
            <article>
                
<p>Health checks in a cloud-native system have a different focus from traditional health checks. Traditional health checks operate at the instance level to identify when a specific instance in a cluster needs to be replaced. Cloud-native systems, however, use fully managed, value-added cloud services, so there are no instances to manage. These serverless capabilities provide high availability across the availability zones within a specific region. As a result, cloud-native systems can focus on providing high availability across regions. This recipe demonstrates how to assert the health of the value-added cloud services that are used within a given region.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To complete this recipe in full, you will need a Pingdom (<a href="https://www.pingdom.com">https://www.pingdom.com</a>) account.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch10/regional-health-check --path cncb-regional-health-check</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-regional-health-check</kbd> directory<span> with </span><kbd>cd cncb-regional-health-check</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-regional-health-check<br/><br/>provider:<br/>  name: aws<br/>  runtime: nodejs8.10<br/>  endpointType: <strong>REGIONAL</strong><br/>  iamRoleStatements:<br/>    ...<br/><br/>functions:<br/>  <strong>check</strong>:<br/>    handler: handler.<strong>check</strong><br/>    events:<br/>      - http:<br/>          path: <strong>check</strong><br/>          method: get<br/>    environment:<br/>      <strong>UNHEALTHY</strong>: false<br/>      TABLE_NAME:<br/>        Ref: Table<br/><br/>resources:<br/>  Resources:<br/>    <strong>Table</strong>:<br/>      Type: AWS::DynamoDB::Table<br/>      ...</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>check</strong> = (request, context, callback) =&gt; {<br/>  Promise.all([<strong>readCheck</strong>, <strong>writeCheck</strong>])<br/>    .catch(handleError)<br/>    .then(response(callback));<br/>};<br/><br/>const db = new aws.DynamoDB.DocumentClient({<br/>  httpOptions: { timeout: 1000 },<br/>  logger: console,<br/>});<br/><br/>const <strong>readCheck</strong> = () =&gt; db.<strong>get</strong>({<br/>  TableName: process.env.TABLE_NAME,<br/>  Key: {<br/>    id: '1',<br/>  },<br/>}).promise();<br/><br/>const <strong>writeCheck</strong> = () =&gt; db.<strong>put</strong>({<br/>  TableName: process.env.TABLE_NAME,<br/>  Item: {<br/>    id: '1',<br/>  },<br/>}).promise();<br/><br/>const handleError = (err) =&gt; {<br/>  console.error(err);<br/>  return true; // <strong>unhealthy</strong><br/>};<br/><br/>const response = callback =&gt; (unhealthy) =&gt; {<br/>  callback(null, {<br/>    statusCode: <strong>unhealthy</strong> || process.env.<strong>UNHEALTHY</strong> === 'true' ? <strong>503</strong> : 200,<br/>    body: JSON.stringify({<br/>      timestamp: Date.now(),<br/>      region: process.env.<strong>AWS_REGION</strong>,<br/>    }),<br/>    headers: {<br/>      '<strong>Cache-Control</strong>': 'no-cache, no-store, must-revalidate',<br/>    },<br/>  });<br/>};</pre>
<ol start="5">
<li>Install the dependencies<span> with </span><kbd>npm install</kbd>.</li>
<li>Run the tests<span> with </span><kbd>npm test</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack in the <kbd>us-east-1</kbd> and <kbd>us-west-2</kbd> regions:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl:e -- -s $MY_STAGE</strong><br/>...<br/>  <strong>GET</strong> - https://0987654321.execute-api.us-east-1.amazonaws.com/john/check<br/><br/><strong>$ npm run dp:lcl:w -- -s $MY_STAGE</strong><br/>...<br/>  <strong>GET</strong> - https://1234567890.execute-api.us-west-2.amazonaws.com/john/check</pre>
<ol start="9">
<li>Review the stack and resources in the AWS console for both regions.</li>
</ol>
<ol start="10">
<li>For each region, invoke the endpoint shown in the stack output as shown here:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl -v https://0987654321.execute-api.us-east-1.amazonaws.com/$MY_STAGE/check</strong><br/><strong>$ curl -v https://1234567890.execute-api.us-west-2.amazonaws.com/$MY_STAGE/check</strong></pre>
<ol start="11">
<li>Create an <strong>Uptime</strong> check in your <strong>Pingdom</strong> account for each regional endpoint with an interval of <kbd>1 minute</kbd>.</li>
<li>Remove the stacks once you have finished:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run rm:lcl:e -- -s $MY_STAGE</strong><br/><strong>$ npm run rm:lcl:w -- -s $MY_STAGE</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>A traditional health check typically asserts that an instance is able to access all the resources that it needs to operate properly. A regional health check does the same thing but from the perspective of the region as a whole. It asserts that all the value-added cloud services (that is, resources) used by the system are operating properly within the given region. If any one resource is unavailable, we will failover the entire region, as discussed in the <em>Triggering regional failover</em> recipe.</p>
<p>The health check service is implemented as a <kbd>REGIONAL</kbd> API Gateway based service and deployed to each region. We then need to periodically invoke the health check in each region to check that the region is healthy. We could have Route53 ping these regional endpoints, but it will ping them so frequently that the health check service could easily become the most expensive service in your entire system. Alternatively, we can use an external service, such as <strong>Pingdom</strong>, to invoke the health check in each region once per minute. Once a minute is sufficient for many systems, but extremely high traffic systems may benefit from the higher frequency provided by Route53.</p>
<p>The health check needs to assert that the required resources are available. The health check itself implicitly asserts that the API Gateway and Lambda services are available because it is built on those services. For all other resources, it will need to perform some sort of ping operation. In this recipe, we assume that DynamoDB is the required resource. The health check service defines its own DynamoDB table and performs a <kbd>readCheck</kbd> and a <kbd>writeCheck</kbd> on each invocation to assert that the service is still available. If either request fails, then the health check service will fail and return a <kbd>503</kbd> status code. For testing, the service provides an <kbd>UNHEALTHY</kbd> environment variable that can be used to simulate a failure, which we will use in the next recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Triggering regional failover</h1>
                </header>
            
            <article>
                
<p>As we discussed, in the <em>Creating a regional health check</em> recipe, our regional health checks assert that the fully managed, value-added cloud services that are used by the system are all up and running properly. When any of these services are down or experiencing a sufficiently high error rate, it is best to fail the entire region over to the next-best active region. This recipe demonstrates how to connect a regional health check to Route53, using <strong>CloudWatch Alarms</strong>, so that Route53 can direct traffic to healthy regions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>You will need a registered domain name and a <strong>Route53 Hosted Zone</strong> that you can use in this recipe to create a subdomain for the service that will be deployed, such as we discussed in the <em>Associating a custom domain name with a CDN</em> recipe. You will also need a wildcard certificate for your domain name in the <kbd>us-east-1</kbd> and <kbd>us-west-2</kbd> regions, such as we discussed in the <em>Creating an SSL certificate for encryption in transit</em> recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create the service and check projects from the following templates:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch10/regional-failover/check --path cncb-regional-failover-check</strong><br/><br/><strong>$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch10/regional-failover/service --path cncb-regional-failover-service</strong></pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-regional-failover-check</kbd> directory<span> with </span><kbd>cd cncb-regional-failover-check</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> <span>with the following content:</span></li>
</ol>
<pre style="padding-left: 30px">service: cncb-regional-failover-check<br/>...<br/>functions:<br/>  check:<br/>    ...<br/><br/>resources:<br/>  Resources:<br/>    <strong>Api5xxAlarm</strong>:<br/>      Type: AWS::CloudWatch::Alarm<br/>      Properties:<br/>        Namespace: AWS/<strong>ApiGateway</strong><br/>        MetricName: <strong>5XXError</strong><br/>        Dimensions:<br/>          - Name: <strong>ApiName</strong><br/>            Value: ${opt:stage}-${self:service}<br/>        Statistic: <strong>Minimum</strong><br/>        ComparisonOperator: <strong>GreaterThanThreshold</strong><br/>        <strong>Threshold</strong>: 0<br/>        <strong>Period</strong>: 60<br/>        <strong>EvaluationPeriods</strong>: 1<br/><br/>    <strong>ApiHealthCheck</strong>:<br/>      DependsOn: Api5xxAlarm<br/>      Type: AWS::Route53::HealthCheck<br/>      Properties:<br/>        HealthCheckConfig:<br/>          Type: CLOUDWATCH_METRIC<br/>          AlarmIdentifier:<br/>            Name:<br/>              Ref: <strong>Api5xxAlarm</strong><br/>            Region: ${opt:region}<br/>          InsufficientDataHealthStatus: LastKnownStatus<br/>        <br/>  Outputs:<br/>    <strong>ApiHealthCheckId</strong>:<br/>      Value:<br/>        Ref: ApiHealthCheck</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd>.</li>
<li>Install the dependencies<span> with </span><kbd>npm install</kbd>.</li>
<li>Run the tests<span> with </span><kbd>npm test</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack in the <kbd>us-east-1</kbd> and <kbd>us-west-2</kbd> regions: </li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl:e -- -s $MY_STAGE</strong><br/>...<br/>  <strong>GET</strong> - https://0987654321.execute-api.us-east-1.amazonaws.com/john/check<br/><br/><strong>$ npm run dp:lcl:w -- -s $MY_STAGE</strong><br/>...<br/>  <strong>GET</strong> - https://1234567890.execute-api.us-west-2.amazonaws.com/john/check</pre>
<ol start="9">
<li>Review the stack and resources in the AWS console for both regions.</li>
<li>Navigate to the <kbd>cncb-regional-failover-service</kbd> directory<span> with </span><kbd>cd cncb-regional-failover-service</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-regional-failover-service<br/><br/>plugins:<br/>  - <strong>serverless-multi-regional-plugin</strong><br/>...<br/>custom:<br/>  dns:<br/>    ...<br/>    us-east-1:<br/>      ...<br/>      <strong>healthCheckId</strong>: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx<br/>    us-west-2:<br/>      ...<br/>      <strong>healthCheckId</strong>: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx<br/>...</pre>
<ol start="12">
<li>Update the following fields in the <kbd>serverless.yml</kbd> file: 
<ul>
<li><kbd>custom.dns.hostedZoneId</kbd></li>
<li><kbd>custom.dns.domainName</kbd></li>
<li><kbd>custom.dns.us-east-1.acmCertificateArn</kbd></li>
<li><kbd>custom.dns.us-east-1.healthCheckId</kbd> from the output of the <kbd>east</kbd> health check stack</li>
<li><kbd>custom.dns.us-west-2.acmCertificateArn</kbd></li>
<li><kbd>custom.dns.us-west-2.healthCheckId</kbd> from the output of the <kbd>west</kbd> health check stack</li>
</ul>
</li>
<li>Install the dependencies<span> with </span><kbd>npm install</kbd>.</li>
<li>Run the tests<span> with </span><kbd>npm test</kbd>.</li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack in the <kbd>us-west-2</kbd> and <kbd>us-east-1</kbd> regions: </li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl:w -- -s $MY_STAGE</strong><br/><strong>$ npm run dp:lcl:e -- -s $MY_STAGE</strong></pre>
<div class="packt_infobox"><span>Deploying a CloudFront distribution can take 20 minutes or longer.</span></div>
<ol start="17">
<li>Review the stack and resources in the AWS console for both regions.</li>
<li>Test the <kbd>global</kbd> endpoint of the service and note the region returned in the payload:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl -v https://cncb-regional-failover-service.example.com/hello</strong><br/><br/>{"message":"Your function executed successfully in <strong>us-east-1</strong>!"}</pre>
<ol start="19">
<li>Go to the <kbd>cncb-regional-failover-check-&lt;stage&gt;-check</kbd> Lambda function in the AWS console for the region that your request was routed to and change the <kbd>UNHEALTHY</kbd> environment variable to <kbd>true</kbd> and s<em>ave</em> the function. For example, the previous output shows <kbd>us-east-1</kbd>, so update the function in <kbd>us-east-1</kbd>.</li>
<li>Invoke the health check endpoint for that region multiple times over the course of several minutes to trigger a failover:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl -v https://0987654321.execute-api.us-east-1.amazonaws.com/$MY_STAGE/check</strong></pre>
<ol start="21">
<li>Test the <kbd>global</kbd> endpoint of the service and note that the region has changed:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl -v https://cncb-regional-failover-service.example.com/hello</strong><br/><br/>{"message":"Your function executed successfully in <strong>us-west-2</strong>!"</pre>
<ol start="22">
<li>Review the status of the Route53 health checks in the AWS Console.</li>
<li>Remove the stacks once you are finished:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ cd cncb-regional-failover-service</strong><br/><strong>$ npm run rm:lcl:e -- -s $MY_STAGE</strong><br/><strong>$ npm run rm:lcl:w -- -s $MY_STAGE</strong><br/><strong>$ cd ../cncb-regional-failover-check</strong><br/><strong>$ npm run rm:lcl:e -- -s $MY_STAGE</strong><br/><strong>$ npm run rm:lcl:w -- -s $MY_STAGE</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Our regional health check service is designed to return a <kbd>5xx</kbd> status code when one or more of the required services returns an error. We add a CloudWatch alarm, named <kbd>Api5xxAlarm</kbd>, to the health check service that monitors the API Gateway <kbd>5xxError</kbd> metric in the given region and raises an alarm when there is at least one <kbd>5xx</kbd> in a minute. You will want to adjust the sensitivity of the alarm to your specific requirements. Next, we add a Route53 health check, named <kbd>ApiHealthCheck</kbd>, to the service that depends on the <kbd>Api5xxAlarm</kbd> and outputs the <kbd>ApiHealthCheckId</kbd> for use by other services. Finally, we associate the <kbd>healthCheckId</kbd> with the Route53 RecordSet for each service in each region, such as the <kbd>cncb-regional-failover-service</kbd>. When the alarm status is <kbd>Unhealthy</kbd>, Route53 will stop routing traffic to the region until the status is <kbd>Healthy</kbd> again.</p>
<p>In this recipe, we used the <kbd>UNHEALTHY</kbd> environment variable to simulate a regional failure and manually invoked the service to trigger the alarm. As we discussed in the <em>Creating a regional health check</em> recipe, the health check will typically be invoked on a regular basis by another service, such as Pingdom, to ensure that there is a constant flow of traffic asserting the health of the region. To increase coverage, we could also expand the alarm to check the <kbd>5xx</kbd> metric of all services in a region by removing the <kbd>ApiName</kbd> dimension from the alarm but still rely on pinging the health check service to assert the status when there is no other traffic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing regional replication with DynamoDB</h1>
                </header>
            
            <article>
                
<p>Timely replication of data across regions is important to facilitate a seamless user experience when a regional failover occurs. During normal execution, regional replication will occur in near real time. During a regional failure, it should be expected that data would replicate more slowly. We can think of this as protracted eventual consistency. Fortunately, our cloud-native systems are designed to be eventually consistent. This means they are tolerant of stale data, regardless of how long it takes to become consistent. This recipe shows how to create global tables to replicate DynamoDB tables across regions and discusses why we do not replicate event streams.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before starting this recipe, you will need an AWS Kinesis Stream in the <kbd>us-east-1</kbd> and <kbd>us-west-2</kbd> regions, such as the one created in the <em>Creating an event stream</em> recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch10/dynamodb-global-table --path cncb-dynamodb-global-table</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-dynamodb-global-table</kbd> directory<span> with </span><kbd>cd cncb-dynamodb-global-table</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-dynamodb-global-table<br/>...<br/><br/>plugins:<br/>  - serverless-dynamodb-autoscaling-plugin<br/>  - <strong>serverless-dynamodb-global-table-plugin</strong><br/><br/>custom:<br/>  autoscaling:<br/>    - table: Table<br/>      <strong>global</strong>: true<br/>      read:<br/>        ...<br/>      write:<br/>        ...<br/><br/>resources:<br/>  Resources:<br/>    Table:<br/>      Type: AWS::DynamoDB::Table<br/>      Properties:<br/>        TableName: ${opt:stage}-${self:service}-things<br/>        ...<br/>        <strong>StreamSpecification</strong>:<br/>          StreamViewType: NEW_AND_OLD_IMAGES</pre>
<ol start="4">
<li>Review the file named <kbd>handler.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">...<br/>module.exports.<strong>trigger</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .filter(<strong>forOrigin</strong>)<br/>    .map(toEvent)<br/>    .flatMap(publish)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>const <strong>forOrigin</strong> = e =&gt; e.dynamodb.NewImage['<strong>aws:rep:updateregion</strong>'] &amp;&amp;<br/>  e.dynamodb.NewImage['aws:rep:updateregion'].S === process.env.<strong>AWS_REGION</strong>;<br/>...</pre>
<ol start="5">
<li>Install the dependencies<span> with </span><kbd>npm install</kbd>.</li>
<li>Run the tests<span> with </span><kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack in the <kbd>us-east-1</kbd> and <kbd>us-west-2</kbd> regions:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl:e -- -s $MY_STAGE</strong><br/>...<br/>Serverless: <strong>Created</strong> global table: john-cncb-dynamodb-global-table-things with region: <strong>us-east-1</strong><br/>...<br/><br/><strong>$ npm run dp:lcl:w -- -s $MY_STAGE</strong><br/>...<br/>Serverless: <strong>Updated</strong> global table: john-cncb-dynamodb-global-table-things with region: <strong>us-west-2</strong><br/>...</pre>
<ol start="9">
<li>Review the stack and resources in the AWS console in both regions.</li>
<li>Invoke the <kbd>command</kbd> function to save data to the <kbd>us-east-1</kbd> region:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -f command -r us-east-1 -s $MY_STAGE -d '{"id":"77777777-4444-1111-1111-111111111111","name":"thing one"}'</strong></pre>
<ol start="11">
<li> Invoke the <kbd>query</kbd> function to retrieve the data from the <kbd>us-west-2</kbd> region:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls invoke -f query -r us-west-2 -s $MY_STAGE -d 77777777-4444-1111-1111-111111111111</strong><br/>{<br/>    "Item": {<br/>        "aws:rep:deleting": false,<br/>        "aws:rep:<strong>updateregion</strong>": "<strong>us-east-1</strong>",<br/>        "aws:rep:updatetime": 1534819304.087001,<br/>        "id": "77777777-4444-1111-1111-111111111111",<br/>        "name": "thing one",<br/>        "latch": "open"<br/>    }<br/>}</pre>
<ol start="12">
<li>Take a look at the logs for the <kbd>trigger</kbd> and <kbd>listener</kbd> functions in both regions: </li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f trigger -r us-east-1 -s $MY_STAGE</strong><br/><strong>$ sls logs -f listener -r us-east-1 -s $MY_STAGE</strong><br/><strong>$ sls logs -f trigger -r us-west-2 -s $MY_STAGE</strong><br/><strong>$ sls logs -f listener -r us-west-2 -s $MY_STAGE</strong></pre>
<ol start="13">
<li>Remove both stacks once you are finished: </li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run rm:lcl:e -- -s $MY_STAGE</strong><br/><strong>$ npm run rm:lcl:w -- -s $MY_STAGE</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>A <strong>DynamoDB Global Table</strong> is responsible for replicating data across all the regional tables that have been associated with the global table and keep the data synchronized, all in near real time. The <kbd>serverless-dynamodb-global-table-plugin</kbd> will create the global tables and is designed to work with the <kbd>serverless-dynamodb-autoscaling-plugin</kbd>. For each table that has the <kbd>global</kbd> flag set to true, the plugin will create the global table when the service is deployed to the first region. For each successive regional deployment the plugin will add the regional table to the global table. Each regional table must have the same name, have streams enabled, and have the same autoscaling policies, which is handled by the plugins. One thing that is not handled by the plugins is that the tables must all be empty when the global table is initially deployed.</p>
<p>We will start with the happy-path scenario, where there is no regional disruption and everything is working smoothly, and walk through the following diagram. When data is written to the table in a region, such as <kbd>us-east-1</kbd>, then the data is replicated to the <kbd>us-west-2</kbd> region. The <kbd>trigger</kbd> in the <kbd>us-east-1</kbd> region is also executed. The trigger has a <kbd>forOrigin</kbd> filter that will ignore all events where the <kbd>aws:rep:updateregion</kbd> field is not equal to the current <kbd>AWS_REGION</kbd>. Otherwise, the trigger will publish an event to the Kinesis Stream in the current region and all subscribers to the event will execute in the current region and replicate their own data to the other regions. The <kbd>listener</kbd> for the current service will ignore any events that it produced itself. In the <kbd>us-west-2</kbd> region the <kbd>trigger</kbd> will also be invoked after the replication, but the <kbd>forOrigin</kbd> filter will short-circuit the logic so that a duplicate event is not published to Kinesis and reprocessed by all the subscribers in that region. The inefficiency of duplicate event processing and the potential for infinite replication loops are two reasons why it is best not to replicate event streams and instead reply on replication at the leaf data stores:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0942f19e-c3d5-442b-b233-5e9a57539aa1.png" style="width:72.83em;height:43.50em;"/></p>
<div class="packt_infobox">Note that this recipe builds on the code from the <em>Implementing bi-directional synchronization</em> recipe, so you can review that recipe for additional details about the code.</div>
<p>During a regional failover, in the best-case scenario, a user's data will have already been replicated and the failover process will be completely seamless. The user's next commands will execute in the new region, the chain of events will process in the new region, and the results will eventually replicate back to the failed region. When there is some replication latency, <strong>session consistency</strong> helps make the failover process appear seamless, as we discussed in the <em>Leveraging session consistency</em> recipe. However, during a regional failover, it is likely that some subscribers in the failing region will fall behind on processing the remaining events in the regional stream. Fortunately, a regional disruption typically means that there is lower throughput in the failing region, as opposed to no throughput. This means that there will be a higher latency for replicating the results of event processing to the other regions but they will eventually become consistent. A user experience that is designed for eventual consistency, such as an email app on a mobile device, will handle this protracted eventual consistentcy in its stride.</p>
<p>The complexity of trying to keep track of which events have processed and which events are stuck in a failing region is another reason why it is best not to replicate event streams. In cases where this protracted eventual consistency cannot be tolerated, then the latest events in the new region can rely on session consistency for more up-to-date information and use the techniques discussed in the <em>Implementing idempotency with an inverse oplock</em> and <em>Implementing idempotency with event sourcing</em> recipes to handle the older events that are received out of order from the slowly recovering region.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing round-robin replication</h1>
                </header>
            
            <article>
                
<p>Not all of the databases in our polyglot persistence architecture will support turnkey regional replication as we have with AWS DynamoDB, yet we still need to replicate their data to multiple regions to improve latency and support regional failover. This recipe demonstrates how to use AWS S3 as a surrogate to add regional replication to any database.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create the project from the following template:</li>
</ol>
<pre style="padding-left: 30px">$ sls create --template-url https://github.com/danteinc/js-cloud-native-cookbook/tree/master/ch10/round-robin-replication --path cncb-round-robin-replication</pre>
<ol start="2">
<li>Navigate to the <kbd>cncb-round-robin-replication</kbd> directory<span> with </span><kbd>cd cncb-round-robin-replication</kbd>.</li>
<li>Review the file named <kbd>serverless.yml</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">service: cncb-round-robin-replication<br/>...<br/>functions:<br/>  listener:<br/>    ...<br/>  trigger:<br/>    ...<br/>  search:<br/>    ...<br/>  <strong>replicator</strong>:<br/>    handler: replicator.trigger<br/>    events:<br/>      - sns:<br/>          arn: <br/>            Ref: <strong>BucketTopic</strong><br/>          topicName: ${self:service}-${opt:stage}-trigger<br/>    environment:<br/>      <strong>REPLICATION_BUCKET_NAME</strong>: ${self:custom.regions.${opt:region}.<strong>replicationBucketName</strong>}<br/><br/>custom:<br/>  regions:<br/>    us-east-1:<br/>      <strong>replicationBucketName</strong>: cncb-round-robin-replication-${opt:stage}-bucket-WWWWWWWWWWWWW<br/>    us-west-2:<br/>      <strong>replicationBucketName</strong>: cncb-round-robin-replication-${opt:stage}-bucket-EEEEEEEEEEEEE<br/>...</pre>
<ol start="4">
<li>Review the file named <kbd>replicator.js</kbd> with the following content:</li>
</ol>
<pre style="padding-left: 30px">module.exports.<strong>trigger</strong> = (event, context, cb) =&gt; {<br/>  _(event.Records)<br/>    .flatMap(messagesToTriggers)<br/>    .flatMap(get)<br/>    .filter(<strong>forOrigin</strong>)<br/>    .flatMap(<strong>replicate</strong>)<br/>    .collect()<br/>    .toCallback(cb);<br/>};<br/><br/>...<br/>const <strong>forOrigin</strong> = uow =&gt; uow.object.<strong>Metadata.origin</strong> !== process.env.<strong>REPLICATION_BUCKET_NAME</strong>;<br/>...<br/>const <strong>replicate</strong> = uow =&gt; {<br/>  const { ContentType, CacheControl, Metadata, Body } = uow.object;<br/><br/>  const params = {<br/>    Bucket: process.env.<strong>REPLICATION_BUCKET_NAME</strong>,<br/>    Key: uow.trigger.s3.object.<strong>key</strong>,<br/>    Metadata: {<br/>      '<strong>origin</strong>': uow.trigger.s3.bucket.name,<br/>      ...Metadata,<br/>    },<br/>    ACL: 'public-read',<br/>    ContentType,<br/>    CacheControl,<br/>    <strong>Body</strong>,<br/>  };<br/><br/>  const s3 = new aws.S3(...);<br/>  return _(<br/>    s3.<strong>putObject</strong>(params).promise()<br/>    ...<br/>  );<br/>};</pre>
<ol start="5">
<li>Install the dependencies<span> with </span><kbd>npm install</kbd>.</li>
<li>Run the tests<span> with </span><kbd>npm test -- -s $MY_STAGE</kbd>.<kbd><br/></kbd></li>
<li>Review the contents generated in the <kbd>.serverless</kbd> directory.</li>
<li>Deploy the stack in the <kbd>us-east-1</kbd> and <kbd>us-west-2</kbd> regions:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ npm run dp:lcl:e -- -s $MY_STAGE</strong><br/><strong>$ npm run dp:lcl:w -- -s $MY_STAGE</strong></pre>
<ol start="9">
<li>Update the <kbd>replicationBucketName</kbd> variables in the <kbd>serverless.yml</kbd> so that <kbd>us-east-1</kbd> replicates to <kbd>us-west-2</kbd> and visa versa, and then redeploy the stacks.</li>
<li>Review the stacks and resources in the AWS Console.</li>
<li>Publish an event from a separate Terminal with the following commands:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ cd &lt;path-to-your-workspace&gt;/cncb-event-stream</strong><br/><strong>$ sls invoke -f publish -r us-east-1 -s $MY_STAGE -d '{"type":"thing-created","thing":{"new":{"name":"thing two","id":"77777777-5555-1111-1111-111111111111"}}}'</strong></pre>
<ol start="12">
<li>Invoke the following curl command to search the data in the <kbd>us-west-2</kbd> region:</li>
</ol>
<pre style="padding-left: 30px"><strong>$ curl https://&lt;API-ID&gt;.execute-api.us-west-2.amazonaws.com/$MY_STAGE/search?q=two | json_pp</strong><br/><br/>[<br/>   {<br/>      "id" : "77777777-5555-1111-1111-111111111111",<br/>      "url" : "https://s3.amazonaws.com/cncb-round-robin-replication-john-bucket-<strong>1cqxst40pvog4</strong>/things/77777777-5555-1111-1111-111111111111",<br/> "name" : "thing two"<br/> }<br/>]</pre>
<ol start="13">
<li>Take a look at the logs for the <kbd>trigger</kbd> and <kbd>listener</kbd> functions in both regions: </li>
</ol>
<pre style="padding-left: 30px"><strong>$ sls logs -f replicator -r us-east-1 -s $MY_STAGE</strong><br/>...<br/>2018-08-19 17:00:05 ... [AWS s3 200 0.04s 0 retries] <strong>getObject</strong>({ Bucket: 'cncb-round-robin-replication-john-bucket-<strong>1a3rh4v9tfedw</strong>',<br/>  Key: 'things/77777777-5555-1111-1111-111111111111' })<br/>2018-08-19 17:00:06 ... [AWS s3 200 0.33s 0 retries] <strong>putObject</strong>({ Bucket: 'cncb-round-robin-replication-john-bucket-<strong>1cqxst40pvog4</strong>',<br/>  Key: 'things/77777777-5555-1111-1111-111111111111',<br/>  Metadata:<br/>   { <strong>origin</strong>: 'cncb-round-robin-replication-john-bucket-<strong>1a3rh4v9tfedw</strong>' },<br/>  ACL: 'public-read',<br/>  ContentType: 'application/json',<br/>  CacheControl: 'max-age=300',<br/>  Body: &lt;Buffer ... &gt; })<br/>...<br/><br/><strong>$ sls logs -f replicator -r us-west-2 -s $MY_STAGE</strong><br/>...<br/>2018-08-19 17:00:06 ... [AWS s3 200 0.055s 0 retries] <strong>getObject</strong>({ Bucket: 'cncb-round-robin-replication-john-bucket-<strong>1cqxst40pvog4</strong>',<br/>  Key: 'things/77777777-5555-1111-1111-111111111111' })<br/>2018-08-19 17:00:06 ... {... "object":{..."Metadata":{"<strong>origin</strong>":"cncb-round-robin-replication-john-bucket-<strong>1a3rh4v9tfedw</strong>"}, ...}}<br/>...</pre>
<ol start="14">
<li>Empty the bucket in each region before removing the stacks</li>
<li>Remove both stacks once you have finished: </li>
</ol>
<pre style="padding-left: 30px">$ npm run rm:lcl:e -- -s $MY_STAGE<br/>$ npm run rm:lcl:w -- -s $MY_STAGE</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we are creating a <strong>materialized view</strong> in Elasticsearch, and we want to allow users to search against the data in the region that they are closest to. However, Elasticsearch does not support regional replication. As we discussed in the <em>Implementing regional replication with DynamoDB</em> recipe, we do not want to replicate the event stream because that solution is complex and too difficult to reason about. Instead, as shown in the following diagram, we will place an S3 bucket in front of Elasticsearch in each region and leverage S3 triggers to update Elasticsearch and to implement a round-robin replication scheme:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e50354ed-46ed-4af0-92f6-e13e47b85f06.png"/></p>
<div class="packt_infobox">Note that this recipe builds on the code from the <em>Implementing a search BFF</em> recipe, so you can review that recipe for additional details about the code.</div>
<p>The service listens to the Kinesis Stream in the current region and writes the data to an S3 bucket in the current region, which generates an S3 trigger that is routed to an SNS topic. A function reacts to the topic and creates the materialized view in Elasticsearch in the current region. Meanwhile, a <kbd>replicator</kbd> function also reacts to the same topic. The replicator copies the contents of the object from the S3 bucket to the matching bucket in the next region, as specified by the <kbd>REPLICATION_BUCKET_NAME</kbd> environment variable. This in turn generates a trigger in that region. Once again, a function responds to the topic and creates the materialized view in Elasticsearch in that region as well. The <kbd>replicator</kbd> in that region also responds and looks to copy the object to the next region. This process of <kbd>trigger</kbd> and <kbd>replicate</kbd> will round robin for as many regions as necessary, until the <kbd>forOrigin</kbd> filter sees that the origin bucket (that is, <kbd>uow.object.Metadata.origin</kbd>) is equal to the target of the current replicator (that is, <kbd>process.env.REPLICATION_BUCKET_NAME</kbd>). In this recipe, we have two regions—<kbd>us-east-1</kbd> and <kbd>us-west-2</kbd>. The data originates in the east region, so the east replicator copies the data to the west bucket (<kbd>1cqxst40pvog4</kbd>). The west replicator does not copy the data to the east bucket (<kbd>1a3rh4v9tfedw</kbd>) because the origin is the east bucket.</p>
<p>This round robin replication technique is a simple and cost-effective approach that builds on the event architecture that is already in place. Note that we cannot leverage the built-in S3 replication feature for this purpose because it only replicates to a single region and does not create a chain reaction. However, we could add S3 replication to these buckets for backup and disaster recovery, as we discussed in the <em>Replicating the data lake for disaster recovery</em> recipe.</p>


            </article>

            
        </section>
    </body></html>