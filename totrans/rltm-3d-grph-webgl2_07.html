<html><head></head><body>
        

                            
                    <h1 class="header-title">Textures</h1>
                
            
            
                
<p>In the previous chapter, we covered colors, multiple lights, and important concepts about depth and alpha testing for various blending techniques. So far, we've added details to our scene with geometry, vertex colors, and lighting; but often, that won't be enough to achieve the results we're looking for. Wouldn't it be great if we could "paint" additional details onto our scene without needing additional geometry? We can! This requires us to use a technique called texture mapping. In this chapter, we'll examine how we can use textures to make our scene more detailed.</p>
<p>In this chapter, you will do the following:</p>
<ul>
<li>Learn how to create a texture.</li>
<li>Learn how to use a texture when rendering.</li>
<li>Learn about filter and wrapping modes and how they affect the texture's use.</li>
<li>Learn how to use multi-texturing.</li>
<li>Learn about cube mapping.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">What Is Texture Mapping?</h1>
                
            
            
                
<p>Texture mapping is simply a method for adding detail to a geometry being rendered by displaying an image on the surface. Consider the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b50e4c7c-20ba-4e9e-8864-10fc993fab91.png" style="width:23.33em;height:21.25em;"/></p>
<p>Using only the techniques we've learned so far, this relatively simple scene would be very difficult to build. The WebGL logo alone would have to be carefully constructed out of many triangle primitives. Although this is a possible approach, the additional geometry construction would be impractical for a marginally complex scene.</p>
<p>Fortunately, texture mapping makes such requirements incredibly simple. All that's required is an image in an appropriate file format, an additional vertex attribute on the mesh, and a few additions to our shader code.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating and Uploading a Texture</h1>
                
            
            
                
<p>Unlike traditional native OpenGL applications, browsers load textures "upside down". As a result, many WebGL applications set textures to be loaded with the <kbd>Y</kbd> coordinate flipped. This is done with a single call:</p>
<div><pre>gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, true);</pre></div>
<p>Inverted Textures<br/>
<br/>
Textures can either be manually flipped or flipped via WebGL. We will programmatically flip them with WebGL.</p>
<p>The process of creating a texture is similar to creating a vertex or an index buffer. We start by creating the texture object, as follows:</p>
<div><pre>const texture = gl.createTexture();</pre></div>
<p>Textures, like buffers, must be bound before we can manipulate them:</p>
<div><pre>gl.bindTexture(gl.TEXTURE_2D, texture);</pre></div>
<p>The first parameter indicates the type of texture we're binding, or the texture target. For now, we'll focus on 2D textures, indicated with <kbd>gl.TEXTURE_2D</kbd>. More targets will be introduced later in this chapter.</p>
<p>Once we've bound the texture, we can provide it with image data. The simplest way to do that is to pass a DOM image into the <kbd>texImage2D</kbd> function, as shown in the following code snippet:</p>
<div><pre>const image = document.getElementById('texture-image');<br/>gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image);</pre></div>
<p>In the previous snippet, we selected an image element from our page with the ID of <kbd>texture-image</kbd> as the source texture. This is <strong>uploading</strong> the texture, since the image will be stored in the GPU's memory for fast access during rendering. The source can be in any image format that can be displayed on a web page, such as JPEG, PNG, GIF, and BMP files.</p>
<p>The image source for the texture is passed in as the last parameter of the <kbd>texImage2D</kbd> call. When <kbd>texImage2D</kbd> is called with an image, WebGL will automatically determine the dimensions of the provided texture. The remainder of the parameters instruct WebGL about the type of information the image contains and how to store it. Most of the time, the only values you need to worry about changing are the third and fourth parameters, which can also be <kbd>gl.RGB</kbd>, to indicate that your texture has no alpha (transparency) channel.</p>
<p>In addition to the image, we also need to instruct WebGL on how to filter the texture when rendering. We'll get into what filtering means and what the different filtering modes do in a bit. In the meantime, let's use the simplest one to get us started:</p>
<div><pre>gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);<br/>gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);</pre></div>
<p>Just as with buffers, it's good practice to unbind a texture when you are finished using it. You can do so by binding <kbd>null</kbd> as the active texture:</p>
<div><pre>gl.bindTexture(gl.TEXTURE_2D, null);</pre></div>
<p>Of course, in many cases, you won't want to have all of the textures for your scene embedded in your web page, so it's often more convenient to create the element in JavaScript and load it without adding it to the document. Putting all of this together gives us a simple function that will load any image URL that we provide as a texture:</p>
<div><pre>const texture = gl.createTexture();<br/><br/>const image = new Image();<br/><br/>image.src = 'texture-file.png';<br/><br/>image.onload = () =&gt; {<br/>  gl.bindTexture(gl.TEXTURE_2D, texture);<br/>  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, <br/>   image);<br/>  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);<br/>  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);<br/>  gl.bindTexture(gl.TEXTURE_2D, null);<br/>};</pre></div>
<p>Asynchronous Loading<br/>
<br/>
There is a slight gotcha when loading images this way. The image loading is <strong>asynchronous</strong>, which means that your program won't stop and wait for the image to finish loading before continuing execution. So what happens if you try to use a texture before it's been populated with image data? Your scene will still render, but any texture values you sample will be black.</p>
<p>In short, creating textures follows the same pattern as using buffers. For every texture we create, we want to do the following:</p>
<ol>
<li>Create a new texture</li>
<li>Bind it to make it the current texture</li>
<li>Pass the texture contents, typically from an image</li>
<li>Set the filter mode or other texture parameters</li>
<li>Unbind the texture</li>
</ol>
<p>If we reach a point where we no longer need a texture, we can remove it and free up the associated memory by using <kbd>deleteTexture</kbd>:</p>
<div><pre>gl.deleteTexture(texture);</pre></div>
<p>After this, the texture is no longer valid. Any attempt to use it will react as though <kbd>null</kbd> has been passed.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Texture Coordinates</h1>
                
            
            
                
<p>Before we apply our texture to our surface, we need to figure out which part of the texture maps onto which part of the surface. We do this through another vertex attribute known as <strong>texture coordinates</strong>.</p>
<p>Texture coordinates are two-element float vectors that describe a location on the texture that coincides with that vertex. You may think that it would be most natural to have this vector be an actual pixel location on the image; instead, WebGL forces all of the texture coordinates into a <kbd>0</kbd> to <kbd>1</kbd> range, where <kbd>(0, 0)</kbd> represents the top left-hand side corner of the texture and <kbd>(1, 1)</kbd> represents the bottom right-hand side corner, as shown in the following image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/6b0f89d4-52ad-43fe-9ae8-d40bf5cbd3f0.png" style="width:17.58em;height:17.67em;"/></p>
<p>This means that, in order to map a vertex to the center of any texture, you would give it a texture coordinate of <kbd>(0.5, 0.5)</kbd>. This coordinate system holds true even for rectangular textures.</p>
<p>This may seem strange at first; after all, it's easier to determine the pixel coordinates of a particular point than the percentage of an image's height and width of the point's location. That said, there is a benefit to the coordinate system that WebGL uses.</p>
<p>For example, we could build a WebGL application comprised of high resolution textures. Then, at some later point, we will receive feedback that the textures are taking too long to load or the application is causing devices to render slowly. As a result, we may decide to offer a lower resolution texture option for these situations.</p>
<p>If your texture coordinates were defined in terms of pixels, you would now have to modify every mesh used by your application to ensure that the texture coordinates match up to the new, smaller textures correctly. However, when using WebGL's normalized <kbd>0</kbd> to <kbd>1</kbd> coordinate range, the smaller textures can use the exact same coordinates as the larger ones and still display correctly.</p>
<p>Figuring out texture coordinates for your mesh is often a tricky part of creating 3D resources, especially with complex meshes.</p>
<p>Polygon Mesh<br/>
<br/>
A polygon <strong>mesh</strong> is a collection of vertices, edges, and faces that defines the shape of a polyhedral object in 3D computer graphics and solid modeling.</p>
<p>Fortunately, most 3D modeling tools come with excellent utilities for laying out textures and generating texture coordinates—this process is called <strong>unwrapping</strong>.</p>
<p>Texture Coordinates<br/>
<br/>
Just as vertex position components are commonly represented with <kbd>(x, y, z)</kbd>, texture coordinates also have a common symbolic representation. Unfortunately, it's not consistent across all 3D software applications. OpenGL and WebGL refer to these coordinates as <kbd>s</kbd> and <kbd>t</kbd> for the <kbd>x</kbd> and <kbd>y</kbd> components, respectively. However, DirectX and many popular modeling packages refer to them as <kbd>u</kbd> and <kbd>v</kbd>. As a result, you'll often see people referring to texture coordinates as "UVs" and unwrapping as "UV Mapping."<br/>
<br/>
To be consistent with WebGL's usage, we will use <kbd>st</kbd> for the remainder of this book.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Textures in a Shader</h1>
                
            
            
                
<p>Texture coordinates are exposed to the shader code in the same way that they are with any other vertex attribute. We'll want to include a two-element vector attribute in our vertex shader that will map to our texture coordinates:</p>
<div><pre>in vec2 aVertexTextureCoords;</pre></div>
<p>Additionally, we will also want to add a new uniform to the fragment shader that uses a type we haven't seen before: <kbd>sampler2D</kbd>. The <kbd>sampler2D</kbd> uniform is what allows us to access the texture data in the shader:</p>
<div><pre>uniform sampler2D uSampler;</pre></div>
<p>In the past, when we've used uniforms, we set them to the value that we want them to be in the shader, such as a light color. <strong>Samplers</strong> work a bit differently. The following code shows how to associate a texture with a specific sampler uniform:</p>
<div><pre>gl.activeTexture(gl.TEXTURE0);<br/>gl.bindTexture(gl.TEXTURE_2D, texture);<br/>gl.uniform1i(program.uSampler, 0);</pre></div>
<p>So, what's going on here? First off, we are changing the active texture index with <kbd>gl.activeTexture</kbd>. WebGL supports the use of multiple textures at once (which we'll talk about later in this chapter), so it's good practice to specify which texture index we're working with, even though it won't change for the duration of this program. Next, we bind the texture we wish to use, which associates it with the currently active texture, <kbd>TEXTURE0</kbd>. Finally, we tell the sampler uniform which texture it should be associated with given the texture unit provided via <kbd>gl.uniform1i</kbd>. Here, we give it <kbd>0</kbd> to indicate that the sampler should use <kbd>TEXTURE0</kbd>.</p>
<p>We are now ready to use our texture in the fragment shader! The simplest way to use a texture is to return its value as the fragment color, as shown here:</p>
<div><pre>texture(uSampler, vTextureCoord);</pre></div>
<p><kbd>texture</kbd> takes in the sampler uniform we wish to query and the coordinates to lookup, and returns the color of the texture image at those coordinates as <kbd>vec4</kbd>. If the image has no alpha channel, <kbd>vec4</kbd> will still be returned with the alpha component always set to <kbd>1</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Texturing the Cube</h1>
                
            
            
                
<p>Let's cover an example where we add a texture map to a cube:</p>
<ol>
<li>Open the <kbd>ch07_01_textured-cube.html</kbd> file in your editor. If you open it in a browser, you should see a scene that looks like the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/1e1e7440-0416-4246-a033-d81b9693dcfe.png"/></p>
<ol start="2">
<li>Let's load the texture image. At the top of the script block, add a new variable to hold the texture:</li>
</ol>
<div><pre style="padding-left: 60px">let texture;</pre></div>
<ol start="3">
<li>At the bottom of the <kbd>configure</kbd> function, add the following code, which creates the texture object, loads an image, and sets the image as the texture data. In this case, we'll use a PNG image with the WebGL logo as our texture:</li>
</ol>
<div><pre style="padding-left: 60px">texture = gl.createTexture();<br/><br/>const image = new Image();<br/><br/>image.src = '/common/images/webgl.png';<br/><br/>image.onload = () =&gt; {<br/>  gl.bindTexture(gl.TEXTURE_2D, texture);<br/>  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, <br/>   gl.UNSIGNED_BYTE, image);<br/>  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, <br/>   gl.NEAREST);<br/>  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, <br/>   gl.NEAREST);<br/>  gl.bindTexture(gl.TEXTURE_2D, null);<br/>};</pre></div>
<ol start="4">
<li>In the <kbd>render</kbd> function after the <kbd>vertexColors</kbd> binding block, add the following code to bind the texture to the shader sampler uniform:</li>
</ol>
<div><pre style="padding-left: 60px">if (object.textureCoords) {<br/>  gl.activeTexture(gl.TEXTURE0);<br/>  gl.bindTexture(gl.TEXTURE_2D, texture);<br/>  gl.uniform1i(program.uSampler, 0);<br/>}</pre></div>
<ol start="5">
<li>Now we need to add the texture-specific code to the shader. In the vertex shader, add the following attribute and varying to the variable declarations:</li>
</ol>
<div><pre style="padding-left: 60px">in vec2 aVertexTextureCoords;<br/>out vec2 vTextureCoords;</pre></div>
<ol start="6">
<li>At the end of the vertex shader's <kbd>main</kbd> function, make sure to copy the texture coordinate attribute into the varying so that the fragment shader can access it:</li>
</ol>
<div><pre style="padding-left: 60px">vTextureCoords = aVertexTextureCoords;</pre></div>
<ol start="7">
<li>The fragment shader also needs two new variable declarations—the sampler uniform and the varying from the vertex shader:</li>
</ol>
<div><pre style="padding-left: 60px">uniform sampler2D uSampler;<br/>in vec2 vTextureCoords;</pre></div>
<ol start="8">
<li>We must also remember to add <kbd>aVertexTextureCoords</kbd> to the <kbd>attributes</kbd> list and <kbd>uSampler</kbd> to the <kbd>uniforms</kbd> list in the <kbd>configure</kbd> function so that the new variables can be accessed from our JavaScript binding code.</li>
<li>To access the texture color, we call <kbd>texture</kbd> with the sampler and the texture coordinates. Since we want the textured surface to retain the lighting, we'll multiply the lighting color and the texture color together, giving us the following line to calculate the fragment color:</li>
</ol>
<div><pre style="padding-left: 60px">fragColor = vColor * texture(uSampler, vTextureCoords);</pre></div>
<ol start="10">
<li>Open the file now in browser and you should see a scene like this one:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/90ee294d-de73-4d47-a9a7-55e606ec2b75.png" style="width:41.42em;height:26.58em;"/></p>
<ol start="11">
<li>If you're having trouble with a particular step and would like a reference, the completed code is available in <kbd>ch07_02_textured-cube-final.html</kbd>.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We've just loaded a texture from a file, uploaded it to the GPU, rendered it on the cube geometry, and blended it with the lighting information that was already being calculated.</p>
<p>The remaining examples in this chapter will omit the calculation of lighting for simplicity and clarity, but lighting could be applied to all of them if desired.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Have a Go: Try a Different Texture</h1>
                
            
            
                
<p>Try one of your own images to see if you can get it to display as the texture. What happens if you provide a rectangular image rather than a square one?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Texture Filter Modes</h1>
                
            
            
                
<p>So far, we've seen how textures can be used to sample image data in a fragment shader, but we've only used them in a limited context. Some interesting issues arise when you start to investigate textures more closely.</p>
<p>For example, if you were to zoom in on the cube from the previous demo, you would see that the texture begins to alias:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c17e305b-65e0-4d6d-b7e3-fb60575854cb.png" style="width:39.08em;height:24.08em;"/></p>
<p>As we zoom in, we can see that jagged edges develop around the WebGL logo. Similar problems become apparent when the texture is very small on the screen. Isolated to a single object, such artifacts are easy to overlook, but they can become very distracting in complex scenes. Why do we see these artifacts in the first place?</p>
<p>From the previous chapter, you should remember how vertex colors are interpolated so that the fragment shader is provided with a smooth gradient of color. Texture coordinates are interpolated in the exact same way, with the resulting coordinates being provided to the fragment shader and used to sample color values from the texture. In a perfect situation, the texture would display at a <kbd>1:1</kbd> ratio on screen, meaning each pixel of the texture (known as <strong>texels</strong>) would take up exactly one pixel on screen. In this scenario, there would be no artifacts:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8311bff0-9b3e-40c9-8739-bc71b889e018.png" style="width:30.50em;height:26.33em;"/></p>
<p>Pixel Versus Texel<strong><br/>
<br/></strong> Sometimes, the pixels in a texture are called <strong>texels</strong>. Pixel is short for Picture Element. Texel is short for Texture Element.</p>
<p>The reality of 3D applications, however, is that textures are almost never displayed at their native resolution. We refer to these scenarios as <strong>magnification</strong> and <strong>minification</strong>, depending on whether the texture has a lower or higher resolution than the screen space it occupies:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/366bbeed-d14a-42df-86cf-27667bf130bb.png" style="width:39.00em;height:18.92em;"/></p>
<p>When a texture is magnified or minified, there can be some ambiguity about what color the texture sampler should return. For example, consider the following diagram of sample points against a slightly magnified texture:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/a672fc80-bf56-4470-a439-6bf6d8ef388a.png" style="width:21.67em;height:20.75em;"/></p>
<p>It's pretty obvious what color you would want the top left-hand side or middle sample points to return, but what about those texels in the middle? What color should they return? The answer is determined by your filter mode. Texture filtering allows us to control how textures are sampled and achieve the look we want.</p>
<p>Setting a texture's filter mode is very straightforward, and we already saw an example of how it works when we talked about creating textures:</p>
<div><pre>gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);<br/>gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);</pre></div>
<p>As with most WebGL calls, <kbd>texParameteri</kbd> operates on the currently bound texture, and must be set for every texture you create. This also means that different textures can have different filters, which can be useful when trying to achieve specific effects.</p>
<p>In this example, we are setting both the magnification filter (<kbd>TEXTURE_MAG_FILTER</kbd>) and the minification filter (<kbd>TEXTURE_MIN_FILTER</kbd>) to <kbd>NEAREST</kbd>. There are several modes that can be passed for the third parameter, and the best way to understand the visual impact they have on a scene is to see the various filter modes in action.</p>
<p>Let's look at a demonstration of the filters in your browser while we discuss different parameters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Trying Different Filter Modes</h1>
                
            
            
                
<p>Let's cover an example of seeing different filter modes in action:</p>
<ol>
<li>Open the <kbd>ch07_03_texture-filters.html</kbd> file using your browser:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/73bca57f-91a9-4594-8999-dfcb0ea4ff29.png"/></p>
<ol start="2">
<li>The controls include a slider to adjust the distance of the box from the viewer, while the buttons modify the magnification and minification filters.</li>
<li>Experiment with different modes to observe the effect they have on the texture. Magnification filters take effect when the cube's textures are being rendered larger than their source image size; minification filters when it is further away. Be sure to rotate the cube as well to observe what the texture looks like when viewed at an angle with each mode.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We learned how to create and load textures into our 3D scene. We also covered various techniques for mapping textures onto objects, along with an interactive example to demonstrate these capabilities.</p>
<p>Let's look at each of the filter modes in depth and discuss how they work.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">NEAREST</h1>
                
            
            
                
<p>Textures using the <kbd>NEAREST</kbd> filter always return the color of the texel whose center is nearest to the sample point. Using this mode, textures will look blocky and pixilated when viewed up close, which can be useful for creating "retro" graphics. <kbd>NEAREST</kbd> can be used for both the <kbd>MIN</kbd> and <kbd>MAG</kbd> filters:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0d78ee73-5df7-4bb9-bfc5-be418fd971f1.png" style="width:28.83em;height:21.17em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">LINEAR</h1>
                
            
            
                
<p>The <kbd>LINEAR</kbd> filter returns the weighted average of the four pixels whose centers are nearest to the sample point. This provides a smooth blending of texel colors when looking at textures close up—it's generally the more desirable effect. This does mean that the graphics hardware has to read four times as many pixels per fragment there is, so naturally, it's slower than <kbd>NEAREST</kbd>, but modern graphics hardware is so fast that this is almost never an issue. <kbd>LINEAR</kbd> can be used for both the <kbd>MIN</kbd> and <kbd>MAG</kbd> filters. This filtering mode is also known as <strong>bilinear filtering</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b70f249f-fa09-400f-985c-8b146c6b0a04.png" style="width:29.92em;height:21.67em;"/></p>
<p>Returning to the close-up example image we showed earlier in this chapter, had we used <kbd>LINEAR</kbd> filtering, it would have looked like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/89d602f9-f910-460a-bf2b-6534964c419d.png" style="width:43.83em;height:26.25em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Mipmapping</h1>
                
            
            
                
<p>Before we can discuss the remaining filter modes that are only applicable to <kbd>TEXTURE_MIN_FILTER</kbd>, we need to introduce a new concept: <strong>mipmapping</strong>.</p>
<p>A problem arises when sampling minified textures. In cases where we use <kbd>LINEAR</kbd> filtering and the sample points are so far apart, we can completely miss some details of the texture. As the view shifts, the texture fragments that we miss change, which results in a shimmering effect. You can see this in action by setting the <kbd>MIN</kbd> filter in the demo to <kbd>NEAREST</kbd> or <kbd>LINEAR</kbd>, zooming out, and rotating the cube:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b8233e1c-b064-4437-885a-e2e089fa7200.png" style="width:27.92em;height:20.00em;"/></p>
<p>To avoid this, graphics cards can utilize a <strong>mipmap chain</strong>.</p>
<p>Mipmaps are scaled-down copies of a texture, with each copy being exactly half the size of the previous one. If you were to show a texture and all of its mipmaps in a row, it would look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/79e5c391-6b74-4fac-a30b-3c0c86d16d85.png" style="width:50.42em;height:24.92em;"/></p>
<p>The advantage is that when rendering, the graphics hardware can choose the copy of the texture that most closely matches the size of the texture on screen and samples from it instead. This reduces the number of skipped texels and the jittery artifacts that accompany them. However, mipmapping is only used if you use the appropriate texture filters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">NEAREST_MIPMAP_NEAREST</h1>
                
            
            
                
<p>This filter will select the mipmap that most closely matches the size of the texture on screen and samples from it using the <kbd>NEAREST</kbd> algorithm.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">LINEAR_MIPMAP_NEAREST</h1>
                
            
            
                
<p>This filter selects the mipmap that most closely matches the size of the texture on screen and samples from it using the <kbd>LINEAR</kbd> algorithm.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">NEAREST_MIPMAP_LINEAR</h1>
                
            
            
                
<p>This filter selects two mipmaps that most closely match the size of the texture on screen and samples from both of them by using the <kbd>NEAREST</kbd> algorithm. The color returned is a weighted average of those two samples.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">LINEAR_MIPMAP_LINEAR</h1>
                
            
            
                
<p>This filter selects two mipmaps that most closely match the size of the texture on screen and samples from both of them using the <kbd>LINEAR</kbd> algorithm. The color returned is a weighted average of those two samples. This mode is also known as <strong>trilinear filtering</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/be2654c4-556f-41c8-9e62-6942d710fdb5.png" style="width:40.92em;height:20.67em;"/></p>
<p>Of the <kbd>*_MIPMAP_*</kbd> filter modes, <kbd>NEAREST_MIPMAP_NEAREST</kbd> is the fastest and lowest quality while <kbd>LINEAR_MIPMAP_LINEAR</kbd> will provide the best quality but the lowest performance. The other two modes sit somewhere in between on the quality/speed scale. In most cases, the performance trade-off will be small enough that it's common to use <kbd>LINEAR_MIPMAP_LINEAR</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Generating Mipmaps</h1>
                
            
            
                
<p>WebGL doesn't automatically create mipmaps for every texture; so, if we want to use one of the <kbd>*_MIPMAP_*</kbd> filter modes, we have to create the mipmaps for the texture first. Fortunately, all this takes is a single function call:</p>
<div><pre>gl.generateMipmap(gl.TEXTURE_2D);</pre></div>
<p><kbd>generateMipmap</kbd> must be called after the texture has been populated with <kbd>texImage2D</kbd> and will automatically create a full mipmap chain for the image.</p>
<p>Alternatively, if you want to provide the mipmaps manually, you can always specify that you are providing a mipmap level rather than the source texture when calling <kbd>texImage2D</kbd> by passing a number other than <kbd>0</kbd> as the second parameter:</p>
<div><pre>gl.texImage2D(gl.TEXTURE_2D, 1, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, mipmapImage);</pre></div>
<p>Here, we're manually creating the first mipmap level, which is half the height and width of the normal texture. The second level would be a quarter of the dimensions of the normal texture, and so on.</p>
<p>This can be useful for some advanced effects or when using compressed textures that cannot be used with <kbd>generateMipmap</kbd>.</p>
<p>If you are familiar with WebGL 1, you'll remember its limit that textures with dimensions that were not a power of two (<strong>not</strong> <kbd>1</kbd>, <kbd>2</kbd>, <kbd>4</kbd>, <kbd>8</kbd>, <kbd>16</kbd>, <kbd>32</kbd>, <kbd>64</kbd>, <kbd>128</kbd>, <kbd>256</kbd>, <kbd>512</kbd>, and so on) could not use mips and could not repeat. In WebGL 2, these restrictions are gone.</p>
<p>Non Power of Two (NPOT)<br/>
<br/>
In order to use mipmaps with a texture in WebGL 1, mipmaps need to satisfy some dimension restrictions. Namely, the texture width and height must both be <strong>Powers of Two</strong> (<strong>POT</strong>). That is, the width and height can be <kbd>pow(2, n)</kbd> pixels, where <kbd>n</kbd> is any integer. Examples are <kbd>16px</kbd>, <kbd>32px</kbd>, <kbd>64px</kbd>, <kbd>128px</kbd>, <kbd>256px</kbd>, <kbd>512px</kbd>, <kbd>1024px</kbd>, and so on. Also, note that the width and height do not have to be the same as long as both are powers of two. For example, a <kbd>512x128</kbd> texture can still be mipmapped. NPOT textures can still be used with WebGL 1, but are restricted to only using <kbd>NEAREST</kbd> and <kbd>LINEAR</kbd> filters.<br/>
<br/>
Why, then, is power restricted for two textures? Recall that the mipmap chain is made up of textures whose sizes are half the previous level. When the dimensions are powers of two, this will always produce integer numbers, which means that the number of pixels never needs to be rounded off, and hence produces clean and fast scaling algorithms.</p>
<p class="mce-root">For all of the texture code samples after this point, we'll be using a simple texture class that cleanly wraps up the texture's download, creation, and setup. Any textures created with the class will automatically have mipmaps generated for them and be set to use <kbd>LINEAR</kbd> for the magnification filter and <kbd>LINEAR_MIPMAP_LINEAR</kbd> for the minification filter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Texture Wrapping</h1>
                
            
            
                
<p>In the previous section, we used <kbd>texParameteri</kbd> to set the filter mode for textures but, as you might expect from the generic function name, that's not all it can do. Another texture behavior that we can manipulate is the <strong>texture wrapping</strong> mode.</p>
<p>Texture wrapping describes the behavior of the sampler when the texture coordinates fall outside the range of <kbd>0</kbd> and <kbd>1</kbd>.</p>
<p>The wrapping mode can be set independently for both the <kbd>S</kbd> and <kbd>T</kbd> coordinates, so changing the wrapping mode typically takes two calls:</p>
<div><pre>gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);<br/>gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);</pre></div>
<p>Here, we're setting both the <kbd>S</kbd> and <kbd>T</kbd> wrapping modes for the currently bound texture to <kbd>CLAMP_TO_EDGE</kbd>, the effects of which we will see in a moment.</p>
<p>As with texture filters, it's easiest to demonstrate the effects of the different wrapping modes via an example and then discuss the results. Please open your browser again for another demonstration.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Trying Different Wrap Modes</h1>
                
            
            
                
<p>Let's cover an example of seeing different wrap modes in action:</p>
<ol>
<li>Open the <kbd>ch07_04_texture-wrapping.html</kbd> file using your browser:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/22809f44-7d56-4dda-97bd-144a8d886e3c.png"/></p>
<ol start="2">
<li>The cube shown in the preceding screenshot has texture coordinates that range from <kbd>-1</kbd> to <kbd>2</kbd>, which forces the texture wrapping mode to be used for everything but the center tile of the texture.</li>
<li>Experiment with the controls to see the effect that different wrap modes have on the texture.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We experimented with various approaches to texture interpolation and mipmapping techniques, along with interactive examples demonstrating these capabilities.</p>
<p>Now, let's investigate each of the wrap modes and discuss how they function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">CLAMP_TO_EDGE</h1>
                
            
            
                
<p>This wrap mode rounds any texture coordinates greater than <kbd>1</kbd> down to <kbd>1</kbd>; any coordinates lower than <kbd>0</kbd> are rounded up to <kbd>0</kbd>, "clamping" the values to the <kbd>0</kbd>-<kbd>1</kbd> range. Visually, this has the effect of repeating the texture's border pixels indefinitely once the coordinates go out of the <kbd>0</kbd>-<kbd>1</kbd> range. Note that this is the only wrapping mode that's compatible with <strong>NPOT</strong> textures:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/7cdc7623-4392-4973-8f88-252489ad40a6.png"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">REPEAT</h1>
                
            
            
                
<p>This is the default wrap mode, and the one that you'll probably use most often. In mathematical terms, this wrap mode simply ignores the integer part of the texture coordinate. This creates the visual effect of the texture repeating as you move outside of the <kbd>0</kbd>-<kbd>1</kbd> range. This is a useful effect for displaying surfaces that have a natural repeating pattern to them, such as a tile floor or brick wall:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1339edfd-2dde-4f8d-a0a1-61d27f982e5d.png"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">MIRRORED_REPEAT</h1>
                
            
            
                
<p>The algorithm for this mode is a little more complicated. If the coordinate's integer portion is even, the texture coordinates will be the same as they were with <kbd>REPEAT</kbd>. If the integer portion of the coordinate is odd, the resulting coordinate is <kbd>1</kbd> minus the fractional portion of the coordinate. This results in a texture that "flip-flops" as it repeats, with every other repetition being a mirror image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/12cd5797-553d-4893-9d29-e4d8e8c4cdab.png"/></p>
<p>As we mentioned earlier, these modes can be mixed and matched. For example, consider the following code snippet:</p>
<div><pre>gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.REPEAT);<br/>gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);</pre></div>
<p>This would produce the following effect on the texture from the sample:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/f3dbeeb7-9e96-431c-8f4b-194618d69cbc.png"/></p>
<p>Samplers Versus Textures<br/>
<br/>
Wondering why the shader uniforms are called <em>samplers</em> instead of <em>textures</em>? A texture is just the image data stored on the GPU, while a sampler contains all of the information about how to look up texture information, including filter and wrap modes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Multiple Textures</h1>
                
            
            
                
<p>So far, we've done all of our rendering by using a single texture. However, there are times when we may want to have multiple textures contribute to a fragment to create more complex effects. In such cases, we can use WebGL's ability to access multiple textures in a single draw call, commonly referred to as <strong>multi-texturing</strong>.</p>
<p>We briefly covered multi-texturing earlier, so let's go back and look at it again. When talking about exposing a texture to a shader as a sampler uniform, we used the following code:</p>
<div><pre>gl.activeTexture(gl.TEXTURE0);<br/>gl.bindTexture(gl.TEXTURE_2D, texture);</pre></div>
<p>The first line, <kbd>gl.activeTexture</kbd>, is the key to utilizing multi-texturing. We use it to tell the WebGL state machine which texture we're going to use in subsequent texture functions. In this case, we passed <kbd>gl.TEXTURE0</kbd>, which means that any following texture calls (such as <kbd>gl.bindTexture</kbd>) will alter the state of the first texture unit. If we want to attach a different texture to the second texture unit, we would use <kbd>gl.TEXTURE1</kbd> instead.</p>
<p>Different devices will support different numbers of texture units, but WebGL specifies that compatible hardware must always support at least two texture units. We can find out how many texture units the current device supports with the following function call:</p>
<div><pre>gl.getParameter(gl.MAX_COMBINED_TEXTURE_IMAGE_UNITS);</pre></div>
<p>WebGL provides explicit enumerations for <kbd>gl.TEXTURE0</kbd> through <kbd>gl.TEXTURE31</kbd>. It may be more convenient to specify the texture unit programmatically or find a need to refer to a texture unit above <kbd>31</kbd>. In such situations, you can always substitute <kbd>gl.TEXTURE0 + i</kbd> for <kbd>gl.TEXTUREi</kbd>, as in the following example:</p>
<div><pre>gl.TEXTURE0 + 2 === gl.TEXTURE2;</pre></div>
<p>Accessing multiple textures in a shader is as simple as declaring multiple samplers:</p>
<div><pre>uniform sampler2D uSampler;<br/>uniform sampler2D uOtherSampler;</pre></div>
<p>When setting up your draw call, tell the shader which texture is associated with which sampler by providing the texture unit to <kbd>gl.uniform1i</kbd>. The code to bind two textures to the samplers above would look something like this:</p>
<div><pre>// bind the first texture<br/>gl.activeTexture(gl.TEXTURE0);<br/>gl.bindTexture(gl.TEXTURE_2D, texture);<br/>gl.uniform1i(program.uSampler, 0);<br/><br/>// bind the second texture<br/>gl.activeTexture(gl.TEXTURE1);<br/>gl.bindTexture(gl.TEXTURE_2D, otherTexture);<br/>gl.uniform1i(program.uOtherSampler, 1);</pre></div>
<p>We now have two textures available for our fragment shader, but what do we want to do with them?</p>
<p>As an example, we're going to implement a simple multi-texture effect that layers another texture on top of a simple textured cube to simulate static lighting.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Using Multi-Texturing</h1>
                
            
            
                
<p>Let's cover an example of multi-texturing in action:</p>
<ol>
<li>Open the <kbd>ch07_05_multi-texture.html</kbd> file with your editor.</li>
<li>At the top of the script block, add another texture variable:</li>
</ol>
<div><pre style="padding-left: 60px">let texture2;</pre></div>
<ol start="3">
<li>At the bottom of the <kbd>configure</kbd> function, add the code to load the second texture. We're using a class to make this process easier, so the new code is as follows:</li>
</ol>
<div><pre style="padding-left: 60px">texture2 = new Texture();<br/>texture2.setImage('/common/images/light.png');</pre></div>
<ol start="4">
<li>The texture we're using is a white radial gradient that simulates a spot light:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/0af51617-be91-4d55-9a8d-9875e6e8df98.png" style="width:17.42em;height:17.42em;"/></p>
<ol start="5">
<li>In the <kbd>render</kbd> function, directly below the code that binds the first texture, add the following to expose the new texture to the shader:</li>
</ol>
<div><pre style="padding-left: 60px">gl.activeTexture(gl.TEXTURE1);<br/>gl.bindTexture(gl.TEXTURE_2D, texture2.tex);<br/>gl.uniform1i(program.uSampler2, 1);</pre></div>
<ol start="6">
<li>We need to add the new sampler uniform to the fragment shader:</li>
</ol>
<div><pre style="padding-left: 60px">uniform sampler2D uSampler2;</pre></div>
<ol start="7">
<li>Don't forget to add the corresponding string to the uniforms list in the <kbd>configure</kbd> function.</li>
<li>We add the code to sample the new texture value and blend it with the first texture. Since we want the second texture to simulate a light, we multiply the two values together as we did with the per-vertex lighting in the first texture example:</li>
</ol>
<div><pre style="padding-left: 60px">fragColor = texture(uSampler2, vTextureCoords) * texture(uSampler, vTextureCoords);</pre></div>
<ol start="9">
<li>Note that we're re-using the same texture coordinate for both textures. This is more convenient but, if needed, a second texture coordinate attribute could be provided or we could calculate a new texture coordinate from the vertex position or some other criteria.</li>
<li>You should see a scene that looks like this when you open the file in your browser:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/3b30f924-5ce4-4574-92d2-46115915aa43.png" style="width:18.75em;height:16.75em;"/></p>
<ol start="11">
<li>You can see the completed example in <kbd>ch07_06_multi-texture-final.html</kbd>.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We've added a second texture to the <kbd>render</kbd> call and blended it with the first to create a new effect, which, in this case, simulates a simple static spotlight.</p>
<p>It's important to realize that the colors sampled from a texture are treated like any other color in the shader—that is, as a generic 4-dimensional vector. As a result, we can combine textures just as we would combine vertex and light colors, or any other color manipulation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Have a Go: Moving Beyond Multiply</h1>
                
            
            
                
<p>Multiplication is one of the most common ways to blend colors in a shader, but there's really no limit to how you can combine color values. Try experimenting with different algorithms in the fragment shader to see what effect it has on the output. What happens when you add values instead of multiply? What if you use the red channel from one texture and the blue and green from the other? Try out the following algorithm and see what the result is:</p>
<div><pre>fragColor = vec4(texture(uSampler2, vTextureCoords).rgb - texture(uSampler, vTextureCoords).rgb, 1.0);</pre>
<p>The result is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/af682a41-586b-4863-b5de-f190665f37ae.png" style="width:20.08em;height:18.75em;"/></p>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Have a Go: Using Multi-Dimensional Textures</h1>
                
            
            
                
<p>As you may have noticed, the challenges in maintaining multiple textures resembles the same challenges we faced in <a href="65e942d0-5402-4006-9b64-6811ade8f46c.xhtml">Chapter 6</a>, <em>Colors, Depth Testing, and Alpha Blending,</em> in managing multiple lights. That being said, does WebGL provide a similar feature as uniform arrays for managing multiple textures? Yes, of course! We can leverage two different solutions that WebGL 2 provides for managing multi-dimensional textures: <strong>3D textures</strong> and <strong>texture arrays</strong>.</p>
<p>Although, we will discuss these features in <a href="9a56a1ad-908b-4201-b95c-0c811b1bd011.xhtml">Chapter 11</a>, <em>WebGL 2 Highlights,</em> it may be useful to think about how these features can be useful in reducing complexity, improving code maintainability, and increasing the number of textures that can be used.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cube Maps</h1>
                
            
            
                
<p>Earlier in this chapter, we mentioned 2D textures and cube maps for creating complex effects using images. We covered textures, but exactly what are cube maps and how do we use them?</p>
<p>A <strong>cube map</strong> is, very much like it sounds, a cube of textures. Six individual textures are created, each assigned to a different face of the cube. The graphics hardware can sample them as a single entity, by using a 3D texture coordinate.</p>
<p>The faces of the cube are identified by the axis they face and whether they are on the positive or negative side of that axis:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/5762d179-58de-4c54-b10f-b50f1d66f965.png" style="width:26.42em;height:19.83em;"/></p>
<p>So far, we have manipulated a texture by specifying a texture target of <kbd>TEXTURE_2D</kbd>. Cube mapping introduces a few new texture targets that indicate we are working with cube maps. These targets also indicate which face of the cube map we're manipulating:</p>
<ul>
<li><kbd>TEXTURE_CUBE_MAP</kbd></li>
<li><kbd>TEXTURE_CUBE_MAP_POSITIVE_X</kbd></li>
<li><kbd>TEXTURE_CUBE_MAP_NEGATIVE_X</kbd></li>
<li><kbd>TEXTURE_CUBE_MAP_POSITIVE_Y</kbd></li>
<li><kbd>TEXTURE_CUBE_MAP_NEGATIVE_Y</kbd></li>
<li><kbd>TEXTURE_CUBE_MAP_POSITIVE_Z</kbd></li>
<li><kbd>TEXTURE_CUBE_MAP_NEGATIVE_Z</kbd></li>
</ul>
<p>These targets are collectively known as the <kbd>gl.TEXTURE_CUBE_MAP_*</kbd> targets. Which one you need to use depends on the function you're calling.</p>
<p>Cube maps are created like a normal texture, but binding and property manipulation happen with the <kbd>TEXTURE_CUBE_MAP</kbd> target, as shown here:</p>
<div><pre>const cubeTexture = gl.createTexture();<br/>gl.bindTexture(gl.TEXTURE_CUBE_MAP, cubeTexture);<br/>gl.texParameteri(gl.TEXTURE_CUBE_MAP, gl.TEXTURE_MAG_FILTER, gl.LINEAR);<br/>gl.texParameteri(gl.TEXTURE_CUBE_MAP, gl.TEXTURE_MIN_FILTER, gl.LINEAR);</pre></div>
<p>When uploading the image data for the texture, you need to specify the side that you are manipulating, as shown here:</p>
<div><pre>gl.texImage2D(gl.TEXTURE_CUBE_MAP_POSITIVE_X, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, positiveXImage);<br/>gl.texImage2D(gl.TEXTURE_CUBE_MAP_NEGATIVE_X, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, negativeXImage);<br/>gl.texImage2D(gl.TEXTURE_CUBE_MAP_POSITIVE_Y, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, positiveYImage);<br/>// ...</pre></div>
<p>Exposing the cube map texture to the shader is done in the same way as a normal texture, just with the cube map target:</p>
<div><pre>gl.activeTexture(gl.TEXTURE0);<br/>gl.bindTexture(gl.TEXTURE_CUBE_MAP, cubeTexture);<br/>gl.uniform1i(program.uCubeSampler, 0);</pre></div>
<p>However, the uniform type within the shader is specific to cube maps:</p>
<div><pre>uniform samplerCube uCubeSampler;</pre></div>
<p>When sampling from the cube map, you also use a cube map-specific function:</p>
<div><pre>texture(uCubeSampler, vCubeTextureCoords);</pre></div>
<p>The 3D coordinates you provide are normalized by the graphics hardware into a unit vector, which specifies a direction from the center of the "cube." A ray is traced along that vector, and where it intersects the cube face is where the texture is sampled:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1e0f5c69-128a-49a8-ad7a-ae4b8ad6bb7b.png" style="width:18.75em;height:18.92em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Time for Action: Trying out Cube Maps</h1>
                
            
            
                
<p>Let's cover an example of seeing cube maps in action:</p>
<ol>
<li>Open the <kbd>ch07_07_cubemap.html</kbd> file in your browser. Once again, this contains a simple textured cube example on top of which we'll build the cube map example. We want to use the cube map to create a reflective-looking surface.</li>
<li>Creating the cube map is a bit more complicated than the textures we've loaded in the past, so this time, we'll use a function to simplify the asynchronous loading of individual cube faces. It's called <kbd>loadCubemapFace</kbd> and has already been to the file. Inside of the <kbd>configure</kbd> function, at the bottom, add the following code, which creates and loads the cube map faces:</li>
</ol>
<div><pre style="padding-left: 60px">cubeTexture = gl.createTexture();<br/><br/>gl.bindTexture(gl.TEXTURE_CUBE_MAP, cubeTexture);<br/>gl.texParameteri(gl.TEXTURE_CUBE_MAP, gl.TEXTURE_MAG_FILTER, gl.LINEAR);<br/>gl.texParameteri(gl.TEXTURE_CUBE_MAP, gl.TEXTURE_MIN_FILTER, gl.LINEAR);<br/><br/>loadCubemapFace(gl, gl.TEXTURE_CUBE_MAP_POSITIVE_X, cubeTexture, '/common/images/cubemap/positive_x.png');<br/><br/>loadCubemapFace(gl, gl.TEXTURE_CUBE_MAP_NEGATIVE_X, cubeTexture, '/common/images/cubemap/negative_x.png');<br/><br/>loadCubemapFace(gl, gl.TEXTURE_CUBE_MAP_POSITIVE_Y, cubeTexture, '/common/images/cubemap/positive_y.png');<br/><br/>loadCubemapFace(gl, gl.TEXTURE_CUBE_MAP_NEGATIVE_Y, cubeTexture, '/common/images/cubemap/negative_y.png');<br/><br/>loadCubemapFace(gl, gl.TEXTURE_CUBE_MAP_POSITIVE_Z, cubeTexture, '/common/images/cubemap/positive_z.png');<br/><br/>loadCubemapFace(gl, gl.TEXTURE_CUBE_MAP_NEGATIVE_Z, cubeTexture, '/common/images/cubemap/negative_z.png');</pre></div>
<ol start="3">
<li>In the <kbd>render</kbd> function, add the code to bind the cube map to the appropriate sampler:</li>
</ol>
<div><pre style="padding-left: 60px">gl.activeTexture(gl.TEXTURE1);<br/>gl.bindTexture(gl.TEXTURE_CUBE_MAP, cubeTexture);<br/>gl.uniform1i(program.uCubeSampler, 1);</pre></div>
<ol start="4">
<li>Turning to the shader now, we want to add a new varying to the vertex shader:</li>
</ol>
<div><pre style="padding-left: 60px">out vec3 vVertexNormal;</pre></div>
<ol start="5">
<li>We'll be using the vertex normals instead of a dedicated texture coordinate to do the cube map sampling, which will give us the mirror effect we're looking for. Unfortunately, the actual normals of each face on the cube point straight out. If we were to use them, we would only get a single color per face from the cube map. In this case, we can "cheat" and use the vertex position as the normal instead (for most models, using the normals would be appropriate):</li>
</ol>
<div><pre style="padding-left: 60px">vVertexNormal = (uNormalMatrix * vec4(-aVertexPosition, 1.0)).xyz;</pre></div>
<ol start="6">
<li>We need to define the following varying inside of the fragment shader:</li>
</ol>
<pre style="padding-left: 60px">in vec3 vVertexNormal;</pre>
<ol start="7">
<li>We also need to add the new sampler uniform inside of the fragment shader. Be sure to also include this in the <kbd>uniforms</kbd> list inside of the <kbd>configure</kbd> function:</li>
</ol>
<div><pre style="padding-left: 60px">uniform samplerCube uCubeSampler;</pre></div>
<ol start="8">
<li>And then, in the fragment shader's <kbd>main</kbd> function, add the code to actually sample the cube map and blend it with the base texture:</li>
</ol>
<div><pre style="padding-left: 60px">fragColor = texture(uSampler, vTextureCoords) * texture(uCubeSampler, vVertexNormal);</pre></div>
<ol start="9">
<li>We should now be able to reload the file in a browser and see the scene shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/ac07b294-5ebb-4afe-b292-d4266523ad4c.png" style="width:24.25em;height:24.92em;"/></p>
<ol start="10">
<li>The completed example is available in <kbd>ch07_08_cubemap-final.html</kbd>.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>As you rotate the cube, you'll notice that the scene displayed on the cube map does not rotate along, creating a "mirror" effect on the cube faces. This is due to multiplication of the normals by the Normal matrix when assigning the <kbd>vVertexNormal</kbd> varying, which puts the normals in world space.</p>
<p>Using cube maps for reflective surfaces is a common technique, but it's not the only use for cube maps. Other common uses include skyboxes and advanced lighting models.</p>
<p>Skybox<strong><br/>
<br/></strong> A skybox is a method that's used for creating backgrounds to make computer and video game levels look bigger than they really are. When a skybox is used, the level is enclosed in a cuboid. The sky, distant mountains, distant buildings, and other unreachable objects are projected onto the cube's faces (using a technique called cube mapping), hence creating the illusion of distant, three-dimensional surroundings. A skydome employs the same concept but uses either a sphere or a hemisphere instead of a cube. For more information, check out the following URL: <a href="https://en.wikipedia.org/wiki/Skybox_(video_games)">https://en.wikipedia.org/wiki/Skybox_(video_games)</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Have a Go: Shiny Logo</h1>
                
            
            
                
<p>In this example, we've created a reflective "mirrored" cube. But what if we only wanted the logo to be reflective? How could we constrain the cube map to only be displayed within the red portion of the texture?</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Let's summarize what we've learned in this chapter:</p>
<ul>
<li>How to use textures to add a new level of detail to our scenes.</li>
<li>How to create and manage texture objects and use HTML images as textures.</li>
<li>We covered texture coordinates and the ability to mipmap for various rendering techniques.</li>
<li>We examined the various filter modes and how they affect the texture's appearance and usage, as well as the available texture wrapping modes and how they alter the way texture coordinates are interpreted.</li>
<li>We learned how to use multiple textures in a single draw call, and how to combine them in a shader.</li>
<li>We learned how to create and render cube maps and saw how they can be used to simulate reflective surfaces.</li>
</ul>
<p>In the next chapter, we will look at selecting and interacting with objects in our WebGL scene by using a clever technique known as picking.</p>


            

            
        
    </body></html>