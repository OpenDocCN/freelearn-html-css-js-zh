<html><head></head><body><div><h1 class="header-title">Deploying Node.js Applications</h1>
                
            
            
                
<p>Now that the Notes application is fairly complete, it's time to think about how to deploy it to a real server. We've created a minimal implementation of the collaborative note concept that works fairly well. To grow, Notes must escape our laptop and live on a real server. The goal is to look at deployment methods for Node.js applications.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Traditional LSB-compliant Node.js deployment</li>
<li>Using PM2 to improve reliability</li>
<li>Deployment to a <strong>Virtual Private Server</strong> (<strong>VPS</strong>) provider</li>
<li>Microservice deployment with Docker (we have four distinct services to deploy)</li>
<li>Deployment to a Docker hosting provider</li>
</ul>
<p>The first task is to duplicate the source code from the previous chapter. It's suggested you create a new directory, <kbd>chap10</kbd>, as a sibling of the <kbd>chap09</kbd> directory, and copy everything from <kbd>chap09</kbd> to <kbd>chap10</kbd>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Notes application architecture and deployment considerations</h1>
                
            
            
                
<p>Before we get into deploying the Notes application, we need to review its architecture. To deploy the Notes application, we must understand what we're planning to do.</p>
<p>We have segmented the services into two groups, as shown in the following diagram:</p>
<div><img src="img/0722ddbb-10a6-4c2d-988c-703ed2474e35.png" style="width:20.33em;height:24.83em;" width="673" height="819"/></div>
<p>The user authentication server should be the more secure portion of the system. On our laptop, we weren't able to create the envisioned protective wall around that service, but we're about to implement such protection.</p>
<p>One strategy to enhance security is to expose as few ports as possible. That reduces the so-called attack surface, simplifying our work in hardening the application against security bugs. With the Notes application, we have exactly one port to expose, the HTTP service through which users access the application. The other ports, the two for MySQL servers and the user authentication service port, should be hidden.</p>
<p>Internally, the Notes application needs to access both the Notes database and the user authentication service. That service, in turn, needs to access the user authentication database. As currently envisaged, no service outside the Notes application requires access to either database or to the authentication service.</p>
<p>Implementation of this segmentation requires either two or three subnets, depending on the lengths you wish to go to. The first, FrontNet, contains the Notes application and its database. The second, AuthNet, contains the authentication service and its database. A third possible subnet would contain the Notes and authentication services. The subnet configuration must limit the hosts with access to the subnet, and create a security wall between subnets.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Traditional Linux Node.js service deployment</h1>
                
            
            
                
<p>Traditional Linux/Unix server application deployment uses an <strong>init script</strong> to manage background processes. They are to start every time the system boots and cleanly shut down when the system is halted. While it's a simple model, the specifics of this vary widely from one <strong>operating system</strong> (<strong>OS</strong>) to another.</p>
<p>A common method is for the <kbd>init</kbd> process to manage background processes using shell scripts in the <kbd>/etc/init.d</kbd> directory. Other OSes use other process managers, such as <kbd>upstart</kbd> or <kbd>launchd</kbd>. </p>
<p>The Node.js project itself does not include any scripts to manage server processes on any OS. Node.js is more like a construction kit, with the pieces and parts to build servers, and is not a complete polished server framework itself. Implementing a complete web service based on Node.js means creating the scripting to integrate with process management on your OS. It's up to us to develop those scripts.</p>
<p>Web services have to be:</p>
<ul>
<li>Reliable: For example, to auto-restart when the server process crashes</li>
<li>Manageable: Meaning it integrates well with system management practices</li>
<li>Observable: Meaning the administrator must be able to get status and activity information from the service</li>
</ul>
<p>To demonstrate what's involved, let's use PM2 to implement background server process management for Notes. PM2 detects the system type and can automatically integrate itself with the process management system. It will create an LSB-style init script (<a href="http://wiki.debian.org/LSBInitScripts">http://wiki.debian.org/LSBInitScripts</a>), or other scripts, as required by the process management system on your server. </p>
<p>For this deployment, we'll set up a single Ubuntu 17.10 server. You should provision a <strong>Virtual Private Server</strong> (<strong>VPS</strong>) from a hosting provider and do all installation and configuration there. Renting a small machine instance from one of the major providers for the time needed to go through this chapter will only cost a couple of dollars.</p>
<p>You can also do the tasks in this section using <strong>VirtualBox</strong> on your laptop. Simply install Debian or Ubuntu as a virtual machine in VirtualBox, then follow the instructions in this section. It won't be quite the same as using a remote VPS hosting provider, but does not require renting a server.</p>
<p>Both the Notes and user authentication services will be on that server, along with a single MySQL instance. While our goal is a strong separation between FrontNet and AuthNet, with two MySQL instances, we won't do so at this time.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Prerequisite – provisioning the databases</h1>
                
            
            
                
<p>The Linux package management system doesn't allow us to install two MySQL instances. Instead, we implement separation in the same MySQL instance by using separate databases with different usernames and access privileges for each database.</p>
<p>The first step is to ensure that MySQL is installed on your server. For Ubuntu, <strong>DigitalOcean</strong> has a fairly good tutorial: <a href="https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-14-04">https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-14-04</a>. While the Ubuntu version for that tutorial is old, the instructions are still accurate enough.</p>
<p>The MySQL server must support TCP connections from <kbd>localhost</kbd>. Edit the configuration file, <kbd>/etc/mysql/my.cnf</kbd>, to have the following line:</p>
<pre>bind-address = 127.0.0.1</pre>
<p>This limits MySQL server connections to the processes on the server. A miscreant would have to break into the server to access your database. Now that our database server is available, let's set up two databases.</p>
<p>In the <kbd>chap10/notes/models</kbd> directory, create a file named <kbd>mysql-create-db.sql</kbd> containing the following:</p>
<pre><strong>CREATE DATABASE notes;
CREATE USER 'notes'@'localhost' IDENTIFIED BY 'notes';
GRANT ALL PRIVILEGES ON notes.* TO 'notes'@'localhost' WITH GRANT OPTION; </strong> </pre>
<p>In the <kbd>chap10/users</kbd> directory, create a file named <kbd>mysql-create-db.sql</kbd> containing the following:</p>
<pre><strong>CREATE DATABASE userauth;
CREATE USER 'userauth'@'localhost' IDENTIFIED BY 'userauth';
GRANT ALL PRIVILEGES ON userauth.* TO 'userauth'@'localhost' WITH GRANT OPTION; </strong> </pre>
<p>We can't run those scripts on the server, because the Notes application has not yet been copied to the server. When that's accomplished, we'll run the scripts as:</p>
<pre><strong>$ mysql -u root -p &lt;chap10/users/mysql-create-db.sql
$ mysql -u root -p &lt;chap10/notes/models/mysql-create-db.sql </strong> </pre>
<p>This will create the two databases, <kbd>notes</kbd> and <kbd>userauth</kbd>, with associated usernames and passwords. Each user can access only their associated database. Later, we'll set up Notes and the user authentication service with YAML configuration files to access these databases.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Installing Node.js on Ubuntu</h1>
                
            
            
                
<p>According to the Node.js documentation (<a href="https://nodejs.org/en/download/package-manager/">https://nodejs.org/en/download/package-manager/</a>), the recommended installation method for Debian or Ubuntu Linux distributions is the following:</p>
<pre><strong>$ curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -</strong><br/><strong>$ sudo apt-get update
$ sudo apt-get install -y nodejs build-essential </strong> </pre>
<p>We've seen this before, so substitute the Node.js desired version number in the URL. Installing this way means that as new Node.js releases are issued, upgrades are easily accomplished with the normal package management procedures. </p>


            

            
        
    </div>



  
<div><h1 class="header-title">Setting up Notes and user authentication on the server</h1>
                
            
            
                
<p>Before copying the Notes and user authentication code to this server, let's do a little coding to prepare for the move. We know that the Notes and authentication services must access the MySQL instance on <kbd>localhost</kbd> using the usernames and passwords given earlier.</p>
<p>Using the approach we've followed so far, this means a pair of YAML files for <kbd>Sequelize</kbd> parameters, and changing environment variables in the <kbd>package.json</kbd> files to match.</p>
<p class="mce-root"/>
<p>Create a <kbd>chap10/notes/models/sequelize-server-mysql.yaml</kbd> file containing:</p>
<pre>dbname: notes 
username: notes 
password: notes12345
params: 
    host: localhost 
    port: 3306 
    dialect: mysql </pre>
<p>It was discovered during testing that a simple password such as <kbd>notes</kbd> was not acceptable to the MySQL server, and that a longer password was required. In <kbd>chap10/notes/package.json</kbd>, add the following line to the <kbd>scripts</kbd> section:</p>
<pre>"on-server": "SEQUELIZE_CONNECT=models/sequelize-server-mysql.yaml NOTES_MODEL=sequelize USER_SERVICE_URL=http://localhost:3333 PORT=3000 node --experimental-modules ./app",  </pre>
<p>Then create a <kbd>chap10/users/sequelize-server-mysql.yaml</kbd> file containing the following code the following code:</p>
<pre>dbname: userauth 
username: userauth 
password: userauth 
params: 
    host: localhost 
    port: 3306 
    dialect: mysql </pre>
<p>The passwords shown in these configuration files obviously will not pass any security audits.</p>
<p>In <kbd>chap10/users/package.json</kbd>, add the following line to the <kbd>scripts</kbd> section:</p>
<pre>"on-server": "PORT=3333 SEQUELIZE_CONNECT=sequelize-server-mysql.yaml node --experimental-modules ./user-server", </pre>
<p>This configures the authentication service to access the databases just created.</p>
<p>Now we need to select a place on the server to install the application code:</p>
<pre><strong># ls /opt </strong> </pre>
<p>This empty directory looks to be as good a place as any. Simply upload <kbd>chap10/notes</kbd> and <kbd>chap10/users</kbd> to your preferred location. Before uploading, remove the <kbd>node_modules</kbd> directory in both directories.</p>
<p>That's both to save time on the upload, and because of the simple fact that any native-code modules installed on your laptop will be incompatible with the server.</p>
<p>On your laptop, you might run a command like this:</p>
<pre><strong>$ rsync --archive --verbose ./ root@159.89.145.190:/opt/</strong></pre>
<p>Use the actual IP address or domain name assigned to the server being used.</p>
<p>You should end up with something like the following:</p>
<pre><strong># ls /opt
notes  users </strong> </pre>
<p>Then, in each directory, run these commands:</p>
<pre><strong># rm -rf node_modules
# npm install </strong> </pre>
<p>We're running these commands as <kbd>root</kbd> rather than a user ID that can use the <kbd>sudo</kbd> command. The machine offered by the chosen hosting provider (DigitalOcean) is configured so users log in as <kbd>root</kbd>. Other VPS hosting providers will provide machines where you log in as a regular user and then use <kbd>sudo</kbd> to perform privileged operations. As you read these instructions, pay attention to the command prompt we show. We've followed the convention where <kbd>$</kbd> is used for commands run as a regular user and <kbd>#</kbd> is used for commands run as <kbd>root</kbd>. If you're running as a regular user, and need to run a <kbd>root</kbd> command, then run the command with <kbd>sudo</kbd>.</p>
<p>The simplest way of doing this is to just delete the whole <kbd>node_modules</kbd> directory and then let <kbd>npm install</kbd> do its job. Remember that we set up the <kbd>PATH</kbd> environment variable the following way:</p>
<pre><strong># export PATH=./node_modules/.bin:${PATH} </strong> </pre>
<p>You can place this in the login script (<kbd>.</kbd><kbd>bashrc</kbd>, <kbd>.cshrc</kbd>, and so on) on your server so it's automatically enabled. </p>
<p>Finally, you can now run the SQL scripts written earlier to set up the database instances:</p>
<pre><strong># mysql -u root -p &lt;users/mysql-create-db.sql
# mysql -u root -p &lt;notes/models/mysql-create-db.sql </strong> </pre>
<p>Then you should be able to start up the services by hand to check that everything is working correctly. The MySQL instance has already been tested, so we just need to start the user authentication and Notes services:</p>
<pre><strong># cd /opt/users
# DEBUG=users:* npm run on-server</strong><br/><br/><strong>&gt; user-auth-server@0.0.1 on-server /opt/users</strong><br/><strong>&gt; PORT=3333 SEQUELIZE_CONNECT=sequelize-server-mysql.yaml node --experimental-modules ./user-server</strong><br/><br/><strong>(node:9844) ExperimentalWarning: The ESM module loader is experimental.</strong><br/><br/></pre>
<p>Then log in to the server on another Terminal session and run the following:</p>
<pre><strong># cd /opt/users/
# PORT=3333 node users-add.js </strong><br/><strong>Created { id: 1, username: 'me', password: 'w0rd', provider: 'local',</strong><br/><strong>  familyName: 'Einarrsdottir', givenName: 'Ashildr', middleName: '',</strong><br/><strong>  emails: '[]', photos: '[]',</strong><br/><strong>  updatedAt: '2018-02-02T00:43:16.923Z', createdAt: '2018-02-02T00:43:16.923Z' }
# PORT=3333 node users-list.js </strong><br/><strong>List [ { id: 'me', username: 'me', provider: 'local',</strong><br/><strong>    familyName: 'Einarrsdottir', givenName: 'Ashildr', middleName: '',</strong><br/><strong>    emails: '[]', photos: '[]' } ]</strong></pre>
<p>The preceding command both tests that the backend user authentication service is functioning and gives us a user account we can use to log in. The <kbd>users-list</kbd> command demonstrates that it works.</p>
<p>You may get an error:</p>
<pre><strong> users:error /create-user Error: Please install mysql2 package manually</strong><br/></pre>
<p>This is generated inside of <kbd>Sequelize</kbd>. The <kbd>mysql2</kbd> driver is an alternate MySQL driver, implemented in pure JavaScript, and includes support for returning Promises for smooth usage in <kbd>async</kbd> functions. If you do get this message, go ahead and install the package and remember to add this dependency to your <kbd>package.json</kbd>.</p>
<p>Now we can start the Notes service:</p>
<pre><strong># cd ../notes
# npm run on-server</strong><br/><br/><strong>&gt; notes@0.0.0 on-server /opt/notes</strong><br/><strong>&gt; SEQUELIZE_CONNECT=models/sequelize-server-mysql.yaml NOTES_MODEL=sequelize USER_SERVICE_URL=http://localhost:3333 PORT=3000 node --experimental-modules ./app</strong><br/><br/><strong>(node:9932) ExperimentalWarning: The ESM module loader is experimental.</strong></pre>
<p>Then we can use our web browser to connect to the application. Since you probably do not have a domain name associated with this server, Notes can be accessed via the IP address of the server, such as <kbd>http://159.89.145.190:3000/</kbd>.</p>
<p>In these examples, we're using the IP address of the VPS used to test the instructions in this section. The IP address you use will, of course, be different.</p>
<p>By now, you know that the drill for verifying Notes is working. Create a few notes, open a few browser windows, see that real-time notifications work, and so on. Once you're satisfied that Notes is working on the server, kill the processes and move on to the next section, where we'll set this up to run when the server starts.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Adjusting Twitter authentication to work on the server</h1>
                
            
            
                
<p>The Twitter application we set up for Notes previously won't work because the authentication URL is incorrect for the server. For now, we can log in using the user profile created previously. If you want to see OAuth work with Twitter, go to <kbd>apps.twitter.com</kbd> and reconfigure the application to use the IP address of your server.</p>
<p>By hosting somewhere other than our laptop, the Twitter <kbd>callbackURL</kbd> must point to the correct location. The default value was <kbd>http://localhost:3000</kbd> for use on our laptop. But we now need to use the IP address for the server. In <kbd>notes/package.json,</kbd> add the following environment variable to the <kbd>on-server</kbd> script:</p>
<div><pre><strong>TWITTER_CALLBACK_HOST=http://159.89.145.190:3000</strong> </pre>
<p>Use the actual IP address or domain name assigned to the server being used. In a real deployment, we'll have a domain name to use here. </p>
</div>


            

            
        
    </div>



  
<div><h1 class="header-title">Setting up PM2 to manage Node.js processes</h1>
                
            
            
                
<p>There are many ways to manage server processes, to ensure restarts if the process crashes, and so on. We'll use <strong>PM2</strong> (<a href="http://pm2.keymetrics.io/">http://pm2.keymetrics.io/</a>) because it's optimized for Node.js processes. It bundles process management and monitoring into one application.</p>
<p>Let's create a directory, <kbd>init</kbd>, in which to use PM2. The PM2 website suggests you install the tool globally but, as students of the Twelve Factor Application model, we recognize it's best to use explicitly declared dependencies and avoid global unmanaged dependencies.</p>
<p>Create a <kbd>package.json</kbd> file containing:</p>
<pre>{<br/>  "name": "pm2deploy",<br/>  "version": "1.0.0",<br/>  "scripts": {<br/>    "start": "pm2 start ecosystem.json",<br/>    "stop": "pm2 stop ecosystem.json",<br/>    "restart": "pm2 restart ecosystem.json",<br/>    "status": "pm2 status",<br/>    "save": "pm2 save",<br/>    "startup": "pm2 startup"<br/>  },<br/>  "dependencies": {<br/>    "pm2": "^2.9.3"<br/>  }<br/>}<br/></pre>
<p>Install PM2 using <kbd>npm install</kbd> as usual.</p>
<p>In normal PM2 usage, we launch scripts with <kbd>pm2 start script-name.js</kbd>. We could make an <kbd>/etc/init</kbd> script which does that, but PM2 also supports a file named <kbd>ecosystem.json</kbd> that can be used to manage a cluster of processes. We have two processes to manage together, the user-facing Notes application, and the user authentication service on the back end.</p>
<p>Create a file named <kbd>ecosystem.json</kbd> containing the following:</p>
<pre>{<br/>  "apps" : [<br/>    {<br/>      "name": "User Authentication",<br/>      "script": "user-server.mjs",<br/>      "cwd": "/opt/users",<br/>      "node_args": "--experimental-modules",<br/>      "env": {<br/>        "PORT": "3333",<br/>        "SEQUELIZE_CONNECT": "sequelize-server-mysql.yaml"<br/>      },<br/>      "env_production": { "NODE_ENV": "production" }<br/>    },<br/>    {<br/>      "name": "Notes",<br/>      "script": "app.mjs",<br/>      "cwd": "/opt/notes",<br/>      "node_args": "--experimental-modules",<br/>      "env": {<br/>        "PORT": "3000",<br/>        "SEQUELIZE_CONNECT": "models/sequelize-server-mysql.yaml",<br/>        "NOTES_MODEL": "sequelize",<br/>        "USER_SERVICE_URL": "http://localhost:3333",<br/>        "TWITTER_CONSUMER_KEY": "..",<br/>        "TWITTER_CONSUMER_SECRET": "..",<br/>        "TWITTER_CALLBACK_HOST": "http://45.55.37.74:3000"<br/>      },<br/>      "env_production": { "NODE_ENV": "production" }<br/>    }<br/>  ]<br/>}<br/> </pre>
<p>This file describes the directories containing both services, the script to run each service, the command-line options, and the environment variables to use. It's the same information that is in the <kbd>package.json</kbd> scripts, but spelled out more clearly. Adjust <kbd>TWITTER_CALLBACK_HOST</kbd> for the IP address of the server. For documentation, see <a href="http://pm2.keymetrics.io/docs/usage/application-declaration/">http://pm2.keymetrics.io/docs/usage/application-declaration/</a>.</p>
<p>We then start the services with <kbd>npm run start</kbd>,  which looks like the following on the screen:</p>
<div><img src="img/641357ce-aa02-4035-93b1-61885a0dd4c0.png" style="width:42.67em;height:22.58em;" width="991" height="523"/></div>
<p>You can again navigate your browser to the URL for your server, such as <kbd>http://159.89.145.190:3000</kbd>, and check that Notes is working. Once started, some useful commands are as follows:</p>
<pre><strong># pm2 list
# pm2 describe 1
# pm2 logs 1 </strong> </pre>
<p>These commands let you query the status of the services.</p>
<p>The <kbd>pm2 monit</kbd> command gives you a pseudo-graphical monitor of system activity. For documentation, see <a href="http://pm2.keymetrics.io/docs/usage/monitoring/">http://pm2.keymetrics.io/docs/usage/monitoring/</a>.</p>
<p>The <kbd>pm2 logs</kbd> command addresses the application log management issue we raised elsewhere. Activity logs should be treated as an event stream, and should be captured and managed appropriately. With PM2, the output is automatically captured, can be viewed, and the log files can be rotated and purged. See <a href="http://pm2.keymetrics.io/docs/usage/log-management/">http://pm2.keymetrics.io/docs/usage/log-management/</a> for documentation.</p>
<p>If we restart the server, these processes don't start with the server. How do we handle that? It's very simple because PM2 can generate an <kbd>init</kbd> script for us:</p>
<pre><strong># pm2 save</strong><br/><strong>[PM2] Saving current process list...</strong><br/><strong>[PM2] Successfully saved in /root/.pm2/dump.pm2</strong><br/> <br/><strong># pm2 startup</strong><br/><strong>[PM2] Init System found: systemd</strong><br/><strong>Platform systemd</strong><br/><strong>Template</strong><br/><strong>[Unit]</strong><br/><strong>Description=PM2 process manager</strong><br/><strong>Documentation=https://pm2.keymetrics.io/</strong><br/><strong>After=network.target</strong><br/><br/><strong>... more output is printed</strong></pre>
<p>The <kbd>pm2 save</kbd> command saves the current state. Whatever services are running at that time will be saved and managed by the generated start up script. </p>
<p>The next step is to generate the startup script, using the <kbd>pm startup</kbd> command. PM2 supports generating start up scripts on several OSes, but when run this way, it autodetects the system type and generates the correct start up script. It also installs the start up script, and starts it running. See the documentation at <a href="http://pm2.keymetrics.io/docs/usage/startup/">http://pm2.keymetrics.io/docs/usage/startup/</a> for more information.<a href="http://pm2.keymetrics.io/docs/usage/startup/"/></p>
<p>If you look closely at the output, some useful commands will be printed. The details will vary based on your operating system, because each operating system has its own commands for managing background processes. In this case, the installation is geared to use the <kbd>systemctl</kbd> command, as verified by this output:</p>
<pre>Command list <br/>[ '<strong>systemctl enable pm2-root</strong>', <br/>  '<strong>systemctl start pm2-root</strong>', <br/>  '<strong>systemctl daemon-reload</strong>', <br/>  '<strong>systemctl status pm2-root</strong>' ] <br/>[PM2] Writing init configuration in /etc/systemd/system/pm2-root.service <br/>[PM2] Making script booting at startup... <br/>...<br/>[DONE]  <br/>&gt;&gt;&gt; Executing systemctl start pm2-root <br/>[DONE]  <br/>&gt;&gt;&gt; Executing systemctl daemon-reload <br/>[DONE]  <br/>&gt;&gt;&gt; Executing systemctl status pm2-root<br/></pre>
<p>You are free to run these commands yourself:</p>
<pre><strong># systemctl status pm2-root </strong><br/><strong>● pm2-root.service - PM2 process manager </strong><br/><strong>   Loaded: loaded (/etc/systemd/system/pm2-root.service; enabled; vendor preset: enabled) </strong><br/><strong>   Active: active (running) since Fri 2018-02-02 22:27:45 UTC; 29min ago </strong><br/><strong>     Docs: https://pm2.keymetrics.io/ </strong><br/><strong>  Process: 738 ExecStart=/opt/init/node_modules/pm2/bin/pm2 resurrect (code=exited, status=0/SUCCESS) </strong><br/><strong> Main PID: 873 (PM2 v2.9.3: God) </strong><br/><strong>    Tasks: 30 (limit: 4915) </strong><br/><strong>   Memory: 171.6M </strong><br/><strong>      CPU: 11.528s </strong><br/><strong>   CGroup: /system.slice/pm2-root.service </strong><br/><strong>           ├─873 PM2 v2.9.3: God Daemon (/root/.pm2) </strong><br/><strong>           ├─895 node /opt/users/user-server.mjs </strong><br/><strong>           └─904 node /opt/notes/app.mjs</strong><br/></pre>
<p>To verify that PM2 starts the services as advertised, reboot your server, then use PM2 to check the status:</p>
<div><img src="img/493c7bae-a569-4c71-a6ff-b5600ce2f63a.png" style="width:44.08em;height:18.25em;" width="957" height="396"/></div>
<p>The first thing to notice is that upon initially logging in to the <kbd>root</kbd> account, the <kbd>pm2 status</kbd> command is not available. We installed PM2 locally to <kbd>/opt/init</kbd>, and the command is only available in that directory.</p>
<p>After going to that directory, we can now run the command and see the status. Remember to set the correct IP address or domain name in the <kbd>TWITTER_CALLBACK_HOST</kbd> environment variable. Otherwise, logging in with Twitter will fail.</p>
<p>We now have the Notes application under a fairly good management system. We can easily update its code on the server and restart the service. If the service crashes, PM2 will automatically restart it. Log files are automatically kept for our perusal.</p>
<p>PM2 also supports deployment from the source on our laptop, which we can push to staging or production environments. To support this, we must add deployment information to the <kbd>ecosystem.json</kbd> file and then run the <kbd>pm2 deploy</kbd> command to push the code to the server. See the PM2 website for more information: <a href="http://pm2.keymetrics.io/docs/usage/deployment/">http://pm2.keymetrics.io/docs/usage/deployment/</a>.<a href="http://pm2.keymetrics.io/docs/usage/deployment/"/></p>
<p>While PM2 does a good job at managing server processes, the system we've developed is insufficient for an internet-scale service. What if the Notes application were to become a viral hit and suddenly we need to deploy a million servers spread around the planet? Deploying and maintaining servers one at a time, like this, is not scalable.</p>
<p>We also skipped over implementing the architectural decisions at the beginning. Putting the user authentication data on the same server is a security risk. We want to deploy that data on a different server, under tighter security.</p>
<p>In the next section, we'll explore a new system, Docker, that solves these problems and more.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Node.js microservice deployment with Docker</h1>
                
            
            
                
<p>Docker (<a href="http://docker.com">http://docker.com</a>) is the new attraction in the software industry. Interest is taking off like crazy, spawning many projects, often with names containing puns around shipping containers.</p>
<p>It is described as <em>an open platform for distributed applications for developers and sysadmins</em>. It is designed around Linux containerization technology and focuses on describing the configuration of software on any variant of Linux.</p>
<p>Docker automates the application deployment within software containers. The basic concepts of Linux containers date back to <kbd>chroot</kbd> jail's first implementation in the 1970s, and other systems such as Solaris Zones. The Docker implementation creates a layer of software isolation and virtualization based on Linux cgroups, kernel namespaces, and union-capable filesystems, which blend together to make Docker what it is. That was some heavy geek-speak, so let's try a simpler explanation.</p>
<p>A Docker container is a running instantiation of a Docker image. An image is a given Linux OS and application configuration designed by developers for whatever purpose they have in mind. Developers describe an image using a <strong>Dockerfile</strong>. The Dockerfile is a fairly simple-to-write script showing Docker how to build an image. Docker images are designed to be copied to any server, where the image is instantiated as a Docker container.</p>
<p>A running container will make you feel like you're inside a virtual server running on a virtual machine. But Docker containerization is very different from a virtual machine system such as VirtualBox. The processes running inside the container are actually running on the host OS. The containerization technology (cgroups, kernel namespaces, and so on) create the illusion of running on the Linux variant specified in the Dockerfile, even if the host OS is completely different. Your host OS could be Ubuntu and the container OS could be Fedora or OpenSUSE; Docker makes it all work. </p>
<p>By contrast, with Virtual Machine software (VirtualBox, and VMWare, among others), you're using what feels like a real computer. There is a virtual BIOS and virtualized system hardware, and you must install a full-fledged guest OS. You must follow every ritual of computer ownership, including securing licenses if it's a closed source system such as Windows. </p>
<p>While Docker is primarily targeted at x86 flavors of Linux, it is available on several ARM-based OSes, as well as other processors. You can even run Docker on single-board computers, such as Raspberry Pis, for hardware-oriented Internet of Things projects. Operating systems such as Resin.IO are optimized to solely run Docker containers.</p>
<p>The Docker ecosystem contains many tools, and their number is quickly increasing. For our purposes, we'll be focusing on the following three specific tools:</p>
<ul>
<li><strong>Docker engine</strong>: This is the core execution system that orchestrates everything. It runs on a Linux host system, exposing a network-based API that client applications use to make Docker requests, such as building, deploying, and running containers.</li>
<li><strong>Docker machine</strong>: This is a client application performing functions around provisioning Docker Engine instances on host computers.</li>
<li><strong>Docker compose</strong>: This helps you define, in a single file, a multi-container application, with all its dependencies defined.</li>
</ul>
<p>With the Docker ecosystem, you can create a whole universe of subnets and services to implement your dream application. That universe can run on your laptop or be deployed to a globe-spanning network of cloud-hosting facilities around the world. The surface area through which miscreants can attack is strictly defined by the developer. A multicontainer application will even limit access so strongly between services that miscreants who do manage to break into a container will find it difficult to break out of the container.</p>
<p>Using Docker, we'll first design on our laptop the system shown in the previous diagram. Then we'll migrate that system to a Docker instance on a server.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Installing Docker on your laptop</h1>
                
            
            
                
<p>The best place to learn how to install Docker on your laptop is the Docker documentation website. What we're looking for is the Docker <strong>Community Edition</strong> (<strong>CE</strong>). There is the Docker <strong>Enterprise Edition</strong> (<strong>EE</strong>), with more features and some opportunities to pay support fees:</p>
<ul>
<li>macOS installation – <a href="https://docs.docker.com/docker-for-mac/install/">https://docs.docker.com/docker-for-mac/install/</a><a href="https://docs.docker.com/docker-for-mac/install/"/></li>
<li>Windows installation – <a href="https://docs.docker.com/docker-for-windows/install/">https://docs.docker.com/docker-for-windows/install/</a></li>
<li>Ubuntu installation – <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/">https://docs.docker.com/install/linux/docker-ce/ubuntu/</a></li>
<li>Instructions are available for several other distros. Some useful post-install Linux instructions are at <a href="https://docs.docker.com/install/linux/linux-postinstall/">https://docs.docker.com/install/linux/linux-postinstall/</a></li>
</ul>
<p>Because Docker runs on Linux, it does not run natively on macOS or Windows. Installation on either OS requires installing Linux inside a virtual machine and then running Docker tools within that virtual Linux machine. The days when you had to handcraft that setup yourself are long gone. The Docker team has made this easy by developing easy-to-use Docker applications for Mac and Windows. The Docker for Windows and Docker for Mac bundles package the Docker tools and lightweight virtual machine software. The result is very lightweight, and the Docker containers can be left running in the background with little impact. </p>
<p>You may find references to Docker Toolbox as the method to install Docker on macOS. That application is long gone, and has been replaced by Docker for Windows and Docker for Mac.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Starting Docker with Docker for Windows/macOS</h1>
                
            
            
                
<p>To start Docker for Windows or Mac is very simple. You simply find and double-click on the application icon. It launches as would any other native application. When started, it manages a virtual machine (not VirtualBox) within which is a Linux instance running the Docker Engine. On macOS, a menu bar icon shows up with which you control <kbd>Docker.app</kbd>, and on Windows, an icon is available in the system tray. </p>
<p>There are settings available so that Docker automatically launches every time you start your laptop.</p>
<p>On both, the CPU must support <strong>Virtualization</strong>. Bundled inside Docker for Windows and Docker for Mac is an ultra-lightweight hypervisor, which, in turn, requires virtualization support from the CPU. </p>
<p>For Windows, this may require BIOS configuration. See <a href="https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled">https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled</a>.<a href="https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled"/></p>
<p>For Mac, this requires hardware from 2010 or newer, with Intel’s hardware support for <strong>memory management unit</strong> (MMU) virtualization, including <strong>Extended Page Tables</strong> (<strong>EPT</strong>) and Unrestricted Mode. You can check for this support by running <kbd>sysctl kern.hv_support</kbd>. It also requires macOS 10.11 or later.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Kicking the tires of Docker</h1>
                
            
            
                
<p>With the setup accomplished, we can use the local Docker instance to create Docker containers, run a few commands, and, in general, learn how to use this amazing system.</p>
<p>As in so many software journeys, this one starts with saying <kbd>Hello World</kbd>:</p>
<pre><strong>$ docker run hello-world  </strong><br/><strong>Unable to find image 'hello-world:latest' locally </strong><br/><strong>latest: Pulling from library/hello-world </strong><br/><strong>ca4f61b1923c: Pull complete  </strong><br/><strong>Digest: sha256:66ef312bbac49c39a89aa9bcc3cb4f3c9e7de3788c944158df3ee0176d32b751 </strong><br/><strong>Status: Downloaded newer image for hello-world:latest </strong><br/> <br/><strong>Hello from Docker! </strong><br/><strong>This message shows that your installation appears to be working correctly. </strong><br/> <br/><strong>To generate this message, Docker took the following steps: </strong><br/><strong> 1. The Docker client contacted the Docker daemon. </strong><br/><strong> 2. The Docker daemon pulled the "hello-world" image from the Docker Hub. </strong><br/><strong>    (amd64) </strong><br/><strong> 3. The Docker daemon created a new container from that image which runs the </strong><br/><strong>    executable that produces the output you are currently reading. </strong><br/><strong> 4. The Docker daemon streamed that output to the Docker client, which sent it </strong><br/><strong>    to your terminal. </strong><br/> <br/><strong>To try something more ambitious, you can run an Ubuntu container with: </strong><br/><strong> $ docker run -it ubuntu bash </strong><br/> <br/><strong>Share images, automate workflows, and more with a free Docker ID: </strong><br/><strong> https://cloud.docker.com/ </strong><br/> <br/><strong>For more examples and ideas, visit: </strong><br/><strong> https://docs.docker.com/engine/userguide/</strong><br/></pre>
<p>The <kbd>docker run</kbd> command downloads a Docker image, named on the command line, initializes a Docker container from that image, and then runs that container. In this case, the image, named <kbd>hello-world</kbd>, was not present on the local computer and had to be downloaded and initialized. Once that was done, the <kbd>hello-world</kbd> container was executed and it printed out these instructions.</p>
<p>You can query your computer to see that while the <kbd>hello-world</kbd> container has executed and finished, it still exists:</p>
<div><img src="img/7b6adbc2-daf3-4940-8f95-25f501d4f895.png" width="1425" height="183"/></div>
<p>The <kbd>docker ps</kbd> command lists the running Docker containers. As we see here, the <kbd>hello-world</kbd> container is no longer running, but with the <kbd>-a</kbd> switch, <kbd>docker ps</kbd> also shows those containers that exist but are not currently running. We also see that this computer has a Nextcloud instance installed along with its associated database.</p>
<p>When you're done using a container, you can clean up with the following command:</p>
<pre><strong>$ docker rm boring_lumiere
boring_lumiere<br/></strong></pre>
<p>The name <kbd>boring_lumiere</kbd> is the container name automatically generated by Docker. While the image name was <kbd>hello-world</kbd>, that's not the container name. Docker generated the container name so you have a more user-friendly identifier for the containers than the hex ID shown in the container ID column. When creating a container, it's easy to specify any container name you like.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Creating the AuthNet for the user authentication service</h1>
                
            
            
                
<p>With all that theory spinning around our heads, it's time to do something practical. Let's start by setting up the user authentication service. In the diagram shown earlier, this will be the box labeled AuthNet containing a MySQL instance and the authentication server.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">MySQL container for Docker</h1>
                
            
            
                
<p>To find publicly available Docker images, go to <a href="https://hub.docker.com/">https://hub.docker.com/</a> and search. You'll find many Docker images ready to go. For example, Nextcloud, and its associated database, was shown earlier installed alongside the <kbd>hello-world</kbd> application when we kicked the tires. Both are available from their respective project teams and it's simply (more or less) a matter of typing <kbd>docker run nextcloud</kbd> to install and run the containers. The process of installing Nextcloud, and its associated database, as well as many other packaged applications, such as GitLab, is very similar to what we're about to do to build AuthNet, so the skills you're about to learn are very practical.</p>
<p>Just for MySQL, there are over 11,000 containers available. Fortunately, the two containers provided by the MySQL team are very popular and easy to use. The <kbd>mysql/mysql-server</kbd> image is a little easier to configure, so let's use that. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>A Docker image name can be specified, along with a <em>tag</em> that is usually the software version number. In this case, we'll use <kbd>mysql/mysql-server:5.7</kbd>, where <kbd>mysql/mysql-server</kbd> is the container name, and <kbd>5.7</kbd> is the tag. MySQL 5.7 is the current GA release. Download the image as follows:</p>
<pre><strong>$ docker pull mysql/mysql-server:5.7</strong><br/><strong>5.7: Pulling from mysql/mysql-server</strong><br/><strong>4040fe120662: Pull complete </strong><br/><strong>d049aa45d358: Pull complete </strong><br/><strong>a6c7ed00840d: Pull complete </strong><br/><strong>853789d8032e: Pull complete </strong><br/><strong>Digest: sha256:1b4c7c24df07fa89cdb7fe1c2eb94fbd2c7bd84ac14bd1779e3dec79f75f37c5</strong><br/><strong>Status: Downloaded newer image for mysql/mysql-server:5.7</strong></pre>
<p>This downloaded four images in total, because this image is built on top of three other images. We'll see later how that works when we learn how to build a Dockerfile. </p>
<p>A container can be started using this image as follows:</p>
<pre><strong>$ docker run --name=mysql --env MYSQL_ROOT_PASSWORD=f00bar  mysql/mysql-server:5.7 </strong><br/><strong>[Entrypoint] MySQL Docker Image 5.7.21-1.1.4 </strong><br/><strong>[Entrypoint] Initializing database </strong><br/><strong>[Entrypoint] Database initialized </strong><br/><strong>... </strong><br/><strong>[Entrypoint] ignoring /docker-entrypoint-initdb.d/* </strong><br/><strong>[Entrypoint] Server shut down </strong><br/><strong>[Entrypoint] MySQL init process done. Ready for start up. </strong><br/><strong>[Entrypoint] Starting MySQL 5.7.21-1.1.4</strong><br/></pre>
<p>We started this service in the foreground. The container name is <kbd>mysql</kbd>. We set an environment variable, which, in turn (according to the image documentation), initializes the <kbd>root</kbd> password as shown. In another window, we can get into the container and run the MySQL client as follows:</p>
<pre><strong>$ docker exec -it mysql mysql -u root -p <br/>Enter password:  <br/>Welcome to the MySQL monitor.  Commands end with ; or \g. <br/>Your MySQL connection id is 4 <br/>Server version: 5.7.21 MySQL Community Server (GPL) <br/> <br/>Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. <br/> <br/>Oracle is a registered trademark of Oracle Corporation and/or its <br/>affiliates. Other names may be trademarks of their respective <br/>owners. <br/> <br/>Type 'help;' or '\h' for help. Type '\c' to clear the current input statement. <br/> <br/>mysql&gt; show databases; <br/>+--------------------+ <br/>| Database           | <br/>+--------------------+ <br/>| information_schema | <br/>| mysql              | <br/>| performance_schema | <br/>| sys                | <br/>+--------------------+ <br/>4 rows in set (0.00 sec) <br/> <br/>mysql&gt; <br/></strong></pre>
<p>The <strong><kbd>docker exec</kbd></strong> command lets you run programs inside the container. The <kbd>-it</kbd> option says the command is run interactively, on an assigned terminal. Substitute <kbd>bash</kbd> for <kbd>mysql</kbd>, and you have an interactive <kbd>bash</kbd> command shell.</p>
<p>This <kbd>mysql</kbd> command instance is running inside the container. The container is configured by default to not expose any external port, and it has a default <kbd>my.cnf</kbd> file. <br/></p>
<p>The database files are locked inside the container. As soon as that container is deleted, the database will go away. Docker containers are meant to be ephemeral, being created and destroyed as needed, while databases are meant to be permanent, with lifetimes measured in decades sometimes.</p>
<p>In other words, it's cool that we can easily install and launch a MySQL instance. But there are several deficiencies:</p>
<p> </p>
<ul>
<li>Access to the database from other software</li>
<li>Storing the database files outside the container for a longer lifespan</li>
<li>Custom configuration, because database admins love to tweak the settings</li>
<li>It needs to be connected to AuthNet along with the user authentication service</li>
</ul>
<p>Before proceeding, let's clean up. In a Terminal window, type:</p>
<pre><strong>$ docker stop mysql <br/>mysql<br/></strong><strong>$ docker rm mysql</strong><br/><strong>mysql</strong><br/></pre>
<p>This closes out and cleans up the containers. And, to reiterate the point made earlier, the database in that container went away.  If that database contained critical information, you just lost it with no chance to recover the data.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Initializing AuthNet</h1>
                
            
            
                
<p>Docker supports creating virtual bridge networks between containers. Remember that a Docker container has many of the features of an installed Linux OS. Each container can have its own IP address(es) and exposed ports. Docker supports creating what amounts to being a virtual Ethernet segment, called a <strong>bridge network</strong>. These networks live solely within the host computer and, by default, are not reachable by anything outside the host computer.</p>
<p>A Docker bridge network, therefore, has strictly limited access. Any Docker container attached to a bridge network can communicate with other containers attached to that network. The containers find each other by hostname, and Docker includes an embedded DNS server to set up the hostnames required. That DNS server is configured to not require dots in domain names, meaning that the DNS/hostname of each container is simply the container name, rather than something such as <kbd>container-name.service</kbd>. This policy of using hostnames to identify containers is Docker's implementation of service discovery.</p>
<p>Create a directory named <kbd>authnet</kbd> as a sibling to the <kbd>users</kbd> and <kbd>notes</kbd> directories. We'll be working on AuthNet in that directory.</p>
<p>Create a file, <kbd>buildauthnet.sh</kbd>, containing the following:</p>
<pre>docker network create --driver bridge authnet</pre>
<p>Type the following:</p>
<pre><strong>$ sh -x buildauthnet.sh</strong><br/><strong>+ docker network create --driver bridge authnet </strong><br/><strong>3021e2069278c2acb08d94a2d31507a43f089db1c02eecc97792414b498eb785</strong></pre>
<p>This creates a Docker bridge network.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Script execution on Windows</h1>
                
            
            
                
<p>Executing scripts on Windows is different because it uses PowerShell rather than <kbd>bash</kbd>, and a large number of other considerations. For this, and the scripts which follow, make these changes.</p>
<p>Powershell script filenames must end with the <kbd>.ps1</kbd> extension. For most of these scripts, that's all that is required because the scripts are so simple. To execute the script, simply type <kbd>.\scriptname.ps1</kbd> in the Powershell window. In other words, on Windows, the script just shown must be named <kbd>buildauthnet.ps1</kbd>, and is executed as <kbd>.\buildauthnet.ps1</kbd>.</p>
<p>To execute the scripts, you may need to change the Powershell Execution Policy:</p>
<pre><strong>PS C:\Users\david\chap10\authnet&gt; Get-ExecutionPolicy</strong><br/><strong>Restricted</strong><br/><strong>PS C:\Users\david\chap10\authnet&gt; Set-ExecutionPolicy Unrestricted</strong></pre>
<p>Obviously, there are security considerations with this change, so change the Execution Policy back when you're done.</p>
<p>A simpler method on Windows is to simply paste these commands into a PowerShell window. </p>


            

            
        
    </div>



  
<div><h1 class="header-title">Linking Docker containers</h1>
                
            
            
                
<p>In the older days of Docker, we were told to link containers using the <kbd>--link</kbd> option. With that option, Docker would create entries in <kbd>/etc/hosts</kbd> so that one container can refer to another container by its hostname. That option also arranged access to TCP ports and volumes between linked containers. This allowed the creation of multicontainer services, using private TCP ports for communication that exposed nothing to processes outside the containers.</p>
<p>Today, we are told that the <kbd>--link</kbd> option is a legacy feature, and that instead we should use <kbd>bridge</kbd> networks.  In this chapter, we'll focus solely on using <kbd>bridge</kbd> networks.</p>
<p>You can list the networks as follows:</p>
<pre><strong>$ docker network ls </strong><br/><strong>NETWORK ID          NAME                DRIVER              SCOPE </strong><br/><strong>3021e2069278        authnet             bridge              local</strong><br/></pre>
<p>Look at details about the network with this command:</p>
<pre><strong>$ docker network inspect authnet</strong><br/><strong>    ... much JSON output</strong><br/></pre>
<p>At the moment, this won't show any containers attached to <kbd>authnet</kbd>. The output shows the network name, the IP range of this network, the default gateway, and other useful network configuration information. Since nothing is connected to the network, let's get started with building the required containers.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">The db-userauth container</h1>
                
            
            
                
<p>Now that we have a network, we can start connecting containers to that network. And then we'll explore the containers to see how private they are.</p>
<p>Create a script, <kbd>startdb.sh</kbd>, containing:</p>
<pre>docker run --name db-userauth --env MYSQL_RANDOM_ROOT_PASSWORD=true \ <br/>    --env MYSQL_USER=userauth --env MYSQL_PASSWORD=userauth \ <br/>    --env MYSQL_DATABASE=userauth \ <br/>    --volume `pwd`/my.cnf:/etc/my.cnf \<br/>    --volume `pwd`/../userauth-data:/var/lib/mysql \ <br/>    --network authnet mysql/mysql-server:5.7<br/></pre>
<p>On Windows, you will need to name the script <kbd>startdb.ps1</kbd> instead, and put the text all on one line rather than extend the lines with backslashes. And, the volume mounted on <kbd>/var/lib/mysql</kbd> must be created separately. Use these commands instead:</p>
<div><pre>docker volume create db-userauth-volume<br/><br/>docker run --name db-userauth --env MYSQL_RANDOM_ROOT_PASSWORD=true --env MYSQL_USER=userauth --env MYSQL_PASSWORD=userauth --env MYSQL_DATABASE=userauth --volume $PSScriptRoot\my.cnf:/etc/my.cnf --volume db-userauth-volume:/var/lib/mysql --network authnet mysql/mysql-server:5.7</pre></div>
<p>When run, the container will be named <kbd>db-userauth</kbd>. To give a little bit of security, the <kbd>root</kbd> password has been randomized. We've instead defined a database named <kbd>userauth</kbd>, accessed by a user named <kbd>userauth</kbd>, using the password <kbd>userauth</kbd>. That's not exactly secure, so feel free to choose better names and passwords. The container is attached to the <kbd>authnet</kbd> network.</p>
<p>There are two <kbd>--volume</kbd> options that we must talk about. In Dockerese, a volume is a thing inside a container that can be mounted from outside the container. In this case, we're defining a volume, <kbd>userauth-data</kbd>, in the host filesystem to be mounted as <kbd>/var/lib/mysql</kbd> inside the container. And, we're defining a local <kbd>my.cnf</kbd> file to be used as <kbd>/etc/my.cnf</kbd> inside the container.</p>
<p>For the Windows version, we have two changes to the <kbd>--volume</kbd> mounts. We specify the mount for <kbd>/etc/my.cnf</kbd> as <kbd>$PSScriptRoot\my.cnf:/etc/my.cnf</kbd>, because that's how you reference a local file in Powershell. </p>
<p>For <kbd>/var/lib/mysql,</kbd> we referenced a separately created volume. The volume is created using the <kbd>volume create</kbd> command, and with that command there is no opportunity to control the location of the volume. It's important that the volume lives outside the container, so that the database files survive the destruction/creation cycle for this container. </p>
<p>Taken together, those settings mean the database files and the configuration file live outside the container and will therefore exist beyond the lifetime of one specific container. To get the <kbd>my.cnf</kbd>, you will have to run the container once without the <kbd>--volume `pwd`/my.cnf:/etc/my.cnf</kbd> option so you can copy the default <kbd>my.cnf</kbd> file into the <kbd>authnet</kbd> directory.</p>
<p>Run the script once without that option:</p>
<pre><strong>$ sh startdb.sh </strong><br/><strong>... much output</strong><br/><strong>[Entrypoint] GENERATED ROOT PASSWORD: UMyh@q]@j4qijyj@wK4s4SkePIkq</strong><br/><strong>... much output</strong><br/></pre>
<p>The output is similar to what we saw earlier, but for this newline giving the randomized password:</p>
<pre><strong>$ docker network inspect authnet</strong></pre>
<p>This will tell you the <kbd>db-userauth</kbd> container is attached to <kbd>authnet</kbd>:</p>
<pre><strong>$ docker exec -it db-userauth mysql -u userauth -p <br/>Enter password:  <br/>Welcome to the MySQL monitor.  Commands end with ; or \g.<br/>   ... much output<br/></strong><strong>mysql&gt; show databases; </strong><br/><strong>+--------------------+ </strong><br/><strong>| Database           | </strong><br/><strong>+--------------------+ </strong><br/><strong>| information_schema | </strong><br/><strong>| userauth           | </strong><br/><strong>+--------------------+ </strong><br/><strong>2 rows in set (0.00 sec) </strong><br/> <br/><strong>mysql&gt; use userauth; </strong><br/><strong>Database changed </strong><br/><strong>mysql&gt; show tables; </strong><br/><strong>Empty set (0.00 sec)</strong><br/></pre>
<p>We see our database has been created and it's empty. But we did this so we could grab the <kbd>my.cnf</kbd> file:</p>
<pre><strong>$ docker cp db-userauth:/etc/my.cnf . </strong><br/><strong>$ ls </strong><br/><strong>my.cnf  mysql-data  startdb.sh</strong> <br/></pre>
<p>The <kbd>docker cp</kbd> command is used for copying files in and out of containers. If you've used <kbd>scp</kbd>, the syntax will be familiar.</p>
<p>Once you have the <kbd>my.cnf</kbd> file, there's a big pile of setting changes you might want to make. The first specific change to make is commenting out the line reading <kbd>socket=/var/lib/mysql/mysql.sock</kbd>, and the second is adding a line reading  <kbd>bind-address = 0.0.0.0</kbd>. The purpose with these changes is to configure the MySQL service to listen on a TCP port rather than a Unix domain socket. This makes it possible to communicate with the MySQL service from outside the container. The result would be:</p>
<pre># socket=/var/lib/mysql/mysql.sock <br/>bind-address = 0.0.0.0</pre>
<p>Now stop the <kbd>db-userauth</kbd> service, and remove the container, as we did earlier. Edit the <kbd>startdb</kbd> script to enable the line mounting <kbd>/etc/my.cnf</kbd> into the container, and then restart the container:</p>
<pre><strong>$ docker stop db-userauth <br/>db-userauth<br/>$ docker rm db-userauth   <br/>db-userauth<br/></strong><strong>$ sh ./startdb.sh  </strong><br/><strong>[Entrypoint] MySQL Docker Image 5.7.21-1.1.4 </strong><br/><strong>[Entrypoint] Starting MySQL 5.7.21-1.1.4</strong><br/></pre>
<p>Now, if we inspect the <kbd>authnet</kbd> network, we see the following:</p>
<pre><strong>$ docker network inspect authnet <br/>        "Name": "authnet",<br/>         ...<br/>                    "Subnet": "172.18.0.0/16", <br/>                    "Gateway": "172.18.0.1"<br/>        ...</strong> <br/><strong>        "Containers": { </strong><br/><strong>                "Name": "db-userauth", </strong><br/><strong>                "MacAddress": "02:42:ac:12:00:02", </strong><br/><strong>                "IPv4Address": "172.18.0.2/16",</strong><br/>   ...</pre>
<p>In other words, the <kbd>authnet</kbd> network has the network number <kbd>172.18.0.0/16</kbd>, and the <kbd>db-userauth</kbd> container was assigned <kbd>172.18.0.2</kbd>. This level of detail is rarely important, but it is useful on our first time through to carefully examine the setup so we understand what we're dealing with:</p>
<pre><strong># cat /etc/resolv.conf  </strong><br/><strong>search attlocal.net </strong><br/><strong>nameserver 127.0.0.11 </strong><br/><strong>options ndots:0</strong><br/></pre>
<p>As we said earlier, there is a DNS server running within the Docker bridge network setup, and domain name resolution is configured to use <kbd>nodots</kbd>. That's so Docker container names are the DNS hostname for the container:</p>
<pre><strong># mysql -h db-userauth -u userauth -p </strong><br/><strong>Enter password:  </strong><br/><strong>Welcome to the MySQL monitor.  Commands end with ; or \g. </strong><br/><strong>Your MySQL connection id is 33 </strong><br/><strong>Server version: 5.7.21 MySQL Community Server (GPL)</strong><br/></pre>
<p>Access the MySQL server using the container name as the hostname.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Dockerfile for the authentication service</h1>
                
            
            
                
<p>In the <kbd>users</kbd> directory, create a file named <kbd>Dockerfile</kbd> containing the following:</p>
<pre><strong>FROM node:10</strong><br/><br/><strong>ENV DEBUG="users:*" </strong><br/><strong>ENV PORT="3333" </strong><br/><strong>ENV SEQUELIZE_CONNECT="sequelize-docker-mysql.yaml" </strong><br/><strong>ENV REST_LISTEN="0.0.0.0" </strong><br/> <br/><strong>RUN mkdir -p /userauth</strong><br/><strong>COPY package.json sequelize-docker-mysql.yaml *.mjs *.js /userauth/</strong><br/><strong>WORKDIR /userauth</strong><br/><strong>RUN apt-get update -y \</strong><br/><strong>    &amp;&amp; apt-get -y install curl python build-essential git ca-certificates \</strong><br/><strong>    &amp;&amp; npm install --unsafe-perm </strong><br/> <br/><strong>EXPOSE 3333 </strong><br/><strong>CMD npm run docker</strong> </pre>
<p>Dockerfiles describe the installation of an application on a server. See <a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a> for documentation. They document assembly of the bits in a Docker container image, and the instructions in a Dockerfile are used to build a Docker image. </p>
<p>The <kbd>FROM</kbd> command specifies a pre-existing image from which to derive a given image. We talked about this earlier; you can build a Docker container starting from an existing image. The official Node.js Docker image (<a href="https://hub.docker.com/_/node/">https://hub.docker.com/_/node/</a>) we're using is derived from <kbd>debian:jessie</kbd>. Therefore, commands available within the container are what Debian offers, and we use <kbd>apt-get</kbd> to install more packages. We use Node.js 10  because it supports ES6 modules and the other features we've been using.</p>
<p>The <kbd>ENV</kbd> commands define environment variables. In this case, we're using the same environment variables defined within the user authentication service, except we have a new <kbd>REST_LISTEN</kbd> variable. We'll take a look at that shortly.</p>
<p>The <kbd>RUN</kbd> commands are where we run the shell commands required to build the container. The first thing is to make a <kbd>/userauth</kbd> directory that will contain the service source code. The <kbd>COPY</kbd> command copies files into that directory. And then we'll need to run an <kbd>npm install</kbd> so that we can run the service. But first we use the <kbd>WORKDIR</kbd> command to move the current working directory into <kbd>/userauth</kbd> so that the <kbd>npm install</kbd> is run in the correct place. We also install the requisite Debian packages so that any native code Node.js packages can be installed.</p>
<p>It's recommended that you always combine <kbd>apt-get update</kbd> with <kbd>apt-get install</kbd> in the same command line, like this, because of the Docker build cache. When rebuilding an image, Docker starts with the first changed line. By putting those two together, you ensure that <kbd>apt-get update</kbd> is executed any time you change the list of packages to be installed. For a complete discussion, see the documentation at <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/">https://docs.docker.com/develop/develop-images/dockerfile_best-practices/</a>.</p>
<p>At the end of this command is <kbd>npm install --unsafe-perm</kbd>. The issue here is that these commands are being run as <kbd>root</kbd>. Normally, when <kbd>npm</kbd> is run as <kbd>root</kbd>, it changes its user ID to a nonprivileged user. This can cause failure, however, and the <kbd>--unsafe-perm</kbd> option prevents changing the user ID.</p>
<p>The <kbd>EXPOSE</kbd> command informs Docker that the container listens on the named TCP port. This does not expose the port beyond the container.</p>
<p>Finally, the CMD command documents the process to launch when the container is executed. The <kbd>RUN</kbd> commands are executed while building the container, while CMD says what's executed when the container starts.</p>
<p>We could have installed PM2 in the container, then used a PM2 command to launch the service.  But Docker is able to fulfill the same function, because it supports automatically restarting a container if the service process dies. We'll see how to do this later.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Configuring the authentication service for Docker</h1>
                
            
            
                
<p>We're using a different file for <kbd>SEQUELIZE_CONNECT</kbd>. Create a new file named <kbd>users/sequelize-docker-mysql.yaml</kbd> containing the following:</p>
<pre>dbname: userauth 
username: userauth 
password: userauth 
params: 
    host: db-userauth 
    port: 3306 
    dialect: mysql </pre>
<p>The difference is that instead of <kbd>localhost</kbd> as the database host, we use <kbd>db-userauth</kbd>. Earlier, we explored the <kbd>db-userauth</kbd> container and determined that was the hostname of the container. By using <kbd>db-userauth</kbd> in this file, the authentication service will use the database in the container.</p>
<p>Now we need to take care of the environment variable named <kbd>REST_LISTEN</kbd>. Previously, the authentication server had listened only to <kbd>http://localhost:3333</kbd>. We'd done this for security purposes, that is, to limit which processes could connect to the service. Under Docker, we need to connect to this service from outside its container so that other containers can connect to this service. Therefore, it must listen to connections from outside the localhost. </p>
<p>In <kbd>users-server.mjs</kbd>, we need to make the following change:</p>
<pre>server.listen(process.env.PORT, 
  process.env.REST_LISTEN ? process.env.REST_LISTEN : "localhost", 
  () =&gt; { log(server.name +' listening at '+ server.url); }); </pre>
<p>That is, if the <kbd>REST_LISTEN</kbd> variable exists, the REST server is told to listen to whatever it says, otherwise the service is to listen to <kbd>localhost</kbd>. With the environment variable in the Dockerfile, the authentication service will listen to the world (<kbd>0.0.0.0</kbd>). Are we throwing caution to the wind and abrogating our fiduciary duty in keeping the sacred trust of storing all this user identification information? No. Be patient. We'll describe momentarily how to connect this service and its database to <kbd>AuthNet</kbd> and will prevent access to <kbd>AuthNet</kbd> by any other process.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Building and running the authentication service Docker container</h1>
                
            
            
                
<p>In <kbd>users/package.json</kbd> add the following line to the <kbd>scripts</kbd> section:</p>
<pre>"docker": "node --experimental-modules ./user-server",<br/>"docker-build": "docker build -t node-web-development/userauth ."</pre>
<p>Previously, we've put the configuration environment variables into <kbd>package.json</kbd>. In this case, the configuration environment variables are in the Dockerfile. This means we need a way to run the server with no environment variables other than those in the Dockerfile. With this <kbd>scripts</kbd>entry, we can do <kbd>npm run docker</kbd> and then the Dockerfile environment variables will supply all configuration.</p>
<p>We can build the authentication service as follows:</p>
<pre><strong>$ npm run docker-build</strong><br/><br/><strong>&gt; user-auth-server@0.0.1 docker-build /Users/david/chap10/users</strong><br/><strong>&gt; docker build -t node-web-development/userauth .</strong><br/><br/><strong>Sending build context to Docker daemon 33.8MB</strong><br/><strong>Step 1/11 : FROM node:9.5</strong><br/><strong> ---&gt; a696309517c6</strong><br/><strong>Step 2/11 : ENV DEBUG="users:*"</strong><br/><strong> ---&gt; Using cache</strong><br/><strong> ---&gt; f8cc103432e8</strong>
<br/><strong>Step 3/11 : ENV PORT="3333"</strong><br/><strong> ---&gt; Using cache</strong><br/><strong> ---&gt; 39b24b8b554e</strong><br/><strong>... more output</strong></pre>
<p>The <kbd>docker build</kbd> command builds a container from a Dockerfile. As we said earlier, the process begins with the image defined in the <kbd>FROM</kbd> command. Then the build proceeds step by step, and the output shows literally each step as it is executed.</p>
<p>Then create a script, <kbd>authnet/startserver.sh</kbd>, or on Windows call it <kbd>startserver.ps1</kbd>, containing the following command:</p>
<div><pre>docker run -it --name userauth --net=authnet node-web-development/userauth</pre></div>
<p>This launches the newly built container, giving it the name <kbd>userauth</kbd>, attaching it to <kbd>authnet</kbd>:</p>
<pre><strong>$ sh -x startserver.sh </strong><br/><strong>+ docker run -it --name userauth --net=authnet node-web-development/userauth</strong><br/><br/><strong>&gt; user-auth-server@0.0.1 docker /userauth</strong><br/><strong>&gt; node --experimental-modules ./user-server</strong><br/><br/><strong>(node:17) ExperimentalWarning: The ESM module loader is experimental.</strong><br/><strong>  users:service User-Auth-Service listening at http://0.0.0.0:3333 +0ms</strong><br/></pre>
<p>That starts the user authentication service. On Windows, start it as <kbd>.\startserver.ps1</kbd>. You should recall that it's a REST service, and therefore running it through its paces is done with <kbd>users-add.js</kbd> and the other scripts. But, since we did not expose a public port from the service we must run those scripts from inside the container.</p>
<p>We determine whether a container exposes a public port in one of two ways. The easiest is running <kbd>docker ps -a</kbd> and viewing the container listing details. There is a column marked PORTS, and for <kbd>userauth</kbd> we see <kbd>3333/tcp</kbd>. This is a side effect of the EXPOSE command in the Dockerfile. If that port were exposed, it would appear in the PORTS column as <kbd>0.0.0.0:3333-&gt;3333/tcp</kbd>. Remember the goal for the <kbd>userauth</kbd> container, and <kbd>authnet</kbd> overall, was that it would not be publicly accessible because of security concerns.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Exploring Authnet</h1>
                
            
            
                
<p>Let's explore what we just created:</p>
<pre><strong>$ docker network inspect authnet </strong> </pre>
<p>This prints out a large JSON object describing the network, and its attached containers, which we've looked at before. If all went well, we'll see there are now two containers attached to <kbd>authnet</kbd> where there'd previously been only one.</p>
<p>Let's go into the <kbd>userauth</kbd> container and poke around:</p>
<pre><strong>$ docker exec -it userauth bash</strong><br/><strong>root@a29d833287bf:/userauth# ls </strong><br/><strong>node_modules                 user-server.mjs  users-list.js </strong><br/><strong>package-lock.json            users-add.js     users-sequelize.mjs </strong><br/><strong>package.json                 users-delete.js </strong><br/><strong>sequelize-docker-mysql.yaml  users-find.js</strong><br/></pre>
<p>The <kbd>/userauth</kbd> directory is inside the container and is exactly the files placed in the container using the <kbd>COPY</kbd> command, plus the installed files in <kbd>node_modules:</kbd></p>
<pre><strong>root@a29d833287bf:/userauth# PORT=3333 node users-list.js  </strong><br/><strong>List [] </strong><br/><strong>root@a29d833287bf:/userauth# PORT=3333 node users-add.js  </strong><br/><strong>Created { id: 1, username: 'me', password: 'w0rd', provider: 'local', </strong><br/><strong>  familyName: 'Einarrsdottir', givenName: 'Ashildr', </strong><br/><strong>  middleName: '', emails: '[]', photos: '[]', </strong><br/><strong>  updatedAt: '2018-02-05T01:54:53.320Z', createdAt: '2018-02-</strong><br/><strong>  05T01:54:53.320Z' } </strong><br/><strong>root@a29d833287bf:/userauth# PORT=3333 node users-list.js </strong><br/><strong>List [ { id: 'me', username: 'me', provider: 'local', </strong><br/><strong>    familyName: 'Einarrsdottir', givenName: 'Ashildr', middleName: '', </strong><br/><strong>    emails: '[]', photos: '[]' } ]</strong><br/></pre>
<p>Our test of adding a user to the authentication service works:</p>
<pre><strong>root@a29d833287bf:/userauth# ps -eafw </strong><br/><strong>UID        PID  PPID  C STIME TTY          TIME CMD </strong><br/><strong>root         1     0  0 01:52 pts/0    00:00:00 /bin/sh -c npm run docker </strong><br/><strong>root         9     1  0 01:52 pts/0    00:00:00 npm                                </strong><br/><strong>root        19     9  0 01:52 pts/0    00:00:00 sh -c node --experimental-modules ./user-server </strong><br/><strong>root        20    19  0 01:52 pts/0    00:00:01 node --experimental-modules ./user-server </strong><br/><strong>root        30     0  0 01:54 pts/1    00:00:00 bash </strong><br/><strong>root        70    30  0 01:57 pts/1    00:00:00 ps -eafw </strong><br/><strong>root@a29d833287bf:/userauth# ping db-userauth </strong><br/><strong>PING db-userauth (172.18.0.2): 56 data bytes </strong><br/><strong>64 bytes from 172.18.0.2: icmp_seq=0 ttl=64 time=0.105 ms </strong><br/><strong>64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.077 ms </strong><br/><strong>^C--- db-userauth ping statistics --- </strong><br/><strong>2 packets transmitted, 2 packets received, 0% packet loss </strong><br/><strong>round-trip min/avg/max/stddev = 0.077/0.091/0.105/0.000 ms </strong><br/><strong>root@a29d833287bf:/userauth# ping userauth </strong><br/><strong>PING userauth (172.18.0.3): 56 data bytes </strong><br/><strong>64 bytes from 172.18.0.3: icmp_seq=0 ttl=64 time=0.132 ms </strong><br/><strong>64 bytes from 172.18.0.3: icmp_seq=1 ttl=64 time=0.095 ms </strong><br/><strong>^C--- userauth ping statistics --- </strong><br/><strong>2 packets transmitted, 2 packets received, 0% packet loss</strong><br/></pre>
<p>The process listing is interesting to study. Process PID 1 is the <kbd>npm run docker</kbd> command in the Dockerfile. Processes proceed from there to the <kbd>node</kbd> process running the actual server.</p>
<p>A <kbd>ping</kbd> command proves the two containers are available as hostnames matching the container names.</p>
<p>Then, you can log in to the <kbd>db-userauth</kbd> container and inspect the database:</p>
<pre><strong>$ docker exec -it db-userauth bash <br/>bash-4.2# mysql -u userauth -p <br/>Enter password:  <br/>Welcome to the MySQL monitor.  Commands end with ; or \g.<br/> ...<br/></strong><strong>mysql&gt; use userauth </strong><br/> <br/><strong>Database changed </strong><br/><strong>mysql&gt; show tables; </strong><br/><strong>+--------------------+ </strong><br/><strong>| Tables_in_userauth | </strong><br/><strong>+--------------------+ </strong><br/><strong>| Users              | </strong><br/><strong>+--------------------+ </strong><br/><strong>1 row in set (0.00 sec) </strong><br/> <br/><strong>mysql&gt; select * from Users; </strong><br/><strong>+----+----------+----------+----------+---------------+-----------+--...</strong><br/><strong>| id | username | password | provider | familyName    | givenName |  ...</strong><br/><strong>+----+----------+----------+----------+---------------+-----------+--...</strong><br/><strong>|  1 | me       | w0rd     | local    | Einarrsdottir | Ashildr   |  ...</strong><br/><strong>+----+----------+----------+----------+---------------+-----------+--...</strong><br/><strong>1 row in set (0.00 sec)</strong><br/></pre>
<p>We have successfully Dockerized the user authentication service in two containers, <kbd>db-userauth</kbd> and <kbd>userauth</kbd>. We've poked around the insides of a running container and found some interesting things. But, our users need the fantastic Notes application to be running, and we can't afford to rest on our laurels.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Creating FrontNet for the Notes application</h1>
                
            
            
                
<p>We have the back half of our system set up in a Docker container, as well as the private bridge network to connect the backend containers. We now need to set up another private bridge network, <kbd>frontnet</kbd>, and attach the other half of our system to that network.</p>
<p>Create a directory, <kbd>frontnet</kbd>, which is where we'll develop the tools to build and run that network. In that directory, create a file, <kbd>buildfrontnet.sh</kbd>, or on Windows, <kbd>buildfrontnet.ps1</kbd>, containing:</p>
<pre><strong>docker network create --driver bridge frontnet</strong></pre>
<p>Let's go ahead and create the <kbd>frontnet</kbd> bridge network:</p>
<pre><strong>$ sh -x buildfrontnet.sh </strong><br/><strong>+ docker network create --driver bridge frontnet</strong><br/><strong>f3df227d4bfff57bc7aed1e096a2ad16f6cebce4938315a54d9386a42d1ae3ed</strong><br/><strong>$ docker network ls</strong><br/><strong>NETWORK ID NAME DRIVER SCOPE</strong><br/><strong>3021e2069278 authnet bridge local</strong><br/><strong>f3df227d4bff frontnet bridge local </strong> </pre>
<p>We'll proceed from here similarly to how <kbd>authnet</kbd> was created. However, we can work more quickly because we've already gone over the basics.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">MySQL container for the Notes application</h1>
                
            
            
                
<p>From the <kbd>authnet</kbd> directory, copy the <kbd>my.cnf</kbd> and <kbd>startdb.sh</kbd> files into the <kbd>frontnet</kbd> directory.</p>
<p>The <kbd>my.cnf</kbd> file can probably be used unmodified, but we have a few changes to make to the <kbd>startdb.sh</kbd> file:</p>
<div><pre><strong>docker run --name db-notes --env MYSQL_RANDOM_ROOT_PASSWORD=true \<br/> --env MYSQL_USER=notes --env MYSQL_PASSWORD=notes12345 \<br/> --env MYSQL_DATABASE=notes \<br/> --volume `pwd`/my.cnf:/etc/my.cnf \<br/> --volume `pwd`/../notes-data:/var/lib/mysql \<br/> --network frontnet mysql/mysql-server:5.7</strong></pre></div>
<p>On Windows, name the file <kbd>startdb.ps1</kbd> containing this:</p>
<div><pre><strong>docker volume create notes-data-volume<br/><br/>docker run --name db-notes --env MYSQL_RANDOM_ROOT_PASSWORD=true --env MYSQL_USER=notes --env MYSQL_PASSWORD=notes12345 --env MYSQL_DATABASE=notes --volume $PSScriptRoot\my.cnf:/etc/my.cnf --volume notes-data-volume:/var/lib/mysql --network frontnet mysql/mysql-server:5.7</strong></pre>
<p>The changes are simple substitutions to transliterate from <kbd>userauth</kbd> to <kbd>notes</kbd>. And then run it:</p>
</div>
<pre><strong>$ mkdir ../notes-data</strong><br/><strong>$ sh -x startdb.sh </strong><br/><strong>+ pwd</strong><br/><strong>+ pwd</strong><br/><strong>+ docker run --name db-notes --env MYSQL_RANDOM_ROOT_PASSWORD=true --env MYSQL_USER=notes --env MYSQL_PASSWORD=notes12345 --env MYSQL_DATABASE=notes --volume /home/david/nodewebdev/node-web-development-code-4th-edition/chap10/frontnet/my.cnf:/etc/my.cnf --volume /home/david/nodewebdev/node-web-development-code-4th-edition/chap10/frontnet/../notes-data:/var/lib/mysql --network frontnet mysql/mysql-server:5.7</strong><br/><strong>[Entrypoint] MySQL Docker Image 5.7.21-1.1.4</strong><br/><strong>[Entrypoint] Initializing database</strong><br/><strong>[Entrypoint] Database initialized</strong><br/><strong>[Entrypoint] GENERATED ROOT PASSWORD: 3kZ@q4hBItYGYj3Mes!AdiP83Nol</strong><br/><strong>[Entrypoint] ignoring /docker-entrypoint-initdb.d/*</strong><br/><strong>[Entrypoint] Server shut down</strong><br/><strong>[Entrypoint] MySQL init process done. Ready for start up.</strong><br/><strong>[Entrypoint] Starting MySQL 5.7.21-1.1.4</strong></pre>
<p>For Windows, simply run <kbd>.\startdb.ps1</kbd>.</p>
<p>This database will be available at the <kbd>db-notes</kbd> domain name on <kbd>frontnet</kbd>. Because it's attached to <kbd>frontnet</kbd>, it won't be reachable by containers connected to <kbd>authnet</kbd>.</p>
<pre><strong>$ docker exec -it userauth bash    </strong><br/><strong>root@0a2009334b79:/userauth# ping db-notes </strong><br/><strong>ping: unknown host</strong><br/></pre>
<p>Since <kbd>db-notes</kbd> is on a different network segment, we've achieved separation.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Dockerizing the Notes application</h1>
                
            
            
                
<p>In the <kbd>notes</kbd> directory, create a file named <kbd>Dockerfile</kbd> containing the following:</p>
<pre><strong>FROM node:10 </strong><br/> <br/><strong>ENV DEBUG="notes:*,messages:*" </strong><br/><strong>ENV SEQUELIZE_CONNECT="models/sequelize-docker-mysql.yaml" </strong><br/><strong>ENV NOTES_MODEL="sequelize" </strong><br/><strong>ENV USER_SERVICE_URL="http://userauth:3333" </strong><br/><strong>ENV PORT="3000" </strong><br/><strong>ENV NOTES_SESSIONS_DIR="/sessions" </strong><br/><strong># ENV TWITTER_CONSUMER_KEY="..."</strong><br/><strong># ENV TWITTER_CONSUMER_SECRET="..."</strong><br/><strong># Use this line when the Twitter Callback URL</strong><br/><strong># has to be other than localhost:3000</strong><br/><strong># ENV TWITTER_CALLBACK_HOST=http://45.55.37.74:3000 </strong><br/> <br/><strong>RUN mkdir -p /notesapp /notesapp/minty /notesapp/partials /notesapp/public /notesapp/routes /notesapp/theme /notesapp/views</strong><br/><strong>COPY minty/ /notesapp/minty/</strong><br/><strong>COPY models/*.mjs models/sequelize-docker-mysql.yaml /notesapp/models/</strong><br/><strong>COPY partials/ /notesapp/partials/</strong><br/><strong>COPY public/ /notesapp/public/</strong><br/><strong>COPY routes/ /notesapp/routes/</strong><br/><strong>COPY theme/ /notesapp/theme/</strong><br/><strong>COPY views/ /notesapp/views/</strong><br/><strong>COPY app.mjs package.json /notesapp/</strong><br/><br/><strong>WORKDIR /notesapp</strong><br/><strong>RUN apt-get update -y \</strong><br/><strong>    &amp;&amp; apt-get -y install curl python build-essential git ca-certificates \</strong><br/><strong>    &amp;&amp; npm install --unsafe-perm</strong><br/><br/><strong># Uncomment to build the theme directory</strong><br/><strong># WORKDIR /notesapp/theme</strong><br/><strong># RUN npm run download &amp;&amp; npm run build &amp;&amp; npm run clean</strong><br/><br/><strong>WORKDIR /notesapp</strong><br/><br/><strong>VOLUME /sessions </strong><br/><strong>EXPOSE 3000 </strong><br/><strong>CMD node --experimental-modules ./app</strong></pre>
<p>This is similar to the Dockerfile we used for the authentication service. We're using the environment variables from <kbd>notes/package.json</kbd>, plus a new one, and there's a couple of new tricks involved here, so let's take a look.</p>
<p>The most obvious change is the number of <kbd>COPY</kbd> commands. The Notes application is a lot more involved given the number of subdirectories full of files that must be installed.  We start by creating the top-level directories of the Notes application deployment tree. Then, one by one, we copy each subdirectory into its corresponding subdirectory in the container filesystem.</p>
<p>In a <kbd>COPY</kbd> command, the trailing slash on the destination directory is important. Why?  Because the documentation says that the trailing slash is important.</p>
<p>The big question is: Why use multiple <kbd>COPY</kbd> commands such as this?  This would have been trivially simple:</p>
<pre><strong>COPY . /notesapp</strong></pre>
<p>But, it is important to avoid copying the <kbd>node_modules</kbd> directory into the container. The container <kbd>node_modules</kbd> must be built inside the container, because the container operating system is almost certainly different to the host operating system. Any native code modules must be built for the correct operating system. That constraint led to the question of concisely copying specific files to the destination.</p>
<p>We've developed a process to build a Bootstrap 4 theme, which we developed in <a href="">Chapter 6</a>,<em> Implementing the Mobile-First Paradigm</em>.  If you have a Bootstrap 4 theme to build, simply uncomment the corresponding lines in the Dockerfile. Those lines move the working directory to <kbd>/notesapp/theme</kbd> and then run the scripts to build the theme. A new script is required in <kbd>theme/package.json</kbd> to remove the <kbd>theme/node_modules</kbd> directory after the theme has been built:</p>
<div><pre> "scripts": {<br/>    ...<br/>    "clean": "rm -rf bootstrap-4.0.0/node_modules"<br/>    ...<br/>}</pre></div>
<p>We also have a new <kbd>SEQUELIZE_CONNECT</kbd> file. Create <kbd>models/sequelize-docker-mysql.yaml</kbd> containing the following:</p>
<pre>dbname: notes 
username: notes 
password: notes12345
params: 
    host: db-notes 
    port: 3306 
    dialect: mysql </pre>
<p>This will access a database server on the <kbd>db-notes</kbd> domain name using the named database, username, and password. </p>
<p>Notice that the <kbd>USER_SERVICE_URL</kbd> variable no longer accesses the authentication service at <kbd>localhost</kbd>, but at <kbd>userauth</kbd>. The <kbd>userauth</kbd> domain name is currently only advertised by the DNS server on AuthNet, but the Notes service is on FrontNet. This means we'll have to connect the <kbd>userauth</kbd> container to the FrontNet bridge network so that its name is known there as well. We'll get to that in a minute.</p>
<p>In <a href="19304f2b-9d3e-45a4-981b-1b7cd269895b.xhtml">Chapter 8</a><em>, Multiuser Authentication the Microservice Way</em> we discussed the need to protect the API keys supplied by Twitter.</p>
<p>We didn't want to commit the keys in the source code, but they have to go somewhere.  Placeholders are in the Dockerfile for specifying <kbd>TWITTER_CONSUMER_KEY</kbd> and <kbd>TWITTER_CONSUMER_SECRET</kbd>.</p>
<p>The value for <kbd>TWITTER_CALLBACK_HOST</kbd> needs to reflect where Notes is deployed. Right now, it is still on your laptop, but by the end of the chapter, it will be deployed to the server, and, at that time, it will need the IP address or domain name of the server.</p>
<p>A new variable is <kbd>NOTES_SESSIONS_DIR</kbd> and the matching <kbd>VOLUME</kbd> declaration. If we were to run multiple Notes instances, they could share session data by sharing this volume.</p>
<p>Supporting the <kbd>NOTES_SESSIONS_DIR</kbd> variable requires one change in <kbd>app.mjs</kbd>:</p>
<pre>const sessionStore  = new FileStore({ 
    path: process.env.NOTES_SESSIONS_DIR ?             
          process.env.NOTES_SESSIONS_DIR : "sessions" 
}); </pre>
<p>Instead of a hardcoded directory name, we can use an environment variable to define the location where session data is stored. Alternatively, there are <kbd>sessionStore</kbd> implementations for various servers such as REDIS, enabling session data sharing between containers on separate host systems.</p>
<p>In <kbd>notes/package.json,</kbd> add these scripts:</p>
<pre>"scripts": {<br/>    ...<br/>    "docker": "node --experimental-modules ./app",<br/>    "docker-build": "docker build -t node-web-development/notes ."<br/>    ...<br/>}</pre>
<div><p>As for the authentication server, this lets us build the container and then, within the container, we can run the service. </p>
</div>
<p>Now we can build the container image:</p>
<pre><strong>$ npm run docker-build</strong><br/><br/><strong>&gt; notes@0.0.0 docker-build /Users/david/chap10/notes</strong><br/><strong>&gt; docker build -t node-web-development/notes .</strong><br/><br/><strong>Sending build context to Docker daemon 76.27MB</strong><br/><strong>Step 1/22 : FROM node:9.5</strong><br/><strong> ---&gt; a696309517c6</strong><br/><strong>Step 2/22 : ENV DEBUG="notes:*,messages:*"</strong><br/><strong> ---&gt; Using cache</strong><br/><strong> ---&gt; 8628ecad9fa4</strong></pre>
<p>Next, in the <kbd>frontnet</kbd> directory, create a file named <kbd>startserver.sh</kbd>, or, on Windows, <kbd>startserver.ps1</kbd>:</p>
<div><pre>docker run -it --name notes --net=frontnet -p 3000:3000 node-web-development/notes</pre></div>
<p>Unlike the authentication service, the Notes application container must export a port to the public. Otherwise, the public will never be able to enjoy this wonderful creation we're building. The <kbd>-p</kbd> option is how we instruct Docker to expose a port. </p>
<p>The first number is a TCP port number published from the container, and the second number is the TCP port inside the container. Generally speaking, this option maps a port inside the container to one reachable by the public.</p>
<p>Then run it as follows:</p>
<pre><strong>$ sh -x startserver.sh </strong><br/><strong>+ docker run -it --name notes --net=frontnet -p 3000:3000 node-web-development/notes</strong><br/><strong>(node:6) ExperimentalWarning: The ESM module loader is experimental.</strong><br/><strong>  notes:debug-INDEX Listening on port 3000 +0ms</strong></pre>
<p>On Windows, run <kbd>.\startserver.ps1</kbd>.</p>
<p>At this point, we can connect our browser to <kbd>http://localhost:3000</kbd> and start using the Notes application. But we'll quickly run into a problem:</p>
<div><img src="img/030b00fc-4919-4709-bbf5-37d41b7e5b6d.png" style="width:39.67em;height:11.83em;" width="826" height="246"/></div>
<p>The user experience team is going to scream about this ugly error message, so put it on your backlog to generate a prettier error screen. For example, a flock of birds pulling a whale out of the ocean is popular.</p>
<p>This error means that Notes cannot access anything at the host named <kbd>userauth</kbd>. That host does exist, because the container is running, but it's not on <kbd>frontnet</kbd>, and is not reachable from the <kbd>notes</kbd> container. Namely:</p>
<pre><strong>$ docker exec -it notes bash </strong><br/><strong>root@125a196c3fd5:/notesapp# ping userauth </strong><br/><strong>ping: unknown host </strong><br/><strong>root@125a196c3fd5:/notesapp# ping db-notes </strong><br/><strong>PING db-notes (172.19.0.2): 56 data bytes </strong><br/><strong>64 bytes from 172.19.0.2: icmp_seq=0 ttl=64 time=0.136 ms </strong><br/><strong>^C--- db-notes ping statistics --- </strong><br/><strong>1 packets transmitted, 1 packets received, 0% packet loss </strong><br/><strong>round-trip min/avg/max/stddev = 0.136/0.136/0.136/0.000 ms </strong><br/><strong>root@125a196c3fd5:/notesapp#</strong> <br/></pre>
<p>If you inspect FrontNet and AuthNet, you'll see the containers attached to each do not overlap:</p>
<pre><strong>$ docker network inspect frontnet</strong><br/><strong>$ docker network inspect authnet</strong></pre>
<p>In the architecture diagram at the beginning of the chapter, we showed a connection between the <kbd>notes</kbd> and <kbd>userauth</kbd> containers. The connection is required so <kbd>notes</kbd> can authenticate its users. But that connection does not exist, yet.</p>
<p>Unfortunately, a simple change to <kbd>startserver.sh</kbd> (<kbd>startserver.ps1</kbd>) does not work:</p>
<pre><strong>docker run -it --name notes --net=authnet --net=frontnet -p 3000:3000 node-web-development/notes</strong></pre>
<p>While it is conceptually simple to specify multiple <kbd>--net</kbd> options when starting a container, Docker does not support this. It silently accepts the command as shown, but only connects the container to the last network mentioned in the options. Instead, Docker requires that you take a second step to attach the container to a second network:</p>
<pre><strong>$ docker network connect authnet notes</strong></pre>
<p>With no other change, the Notes application will now allow you to log in and start adding and editing notes. </p>
<p>There is a glaring architecture question staring at us. Do we connect the <kbd>userauth</kbd> service to <kbd>frontnet</kbd>, or do we connect the <kbd>notes</kbd> service to <kbd>authnet</kbd>?    To verify that either direction solves the problem, run these commands:</p>
<pre><strong>$ docker network disconnect authnet notes </strong><br/><strong>$ docker network connect frontnet userauth</strong>
 </pre>
<p>The first time around, we connected <kbd>notes</kbd> to <kbd>authnet</kbd>, then we disconnected it from <kbd>authnet</kbd>, and then connected <kbd>userauth</kbd> to <kbd>frontnet</kbd>. That means we tried both combinations and, as expected, in both cases <kbd>notes</kbd> and <kbd>userauth</kbd> were able to communicate.</p>
<p>This is a question for security experts since the consideration is the attack vectors available to any intruders. Suppose Notes has a security hole allowing an invader to gain access. How do we limit what is reachable via that hole?  </p>
<p>The primary observation is that by connecting <kbd>notes</kbd> to <kbd>authnet</kbd>, <kbd>notes</kbd> not only has access to <kbd>userauth</kbd>, but also to <kbd>db-userauth</kbd>:</p>
<pre><strong>$ docker network disconnect frontnet userauth</strong><br/><strong>$ docker network connect authnet notes</strong><br/><strong>$ docker exec -it notes bash</strong><br/><strong>root@7fce818e9a4d:/notesapp# ping userauth</strong><br/><strong>PING userauth (172.18.0.3): 56 data bytes</strong><br/><strong>64 bytes from 172.18.0.3: icmp_seq=0 ttl=64 time=0.103 ms</strong><br/><strong>^C--- userauth ping statistics ---</strong><br/><strong>1 packets transmitted, 1 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max/stddev = 0.103/0.103/0.103/0.000 ms</strong><br/><strong>root@7fce818e9a4d:/notesapp# ping db-userauth</strong><br/><strong>PING db-userauth (172.18.0.2): 56 data bytes</strong><br/><strong>64 bytes from 172.18.0.2: icmp_seq=0 ttl=64 time=0.201 ms</strong><br/><strong>^C--- db-userauth ping statistics ---</strong><br/><strong>1 packets transmitted, 1 packets received, 0% packet loss</strong><br/><strong>round-trip min/avg/max/stddev = 0.201/0.201/0.201/0.000 ms</strong><br/><strong>root@7fce818e9a4d:/notesapp#</strong> </pre>
<p>This sequence reconnects <kbd>notes</kbd> to <kbd>authnet</kbd>, and demonstrates the ability to access both the <kbd>userauth</kbd> and <kbd>db-userauth</kbd> containers. Therefore, a successful invader could access the <kbd>db-userauth</kbd> database, a result we wanted to prevent. Our diagram at the beginning showed no such connection between <kbd>notes</kbd> and <kbd>db-userauth</kbd>.</p>
<p>Given that our goal for using Docker was to limit the attack vectors, we have a clear distinction between the two container/network connection setups. Attaching <kbd>userauth</kbd> to <kbd>frontnet</kbd> limits the number of containers that can access <kbd>db-userauth</kbd>. For an intruder to access the user information database, they must first break into <kbd>notes</kbd>, and then break into <kbd>userauth</kbd>. Unless, that is, our amateur attempt at a security audit is flawed.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Controlling the location of MySQL data volumes</h1>
                
            
            
                
<p>The <kbd>db-userauth</kbd> and <kbd>db-notes</kbd> Dockerfiles contain <kbd>VOLUME /var/lib/mysql</kbd>, and when we started the containers, we gave <kbd>--volume</kbd> options, assigning a host directory for that container directory:</p>
<div><pre>docker run --name db-notes \<br/>  ...<br/>  --volume `pwd`/../notes-data:/var/lib/mysql \<br/>  ...</pre></div>
<p>We can easily see this connects a host directory, so it appears within the container at that location. Simply inspecting the host directory with tools such as <kbd>ls</kbd> shows that files are created in that directory corresponding to a MySQL database.</p>
<p>The <kbd>VOLUME</kbd> instruction instructs Docker to create a directory outside the container and to map that directory so that it's mounted inside the container on the named path. The <kbd>VOLUME</kbd> instruction by itself doesn't control the directory name on the host computer. If no <kbd>--volume</kbd> option is given, Docker still arranges for the content of said directory to be kept outside the container. That's useful, and at least the data is available outside the container, but you haven't controlled the location.</p>
<p>If we restart the <kbd>db-notes</kbd> container without using the <kbd>--volume</kbd> option for <kbd>/var/lib/mysql</kbd>, we can inspect the container to  discover where Docker put the volume:</p>
<pre><strong>$ docker inspect --format '{{json .Mounts}}' db-notes</strong><br/><strong>[{"Type":"bind",</strong><br/><strong>"Source":"/Users/david/chap10/frontnet/my.cnf","Destination":"/etc/my.cnf",</strong><br/><strong>"Mode":"","RW":true,"Propagation":"rprivate"},{"Type":"volume","Name":"39f9a80b49e3ecdebc7789de7b7dd2366c400ee7fbfedd6e4df18f7e60bad409",</strong><br/><strong>"Source":"/var/lib/docker/volumes/39f9a80b49e3ecdebc7789de7b7dd2366c400ee7fbfedd6e4df18f7e60bad409/_data","Destination":"/var/lib/mysql",</strong><br/><strong>"Driver":"local","Mode":"","RW":true,"Propagation":""}]</strong></pre>
<p>That's not exactly a user-friendly pathname, but you can snoop into that directory and see that indeed the MySQL database is stored there. The simplest way to use a user-friendly pathname for a volume is with the <kbd>--volume</kbd> options we showed earlier.</p>
<p>Another advantage we have is to easily switch databases. For example, we could test Notes with pre-cooked test databases full of notes written in Swahili (<kbd>notes-data-swahili</kbd>), Romanian (<kbd>notes-data-romanian</kbd>), German (<kbd>notes-data-german</kbd>) and English (<kbd>notes-data-english</kbd>). Each test database could be stored in the named directory, and testing against the specific language is as simple as running the notes container with different <kbd>--volume</kbd> options. </p>
<p>In any case, if you restart the <kbd>notes</kbd> container with the <kbd>--volume</kbd> option, you can inspect the container and see the directory is mounted on the directory you specified:</p>
<pre><strong>$ docker inspect --format '{{json .Mounts}}' db-notes </strong><br/><strong>[{"Type":"bind",</strong><br/><strong>"Source":"/Users/david/chap10/frontnet/my.cnf","Destination":"/etc/my.cnf",</strong><br/><strong>"Mode":"","RW":true,"Propagation":"rprivate"},</strong><br/><strong>{"Type":"bind",</strong><br/><strong>"Source":"/Users/david/chap10/notes-data","Destination":"/var/lib/mysql",</strong><br/><strong>"Mode":"","RW":true,"Propagation":"rprivate"}]</strong><br/></pre>
<p>With the <kbd>--volume</kbd> options, we have controlled the location of the host directory corresponding to the container directory.</p>
<p>The last thing to note is that controlling the location of such directories makes it easier to make backups and take other administrative actions with that data.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Docker deployment of background services</h1>
                
            
            
                
<p>With the scripts we've written so far, the Docker container is run in the foreground.  That makes it easier to debug the service since you see the errors.   For a production deployment, we need the Docker container detached from the terminal, and an assurance that it will restart itself automatically.   Those two attributes are simple to implement.</p>
<p>Simply change this pattern:</p>
<pre><strong>$ docker run -it ...</strong></pre>
<p>To this pattern:</p>
<pre><strong>$ docker run --detach --restart always ...</strong></pre>
<p>The <kbd>-it</kbd> option is what causes the Docker container to run in the foreground.  Using these options causes the Docker container to run detached from your terminal, and if the service process dies, the container will automatically restart.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Deploying to the cloud with Docker compose</h1>
                
            
            
                
<p>This is cool that we can create encapsulated instantiations of the software services we've created. But the promise was to use the Dockerized application for deployment on cloud services. In other words, we need to take all this learning and apply it to the task of deploying Notes on a public internet server with a fairly high degree of security.</p>
<p>We've demonstrated that, with Docker, Notes can be decomposed into four containers that have a high degree of isolation from each other, and from the outside world. </p>
<p>There is another glaring problem: our process in the previous section was partly manual, partly automated. We created scripts to launch each portion of the system, which is a good practice according to the Twelve Factor Application model. But we did not automate the entire process to bring up Notes and the authentication services. Nor is this solution scalable beyond one machine.</p>
<p>Let's start with the last issue first—scalability. Within the Docker ecosystem, several <strong>Docker orchestrator</strong> services are available. An Orchestrator automatically deploys and manages Docker containers over a group of machines. Some examples of Docker Orchestrators are Docker Swarm (which is built into the Docker CLI), Kubernetes, CoreOS Fleet, and Apache Mesos. These are powerful systems able to automatically increase/decrease resources as needed, to move containers from one host to another, and more. We mention these systems for your further study as your needs grow.</p>
<p>Docker compose (<a href="https://docs.docker.com/compose/overview/">https://docs.docker.com/compose/overview/</a>) will solve the other problems we've identified. It lets us easily define and run several Docker containers together as a complete application. It uses a YAML file, <kbd>docker-compose.yml</kbd>, to describe the containers, their dependencies, the virtual networks, and the volumes. While we'll be using it to describe the deployment onto a single host machine, Docker compose can be used for multimachine deployments, especially when combined with Docker Swarm. Understanding Docker compose will provide a basis upon which to understand/use the other tools, such as Swarm or Kubernetes.</p>
<p>Docker machine (<a href="https://docs.docker.com/machine/overview/">https://docs.docker.com/machine/overview/</a>) is a tool for installing Docker Engine on virtual hosts, either local or remote, and for managing Docker containers on those hosts. We'll be using this to provision a server on a cloud hosting service, and push containers into that server. It can also be used to provision a virtual host on your laptop within a VirtualBox instance.</p>
<p>Before proceeding, ensure Docker compose and Docker machine are installed. If you've installed Docker for Windows or Docker for Mac, both are installed along with everything else. On Linux, you must install both separately by following the instructions at the links given earlier.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Docker compose files</h1>
                
            
            
                
<p>Let's start by creating a directory, <kbd>compose</kbd>, as a sibling to the <kbd>users</kbd> and <kbd>notes</kbd> directories. In that directory, create a file named <kbd>docker-compose.yml</kbd>:</p>
<pre>version: '3'<br/>services:<br/><br/>  db-userauth:<br/>    image: "mysql/mysql-server:5.7"<br/>    container_name: db-userauth<br/>    command: [ "mysqld", "--character-set-server=utf8mb4", <br/>              "--collation-server=utf8mb4_unicode_ci",<br/>              "--bind-address=0.0.0.0" ]<br/>    expose:<br/>      - "3306"<br/>    networks:<br/>      - authnet<br/>    volumes:<br/>      - db-userauth-data:/var/lib/mysql<br/>      - ../authnet/my.cnf:/etc/my.cnf<br/>    environment:<br/>      MYSQL_RANDOM_ROOT_PASSWORD: "true"<br/>      MYSQL_USER: userauth<br/>      MYSQL_PASSWORD: userauth<br/>      MYSQL_DATABASE: userauth<br/>    restart: always<br/><br/>  userauth:<br/>    build: ../users<br/>    container_name: userauth<br/>    depends_on:<br/>      - db-userauth<br/>    networks:<br/>      - authnet<br/>      - frontnet<br/>    restart: always<br/><br/>  db-notes:<br/>    image: "mysql/mysql-server:5.7"<br/>    container_name: db-notes<br/>    command: [ "mysqld", "--character-set-server=utf8mb4", <br/>              "--collation-server=utf8mb4_unicode_ci",<br/>              "--bind-address=0.0.0.0" ]<br/>    expose:<br/>      - "3306"<br/>    networks:<br/>      - frontnet<br/>    volumes:<br/>      - db-notes-data:/var/lib/mysql<br/>      - ../frontnet/my.cnf:/etc/my.cnf<br/>    environment:<br/>      MYSQL_RANDOM_ROOT_PASSWORD: "true"<br/>      MYSQL_USER: notes<br/>      MYSQL_PASSWORD: notes12345<br/>      MYSQL_DATABASE: notes<br/>    restart: always<br/><br/>  notes:<br/>    build: ../notes<br/>    container_name: notes<br/>    restart: always<br/>    depends_on:<br/>      - db-notes<br/>    networks:<br/>      - frontnet<br/>    ports:<br/>      - "3000:3000"<br/>    restart: always<br/><br/>networks:<br/>  frontnet:<br/>    driver: bridge<br/>  authnet:<br/>    driver: bridge<br/><br/>volumes: <br/>  db-userauth-data: <br/>  db-notes-data:</pre>
<p>That's the description of the entire Notes deployment. It's at a fairly high level of abstraction, roughly equivalent to the options on the command-line tools we've used so far. Further details are located inside the Dockerfiles, which are referenced from this compose file.</p>
<p>The <kbd>version</kbd> line says that this is a version 3 compose file. The version number is inspected by the <kbd>docker-compose</kbd> command, so it can correctly interpret its content. The full documentation is worth reading at  <a href="https://docs.docker.com/compose/compose-file/">https://docs.docker.com/compose/compose-file/</a>.</p>
<p>There are three major sections used here: <strong>services</strong>, <strong>volumes</strong>, and <strong>networks</strong>. The services section describes the containers being used, the networks section describes the networks, and the volumes section describes the volumes. The content of each section matches the intent/purpose of the commands we ran earlier. The information we've already dealt with is all here, just rearranged. </p>
<p>There are two database containers, <kbd>db-userauth</kbd> and <kbd>db-notes</kbd>. Both reference the Dockerhub image using the <kbd>image</kbd> tag. For the databases, we did not create a Dockerfile, but instead built directly from the Dockerhub image. The same happens here in the compose file.</p>
<p>For the <kbd>userauth</kbd> and <kbd>notes</kbd> containers, we created a Dockerfile. The directory containing that file is referenced by the <kbd>build</kbd> tag. To build the container, <kbd>docker-compose</kbd> looks for a file named <kbd>Dockerfile</kbd> in the named directory. There are more options for the <kbd>build</kbd> tag, which are discussed in the official documentation.</p>
<p>The <kbd>container_name</kbd> attribute is equivalent to the <kbd>--name</kbd> attribute and specifies a user-friendly name for the container. We must specify the container name in order to specify the container hostname in order to do Docker-style service discovery.</p>
<p>The <kbd>command</kbd> tag overrides the <kbd>CMD</kbd> tag in the Dockerfile. We've specified this for the two database containers, so we can instruct MySQL to bind to IP address <kbd>0.0.0.0</kbd>. Even though we didn't create a Dockerfile for the database containers, there is a Dockerfile created by the MySQL maintainers. </p>
<p>The <kbd>networks</kbd> attribute lists the networks to which this container must be connected and is exactly equivalent to the <kbd>--net</kbd> argument. Even though the <kbd>docker</kbd> command doesn't support multiple <kbd>--net</kbd> options, we can list multiple networks in the compose file. In this case, the networks are bridge networks. As we did earlier, the networks themselves must be created separately, and in a compose file, that's done in the <em>networks</em> section.</p>
<p>Each of the networks in our system is a <kbd>bridge</kbd> network. This fact is described in the compose file.</p>
<p>The <kbd>expose</kbd> attribute declares which ports are exposed from the container, and is equivalent to the <kbd>EXPOSE</kbd> tag. The exposed ports are not published outside the host machine, however. The <kbd>ports</kbd> attribute declares the ports that are to be published. In the ports declaration, we have two port numbers: the first being the published port number and the second being the port number inside the container. This is exactly equivalent to the <kbd>-p</kbd> option used earlier.</p>
<p>The <kbd>notes</kbd> container has a few environment variables, such as <kbd>TWITTER_CONSUMER_KEY</kbd> and <kbd>TWITTER_CONSUMER_SECRET</kbd>, that you may prefer to store in this file rather than in the Dockerfile. </p>
<p>The <kbd>depends_on</kbd> attribute lets us control the start up order. A container that depends on another will wait to start until the depended-upon container is running.</p>
<p>The <kbd>volumes</kbd> attribute describes mappings of a container directory to a <kbd>host</kbd> directory. In this case, we've defined two volume names, <kbd>db-userauth-data</kbd> and <kbd>db-notes-data</kbd>, and then used them for the volume mapping.</p>
<p>To explore the volumes, start with this command:</p>
<pre><strong>$ docker volume ls </strong><br/><strong>DRIVER              VOLUME NAME </strong><br/><strong>... </strong><br/><strong>local               compose_db-notes-data </strong><br/><strong>local               compose_db-userauth-data </strong><br/><strong>...</strong><br/></pre>
<p>The volume names are the same as in the compose file, but with <kbd>compose<em>_</em></kbd> tacked on the front.</p>
<p>You can inspect the volume location using the <kbd>docker</kbd> command line:</p>
<pre><strong>$ docker volume inspect compose_db-notes-data
$ docker volume inspect compose_db-userauth-data </strong> </pre>
<p>If it's preferable, you can specify a pathname in the <kbd>compose</kbd> file:</p>
<pre><strong>db-auth: 
  .. 
  volumes: 
    # - db-userauth-data:/var/lib/mysql 
    - ../userauth-data:/var/lib/mysql 
 
db-notes: 
  .. 
  volumes: 
    # - db-notes-data:/var/lib/mysql 
    - ../notes-data:/var/lib/mysql</strong> </pre>
<p>This is the same configuration we made earlier. It uses the <kbd>userauth-data</kbd> and <kbd>notes-data</kbd> directories for the MySQL data files for their respective database containers.</p>
<p>The <kbd>environment</kbd> tag describes the environment variables that will be received by the container. As before, environment variables should be used to inject configuration data. </p>
<p>The <kbd>restart</kbd> attribute controls what happens if, or when, the container dies. When a container starts, it runs the program named in the <kbd>CMD</kbd> instruction, and when that program exits, the container exits. But what if that program is meant to run <em>forever</em>, shouldn't Docker know it should restart the process? We could use a background process supervisor, such as Supervisord or PM2. But, we can also use the Docker <kbd>restart</kbd> option.</p>
<p>The <kbd>restart</kbd> attribute can take one of the following four values:</p>
<ul>
<li><kbd>no</kbd> – do not restart</li>
<li><kbd>on-failure:count</kbd> – restart up to <em>N</em> times</li>
<li><kbd>always</kbd> – always restart</li>
<li><kbd>unless-stopped</kbd> – start the container unless it was explicitly stopped</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Running the Notes application with Docker compose</h1>
                
            
            
                
<p>On Windows, we're able to run the commands in this section unchanged.</p>
<p>Before deploying this to a server, let's run it on our laptop using <kbd>docker-compose</kbd>:</p>
<pre><strong>$ docker stop db-notes userauth db-auth notesapp
db-notes
userauth
db-auth
notesapp
$ docker rm db-notes userauth db-auth notesapp
db-notes
userauth
db-auth
notesapp</strong>  </pre>
<p>We first needed to stop and delete the existing containers. Because the compose file wants to launch containers with the same names as we'd built earlier, we also have to remove the existing containers:</p>
<pre><strong>$ docker-compose build
Building db-auth
.. lots of output
$ docker-compose up
Creating db-auth
Recreating compose_db-notes_1
Recreating compose_userauth_1
Recreating compose_notesapp_1
Attaching to db-auth, db-notes, userauth, notesapp  </strong></pre>
<p>Once that's done, we can build the containers, <kbd>docker-compose build</kbd>, and then start them running, <kbd>docker-compose up</kbd>.</p>
<p class="mce-root"/>
<p>The first test is to execute a shell in <kbd>userauth</kbd> to run our user database script:</p>
<pre><strong>$ docker exec -it userauth bash</strong><br/><strong>root@9972adbbdbb3:/userauth# PORT=3333 node users-add.js </strong><br/><strong>Created { id: 2,</strong><br/><strong>  username: 'me', password: 'w0rd', provider: 'local',</strong><br/><strong>  familyName: 'Einarrsdottir', givenName: 'Ashildr', middleName: '',</strong><br/><strong>  emails: '[]', photos: '[]',</strong><br/><strong>  updatedAt: '2018-02-07T02:24:04.257Z', createdAt: '2018-02-07T02:24:04.257Z' }</strong><br/><strong>root@9972adbbdbb3:/userauth#</strong> </pre>
<p>Now that we've proved that the authentication service will work, and, by the way, created a user account, you should be able to browse to the Notes application and run it through its paces.</p>
<p>You can also try pinging different containers to ensure that the application network topology has been created correctly.</p>
<p>If you use Docker command-line tools to explore the running containers and networks, you'll see they have new names. The new names are similar to the old names, but prefixed with the string <kbd>compose_</kbd>. This is a side effect of using Docker compose.</p>
<p>By default, <kbd>docker-compose</kbd> attaches to the containers so that logging output is printed on the Terminal. Output from all four containers will be intermingled together. Thankfully, each line is prepended by the container name.</p>
<p>When you're done testing the system, simply type <em>CTRL</em> +<em> C</em> on the Terminal:</p>
<pre><strong>^CGracefully stopping... (press Ctrl+C again to force) </strong><br/><strong>Stopping db-userauth ... done </strong><br/><strong>Stopping userauth    ... done </strong><br/><strong>Stopping db-notes    ... done </strong><br/><strong>Stopping notes       ... done</strong><br/></pre>
<p>To avoid running with the containers attached to the Terminal, use the <kbd>-d</kbd> option. This says to detach from the Terminal and run in the background.</p>
<p>An alternate way to bring down the system described in the compose file is with the <kbd>docker-compose down</kbd> command. </p>
<p>The <kbd>up</kbd> command builds, recreates, and starts the containers. The build step can be handled separately using the <kbd>docker-compose build</kbd> command. Likewise, starting and stopping the containers can be handled separately by using the <kbd>docker-compose start</kbd> and <kbd>docker-compose-stop</kbd> commands.</p>
<p>In all cases, your command shell should be in the directory containing the <kbd>docker-compose.yml</kbd> file. That's the default name for this file. This can be overridden with the <kbd>-f</kbd> option to specify a different filename.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Deploying to cloud hosting with Docker compose</h1>
                
            
            
                
<p>We've verified on our laptop that the services described by the compose file work as intended. Launching the containers is now automated, fixing one of the issues we named earlier. It's now time to see how to deploy to a cloud-hosting provider. This is where we turn to Docker machine.</p>
<p>Docker machine can be used to provision Docker instances inside a VirtualBox host on your laptop. What we'll be doing is provisioning a Docker system on DigitalOcean. The <kbd>docker-machine</kbd> command comes with drivers supporting a long list of cloud-hosting providers.  It's easy to adapt the instructions shown here for other providers, simply by substituting a different driver.</p>
<p>After signing up for a DigitalOcean account, click on the API link in the dashboard. We need an API token to grant <kbd>docker-machine</kbd> access to the account. Go through the process of creating a token and save away the token string you're given. The Docker website has a tutorial at <a href="https://docs.docker.com/machine/examples/ocean/">https://docs.docker.com/machine/examples/ocean/</a>.</p>
<p>With the token in hand, type the following:</p>
<pre><strong>$ docker-machine create --driver digitalocean --digitalocean-size 2gb \</strong><br/><strong>    --digitalocean-access-token TOKEN-FROM-PROVIDER \</strong><br/><strong>    sandbox</strong><br/><strong>Running pre-create checks...</strong><br/><strong>Creating machine...</strong><br/><strong>(sandbox) Creating SSH key...</strong><br/><strong>(sandbox) Creating Digital Ocean droplet...</strong><br/><strong>(sandbox) Waiting for IP address to be assigned to the Droplet...</strong><br/><strong>Waiting for machine to be running, this may take a few minutes...</strong><br/><strong>Detecting operating system of created instance...</strong><br/><strong>Waiting for SSH to be available...</strong><br/><strong>Detecting the provisioner...</strong><br/><strong>Provisioning with ubuntu(systemd)...</strong><br/><strong>Installing Docker...</strong><br/><strong>Copying certs to the local machine directory...</strong><br/><strong>Copying certs to the remote machine...</strong><br/><strong>Setting Docker configuration on the remote daemon...</strong><br/><strong>Checking connection to Docker...</strong><br/><strong>Docker is up and running!</strong><br/><strong>To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env sandbox</strong></pre>
<p>The <kbd>digitalocean</kbd> driver is, as we said earlier, used with Digital Ocean. The Docker website has a list of drivers at <a href="https://docs.docker.com/machine/drivers/">https://docs.docker.com/machine/drivers/</a>.<a href="https://docs.docker.com/machine/drivers/"/></p>
<p>A lot of information is printed here about things being set up. The most important is the message at the end. A series of environment variables are used to tell the <kbd>docker</kbd> command where to connect to the Docker Engine instance. As the messages say, run: <kbd>docker-machine env sandbox</kbd>:</p>
<pre><strong>$ docker-machine env sandbox</strong><br/><strong>export DOCKER_TLS_VERIFY="1"</strong><br/><strong>export DOCKER_HOST="tcp://45.55.37.74:2376"</strong><br/><strong>export DOCKER_CERT_PATH="/home/david/.docker/machine/machines/sandbox"</strong><br/><strong>export DOCKER_MACHINE_NAME="sandbox"</strong><br/><strong># Run this command to configure your shell: </strong><br/><strong># eval $(docker-machine env sandbox)</strong></pre>
<p>That's the environment variables used to access the Docker host we just created. You should also go to your cloud-hosting provider dashboard and see that the host has been created. This command also gives us some instructions to follow:</p>
<pre><strong>$ eval $(docker-machine env sandbox) </strong><br/><strong>$ docker-machine ls </strong><br/><strong>NAME      ACTIVE   DRIVER         STATE     URL                      SWARM   DOCKER        ERRORS </strong><br/><strong>sandbox   *        digitalocean   Running   tcp://45.55.37.74:2376           v18.01.0-ce   </strong> <br/></pre>
<p>This shows that we have a Docker Engine instance running in a host at our chosen cloud-hosting provider.</p>
<p>One interesting test at this point is to run <kbd>docker ps -a</kbd> on this Terminal, and then to run it in another Terminal that does not have these environment variables. That should show the cloud host has no containers at all, while your local machine may have some containers (depending on what you currently have running):</p>
<pre><strong>$ docker run hello-world <br/>Unable to find image 'hello-world:latest' locally <br/>latest: Pulling from library/hello-world <br/>ca4f61b1923c: Pull complete  <br/>Digest: sha256:66ef312bbac49c39a89aa9bcc3cb4f3c9e7de3788c944158df3ee0176d32b751 <br/>Status: Downloaded newer image for hello-world:latest <br/> ...<br/></strong><strong>$ docker images </strong><br/><strong>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE </strong><br/><strong>hello-world         latest              f2a91732366c        2 months ago        1.85kB</strong><br/></pre>
<p>Here, we've verified that we can launch a container on the remote host.</p>
<p>The next step is to build our containers for the new machine. Because we've switched the environment variables to point to the new server, these commands cause action to happen there rather than inside our laptop:</p>
<pre><strong>$ docker-compose build</strong><br/><strong>db-userauth uses an image, skipping</strong><br/><strong>db-notes uses an image, skipping</strong><br/><strong>Building notes</strong><br/><strong>Step 1/22 : FROM node:9.5</strong><br/><strong>9.5: Pulling from library/node</strong><br/><strong>f49cf87b52c1: Pull complete</strong><br/><strong>7b491c575b06: Pull complete</strong><br/><strong>b313b08bab3b: Pull complete</strong><br/><strong>51d6678c3f0e: Pull complete</strong><br/><strong>...</strong></pre>
<p>Because we changed the environment variables, the build occurs on the <kbd>sandbox</kbd> machine rather than on our laptop, as previously.</p>
<p>This will take a while because the Docker image cache on the remote machine is empty. Additionally, building the <kbd>notesapp</kbd> and <kbd>userauth</kbd> containers copies the entire source tree to the server and runs all build steps on the server.</p>
<p>The build may fail if the default memory size is 500 MB, the default on DigitalOcean at the time of writing. If so, the first thing to try is resizing the memory on the host to at least 2 GB.</p>
<p>Once the build is finished, launch the containers on the remote machine:</p>
<pre><strong>$ docker-compose up </strong><br/><strong>Creating notes ... done</strong><br/><strong>Recreating db-userauth ... done</strong><br/><strong>Recreating db-notes ... done</strong><br/><strong>Creating notes ... </strong><br/><strong>Attaching to db-userauth, db-notes, userauth, notes</strong></pre>
<p>Once the containers start, you should test the <kbd>userauth</kbd> container as we've done previously. Unfortunately, the first time you do this, that command will fail. The problem is these lines in the <kbd>docker-compose.yml</kbd>:</p>
<pre> - ../authnet/my.cnf:/etc/my.cnf<br/>...<br/> - ../frontnet/my.cnf:/etc/my.cnf</pre>
<p>In this case, the build occurs on the remote machine, and the <kbd>docker-machine</kbd> command does not copy the named file to the server. Hence, when Docker attempts to start the container, it is unable to do so because that volume mount cannot be satisfied because the file is simply not there. This, then, means some surgery on <kbd>docker-compose.yml</kbd>, and to add two new Dockerfiles.</p>
<p>First, make these changes to <kbd>docker-compose.yml</kbd>:</p>
<div><pre><strong>  ...<br/>  db-userauth:<br/>    build: ../authnet<br/>    container_name: db-userauth<br/>    networks:<br/>      - authnet<br/>    volumes:<br/>      - db-userauth-data:/var/lib/mysql<br/>    restart: always<br/>  ...</strong><br/><strong>  db-notes:<br/>    build: ../frontnet<br/>    container_name: db-notes<br/>    networks:<br/>      - frontnet<br/>    volumes:<br/>      - db-notes-data:/var/lib/mysql<br/>    restart: always</strong></pre></div>
<p>Instead of building the database containers from a Docker image, we're now building them from a pair of Dockerfiles. Now we must create those two Dockerfiles.</p>
<p>In <kbd>authnet</kbd>, create a file named <kbd>Dockerfile</kbd> containing the following:</p>
<div><pre>FROM mysql/mysql-server:5.7<br/>EXPOSE 3306<br/>COPY my.cnf /etc/<br/>ENV MYSQL_RANDOM_ROOT_PASSWORD="true"<br/>ENV MYSQL_USER=userauth<br/>ENV MYSQL_PASSWORD=userauth<br/>ENV MYSQL_DATABASE=userauth<br/>CMD [ "mysqld", "--character-set-server=utf8mb4", \<br/>  "--collation-server=utf8mb4_unicode_ci", "--bind-address=0.0.0.0" ]</pre></div>
<p>This copies certain settings from what had been the <kbd>db-userauth</kbd> description in <kbd>docker-compose.yml</kbd>. The important thing is that we now <kbd>COPY</kbd> the <kbd>my.cnf</kbd> file rather than use a volume mount.</p>
<p>In <kbd>frontnet</kbd>, create a <kbd>Dockerfile</kbd> containing the following:</p>
<div><pre>FROM mysql/mysql-server:5.7<br/>EXPOSE 3306<br/>COPY my.cnf /etc/<br/>ENV MYSQL_RANDOM_ROOT_PASSWORD="true"<br/>ENV MYSQL_USER=notes<br/>ENV MYSQL_PASSWORD=notes12345<br/>ENV MYSQL_DATABASE=notes<br/>CMD [ "mysqld", "--character-set-server=utf8mb4", \<br/>  "--collation-server=utf8mb4_unicode_ci", "--bind-address=0.0.0.0" ]</pre></div>
<p>This is the same, but with a few critical values changed.</p>
<p>After making these changes, we can now build the containers, and launch them:</p>
<pre><strong>$ docker-compose build</strong><br/><strong>... much output</strong><br/><strong>$ docker-compose up --force-recreate</strong><br/><strong>... much output</strong></pre>
<p>Now that we have a working build, and can bring up the containers, let's inspect them and verify everything works.</p>
<p>Execute a shell in <kbd>userauth</kbd> to test and set up the user database:</p>
<pre><strong>$ docker exec -it userauth bash</strong><br/><strong>root@931dd2a267b4:/userauth# PORT=3333 node users-list.js </strong><br/><strong>List [ { id: 'me', username: 'me', provider: 'local',</strong><br/><strong>    familyName: 'Einarrsdottir', givenName: 'Ashildr', middleName: '',</strong><br/><strong>    emails: '[]', photos: '[]' } ] </strong> </pre>
<p>As mentioned previously, this verifies that the <kbd>userauth</kbd> service works, that the remote containers are set up, and that we can proceed to using the Notes application.</p>
<p>The question is: What's the URL to use?  The service is not on <kbd>localhost</kbd>, because it's on the remote server. We don't have a domain name assigned, but there is an IP address for the server.</p>
<p>Run the following command:</p>
<pre><strong>$ docker-machine ip sandbox</strong><br/><strong>45.55.37.74</strong></pre>
<p>Docker tells you the IP address, which you should use as the basis of the URL. Hence, in your browser, visit <kbd>http://IP-ADDRESS:3000</kbd></p>
<p>With Notes deployed to the remote server, you should check out all the things we've looked at previously. The bridge networks should exist, as shown previously, with the same limited access between containers. The only public access should be port <kbd>3000</kbd> on the <kbd>notes</kbd> container. </p>
<p>Remember to set the <kbd>TWITTER_CALLBACK_HOST</kbd> environment variable appropriately for your server.</p>
<p>Because our database containers mount a volume to store the data, let's see where that volume landed on the server:</p>
<pre><strong>$ docker volume ls</strong><br/><strong>DRIVER VOLUME NAME</strong><br/><strong>local compose_db-notes-data</strong><br/><strong>local compose_db-userauth-data</strong></pre>
<p>Those are the expected volumes, one for each container:</p>
<pre><strong>$ docker volume inspect compose_db-notes-data</strong><br/><strong>[</strong><br/><strong>    {</strong><br/><strong>        "CreatedAt": "2018-02-07T06:30:06Z",</strong><br/><strong>        "Driver": "local",</strong><br/><strong>        "Labels": {</strong><br/><strong>            "com.docker.compose.project": "compose",</strong><br/><strong>            "com.docker.compose.volume": "db-notes-data"</strong><br/><strong>        },</strong><br/><strong>        "Mountpoint": "/var/lib/docker/volumes/compose_db-notes-</strong><br/><strong>        data/_data",</strong><br/><strong>        "Name": "compose_db-notes-data",</strong><br/><strong>        "Options": {},</strong><br/><strong>        "Scope": "local"</strong><br/><strong>    }</strong><br/><strong>]</strong></pre>
<p>Those are the directories, but they're not located on our laptop. Instead, they're on the remote server. Accessing these directories means logging into the remote server to take a look:</p>
<pre><strong>$ docker-machine ssh sandbox</strong><br/><strong>Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-112-generic x86_64)</strong><br/><br/><strong> * Documentation: https://help.ubuntu.com</strong><br/><strong> * Management: https://landscape.canonical.com</strong><br/><strong> * Support: https://ubuntu.com/advantage</strong><br/><br/><strong>  Get cloud support with Ubuntu Advantage Cloud Guest:</strong><br/><strong>    http://www.ubuntu.com/business/services/cloud</strong><br/><br/><strong>4 packages can be updated.</strong><br/><strong>0 updates are security updates.</strong><br/><br/><strong>Last login: Wed Feb 7 04:00:29 2018 from 108.213.68.139</strong><br/><strong>root@sandbox:~#</strong></pre>
<p>From this point, you can inspect the directories corresponding to these volumes and see that they indeed contain MySQL configuration and data files:</p>
<pre><strong>root@sandbox:~# ls /var/lib/docker/volumes/compose_db-notes-data/_data </strong><br/><strong>auto.cnf         client-key.pem  ib_logfile1  mysql.sock.lock     public_key.pem </strong><br/><strong>ca-key.pem       ib_buffer_pool  ibtmp1       notes               server-cert.pem </strong><br/><strong>ca.pem           ibdata1         mysql        performance_schema  server-key.pem </strong><br/><strong>client-cert.pem  ib_logfile0     mysql.sock   private_key.pem     sys</strong><br/></pre>
<p>You'll also find that the Docker command-line tools will work. The process list is especially interesting:</p>
<div><img src="img/70d8826e-ebbb-4a78-bc23-074165ed0bfe.png" style="width:62.25em;height:14.67em;" width="1163" height="273"/></div>
<p>Look closely at this and you see a process corresponding to every container in the system. These processes are running in the host operating system. Docker creates layers of configuration/containment around those processes to create the appearance that the process is running under a different operating system, and with various system/network configuration files, as specified in the container screenshot. </p>
<p>The claimed advantage Docker has over virtualization approaches, such as VirtualBox, is that Docker is very lightweight. We see right here why Docker is lightweight: there is no virtualization layer, there is only a containerization process (<kbd>docker-containerd-shim</kbd>).</p>
<p>Once you're satisfied that Notes is working on the remote server, you can shut it down and remove it as follows:</p>
<pre><strong>$ docker-compose stop
Stopping notesapp ... done
Stopping userauth ... done
Stopping db-notes ... done
Stopping db-auth ... done</strong></pre>
<p>This shuts down all the containers at once:</p>
<pre><strong>$ docker-machine stop sandbox
Stopping "sandbox"...
Machine "sandbox" was stopped.</strong></pre>
<p>This shuts down the remote machine. The cloud-hosting provider dashboard will show that the Droplet has stopped.</p>
<p>At this point, you can go ahead and delete the Docker machine instance as well, if you like:</p>
<pre><strong>$ docker-machine rm sandbox
About to remove sandbox
Are you sure? (y/n): y
Successfully removed sandbox </strong> </pre>
<p>And, if you're truly certain you want to delete the machine, the preceding command does the deed. As soon as you do this, the machine will be erased from your cloud-hosting provider dashboard.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter has been quite a journey. We went from an application that existed solely on our laptop, to exploring two ways to deploy Node.js applications to a production server.</p>
<p>We started by reviewing the Notes application architecture and how that will affect deployment. That enabled you to understand what you had to do for server deployment.</p>
<p>Then you learned the traditional way to deploy services on Linux using an init script. PM2 is a useful tool for managing background processes in such an environment. You also learned how to provision a remote server using a virtual machine hosting service.</p>
<p>Then you took a long trip into the land of Docker, a new and exciting system for deploying services on machines. You learned how to write a Dockerfile so that Docker knows how to construct a service image. You learned several ways to deploy Docker images on a laptop or on a remote server. And you learned how to describe a multi-container application using Docker compose.</p>
<p>You're almost ready to wrap up this book. You've learned a lot along the way; there are two final things to cover.</p>
<p>In the next chapter, we will learn about both unit testing and functional testing. While a core principle of test-driven development is to write the unit tests before writing the application, we've done it the other way around and put the chapter about unit testing at the end of this book. That's not to say unit testing is unimportant, because it is extremely important. </p>
<p>In the final chapter, we'll explore how to harden our application, and application infrastructure, against attackers.</p>


            

            
        
    </div>



  </body></html>