- en: Implementing Autonomous Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a GraphQL CRUD BFF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a search BFF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an analytics BFF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an inbound External Service Gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an outbound External Service Gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestrating collaboration between services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a Saga
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](a3041ef8-acc9-4585-8b31-11fe972d59da.xhtml), *Getting Started
    with Cloud-Native*, we began our journey to understand why cloud-native is lean
    and autonomous. We focused on recipes that demonstrate how leveraging fully managed
    cloud services empower self-sufficient, full-stack teams to rapidly and continuously
    deliver innovation with confidence. In [Chapter 2](129afdee-aa82-4ba6-9d80-5ec70c4a766e.xhtml),
    *Applying the Event Sourcing and CQRS Patterns*, we worked through recipes that
    showcase how these patterns establish the bulkheads that enable the creation of
    autonomous services.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we bring all these foundational pieces together with recipes
    for implementing autonomous service patterns. In my book, [Cloud Native Development
    Patterns and Best Practices](https://www.packtpub.com/application-development/cloud-native-development-patterns-and-best-practices),
    I discuss various approaches for decomposing a cloud-native system into bound,
    isolated, and autonomous services.
  prefs: []
  type: TYPE_NORMAL
- en: Every service should certainly have a bounded context and a single responsibility,
    but we can decompose services along additional dimensions as well. The life cycle
    of data is an important consideration for defining services, because the users,
    requirements, and persistence mechanisms will likely change as data ages. We also
    decompose services based on boundary and control patterns. Boundary services,
    such as a **Backend for Frontend** (**BFF**) or an **External Service Gateway**
    (**ESG**), interact with things that are external to the system, such as humans
    and other systems. Control services orchestrate the interactions between these
    decoupled boundary services. The recipes in this chapter demonstrate common permutations
    of these decomposition strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a GraphQL CRUD BFF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The BFF pattern accelerates innovation because the team that implements the
    frontend also owns and implements the backend service that supports the frontend.
    This enables teams to be self-sufficient and unencumbered by competing demands
    for a shared backend service. In this recipe, we will create a CRUD BFF service
    that supports data at the beginning of its life cycle. The single responsibility
    of this service is authoring data for a specific bounded context. It leverages
    *database-first* Event Sourcing to publish domain events to downstream services.
    The service exposes a GraphQL-based API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need an AWS Kinesis Stream, such as the
    one created in the *Creating an event stream* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-bff-graphql-crud` directory with `cd cncb-bff-graphql-crud`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `./schema/thing/typedefs.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `./schema/thing/resolvers.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the function with the following `curl` commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure to replace the API Gateway ID (that is, `ac0n4oyzm6`) in the endpoints
    with the value output during deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the same mutations and queries using GraphiQL by using the endpoint
    output during deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e8117ea3-faa6-45bd-98fb-35c1a5e0e7a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the `trigger` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Review the events collected in the data lake bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe builds on the *Applying the database-first variant of the Event
    Sourcing pattern with DynamoDB* recipe in [Chapter 2](129afdee-aa82-4ba6-9d80-5ec70c4a766e.xhtml), *Applying
    the Event Sourcing and CQRS Patterns* by exposing the ability to author data in
    a bounded context through a GraphQL API. *GraphQL* is becoming increasingly popular
    because of the flexibility of the resulting API and the power of client libraries,
    such as the Apollo Client. We implement a single `graphql` function to support
    our API and then add the necessary functionality through the `schema`, `resolvers`,
    `models`, and `connectors`.
  prefs: []
  type: TYPE_NORMAL
- en: The GraphQL schema is where we define our `types`, `queries`, and `mutations`.
    In this recipe, we can query `thing` types by ID and by name, and `save` and `delete`.
    The `resolvers` map the GraphQL requests to `model` objects that encapsulate the
    business logic. The `models`, in turn, talk to `connectors` that encapsulate the
    details of the database API. The `models` and `connectors` are registered with
    the `schema` in the `handler` function with a very simple but effective form of
    constructor-based dependency injection. We don't use dependency injection very
    often in cloud-native, because functions are so small and focused that it is overkill
    and can impede performance. With GraphQL, this simple form is very effective for
    facilitating testing. The `Graphiql` tool is very useful for exposing the self-documenting
    nature of APIs.
  prefs: []
  type: TYPE_NORMAL
- en: The single responsibility of this service is authoring data and publishing the
    events, using database-first Event Sourcing, for a specific bounded context. The
    code within the service follows a very repeatable coding convention of `types`,
    `resolvers`, `models`, `connectors`, and `triggers`. As such, it is very easy
    to reason about the correctness of the code, even as the number of business domains
    in the service increases. Therefore, it is reasonable to have a larger number
    of domains in a single authoring BFF services, so long as the domains are cohesive,
    part of the same bounded context, and authored by a consistent group of users.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a search BFF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Implementing a GraphQL CRUD BFF* recipe, we discussed how the *BFF*
    pattern accelerates innovation. We have also discussed how different user groups
    interact with data at different stages in the data life cycle, and how different
    persistent mechanisms are more appropriate at the different stages. In this recipe,
    we will create a BFF service that supports the read-only consumption of data.
    The single responsibility of this service is *indexing* and retrieving data for
    a specific bounded context. It applies the *CQRS* pattern to create two *materialized
    views* that work in tandem, one in Elasticsearch and another in S3\. The service
    exposes a RESTful API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-bff-rest-search` directory with `cd cncb-bff-rest-search`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the following `curl` commands, after updating the `API-ID` and  `BUCKET-SUFFIX`,
    to search the data and retrieve the detailed data from `S3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the trigger function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe combines and builds on the *Creating a materialized view in S3*
    and the *Creating a materialized view in Elasticsearch* recipes to create a highly
    scalable, efficient, and cost-effective read-only view of the data in a bounded
    context. First, the `listener` function atomically creates the materialized view
    in *S3*. The S3 `Bucket` is configured to send events to a **Simple Notification
    Service** (**SNS**) topic called `BucketTopic`. We use SNS to deliver the S3 events
    because only a single observer can consume S3 events, while SNS, in turn, can
    deliver to any number of observers. Next, the `trigger` function atomically indexes
    the data in the *Elasticsearch* `Domain` and includes the `url` to the materialized
    view in S3.
  prefs: []
  type: TYPE_NORMAL
- en: The RESTful search service exposed by the API Gateway can explicitly scale to
    meet demand and efficiently search a large amount of indexed data. The detailed
    data can then be cost-effectively retrieved from S3, based on the returned URL,
    without the need to go through the API Gateway, a function, and the database.
    We create the data in S3 first and then index the data in Elasticsearch to ensure
    that the search results do not include data that has not been successfully stored
    in S3.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an analytics BFF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Implementing a GraphQL CRUD BFF* recipe, we discussed how the *BFF*
    pattern accelerates innovation. We have also discussed how different user groups
    interact with data at different stages in the data life cycle, and how different
    persistent mechanisms are more appropriate at the different stages. In this recipe,
    we will create a BFF service that provides statistics about the life cycle of
    data. The single responsibility of this service is accumulating and aggregating
    metrics about data in a specific bounded context. It applies the Event Sourcing
    pattern to create a *micro event store* that is used to continuously calculate
    a *materialized view* of the metrics. The service exposes a RESTful API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-bff-rest-analytics` directory with `cd cncb-bff-rest-analytics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish several events from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the following `curl` command, after updating the `<API-ID>`, to query
    the analytics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `trigger` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe combines and builds on the *Creating a micro event store* and the
    *Creating a materialized view in DynamoDB* recipes in [Chapter 2](129afdee-aa82-4ba6-9d80-5ec70c4a766e.xhtml), *Applying
    the Event Sourcing and CQRS Patterns* to create an advanced materialized view
    that counts events by `type`, `user`, `month`, and `year`. The service employs
    two DynamoDB tables, the micro event store, and the materialized view. The `HASH`
    key for the event store is the `partitionKey`, which contains the `userId` so
    that we can correlate events by the user. The range key is the `timestamp` so
    that we can collate the events and query by month. The hash key for the view table
    is also `userId`, and the range key is `monthyear`, so that we can retrieve the
    statistics by user, month, and year. In this example, we are counting all events,
    but in a typical solution, you would be filtering `byType` for a specific set
    of event types.
  prefs: []
  type: TYPE_NORMAL
- en: The `listener` function performs the crucial job of *filtering*, *correlating,*
    and *collating* the events into the micro event store, but the real interesting
    logic in this recipe is in the `trigger` function. The logic is based on the concepts
    of the *ACID 2.0* transaction model. **ACID** 2.0 stands for **Associative, Commutative,
    Idempotent, and Distributed**. In essence, this model allows us to arrive at the
    same, correct answer, regardless of whether or not the events arrive in the correct
    order or even if we receive the same events multiple times. Our hash and range
    key in the micro event store handles the idempotency. For each new key, we recalculate
    the materialized view by querying the event store based on the context of the
    new event, and performing the calculation based on the latest known data. If an
    event arrives out of order, it simply triggers a recalculation. In this specific
    example, the end user would expect the statistics to eventually become consistent
    by the end of the month or shortly thereafter.
  prefs: []
  type: TYPE_NORMAL
- en: The calculations can be arbitrarily complex. The calculation is performed in
    memory and the results of the micro event store query can be sliced and diced
    in many different ways. For this recipe, the `reduce` method on the stream is
    perfect for counting. It is important to note that the `sub-stream` ensures that
    the count is performed by `userId`, because that was the hash key of the results
    returned from the event store. The results are stored in the materialized view
    as a JSON document so that they can be retrieved efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: The **TimeToLive** (**TTL**) feature is set up on the events table. This feature
    can be used to keep the event store from growing unbounded, but it can also be
    used to trigger periodic rollup calculations. I set TTL to one hour so that you
    can see it execute if you wait long enough, but you would typically set this to
    a value suitable for your calculations, on the order of a month, quarter, or year.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an inbound External Service Gateway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **External Service Gateway** (**ESG**) pattern provides an anti-corruption
    layer between a cloud-native system and any external services that it interacts
    with. Each gateway acts as a bridge to exchange events between the system and
    a specific external system. In this recipe, we will create an ESG service that
    allows events to flow inbound from an external service. The single responsibility
    of this service is to encapsulate the details of the external system. The service
    exposes a RESTful webhook to the external system. The external events are transformed
    into an internal format and published using *event-first* Event Sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-esg-inbound` directory with `cd cncb-esg-inbound`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up a webhook in your GitHub project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the Payload URL to the endpoint of your webhook
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the content type to `application/json`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the secret to a random value
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select just the `Issues` events checkbox
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For detailed instructions on creating a GitHub webhook, see [https://developer.github.com/webhooks/creating.](https://developer.github.com/webhooks/creating)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e9442a6-eee5-4020-aaaa-19afb2cccd7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Create and/or update one or more issues in your GitHub project to trigger the
    webhook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a look at the `webhook` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Review the events in the data lake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I chose to use GitHub as the external system in this recipe because it is freely
    available to everyone and representative of typical requirements. In this recipe,
    our inbound ESG service needs to provide an API that will be invoked by the external
    system and conforms to the signature of the external system's webhook. We implement
    this webhook using the API Gateway and a `webhook` function. The single responsibility
    of this function is to transform the external event to an internal event and atomically
    publish it using event-first Event Sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the external event ID is used as the internal event ID to provide
    for idempotency. The external event data is included in the internal event in
    its raw format so that it can be recorded as an audit in the data lake. The external
    format is also transformed in an internal canonical format to support the pluggability
    of different external systems. The logic in an inbound ESG service is intentionally
    kept simple to minimize the chance of errors and help ensure the atomic exchange
    of the events between the systems.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an outbound External Service Gateway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Implementing an inbound External Service Gateway* recipe, we discussed
    how the ESG pattern provides an anti-corruption layer between the cloud-native
    system and its external dependencies. In this recipe, we will create an ESG service
    that allows events to flow outbound to an external service. The single responsibility
    of this service is to encapsulate the details of the external system. The service
    applies the CQRS pattern. The internal events are transformed to the external
    format and forwarded to the external system via its API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting this recipe, you will need an AWS Kinesis Stream, such as the
    one created in the *Creating an event stream* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need a GitHub account and a repository. I recommend creating a repository
    named `sandbox`. Use the following command to create a GitHub personal access
    token, or follow the instructions in the GitHub UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-esg-outbound` directory with `cd cncb-esg-outbound`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Update the `REPO`, `OWNER`, and `TOKEN` environment variables in the `serverless.yml`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish an event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Confirm that the issue was created in your GitHub project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a look at the `listener` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I chose to use GitHub as the external system in this recipe because it is freely
    available to everyone and its API is representative of typical requirements. One
    of the major details that are encapsulated by an ESG service is the security credentials
    needed to access the external API. In this recipe, we must create and secure a
    long-lived *personal access token* and include it as an authorization header in
    every API request. The details of how to secure a token are out of scope for this
    recipe, however, a service such as AWS Secret Manager is typically employed. For
    this recipe, the token is stored as an environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: The `listener` function consumes the desired events, transforms them into the
    external format, and atomically invokes the external API. That is the limit of
    the responsibility of an ESG service. These services effectively make external
    services look like any other service in the system, while also encapsulating the
    details so that these external dependencies can be easily switched in the future.
    The transformation logic can become complex. The latching technique, discussed
    in the *Implementing bi-directional synchronization* recipe, may come into play,
    as well as the need to cross-reference external IDs to internal IDs. In many cases,
    the external data can be thought of as a materialized view, in which case the
    *micro event store* techniques may be useful. In a system that is offered as a
    service, an ESG service would provide your own outbound webhook feature.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating collaboration between services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autonomous cloud-native services perform all inter-service communication asynchronously
    via streams to decouple upstream services from downstream services. Although the
    upstream and downstream services are not directly coupled to each other, they
    are coupled to the event types that they produce and consume. The *Event Orchestration*
    control pattern acts as a *mediator* to completely decouple event producers from
    event consumers by translating between event types.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a control service that orchestrates the interaction
    between two boundary services. The single responsibility of this service is to
    encapsulate the details of the collaboration. The upstream events are transformed
    to the event types' expected downstream, and published using *event-first* Event
    Sourcing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-event-orchestration` directory with `cd cncb-event-orchestration`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Publish these events from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `listener` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Review the events in the data lake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This *control* service has a single *stream processor* function that listens
    for specific events and reacts by emitting more events using event-first Event
    Sourcing. The events it listens for are described in the `transitions` metadata,
    which essentially defines the state machine of a long-lived business process.
    Each business process is implemented as an autonomous control service that orchestrates
    the collaboration between a set of completely decoupled boundary services. Each
    *boundary* service involved in the collaboration defines the set of events it
    produces and consumes independently of the other services. The control service
    provides the glue that brings these services together to deliver a higher value
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: The *downstream* services that are triggered by the emitted events do have one
    requirement that they must support when defining their incoming and outgoing event
    types. The incoming event types must accept the `context` element as an opaque
    set of data and pass the context data along in the outgoing event. A downstream
    service can leverage the context data, but should not explicitly change the context
    data. The context data allows the control service to correlate the events in a
    specific collaboration instance without needing to explicitly store and retrieve
    data. However, a control service could maintain its own *micro event store* to
    facilitate complex transition logic, such as joining multiple parallel flows back
    together before advancing.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Saga
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Saga pattern is a solution to long-lived transactions that is based on eventual
    consistency and compensating transactions. It was first discussed in a paper by
    Hector Garcia-Molina and Kenneth Salem ([https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf](https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf)).
    Each step in a long-lived transaction is atomic. When a downstream step fails,
    it produces a violation event. The upstream services react to the violation event
    by performing a compensating action. In this recipe, we will create a service
    that submits data for downstream processing. The service also listens for a violation
    event and takes corrective action.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create the project from the following template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to the `cncb-saga` directory with `cd cncb-sage`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Review the file named `serverless.yml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Review the file named `handler.js` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Install the dependencies with `npm install`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the tests with `npm test -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the contents generated in the `.serverless` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy the stack:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Review the stack and resources in the AWS Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Invoke the `submit` and `query` functions with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Publish a violation event from a separate Terminal with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke the `query` function again with the following command, and note the
    updated `status`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `trigger` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the `listener` function logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Remove the stack once you are finished with `npm run rm:lcl -- -s $MY_STAGE`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe builds on recipes that we have already covered. We submit an order
    domain object using *database-first* Event Sourcing, and we have a `query` function
    to retrieve the current status of the order. The `listener` function is the most
    interesting part of this recipe. It listens for the `reservation-violation` events
    and performs a compensating action. In this case, the `compensation` is simply
    to change the `status` to `cancelled`. Compensating actions can be arbitrarily
    complex and are specific to a given service. For example, a service may need to
    reverse a complex calculation, while accounting for intermediate changes and triggering
    cascading changes as well. The audit trail provided by Event Sourcing and a *micro
    event store* may be useful for recalculating the new state.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to note in this example is that the collaboration is implemented
    using *event* *choreography* instead of *Event Orchestration*. In other words,
    this service is explicitly coupled to the `reservation-violation` event type.
    Event choreography is typically used in smaller and/or younger systems, or between
    highly related services. As a system matures and grows, the flexibility of *Event
    Orchestration* becomes more valuable. We employed Event Orchestration in the *Orchestrating
    collaboration between services* recipe.
  prefs: []
  type: TYPE_NORMAL
