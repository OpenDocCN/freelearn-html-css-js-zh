<html><head></head><body>
		<div><h1 id="_idParaDest-203" class="chapter-number"><a id="_idTextAnchor204"/>11</h1>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor205"/>Caching and Asynchronous Messaging in Microservices</h1>
			<p>When working with microservices architecture and Node.js, you need to master caching and asynchronous messaging to build the next generation of successful applications.</p>
			<p>We’ll start this chapter by understanding better how to work with caching and asynchronous messaging in microservices with Node.js. Caching and asynchronous messaging are two important techniques used in microservices architecture to improve performance, scalability, and decoupling. Caching involves storing frequently accessed data in a cache to improve response times and reduce the load on the underlying data sources. Asynchronous messaging enables loose coupling and scalability in microservices by decoupling services through message queues or publish-subscribe patterns.</p>
			<p>By the end of this chapter, you will have learned how to work with caching and asynchronous messaging in Node.js.</p>
			<p>In this chapter, we’re going to cover the following main topics:</p>
			<ul>
				<li>Client-side caching and edge caching</li>
				<li>Microservice-level caching and database query caching</li>
				<li>Message queues and publish-subscribe</li>
				<li>Event-driven architecture</li>
			</ul>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor206"/>Client-side caching and edge caching</h1>
			<p>In this section, we’re going to show you how to work with client-side caching and edge caching. Client-side caching and edge caching are strategies used to improve performance and reduce the load on servers by storing and serving content closer to the user.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor207"/>Client-side caching</h2>
			<p><strong class="bold">Client-side caching</strong> involves storing<a id="_idIndexMarker891"/> resources (e.g., HTML pages, stylesheets, scripts, images) on the client device (such as a web browser) to avoid repeated requests to the server.</p>
			<p>Here are some of its<a id="_idIndexMarker892"/> advantages:</p>
			<ul>
				<li>Client-side caching reduces server load by serving cached content directly from the client. This means that client-side caching can improve the performance and efficiency of both the web server and the web browser. By serving cached content directly from the client, the web server does not have to process and send the same data repeatedly to the same or different users. This reduces the server load, meaning the amount of work or requests that the server has to handle at any given time.</li>
				<li>It improves page load times for subsequent visits. This means that client-side caching can enhance the user experience by making the web pages load faster when the user visits them again. By storing a copy of a web page in the browser memory, the browser does not have to request and download the same web page again from the server.</li>
				<li>It enhances user experience by minimizing network requests. This means that client-side caching can reduce the number and size of network requests that the browser has to make to the server. Network requests are the messages that the browser and the server exchange to communicate and transfer data. Network requests can take time and consume bandwidth, depending on the distance, speed, and quality of the connection. By minimizing network requests, client-side caching can save time and bandwidth as well as avoid potential errors or delays that might occur during communication.</li>
				<li>An API contract outlines the rules and specifications for how services should interact. In the context of client-side caching, an API contract can outline the rules and specifications for how services should interact with the cached data stored on the client’s device, such as the browser.</li>
				<li>The caching behavior is controlled by HTTP headers, such as <strong class="bold">Cache-Control</strong> and <strong class="bold">Expires</strong>. This means that<a id="_idIndexMarker893"/> client-side caching<a id="_idIndexMarker894"/> can be configured and customized by using certain HTTP headers that specify for how long and under what conditions the data can be cached. HTTP headers are the metadata that accompany the HTTP requests<a id="_idIndexMarker895"/> and responses between the client and the server.</li>
			</ul>
			<p><em class="italic">Figure 11</em><em class="italic">.1</em> illustrates client-side caching:</p>
			<div><div><img src="img/B14980_11_01.jpg" alt="Figure 11.1: Client-side caching"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1: Client-side caching</p>
			<p>We have learned the basics of client-side caching; now, let’s move on to edge caching.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor208"/>Edge caching</h2>
			<p><strong class="bold">Edge caching</strong>, or <strong class="bold">content delivery network</strong> (<strong class="bold">CDN</strong>) caching, involves caching content on servers strategically <a id="_idIndexMarker896"/>positioned at the<a id="_idIndexMarker897"/> edge of the network (closer to users) to reduce latency and improve content delivery speed.</p>
			<p>Here are some of its<a id="_idIndexMarker898"/> advantages:</p>
			<ul>
				<li>Edge caching minimizes latency by serving content from servers closer to the user. This means that edge caching can reduce the time it takes for the data to travel from the server to the user. <strong class="bold">Latency</strong> is the delay or lag that occurs <a id="_idIndexMarker899"/>when data is transferred over a network. Latency can affect the performance and user experience of web applications, especially for dynamic or interactive content. By serving content from servers closer to the user, edge caching can minimize latency and improve the speed and responsiveness of web applications.</li>
				<li>It distributes content globally, reducing the load on the origin server. This means that edge caching can improve the scalability and reliability of the web application by spreading the data across multiple servers around the world. This reduces the load on the origin server, meaning the main server that hosts the original data and application logic.</li>
				<li>It enhances scalability and reliability. This means that edge caching can improve the ability of the web application to handle more traffic and requests without compromising the quality and availability of the service. By distributing the data across multiple servers around the world, edge caching can reduce the dependency and load on the central server, which might have limited resources and capacity.</li>
				<li>CDN providers deploy servers worldwide, and content is cached on these servers for quick retrieval. This means that edge caching is often implemented by using a CDN, which is a network of servers distributed across the globe that can store and deliver data to users. A CDN provider is a company that offers CDN services to web applications and websites. By using a CDN provider, web applications and websites can cache their data on the CDN servers, which are closer to the users than the original server. This way, when a user requests the data, it can be retrieved quickly<a id="_idIndexMarker900"/> from the CDN server, rather than from the original server.</li>
			</ul>
			<p>Remember, it is important to apply the caching strategies while working with microservices.</p>
			<p>In summary, client-side<a id="_idIndexMarker901"/> caching and edge caching are powerful techniques for optimizing web performance, reducing server loads, and enhancing the overall user experience. Understanding cache control headers, cache invalidation strategies, and leveraging CDNs is crucial for effective implementation.</p>
			<p>With the understanding of these concepts, let’s now move on to microservice-level caching and database query caching.</p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor209"/>Microservice-level caching and database query caching</h1>
			<p>Microservice-level aching and database query caching are strategies employed to enhance the performance and scalability of microservices by reducing the need for repeated computations and database queries.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor210"/>Microservice-level caching</h2>
			<p><strong class="bold">Microservice-level caching</strong> involves storing<a id="_idIndexMarker902"/> and retrieving frequently accessed data within individual microservices to avoid redundant computations or external calls. Each microservice maintains its own cache, and caching decisions are made within the microservice boundaries.</p>
			<p>Caching can allow microservices<a id="_idIndexMarker903"/> to improve fault tolerance, which is the ability of a system to continue functioning despite failures or errors. Caching can act as a buffer during temporary service outages or network issues, which can affect the availability and performance of microservices. Caching<a id="_idIndexMarker904"/> can help microservices to do the following:</p>
			<ul>
				<li>It can reduce the dependency on external services or databases that might be slow, unreliable, or unavailable due to network problems or maintenance. By storing the data in a cache, microservices can avoid making unnecessary or repeated requests to the original data source and instead serve the data from the cache.</li>
				<li>It can handle spikes in traffic or demand that might overload the system or cause bottlenecks. By storing the data in a cache, microservices can reduce the load on the system and improve the response time and throughput of the system.</li>
				<li>It can recover from failures or errors that might cause data loss or corruption. By storing the data in a cache, microservices can preserve the data and restore it from the cache if the original<a id="_idIndexMarker905"/> data source is compromised or damaged.</li>
			</ul>
			<p>Here are some<a id="_idIndexMarker906"/> of its use cases:</p>
			<ul>
				<li>Caching results of computationally expensive operations.</li>
				<li>Storing frequently accessed static data.</li>
				<li>Reducing the load on downstream microservices or databases.</li>
			</ul>
			<p>Here are some key considerations <a id="_idIndexMarker907"/>for microservice-level caching:</p>
			<ul>
				<li><strong class="bold">Granularity</strong>: Determine the appropriate granularity for caching whether it’s at the level of individual API endpoints, specific operations, or entire datasets.</li>
				<li><strong class="bold">Cache invalidation</strong>: Implement strategies to invalidate or update the cache when underlying data changes to ensure consistency.</li>
				<li><strong class="bold">Cache eviction</strong>: Define policies for removing stale or less frequently used items from the cache to manage memory efficiently.</li>
				<li><strong class="bold">Time-to-live</strong> (<strong class="bold">TTL</strong>): Set time-to-live values for cached items to control how long they are considered valid.</li>
			</ul>
			<p>Here are the benefits<a id="_idIndexMarker908"/> of microservice-level caching:</p>
			<ul>
				<li><strong class="bold">Improved performance</strong>: It reduces response times by serving cached data locally without making redundant calls to downstream services or databases.</li>
				<li><strong class="bold">Increased scalability</strong>: It reduces the load on backend services, enhancing overall system scalability.</li>
				<li><strong class="bold">Resilience</strong>: It provides a level of resilience by allowing microservices to continue functioning even when downstream services are temporarily unavailable.</li>
			</ul>
			<p>In this section, we have learned some of the concepts, use cases, and key considerations of microservice-level caching.</p>
			<p>With these concepts learned, we can continue with database query caching.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor211"/>Database query caching</h2>
			<p><strong class="bold">Database query caching</strong> is a technique that stores the results<a id="_idIndexMarker909"/> of frequently executed queries<a id="_idIndexMarker910"/> in a temporary memory, called a cache, for faster access. When a query is requested, the database first checks whether the query result is already in the cache. If it is, the database returns the cached result without having to execute the query again. Database query caching can improve the performance and efficiency of the database by reducing the workload and response time of the database.</p>
			<p>Here are some<a id="_idIndexMarker911"/> of its use cases:</p>
			<ul>
				<li>Caching the results of read-heavy queries.</li>
				<li>Avoiding redundant database access for static or slowly changing data.</li>
				<li>Offloading the database by serving cached results for common queries.</li>
			</ul>
			<p>Here are some key considerations<a id="_idIndexMarker912"/> for database query caching:</p>
			<ul>
				<li><strong class="bold">Query identifiers</strong>: Use unique identifiers for queries to manage and reference cached results effectively.</li>
				<li><strong class="bold">Cache invalidation</strong>: Implement strategies to invalidate the cache when underlying data changes to maintain data consistency.</li>
				<li><strong class="bold">Query complexity</strong>: Consider the complexity and cost of queries when deciding which ones to cache.</li>
			</ul>
			<p>Here are the benefits<a id="_idIndexMarker913"/> of database query caching:</p>
			<ul>
				<li><strong class="bold">Reduced database load</strong>: Caching query results reduces the need for repeated, resource-intensive database access.</li>
				<li><strong class="bold">Lower latency</strong>: It improves response times by serving cached results instead of re-executing queries against the database.</li>
				<li><strong class="bold">Improved scalability</strong>: It enhances the scalability of the overall system by reducing the load<a id="_idIndexMarker914"/> on the database.</li>
			</ul>
			<p>In summary, microservice-level caching and database query caching are essential techniques for optimizing microservices architectures. By strategically caching data at both the microservice and database layers, organizations can achieve improved performance, scalability, and responsiveness in their distributed systems.</p>
			<p>Now, we can continue to the next section, in which we will talk about message queues and publish-subscribe.</p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor212"/>Message queues and publish-subscribe</h1>
			<p>Message queues and publish-subscribe (Pub/Sub) are communication patterns commonly used in microservices<a id="_idIndexMarker915"/> architectures to facilitate asynchronous communication between services.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor213"/>Message queues</h2>
			<p>A <strong class="bold">message queue</strong> is a communication mechanism<a id="_idIndexMarker916"/> that allows microservices to send and receive messages asynchronously. Messages are placed in a queue by the sender and processed by the receiver.</p>
			<p>Here are some<a id="_idIndexMarker917"/> of its use cases:</p>
			<ul>
				<li><strong class="bold">Task distribution</strong>: A web application that processes user-uploaded files. Each file processing task is placed in a message queue, and multiple worker processes consume tasks from the queue to handle file processing concurrently.</li>
				<li><strong class="bold">Event sourcing</strong>: A system that maintains a log of events to capture changes in state. Events are published to a message queue, and various microservices subscribe to these events to update their own state.</li>
				<li><strong class="bold">Microservices communication</strong>: A system with multiple microservices where one microservice generates an event (e.g., user registration) and publishes it to a message queue. Other microservices interested in this event can subscribe to the queue to perform related actions.</li>
				<li><strong class="bold">Load leveling</strong>: A system with a peak load of requests. Instead of overwhelming a service, incoming requests are placed in a message queue. Workers consume requests from the queue, allowing the system to handle peaks more gracefully.</li>
				<li><strong class="bold">Scalability</strong>: A system where certain components have varying processing loads. By using a message queue, these components can scale independently based on their own demand, ensuring efficient resource utilization.</li>
				<li><strong class="bold">Background processing</strong>: An e-commerce platform that sends order confirmation emails. Instead of sending emails synchronously during the checkout process, the system places email tasks in a message queue, and a separate service process and sends the emails.</li>
				<li><strong class="bold">Cross-application integration</strong>: A company using multiple software applications (e.g., CRM, ERP). Integrating these applications can be achieved by placing messages in a queue when specific events occur in one application, triggering actions in another application.</li>
				<li><strong class="bold">Workflow orchestration</strong>: An order processing system where each step (e.g., order validation, payment processing, shipping) is a separate task. Each step publishes a message to a queue upon completion, triggering the next step in the workflow.</li>
				<li><strong class="bold">Delayed or scheduled tasks</strong>: A system that allows users to schedule emails to be sent at a later time. The email content and recipient details are placed in a message queue with a scheduled delivery time.</li>
				<li><strong class="bold">Log and event aggregation</strong>: Distributed applications generate logs and events. Instead of relying on individual logs, events are sent to a message queue and a centralized logging <a id="_idIndexMarker918"/>service consumes and aggregates them for analysis.</li>
			</ul>
			<p>The following are some<a id="_idIndexMarker919"/> of its key components:</p>
			<ul>
				<li><strong class="bold">Queue</strong>: A storage mechanism where messages are temporarily held until they are consumed by a service.</li>
				<li><strong class="bold">Producer</strong>: A microservice responsible for sending messages to the queue.</li>
				<li><strong class="bold">Consumer</strong>: A microservice that retrieves<a id="_idIndexMarker920"/> and processes messages from the queue.</li>
			</ul>
			<p>Here are some of its<a id="_idIndexMarker921"/> advantages:</p>
			<ul>
				<li><strong class="bold">Decoupling</strong>: It allows services to be decoupled, as the sender and receiver are not directly dependent on each other.</li>
				<li><strong class="bold">Asynchronous processing</strong>: It enables asynchronous communication, which can improve system responsiveness and scalability.</li>
				<li><strong class="bold">Load balancing</strong>: It distributes the processing load by allowing multiple instances of a service to consume messages from the queue.</li>
			</ul>
			<p><em class="italic">Figure 11</em><em class="italic">.2</em> illustrates message queues:</p>
			<div><div><img src="img/B14980_11_02.jpg" alt="Figure 11.2: Message queues"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2: Message queues</p>
			<p>Having these concepts in mind can help create a better architecture for message queues.</p>
			<p>We can continue now with publish-subscribe.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor214"/>Publish-subscribe (Pub/Sub)</h2>
			<p><strong class="bold">Pub/Sub</strong> is a messaging<a id="_idIndexMarker922"/> pattern where a microservice (the publisher) broadcasts messages to multiple subscribers. Subscribers express interest in certain types of messages and receive relevant notifications.</p>
			<p>Here are some<a id="_idIndexMarker923"/> of its use cases:</p>
			<ul>
				<li><strong class="bold">Real-time updates</strong>: A social media platform notifying users about new posts, comments, or likes in real-time.</li>
				<li><strong class="bold">Event notification</strong>: A payment gateway notifying multiple services about a successful payment transaction.</li>
				<li><strong class="bold">Distributed systems coordination</strong>: A microservices architecture where changes in user authentication trigger updates in various services, such as user profiles, permissions, and analytics.</li>
				<li><strong class="bold">Cross-cutting concerns</strong>: Publishing events related to system logs, errors, or performance metrics, allowing multiple services to subscribe and react accordingly.</li>
				<li><strong class="bold">Workflow orchestration</strong>: The orchestration of a series of tasks or processes.</li>
				<li><strong class="bold">Cross-application integration</strong>: An ecosystem of applications (CRM, ERP, Analytics) where changes in one application trigger actions in others, ensuring data consistency.</li>
				<li><strong class="bold">IoT device communication</strong>: Smart home devices publishing events related to status changes (e.g., temperature, motion detection) and multiple applications subscribing to these events for automation or monitoring</li>
				<li><strong class="bold">User notifications</strong>: A messaging application publishing events for new messages, and different clients (web, mobile, desktop) subscribing to receive real-time notifications.</li>
				<li><strong class="bold">Log aggregation and analytics</strong>: Services publishing events related to user interactions, and an analytics service subscribing to these events for centralized analysis and reporting.</li>
				<li><strong class="bold">Multi-tenant systems</strong>: A <strong class="bold">software as a service</strong> (<strong class="bold">SaaS</strong>) platform where different organizations<a id="_idIndexMarker924"/> subscribe to events related to their specific data or customizations.</li>
				<li><strong class="bold">Chat applications</strong>: Users subscribing to chat channels or rooms, and messages being published to the relevant channels for real-time delivery.</li>
				<li><strong class="bold">Dynamic configuration updates</strong>: Services subscribing to configuration change events, ensuring that they dynamically<a id="_idIndexMarker925"/> adjust their behavior based on changes.</li>
			</ul>
			<p>Here are some<a id="_idIndexMarker926"/> of its key components:</p>
			<ul>
				<li><strong class="bold">Publisher</strong>: A microservice responsible for broadcasting messages to the system.</li>
				<li><strong class="bold">Topic</strong>: Logical channels or categories to which messages are published.</li>
				<li><strong class="bold">Subscriber</strong>: A microservice that expresses interest in specific topics and receives relevant <a id="_idIndexMarker927"/>messages.</li>
			</ul>
			<p>The following are some<a id="_idIndexMarker928"/> of its advantages:</p>
			<ul>
				<li><strong class="bold">Scalability</strong>: It is well-suited for scenarios where multiple services need to react to the same event or type of information.</li>
				<li><strong class="bold">Flexibility</strong>: It allows services to subscribe to specific topics of interest, receiving only the messages they need.</li>
				<li><strong class="bold">Event-driven architecture</strong>: It supports the creation of event-driven systems where services can react to changes in state.</li>
			</ul>
			<p>You need to learn these concepts fast in order to keep updated with the latest patterns in microservices.</p>
			<p>In summary, message queues<a id="_idIndexMarker929"/> and Pub/Sub patterns are fundamental to building resilient, scalable, and loosely coupled microservices architectures. The choice between them depends on the specific requirements of the system and the<a id="_idIndexMarker930"/> desired communication<a id="_idIndexMarker931"/> patterns between services.</p>
			<p>In the next section, we will learn about event-driven architecture.</p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor215"/>Event-driven architecture</h1>
			<p><strong class="bold">Event-driven architecture</strong> (<strong class="bold">EDA</strong>) is a design paradigm that emphasizes<a id="_idIndexMarker932"/> the production, detection, consumption, and reaction to events in a system. In the context of microservices, event-driven architecture provides a flexible and scalable approach to handle communication and coordination between services.</p>
			<p>Here is the use case<a id="_idIndexMarker933"/> of event-driven architecture:</p>
			<ul>
				<li><strong class="bold">Event sourcing</strong>: Storing changes to the state of an application as a sequence of events. This helps in reconstructing the current state and auditing.</li>
				<li><strong class="bold">Real-time updates</strong>: Broadcasting real-time updates to multiple services or clients in response to certain events.</li>
				<li><strong class="bold">Workflow orchestration</strong>: Coordinating the execution of business processes across multiple microservices.</li>
				<li><strong class="bold">Log and monitoring events</strong>: Capturing events related to system logs, errors, or performance metrics for monitoring purposes.</li>
			</ul>
			<p>The following are its key<a id="_idIndexMarker934"/> concepts:</p>
			<ul>
				<li><strong class="bold">Events</strong>: Events represent occurrences or state changes in a system. Examples include user actions, system alerts, or changes in data.</li>
				<li><strong class="bold">Event producer</strong>: Microservices that generate and emit events are known as event producers. They publish events<a id="_idIndexMarker935"/> to a message broker or event bus.</li>
				<li><strong class="bold">Event consumer</strong>: Microservices that subscribe to and process events are event consumers. They react to events based on predefined logic.</li>
				<li><strong class="bold">Event bus or message broker</strong>: This acts as a communication channel that facilitates the distribution<a id="_idIndexMarker936"/> of events from producers to consumers.</li>
			</ul>
			<p>The following are the advantages<a id="_idIndexMarker937"/> of event-driven architecture:</p>
			<ul>
				<li><strong class="bold">Decoupling</strong>: Microservices become loosely coupled as they communicate through events. This reduces dependencies between services.</li>
				<li><strong class="bold">Scalability</strong>: It allows for easy scalability, as services can be added or removed without affecting the entire system.</li>
				<li><strong class="bold">Flexibility</strong>: It supports flexibility in system design, as services can be added or modified independently.</li>
				<li><strong class="bold">Asynchronicity</strong>: It enables asynchronous communication between services, promoting responsiveness and agility.</li>
			</ul>
			<p>Here is the implementation<a id="_idIndexMarker938"/> of event-driven architecture:</p>
			<ul>
				<li><strong class="bold">Message brokers</strong>: Systems often use message<a id="_idIndexMarker939"/> brokers such as <strong class="bold">Kafka</strong>, <strong class="bold">RabbitMQ</strong>, or <strong class="bold">Apache Pulsar</strong> as the underlying infrastructure<a id="_idIndexMarker940"/> to manage the flow<a id="_idIndexMarker941"/> of events.</li>
				<li><strong class="bold">Event schema</strong>: Defining a clear schema for events helps to ensure consistency and understanding between producers and consumers.</li>
				<li><strong class="bold">Event handlers</strong>: Microservices have event handlers that subscribe to specific types of events and execute predefined logic in response.</li>
				<li><strong class="bold">Event-driven microservices</strong>: Each microservice in the system can act as both a producer and a consumer of events, interacting with other services based on events.</li>
			</ul>
			<p>In summary, event-driven architecture is a powerful paradigm for building resilient and scalable microservices systems. It enables a more responsive and adaptable architecture by fostering loose coupling between microservices, allowing them to evolve independently. Properly implemented, EDA contributes to a more agile and efficient microservices ecosystem.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor216"/>Summary</h1>
			<p>In this chapter, we have learned a lot about microservices, how to deal with caching, and the different types of caching.</p>
			<p>In summary, caching and asynchronous messaging are two techniques that can improve the performance, scalability, and reliability of microservice-based applications. Caching is the process of storing frequently accessed or expensive data in a temporary storage area, such as <strong class="bold">Redis</strong>, to reduce the latency and the load on the primary data source. Asynchronous messaging is the process of exchanging data between microservices or clients in a non-blocking and event-driven manner, using a message broker such as <strong class="bold">Amazon SQS</strong> or <strong class="bold">Amazon SNS</strong>. Caching and asynchronous messaging can help to overcome some of the challenges of microservices, such as complexity, eventual consistency, and network failures. However, they also require careful design and trade-offs, such as data freshness, data synchronization, and message ordering.</p>
			<p>In the next chapter, we are going to learn about ensuring data security with the saga pattern, encryption, and security measures.</p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor217"/>Quiz time</h1>
			<ul>
				<li>What is client-side caching and edge caching?</li>
				<li>What is microservice-level caching?</li>
				<li>What are message queues and publish-subscribe?</li>
				<li>What is event-driven architecture?</li>
			</ul>
		</div>
	</body></html>