<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Advanced Techniques</h1>
                </header>
            
            <article>
                
<p>In the previous chapters of this book, we covered many foundational computer graphics concepts that, ultimately, gave us the knowledge and skills to build a 3D virtual car showroom. This means that at this point, you have all of the information you need to create rich 3D applications with WebGL. However, we've only just scratched the surface of what's possible with WebGL! Creative use of shaders, textures, and vertex attributes can yield fantastic results. In these final chapters, we'll cover a few advanced WebGL concepts that should leave you eager to explore more.</p>
<p>In this chapter, we will cover the following:</p>
<ul>
<li>Learn various post-processing effects</li>
<li>Implementing a particle system using point sprites</li>
<li>Understand how to use normal mapping</li>
<li>Implement how to use ray tracing</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Post-Processing</h1>
                </header>
            
            <article>
                
<p><strong>Post-processing</strong> is the process of adding effects by re-rendering the image of the scene with a shader that alters the final image. You can think of this as the process of taking a screenshot of your scene (ideally at <kbd>60+</kbd> frames per second), opening it up in your favorite image editor, and applying various filters. The difference is, of course, that we can do so in real time!</p>
<p class="mce-root"/>
<p>Some examples of simple post-processing effects include the following:</p>
<ul>
<li>Grayscale</li>
<li>Sepia tone</li>
<li>Inverted colors</li>
<li>Film grain</li>
<li>Blur</li>
<li>Wavy/dizzy effect</li>
</ul>
<p>The basic technique for creating these effects is relatively simple: create a framebuffer with the same dimensions as the <kbd>canvas</kbd> and have the entire scene rendered to it at the beginning of the <kbd>draw</kbd> cycle. Then, a quad is rendered to the default framebuffer using the texture that makes up the framebuffer's color attachment. The shader used during the rendering of the quad is what contains the post-process effect. That shader can transform the color values of the rendered scene as they get written to the quad to produce the desired visuals.</p>
<p>Let's investigate the individual steps of this process more closely.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the Framebuffer</h1>
                </header>
            
            <article>
                
<p>The code we will use to create the framebuffer is nearly the same as what we created earlier in <a href="b06d92d3-3687-476f-a181-e7dd3aac1b8f.xhtml" target="_blank">Chapter 8</a>, <em>Picking</em>. There are, however, a few key differences worth noting:</p>
<div>
<pre><span>const </span>{ width<span>, </span>height } = canvas<span>;<br/></span><span><br/></span><span>// 1. Init Color Texture<br/></span><span>const </span>texture = gl.<span>createTexture</span>()<span>;<br/></span>gl.<span>bindTexture</span>(gl.<span>TEXTURE_2D</span><span>, </span>texture)<span>;<br/></span>gl.<span>texParameteri</span>(gl.<span>TEXTURE_2D</span><span>, </span>gl.<span>TEXTURE_MAG_FILTER</span><span>, </span>gl.<span>NEAREST</span>)<span>;<br/></span>gl.<span>texParameteri</span>(gl.<span>TEXTURE_2D</span><span>, </span>gl.<span>TEXTURE_MIN_FILTER</span><span>, </span>gl.<span>NEAREST</span>)<span>;<br/></span>gl.<span>texParameteri</span>(gl.<span>TEXTURE_2D</span><span>, </span>gl.<span>TEXTURE_WRAP_S</span><span>, </span>gl.<span>CLAMP_TO_EDGE</span>)<span>;<br/></span>gl.<span>texParameteri</span>(gl.<span>TEXTURE_2D</span><span>, </span>gl.<span>TEXTURE_WRAP_T</span><span>, </span>gl.<span>CLAMP_TO_EDGE</span>)<span>;<br/></span>gl.<span>texImage2D</span>(gl.<span>TEXTURE_2D</span><span>, </span><span>0</span><span>, </span>gl.<span>RGBA</span><span>, </span>width<span>, </span>height<span>, </span><span>0</span><span>, </span>gl.<span>RGBA</span><span>, </span>gl.<span>UNSIGNED_BYTE</span><span>, </span><span>null</span>)<span>;<br/></span><span><br/></span><span>// 2. Init Renderbuffer<br/></span><span>const </span>renderbuffer = gl.<span>createRenderbuffer</span>()<span>;<br/></span>gl.<span>bindRenderbuffer</span>(gl.<span>RENDERBUFFER</span><span>, </span>renderbuffer)<span>;<br/></span>gl.<span>renderbufferStorage</span>(gl.<span>RENDERBUFFER</span><span>, </span>gl.<span>DEPTH_COMPONENT16</span><span>, </span>width<span>, </span>height)<span>;<br/></span><span><br/></span><span>// 3. Init Framebuffer<br/></span><span>const </span>framebuffer = gl.<span>createFramebuffer</span>()<span>;<br/></span>gl.<span>bindFramebuffer</span>(gl.<span>FRAMEBUFFER</span><span>, </span>framebuffer)<span>;<br/></span>gl.<span>framebufferTexture2D</span>(gl.<span>FRAMEBUFFER</span><span>, </span>gl.<span>COLOR_ATTACHMENT0</span><span>, </span>gl.<span>TEXTURE_2D</span><span>, </span>texture<span>, </span><span>0</span>)<span>;<br/></span>gl.<span>framebufferRenderbuffer</span>(gl.<span>FRAMEBUFFER</span><span>, </span>gl.<span>DEPTH_ATTACHMENT</span><span>, </span>gl.<span>RENDERBUFFER</span><span>, </span>renderbuffer)<span>;<br/></span><span><br/></span><span>// 4. Clean up<br/></span>gl.<span>bindTexture</span>(gl.<span>TEXTURE_2D</span><span>, </span><span>null</span>)<span>;<br/></span>gl.<span>bindRenderbuffer</span>(gl.<span>RENDERBUFFER</span><span>, </span><span>null</span>)<span>;<br/></span>gl.<span>bindFramebuffer</span>(gl.<span>FRAMEBUFFER</span><span>, </span><span>null</span>)<span>;</span></pre></div>
<p>We use the <kbd>width</kbd> and <kbd>height</kbd> of the <kbd>canvas</kbd> to determine our buffer size, instead of using the arbitrary values that were used for the picker. Because the content of the picker buffer is not for rendering to the screen, we don't need to worry about resolution as much. For the post-process buffer, however, we'll get the best results if the output matches the dimensions of the <kbd>canvas</kbd>.</p>
<p>Since the texture will be exactly the same size as the <kbd>canvas</kbd>, and since we're rendering it as a full-screen quad, we've created a situation where the texture will be displayed at exactly a <kbd>1:1</kbd> ratio on the screen. This means that no filters need to be applied and that we can use <kbd>NEAREST</kbd> filtering with no visual artifacts. Also, in post-processing cases where we want to warp the texture coordinates (such as the wavy effect), we would benefit from using <kbd>LINEAR</kbd> filtering. We also need to use a wrap mode of <kbd>CLAMP_TO_EDGE</kbd>. That being said, the code is nearly identical to the <kbd>Picker</kbd> we used for framebuffer creation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the Geometry</h1>
                </header>
            
            <article>
                
<p>Although we could load the quad from a file, the geometry is simple enough that we can include it directly in the code. All that's needed are the vertex positions and texture coordinates:</p>
<pre><span>// 1. Define the geometry for the full-screen quad<br/></span><span>const </span>vertices = [<br/>  -<span>1</span><span>, </span>-<span>1</span><span>,<br/></span><span>   </span><span>1</span><span>, </span>-<span>1</span><span>,<br/></span><span>  </span>-<span>1</span><span>,  </span><span>1</span><span>,<br/></span><span><br/></span><span>  </span>-<span>1</span><span>,  </span><span>1</span><span>,<br/></span><span>   </span><span>1</span><span>, </span>-<span>1</span><span>,<br/></span><span>   </span><span>1</span><span>,  </span><span>1<br/></span>]<span>;<br/></span><span><br/></span><span>const </span>textureCoords = [<br/>  <span>0</span><span>, </span><span>0</span><span>,<br/></span><span>  </span><span>1</span><span>, </span><span>0</span><span>,<br/></span><span>  </span><span>0</span><span>, </span><span>1</span><span>,<br/></span><span><br/></span><span>  </span><span>0</span><span>, </span><span>1</span><span>,<br/></span><span>  </span><span>1</span><span>, </span><span>0</span><span>,<br/></span><span>  </span><span>1</span><span>, </span><span>1<br/></span>]<span>;<br/><br/>// 2. Create and bind VAO<br/>const vao = gl.createVertexArray();<br/>gl.bindVertexArray(vao);<br/></span><span><br/></span><span>// 3. Init the buffers<br/></span><span>const vertexBuffer </span>= gl.<span>createBuffer</span>()<span>;<br/></span>gl.<span>bindBuffer</span>(gl.<span>ARRAY_BUFFER</span><span>, </span><span>vertexBuffer</span>)<span>;<br/></span>gl.<span>bufferData</span>(gl.<span>ARRAY_BUFFER</span><span>, </span><span>new </span>Float32Array(vertices)<span>, <br/>// Configure instructions for VAO<br/></span>gl.<span>STATIC_DRAW</span>)<span>;</span><span>gl</span>.<span>enableVertexAttribArray</span>(<span>program</span>.<span>aVertexPosition</span>)<span>;<br/></span><span>gl</span>.<span>vertexAttribPointer</span>(<span>program</span>.<span>aVertexPosition</span><span>, </span><span>3</span><span>, </span><span>gl</span>.<span>FLOAT</span><span>, </span><span>false</span><span>, </span><span>0</span><span>, </span><span>0</span>)<span>;<br/><br/></span><span>const textureBuffer </span>= gl.<span>createBuffer</span>()<span>;<br/></span>gl.<span>bindBuffer</span>(gl.<span>ARRAY_BUFFER</span><span>, </span><span>textureBuffer</span>)<span>;<br/></span>gl.<span>bufferData</span>(gl.<span>ARRAY_BUFFER</span><span>, </span><span>new </span>Float32Array(textureCoords)<span>, </span>gl.<span>STATIC_DRAW</span>)<span>;<br/>// Configure instructions for VAO<br/></span><span>gl</span>.<span>enableVertexAttribArray</span>(<span>program</span>.aVertexTextureCoords)<span>;<br/></span><span>gl</span>.<span>vertexAttribPointer</span>(<span>program</span>.aVertexTextureCoords<span>, </span><span>2</span><span>, </span><span>gl</span>.<span>FLOAT</span><span>, </span><span>false</span><span>, </span><span>0</span><span>, </span><span>0</span>)<span>;<br/><br/></span><span>// 4. Clean up<br/>gl.bindVertexArray(null);<br/></span>gl.<span>bindBuffer</span>(gl.<span>ARRAY_BUFFER</span><span>, </span><span>null</span>)<span>;</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the Shader</h1>
                </header>
            
            <article>
                
<p>The vertex shader for the post-process draw is quite simple:</p>
<div>
<pre>#version 300 es<br/>precision mediump float;<br/><br/>in vec2 aVertexPosition;<br/>in vec2 aVertexTextureCoords;<br/><br/>out vec2 vTextureCoords;<br/><br/>void main(void) {<br/>  vTextureCoords = aVertexTextureCoords;<br/>  gl_Position = vec4(aVertexPosition, 0.0, 1.0);<br/>}</pre></div>
<p>Notice that unlike the other vertex shaders we've worked with so far, this one doesn't use any matrices. That's because the vertices we declared in the previous step are <strong>pre-transformed</strong>.</p>
<p>Recall from <a href="62d4de32-0b5b-4339-8fcc-80f739e80ec2.xhtml" target="_blank">Chapter 4</a>, <em>Cameras, </em>that we retrieved normalized device coordinates by multiplying the vertex position by the Projection matrix. Here, the coordinates mapped all positions to a <kbd>[-1, 1]</kbd> range on each axis, which represents the full viewport. In this case, however, our vertex positions are already mapped to a <kbd>[-1, 1]</kbd> range; therefore, no transformation is needed because they will map perfectly to the viewport bounds when we render.</p>
<p>The fragment shader is where most of the interesting operations happen. The fragment shader will be different for every post-process effect. Let's look at a simple <strong>grayscale effect</strong> as an example:</p>
<div>
<pre>#version 300 es<br/>precision mediump float;<br/><br/>uniform sampler2D uSampler;<br/><br/>in vec2 vTextureCoords;<br/><br/>out vec4 fragColor;<br/><br/>void main(void) {<br/>  vec4 frameColor = texture(uSampler, vTextureCoords);<br/>  float luminance = frameColor.r * 0.3 + frameColor.g * 0.59 + frameColor.b <br/>   * 0.11;<br/>  fragColor = vec4(luminance, luminance, luminance, frameColor.a);<br/>}</pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the preceding code, we sample the original color rendered by the scene (available through <kbd>uSampler</kbd>) and output a color that is a weighted average of the red, green, and blue channels. The result is a simple grayscale version of the original scene:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d0e14317-62cd-42b0-9c74-edfff270a994.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architectural Updates</h1>
                </header>
            
            <article>
                
<p>We've added a new class, <kbd>PostProcess</kbd>, to assist with the post-processing effects. This code can be found in the <kbd>common/js/PostProcess.js</kbd> file. This class will create the appropriate framebuffer and quad geometry, compile the post-process shader, and set up the render needed to draw the scene out to the quad.</p>
<p>Let's see how this component works with an example!</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Time for Action: Post-Process Effects</h1>
                </header>
            
            <article>
                
<p>Let's see a few post-processing effects in action:</p>
<ol>
<li>Open the <kbd>ch10_01_post-process.html</kbd> file in your browser, like so:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2f595736-ad10-4585-b60e-27a2d09232b3.png"/></p>
<ol start="2">
<li>The controls dropdown allows you to switch between different sample effects. Try them out to get a feel for the effects they have on the scene. We've already looked at grayscale, so let's examine the rest of the filters individually.</li>
<li>The <strong>invert effect</strong>, similar to grayscale in that it only modifies the color output, inverts each color channel:</li>
</ol>
<div>
<pre style="padding-left: 60px">#version 300 es<br/>precision mediump float;<br/><br/>uniform sampler2D uSampler;<br/><br/>in vec2 vTextureCoords;<br/><br/>out vec4 fragColor;<br/><br/>void main(void) {<br/>  vec4 frameColor = texture(uSampler, vTextureCoords);<br/>  fragColor = vec4(vec3(1.0) - frameColor.rgb, frameColor.a);<br/>}</pre></div>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b6c2a988-ed35-4c64-9093-11daeb61d673.png"/></p>
<ol start="4">
<li>The <strong>wavy effect </strong>manipulates the texture coordinates to make the scene swirl and sway. In this effect, we also provide the current time to allow the distortion to change as time progresses:</li>
</ol>
<div>
<pre style="padding-left: 60px">#version 300 es<br/>precision mediump float;<br/><br/>const float speed = 15.0;<br/>const float magnitude = 0.015;<br/><br/>uniform sampler2D uSampler;<br/>uniform float uTime;<br/><br/>in vec2 vTextureCoords;<br/><br/>out vec4 fragColor;<br/><br/>void main(void) {<br/>  vec2 wavyCoord;<br/>  wavyCoord.s = vTextureCoords.s + sin(uTime + vTextureCoords.t * <br/>   speed) * magnitude;<br/>  wavyCoord.t = vTextureCoords.t + cos(uTime + vTextureCoords.s * <br/>   speed) * magnitude;<br/>  fragColor = texture(uSampler, wavyCoord);<br/>}</pre></div>
<p class="CDPAlignCenter CDPAlign"><img src="assets/249f5635-ab2e-4923-94c9-8ba0f2c4167d.png"/></p>
<p class="mce-root"/>
<ol start="5">
<li>The <strong>blur effect </strong>samples several pixels around the current pixel and uses a weighted blend to produce a fragment output that is the average of its neighbors. This gives a blurry feel to the scene. A new uniform, <kbd>uInverseTextureSize</kbd><span>, provides values that are</span> <kbd>1</kbd> <span>over the</span> <kbd>width</kbd> <span>and</span> <kbd>height</kbd> <span>of the viewport. We use these values to accurately target individual pixels within the texture. For example, </span><kbd>vTextureCoords.x + 2 * uInverseTextureSize.x</kbd><span> will be exactly 2 pixels to the left of the original texture coordinate:</span></li>
</ol>
<div>
<pre style="padding-left: 60px">#version 300 es<br/>precision mediump float;<br/><br/>uniform sampler2D uSampler;<br/>uniform vec2 uInverseTextureSize;<br/><br/>in vec2 vTextureCoords;<br/><br/>out vec4 fragColor;<br/><br/>vec4 offsetLookup(float xOff, float yOff) {<br/>  return texture(<br/>    uSampler, <br/>    vec2(<br/>      vTextureCoords.x + xOff * uInverseTextureSize.x, <br/>      vTextureCoords.y + yOff * uInverseTextureSize.y<br/>    )<br/>  );<br/>}<br/><br/>void main(void) {<br/>  vec4 frameColor = offsetLookup(-4.0, 0.0) * 0.05;<br/>  frameColor += offsetLookup(-3.0, 0.0) * 0.09;<br/>  frameColor += offsetLookup(-2.0, 0.0) * 0.12;<br/>  frameColor += offsetLookup(-1.0, 0.0) * 0.15;<br/>  frameColor += offsetLookup(0.0, 0.0) * 0.16;<br/>  frameColor += offsetLookup(1.0, 0.0) * 0.15;<br/>  frameColor += offsetLookup(2.0, 0.0) * 0.12;<br/>  frameColor += offsetLookup(3.0, 0.0) * 0.09;<br/>  frameColor += offsetLookup(4.0, 0.0) * 0.05;<br/>  fragColor = frameColor;<br/>}</pre></div>
<p class="CDPAlignCenter CDPAlign"><img src="assets/938d5656-3092-425d-9bbb-024feb693459.png"/></p>
<ol start="6">
<li>Our final example is a <strong>film grain </strong>effect. This uses a noisy texture to create a grainy scene, which simulates the use of an old camera. This example is significant because it demonstrates the use of a second texture besides the framebuffer when rendering:</li>
</ol>
<div>
<pre style="padding-left: 60px">#version 300 es<br/>precision mediump float;<br/><br/>const float grainIntensity = 0.1;<br/>const float scrollSpeed = 4000.0;<br/><br/>uniform sampler2D uSampler;<br/>uniform sampler2D uNoiseSampler;<br/>uniform vec2 uInverseTextureSize;<br/>uniform float uTime;<br/><br/>in vec2 vTextureCoords;<br/><br/>out vec4 fragColor;<br/><br/>void main(void) {<br/>  vec4 frameColor = texture(uSampler, vTextureCoords);<br/>  vec4 grain = texture(<br/>    uNoiseSampler,<br/>    vTextureCoords * 2.0 + uTime * scrollSpeed * <br/>     uInverseTextureSize<br/>  );<br/>  fragColor = frameColor - (grain * grainIntensity);<br/>}</pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/164be51d-f5d2-44f8-9653-7d214157d51d.png" style="width:44.25em;height:28.00em;"/></p>
</div>
<p><em><strong>What just happened?</strong></em></p>
<p>All of these effects are achieved by manipulating the rendered image before it is outputted to the screen. Since the amount of geometry processed for these effects is small, they are efficient, regardless of the scene's complexity. That being said, performance may be affected as the size of the <kbd>canvas</kbd> or the complexity of the post-process shader increases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Have a Go: Funhouse Mirror Effect</h1>
                </header>
            
            <article>
                
<p>What would it take to create a post-process effect that stretches the image near the center of the viewport and squashes it toward the edges?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Point Sprites</h1>
                </header>
            
            <article>
                
<p><strong>Particle effects </strong>is a common technique used in many 3D applications and games. A particle effect is a generic term for any special effect created by rendering groups of <strong>particles</strong> (displayed as points, textured quads, or repeated geometry), typically with some simple physics simulation acting on the individual particles. They can be used for simulating smoke, fire, bullets, explosions, water, sparks, and many other effects that are difficult to represent by a single geometric model.</p>
<p>One very efficient way of rendering particles is to use <strong>point sprites</strong>. Throughout this book, we've been rendering triangle primitives, but if you render vertices with the <kbd>POINTS</kbd> primitive type, then each vertex will be rendered as a single pixel on the screen. A point sprite is an extension of the <kbd>POINTS</kbd> primitive rendering, where each point is provided a size and is textured in the shader.</p>
<p>A point sprite is created by setting the <kbd>gl_PointSize</kbd> value in the vertex shader. It can be set to either a constant value or a value calculated from shader inputs. If it's set to a number greater than one, the point is rendered as a quad that always faces the screen (also known as a <strong>billboard</strong>). The quad is centered on the original point and has a width and height equal to the <kbd>gl_PointSize</kbd> in pixels:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/938e6674-d890-4607-9d57-6a9fb0876d0e.png" style="width:34.17em;height:26.00em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When the point sprite is rendered, it also generates texture coordinates for the quad, covering a simple <kbd>0-1</kbd> range from the upper left to the lower right:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9f53f4e7-6be9-4a49-b2ad-a0739bb6197b.png" style="width:41.42em;height:33.42em;"/></p>
<p>The texture coordinates are accessible in the fragment shader by the built-in <kbd>vec2 gl_PointCoord</kbd>. Combining these properties gives us a simple point sprite vertex shader that looks like this:</p>
<div>
<pre>#version 300 es<br/>precision mediump float;<br/><br/>uniform mat4 uModelViewMatrix;<br/>uniform mat4 uProjectionMatrix;<br/>uniform float uPointSize;<br/><br/>in vec4 aParticle;<br/><br/>out float vLifespan;<br/><br/>void main(void) {<br/>  gl_Position = uProjectionMatrix * uModelViewMatrix * vec4(aParticle.xyz, <br/>   1.0);<br/>  vLifespan = aParticle.w;<br/>  gl_PointSize = uPointSize * vLifespan;<br/>}</pre></div>
<p>The corresponding fragment shader looks like this:</p>
<div>
<pre>#version 300 es<br/>precision mediump float;<br/><br/>uniform sampler2D uSampler;<br/><br/>in float vLifespan;<br/><br/>out vec4 fragColor;<br/><br/>void main(void) {<br/>  vec4 texColor = texture(uSampler, gl_PointCoord);<br/>  fragColor = vec4(texColor.rgb, texColor.a * vLifespan);<br/>}</pre></div>
<p>The following is an example of the appropriate draw command:</p>
<div>
<pre>gl.drawArrays(gl.POINTS, 0, vertexCount);</pre></div>
<p>This renders each point in the vertex buffer as a <kbd>16x16</kbd> texture.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Time for Action: Fountain of Sparks</h1>
                </header>
            
            <article>
                
<p>Let's see how we can use point sprites to create a fountain of sparks:</p>
<ol>
<li>Open the <kbd>ch10_02_point-sprites.html</kbd> file in your browser:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/382fc860-9d2a-42be-a19c-2f298a87d2b9.png"/></p>
<ol start="2">
<li>This example showcases a simple <em>fountain of sparks </em>effect with point sprites. You can adjust the size and lifetime of the particles by using the sliders.</li>
<li>The particle simulation is performed by maintaining a list of particles that are comprised of position, velocity, and lifespan. In every frame, we iterate through the list and move the particles according to the velocity; we also apply gravity and reduce the remaining lifespan. Once a particle's lifespan has reached <kbd><span><span>0</span></span></kbd>, it's reset to the origin with a random velocity and updated lifespan.</li>
<li>With every iteration of the particle's simulation, the particle positions and lifespans are copied to an array that is then used to update a vertex buffer. This vertex buffer is what is rendered to produce the onscreen sprites.</li>
<li>Let's experiment with some of the other values that control the simulation and see how they affect the scene. Open up <kbd><span>ch10_02_point-sprites.html</span></kbd> in your editor.</li>
</ol>
<ol start="6">
<li>First, locate the call to <kbd>configureParticles</kbd> at the bottom of the <kbd>configure</kbd> function. The number passed as an argument, initially set to <kbd>1024</kbd>, determines how many particles are created. Try changing it to a lower or higher value to see the effect it has on the particle system. Be careful, though, since extremely high values (for example, in the millions) may cause performance issues.</li>
<li>Next, find the <kbd>resetParticle</kbd> function. This function is called any time a particle is created or reset. There are several values here that can have a significant effect on how the scene renders:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>function </span><span>resetParticle</span>(particle) {<br/>  particle.<span>position </span>= [<span>0</span><span>, </span><span>0</span><span>, </span><span>0</span>]<span>;<br/></span><span><br/></span><span>  </span>particle.<span>velocity </span>= [<br/>    Math.<span>random</span>() * <span>20 </span>- <span>10</span><span>,<br/></span><span>    </span>Math.<span>random</span>() * <span>20</span><span>,<br/></span><span>    </span>Math.<span>random</span>() * <span>20 </span>- <span>10</span><span>,<br/></span><span>  </span>]<span>;<br/></span><span><br/></span><span>  </span>particle.<span>lifespan </span>= Math.<span>random</span>() * particleLifespan<span>;<br/></span><span>  </span>particle.<span>remainingLife </span>= particle.<span>lifespan</span><span>;<br/></span>}</pre></div>
<ol start="8">
<li>The <kbd>particle.position</kbd> is the <kbd>x</kbd>, <kbd>y</kbd>, <kbd>z</kbd> starting coordinates for the particle. Initially, all points start at the world origin <kbd>(0, 0, 0)</kbd>, but this could be set to anything. It's often desirable to have the particles originate from the location of another object so as to give the impression that the object is producing the particles. You can also randomize the position to make the particles appear within a given area.</li>
<li><kbd>particle.velocity</kbd> is the initial velocity of the particle. Here, you can see that it has been randomized so that particles spread out as they move away from the origin. Particles that move in random directions tend to look more like explosions or sprays, while those that move in the same direction give the appearance of a steady stream. In this case, the <kbd>y</kbd> value is designed to always be positive, while the <kbd>x</kbd> and <kbd>z</kbd> values may either be positive or negative. Experiment with what happens when you increase or decrease these velocity values or remove the random element from one of the components.</li>
<li>Finally, <kbd>particle.lifespan</kbd> determines how long a particle is displayed before being reset. This uses the value from the controls while being randomized to provide visual variety. If you remove the random element from the particle lifespan, all of the particles will expire and reset at the same time, resulting in fireworks-like <em>bursts </em>of particles.</li>
</ol>
<ol start="11">
<li>Next, find the <kbd>updateParticles</kbd> function. This function is called once per frame to update the position and velocity of all particles before pushing the new values to the vertex buffer. It's interesting to note that in terms of manipulating the simulation behavior, gravity is applied mid-way through the function:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>function </span><span>updateParticles</span>(elapsed) {<br/>  <span>// Loop through all the particles in the array<br/></span><span>  </span>particles.<span>forEach</span>((particle<span>, </span>i) =&gt; {<br/>    <span>// Track the particles lifespan<br/></span><span>    </span>particle.<span>remainingLife </span>-= elapsed<span>;<br/></span><span>    <br/></span><span>    </span><span>if </span>(particle.<span>remainingLife </span>&lt;= <span>0</span>) {<br/>      <span>// Once the particle expires, reset it to the origin with a <br/>      // new velocity<br/></span><span>      </span><span>resetParticle</span>(particle)<span>;<br/></span><span>    </span>}<br/><br/>    <span>// Update the particle position<br/></span><span>    </span>particle.<span>position</span>[<span>0</span>] += particle.<span>velocity</span>[<span>0</span>] * elapsed<span>;<br/></span><span>    </span>particle.<span>position</span>[<span>1</span>] += particle.<span>velocity</span>[<span>1</span>] * elapsed<span>;<br/></span><span>    </span>particle.<span>position</span>[<span>2</span>] += particle.<span>velocity</span>[<span>2</span>] * elapsed<span>;<br/></span><span><br/></span><span>    </span><span>// Apply gravity to the velocity<br/></span><span>    </span>particle.<span>velocity</span>[<span>1</span>] -= <span>9.8 </span>* elapsed<span>;<br/></span><span>    <br/></span><span>    </span><span>if </span>(particle.<span>position</span>[<span>1</span>] &lt; <span>0</span>) {<br/>      <span>// Allow particles to bounce off the floor<br/></span><span>      </span>particle.<span>velocity</span>[<span>1</span>] *= -<span>0.75</span><span>;<br/></span><span>      </span>particle.<span>position</span>[<span>1</span>] = <span>0</span><span>;<br/></span><span>    </span>}<br/><br/>    <span>// Update the corresponding values in the array<br/></span><span>    </span><span>const </span>index = i * <span>4</span><span>;<br/></span><span>    </span>particleArray[index] = particle.<span>position</span>[<span>0</span>]<span>;<br/></span><span>    </span>particleArray[index + <span>1</span>] = particle.<span>position</span>[<span>1</span>]<span>;<br/></span><span>    </span>particleArray[index + <span>2</span>] = particle.<span>position</span>[<span>2</span>]<span>;<br/></span><span>    </span>particleArray[index + <span>3</span>] = particle.<span>remainingLife </span>/ <br/>     particle.<span>lifespan</span><span>;<br/></span><span>  </span>})<span>;<br/></span><span><br/></span><span>  </span><span>// Once we are done looping through all the particles, update the <br/>  // buffer once<br/></span><span>  </span>gl.<span>bindBuffer</span>(gl.<span>ARRAY_BUFFER</span><span>, </span>particleBuffer)<span>;<br/></span><span>  </span>gl.<span>bufferData</span>(gl.<span>ARRAY_BUFFER</span><span>, </span>particleArray<span>, </span>gl.<span>STATIC_DRAW</span>)<span>;<br/></span><span>  </span>gl.<span>bindBuffer</span>(gl.<span>ARRAY_BUFFER</span><span>, </span><span>null</span>)<span>;<br/></span>}</pre></div>
<p class="mce-root"/>
<ol start="12">
<li>The <kbd>9.8</kbd> here is the acceleration applied to the <kbd>y</kbd> component over time. In other words, this is the gravity. We can remove this calculation entirely to create an environment where the particles float indefinitely along their original trajectories. We can increase the value to make the particles fall very quickly (giving them a <em>heavy</em> appearance), or we can change the component that the deceleration is applied to so that we can change the direction of gravity. For example, subtracting from <kbd>velocity[0]</kbd> makes the particles <em>fall</em> sideways.</li>
<li>This is also where we apply a simple collision response with the <em>floor</em>. Any particles with a <kbd>y</kbd><em> </em>position less than <kbd>0</kbd> (below the floor) have their velocities reversed and reduced. This gives us a realistic bouncing motion. We can make the particles less bouncy by reducing the multiplier (that is, <kbd>0.25</kbd> instead of <kbd>0.75</kbd>) or even eliminate bouncing altogether by simply setting the <kbd>y</kbd> velocity to <kbd>0</kbd>. Additionally, we can remove the floor by taking away the check for <kbd>y &lt; 0</kbd>, which will allow the particles to fall indefinitely.</li>
<li>It's also worth seeing the different effects we can achieve with different textures. Try changing the path for the <kbd>spriteTexture</kbd> in the <kbd>configure</kbd> function to see what it looks like when you use different images.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We've seen how point sprites can be used to efficiently render particle effects. We've also seen the different ways that we can manipulate a particle simulation to achieve various effects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Have a Go: Bubbles!</h1>
                </header>
            
            <article>
                
<p>The particle system in place here could be used to simulate bubbles or smoke floating upward just as easily as bouncing sparks. How would you change the simulation to make the particles float rather than fall?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normal Mapping</h1>
                </header>
            
            <article>
                
<p>A very powerful and popular technique among real-time 3D applications is <strong>normal mapping</strong>. Normal mapping creates the illusion of highly detailed geometry on a low-poly model by storing surface normals in a texture map that can then be used to calculate the lighting of the objects. This method is especially popular in modern games, since this allows developers to strike a balance between high performance and scene detail.</p>
<p>Typically, lighting is calculated by using the surface normals of the triangles being rendered, meaning that the entire polygon will be lit as a continuous, smooth surface:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ee00b4a8-4719-401e-a59e-bd24ef025568.png" style="width:56.67em;height:15.75em;"/></p>
<p>With normal mapping, the surface normals are replaced by normals that are encoded in a texture that give the appearance of a rough or bumpy surface. Note that the actual geometry is not changed when using a normal map <span>– </span>only how it's lit changes. If you look at a normal mapped polygon from the side, it will still appear to be perfectly flat:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/60c3eee4-74e2-4112-a026-ef7161857799.png" style="width:55.75em;height:18.92em;"/></p>
<p>The texture used to store the normals is called a <strong>normal map</strong>, and it's typically paired with a specific diffuse texture that complements the surface that the normal map is trying to simulate. For example, here is a diffuse texture of some flagstones and the corresponding normal map:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8b264e92-611f-4032-bc33-bcef53831320.png" style="width:38.42em;height:21.75em;"/></p>
<p>You can see that the normal map contains a similar pattern to the diffuse texture. Together, the two textures give the appearance that the stones are raised with a rough finish, while the grout is sunken in.</p>
<div class="packt_infobox"><strong><span class="packt_screen">Mapping Techniques</span><br/></strong>Although normal mapping is a powerful technique for efficiently adding more detail to assets, there are many other mapping techniques that follow the same line of reasoning. You can read about some of the other techniques that are available for use here: <a href="https://en.wikipedia.org/wiki/Category:Texture_mapping">https://en.wikipedia.org/wiki/Category:Texture_mapping</a>.</div>
<p>The normal map contains custom-formatted color information that can be interpreted by the shader at runtime as a fragment normal. A fragment normal is essentially the same as a vertex normal: it is a three-component vector that points away from the surface. The normal texture encodes the three components of the normal vector into the three channels of the texture's texel color. Red represents the <kbd>x-axis</kbd>, green represents the <kbd>y-axis</kbd>, and blue represents the <kbd>z-axis</kbd>.</p>
<p>The normals that have been encoded are typically stored in <strong>tangent space</strong>, as opposed to world or object space. Tangent space is the coordinate system for the texture coordinates of a face. Normal maps are commonly blue, since the normals they represent generally point away from the surface and thus have larger <kbd>z</kbd> components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Time for Action: Normal Mapping in Action</h1>
                </header>
            
            <article>
                
<p>Let's cover an example showcasing normal mapping in action:</p>
<ol>
<li>Open the <kbd>ch10_03_normal-map.html</kbd> file in a browser:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/be34f773-74b6-4a56-b78a-89b58c7f91bd.png" style="width:42.92em;height:34.92em;"/></p>
<ol start="2">
<li>Rotate the cube to see the effect that the normal map has on the lit cube. Keep in mind that the profile of the cube has not changed. Let's examine how this effect is achieved.</li>
<li>First, we need to add a new attribute to our vertex buffers. There are three vectors needed to calculate the tangent space coordinates for lighting: the <strong>normal</strong>, the <strong>tangent</strong>,<strong> </strong>and the <strong>bitangent</strong>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aa0741c1-0221-41ff-ae40-fd2904f6bc31.png" style="width:15.92em;height:16.92em;"/></p>
<ol start="4">
<li>We have already covered normals, so let's investigate the other two vectors. The tangent represents the <em>up</em><span> (positive <kbd>y</kbd>) vector for the texture relative to the polygon surface. The bitangent represents the </span><em>left</em><span> (positive <kbd>x</kbd>) vector for the texture relative to the polygon surface.</span></li>
<li>We only need to provide two of the three vectors as vertex attributes. Traditionally, the normal and tangent suffice, as the third vector is calculated as the cross-product of the other two in the vertex shader.</li>
<li>It is common for 3D modeling packages to generate tangents for you. However, if they aren't provided, they can be calculated from the vertex positions and texture coordinates, similar to calculating vertex normals:</li>
</ol>
<div class="packt_infobox"><strong><span class="packt_screen">Tangent Generation Algorithm</span></strong><br/>
We won't cover this algorithm here, but for reference, it has been implemented in <kbd>common/js/utils.js</kbd> as <kbd>calculateTangents</kbd> and used in <kbd>scene.add</kbd>.</div>
<div>
<pre style="padding-left: 60px"><span>const </span>tangentBufferObject = gl.<span>createBuffer</span>()<span>;<br/></span><br/>gl.<span>bindBuffer</span>(gl.<span>ARRAY_BUFFER</span><span>, </span>tangentBufferObject)<span>;<br/><br/></span>gl.<span>bufferData</span>(<br/>  gl.<span>ARRAY_BUFFER</span><span>,<br/></span><span>  </span><span>new </span>Float32Array(utils.<span>calculateTangents</span>(<br/>    object.<span>vertices</span><span>,<br/></span><span>    </span>object.textureCoords<span>,<br/></span><span>    </span>object.<span>indices<br/></span>  ))<span>,<br/></span><span>  </span>gl.<span>STATIC_DRAW<br/></span>)<span>;</span></pre></div>
<ol start="7">
<li>In the vertex shader, at the top of <kbd><span>ch10_03_normal-map.html</span></kbd>, the tangent needs to be transformed by the Normal matrix. The two transformed vectors can be used to calculate the third:</li>
</ol>
<div>
<pre style="padding-left: 60px">// Transformed normal position<br/>vec3 normal = vec3(uNormalMatrix * vec4(aVertexNormal, 1.0));<br/>vec3 tangent = vec3(uNormalMatrix * vec4(aVertexTangent, 1.0));<br/>vec3 bitangent = cross(normal, tangent);</pre></div>
<ol start="8">
<li>The three vectors can then be used to create a matrix that transforms vectors into tangent space:</li>
</ol>
<div>
<pre style="padding-left: 60px">mat3 tbnMatrix = mat3(<br/>  tangent.x, bitangent.x, normal.x,<br/>  tangent.y, bitangent.y, normal.y,<br/>  tangent.z, bitangent.z, normal.z<br/>);</pre></div>
<ol start="9">
<li>Unlike before, where we applied lighting in the vertex shader, the bulk of the lighting calculations needs to happen in the fragment shader so that we can incorporate normals from the texture. That being said, we do transform the light direction into tangent space in the vertex shader before passing it to the fragment shader as a varying:</li>
</ol>
<pre style="padding-left: 60px">// Light direction, from light position to vertex<br/>vec3 lightDirection = uLightPosition - vertex.xyz;<br/><br/>vTangentEyeDir = eyeDirection * tbnMatrix;</pre>
<ol start="10">
<li>In the fragment shader, we start by extracting the tangent space normal from the normal map texture. Since texture texels don't store negative values, the normal components must be encoded to map from a <kbd>[-1, 1]</kbd> to a <kbd>[0, 1]</kbd> range. Therefore, they must be <em>unpacked</em> into the correct range before being used in the shader. The algorithm to perform this operation can be easily expressed in ESSL:</li>
</ol>
<pre style="padding-left: 60px">// Unpack tangent-space normal from texture<br/>vec3 normal = normalize(2.0 * (texture(uNormalSampler, vTextureCoords).rgb - 0.5));</pre>
<p class="mce-root"/>
<ol start="11">
<li>Lighting is calculated nearly the same as the vertex-lit model, which is done by using the texture normal and tangent space light direction:</li>
</ol>
<div>
<pre style="padding-left: 60px">// Normalize the light direction and determine how much light is hitting this point<br/>vec3 lightDirection = normalize(vTangentLightDir);<br/>float lambertTerm = max(dot(normal, lightDirection), 0.20);<br/><br/>// Calculate Specular level<br/>vec3 eyeDirection = normalize(vTangentEyeDir);<br/>vec3 reflectDir = reflect(-lightDirection, normal);<br/>float Is = pow(clamp(dot(reflectDir, eyeDirection), 0.0, 1.0), 8.0);<br/><br/>// Combine lighting and material colors<br/>vec4 Ia = uLightAmbient * uMaterialAmbient;<br/>vec4 Id = uLightDiffuse * uMaterialDiffuse * texture(uSampler, vTextureCoords) * lambertTerm;<br/><br/>fragColor = Ia + Id + Is;</pre></div>
<ol start="12">
<li>To help accentuate the normal mapping effect, the code sample also includes the calculation of a specular term.</li>
</ol>
<p><em><strong>What just happened?</strong></em></p>
<p>We've seen how we can use normal information that's been encoded into a texture to add a new level of complexity to our lit models without additional geometry.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ray Tracing in Fragment Shaders</h1>
                </header>
            
            <article>
                
<p>A common (if somewhat impractical) technique used to demonstrate how powerful shaders can be is to use them to <strong>ray trace</strong> a scene. Thus far, all of our rendering has been done with <strong>polygon rasterization</strong>, which is the technical term for the triangle-based rendering that WebGL incorporates. Ray tracing is an alternate rendering technique that traces the path of light through a scene as it interacts with mathematically defined geometry.</p>
<p>Ray tracing has several advantages compared to traditional polygonal rendering. Primarily, this includes creating more realistic scenes due to a more accurate lighting model that can easily account for things like reflection and reflected lighting. That said, ray tracing tends to be considerably slower than polygonal rendering, which is the reason it's not often used for real-time applications.</p>
<p class="mce-root"/>
<p>Ray tracing a scene is achieved by creating a series of rays (represented by an origin and direction) that start at the camera's location and pass through each pixel in the viewport. These rays are then tested against every object in the scene to determine whether there are any intersections. If an intersection occurs, the closest intersection to the ray origin is returned, determining the color of the rendered pixel:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3aa28732-30bc-4a8a-8592-54763c70778e.png" style="width:35.75em;height:16.83em;"/></p>
<p>Although there are many algorithms that can be used to determine the color of the intersection point <span>– </span>ranging from simple diffuse lighting to multiple bounces of rays coming off other objects to simulate reflection <span>– </span>we'll keep our example simple. It's important to note that the rendered scene will entirely be the product of the shader code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Time for Action: Examining the Ray Traced Scene</h1>
                </header>
            
            <article>
                
<p>Let's cover an example showcasing the power of ray tracing:</p>
<ol>
<li>Open the <kbd>ch10_04_ray-tracing.html</kbd> file in your browser. You should see a scene with a simple lit, bobbing sphere like the one shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a6829bb4-69c0-4d16-b69b-0e9a56925b16.png" style="width:39.92em;height:25.17em;"/></p>
<ol start="2">
<li>In order to trigger the shader, we need a way to draw a full-screen quad. Fortunately, we have a class from our post-processing examples earlier in this chapter to help us do just that. Since we don't have a scene to process, we can omit a large part of the rendering code and simplify JavaScript's <kbd>draw</kbd> function:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>function </span><span>draw</span>() {<br/>  gl.<span>viewport</span>(<span>0</span><span>, </span><span>0</span><span>, gl.</span>canvas.<span>width</span><span>, gl.</span>canvas.<span>height</span>)<span>;<br/></span><span>  </span>gl.<span>clear</span>(gl.<span>COLOR_BUFFER_BIT </span>| gl.<span>DEPTH_BUFFER_BIT</span>)<span>;<br/></span><span><br/></span><span>  </span><span>// Checks to see if the framebuffer needs to be re-sized to match<br/>  // the canvas<br/></span><span>  </span>post.<span>validateSize</span>()<span>;<br/></span><span>  </span>post.<span>bind</span>()<span>;<br/></span><span><br/></span><span>  </span><span>// Render the fullscreen quad<br/></span><span>  </span>post.<span>draw</span>()<span>;<br/></span>}</pre></div>
<ol start="3">
<li>That's it. The remainder of our scene will be built in to the fragment shader.</li>
</ol>
<ol start="4">
<li>There are two functions at the core of our shader: one that determines if a ray is intersecting a sphere and one that determines the normal of a point on the sphere. We're using spheres because they're typically the easiest type of geometry to raycast, and they also happen to be a type of geometry that is difficult to represent accurately with polygons:</li>
</ol>
<div>
<pre style="padding-left: 60px">// ro is the ray origin.<br/>// rd is the ray direction.<br/>// s is the sphere<br/>float sphereIntersection(vec3 ro, vec3 rd, vec4 s) {<br/>  // Transform the ray into object space<br/>  vec3 oro = ro - s.xyz;<br/><br/>  float a = dot(rd, rd);<br/>  float b = 2.0 * dot(oro, rd);<br/>  // w is the sphere radius<br/>  float c = dot(oro, oro) - s.w * s.w;<br/><br/>  float d = b * b - 4.0 * a * c;<br/><br/>  // No intersection<br/>  if (d &lt; 0.0) return d;<br/><br/>  return (-b - sqrt(d)) / 2.0;<br/>}<br/><br/>vec3 sphereNormaml(vec3 pt, vec4 s) {<br/>  return (pt - s.xyz) / s.w;<br/>}</pre></div>
<ol start="5">
<li>Next, we will use these two functions to determine where the ray is intersecting with a sphere (if at all), along with what the normal and color of the sphere are at that point. To keep things simple, the sphere information is hardcoded as global variables, but they could just as easily be provided as uniforms from JavaScript:</li>
</ol>
<div>
<pre style="padding-left: 60px">vec4 sphere = vec4(1.0);<br/>vec3 sphereColor = vec3(0.9, 0.8, 0.6);<br/>float maxDistance = 1024.0;<br/><br/>float intersect(vec3 ro, vec3 rd, out vec3 norm, out vec3 color) {<br/>  float distance = maxDistance;<br/><br/>  // If we wanted multiple objects in the scene you would loop <br/>  // through them here and return the normal and color with the<br/>  // closest intersection point (lowest distance).<br/><br/> float intersectionDistance = sphereIntersection(ro, rd, sphere);<br/><br/>  if (intersectionDistance &gt; 0.0 &amp;&amp; intersectionDistance &lt; <br/>   distance) {<br/>    distance = intersectionDistance;<br/>    // Point of intersection<br/>    vec3 pt = ro + distance * rd;<br/>    // Get normal for that point<br/>    norm = sphereNormaml(pt, sphere);<br/>    // Get color for the sphere<br/>    color = sphereColor;<br/>  }<br/><br/>  return distance;<br/>}</pre></div>
<ol start="6">
<li>Now that we can determine the normal and color of a point with a ray, we need to generate the rays for casting. We can do this by determining the pixel that the current fragment represents and then creating a ray that points from the camera position through that pixel. To do so, we will utilize the <kbd>uInverseTextureSize</kbd> uniform that the <kbd>PostProcess</kbd> class provides to the shader:</li>
</ol>
<pre style="padding-left: 60px">// Pixel coordinate of the fragment being rendered<br/>vec2 uv = gl_FragCoord.xy * uInverseTextureSize;<br/>float aspectRatio = uInverseTextureSize.y / uInverseTextureSize.x;<br/><br/>// Cast a ray out from the eye position into the scene<br/>vec3 ro = eyePos;<br/><br/>// The ray we cast is tilted slightly downward to give a better<br/>// view of the scene<br/>vec3 rd = normalize(vec3(-0.5 + uv * vec2(aspectRatio, 1.0), -1.0));</pre>
<ol start="7">
<li>Using the ray we just generated, we call the <kbd>intersect</kbd> function to get the information about the sphere's intersection. Then, we apply the same diffuse lighting calculations we've been using all along! To keep things simple, we're using directional lighting here, but it would be easy enough to update the lighting model to point or spot lights:</li>
</ol>
<div>
<pre style="padding-left: 60px">// Default color if we don't intersect with anything<br/>vec3 rayColor = backgroundColor;<br/><br/>// See if the ray intersects with any objects.<br/>// Provides the normal of the nearest intersection point and color<br/>vec3 objectNormal, objectColor;<br/>float t = intersect(ro, rd, objectNormal, objectColor);<br/><br/>if (t &lt; maxDistance) {<br/>  // Diffuse factor<br/>  float diffuse = clamp(dot(objectNormal, lightDirection), 0.0,<br/>   1.0);<br/>  rayColor = objectColor * diffuse + ambient;<br/>}<br/><br/>fragColor = vec4(rayColor, 1.0);</pre></div>
<ol start="8">
<li>Thus far, our example is a static lit sphere. How do we add a bit of motion to the scene to give us a better sense of how fast the scene renders and how the lighting interacts with the sphere? We do so by adding a simple looping circular motion to the sphere by using the <kbd>uTime</kbd> uniform to modify the <kbd>x</kbd> and <kbd>z</kbd> coordinates at the beginning of the shader:</li>
</ol>
<div>
<pre style="padding-left: 60px">// Wiggle the sphere back and forth a bit<br/>sphere.x = 1.5 * sin(uTime);<br/>sphere.z = 0.5 * cos(uTime * 3.0);</pre></div>
<p><strong><em>What just happened?</em></strong></p>
<p>We covered how we can construct a 3D scene, lighting and all, entirely in a fragment shader. It's a simple scene, of course, but also one that would be nearly impossible to render using polygon-based rendering. That's because perfect spheres can only be approximated with triangles.</p>
<div class="packt_infobox"><strong><span class="packt_screen">Shader Toy<br/></span></strong>Now that you've seen how to construct 3D scenes entirely in fragment shaders, you will find the demos on <a href="https://www.shadertoy.com/">ShaderToy.com</a> both beautiful and inspiring.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Have a Go: Multiple Spheres</h1>
                </header>
            
            <article>
                
<p>In our example, we've kept things simple by rendering only one single sphere. That being said, all of the pieces needed to render several spheres are in place! How would you render a scene of multiple spheres with different colors and motion?</p>
<div class="packt_tip"><strong><span class="packt_screen">Hint</span><br/></strong>The main shader function that needs editing is <kbd>intersect</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Let's summarize what we've learned in this chapter:</p>
<ul>
<li>We covered a variety of advanced techniques to create more visually complex and compelling scenes.</li>
<li>We learned how to apply post-processing effects by leveraging a framebuffer.</li>
<li>We rendered particle effects using point sprites.</li>
<li>We created the illusion of complex geometry by using normal maps.</li>
<li>Finally, we rendered a scene entirely in a fragment shader using ray casting.</li>
</ul>
<p>These advanced effects are only a glimpse into the vast landscape of effects possible with WebGL. Given the power and flexibility of shaders, the possibilities are endless!</p>
<p>In the next chapter, we will cover the major differences between WebGL 1 (OpenGL ES 2.0) and WebGL 2 (OpenGL ES 3.0), along with a migration plan to WebGL 2.</p>


            </article>

            
        </section>
    </body></html>