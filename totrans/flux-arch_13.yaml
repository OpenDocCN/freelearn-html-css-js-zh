- en: Chapter 13. Testing and Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want the architecture of our application to be the best that it can possibly
    be. It may sound silly to have to state this, but it does bear repeating from
    time to time, as a reminder that the work we're doing with Flux has the potential
    to make or break the success of the application. The best tools we have in our
    arsenal are unit tests and performance tests. These two activities are equally
    important. Being functionally-correct but slow as hell is a failure. Being fast
    as hell and riddled with bugs is a failure.
  prefs: []
  type: TYPE_NORMAL
- en: A huge contributing factor to implementing successful tests is to focus on what's
    relevant. We'll spend time in this chapter thinking about what the important tests
    are for Flux architectures—from both a functional and a performance perspective.
    This is especially important to think about given how new Flux is to the community.
    We'll focus on specific Flux components and design some unit tests for them. We'll
    then think about the difference between benchmarking low-level code versus performance
    testing end-to-end scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Hello Jest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jasmine is the widely accepted tool of choice when it comes to writing effective
    unit tests for JavaScript code. There's no shortage of add-on tools for Jasmine
    that make it possible to test just about anything and to use any tool to run your
    tests. For example, it's common practice to use a task runner such as Grunt or
    Gulp to run tests, along with the other various build tasks associated with the
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Jest is a unit testing tool, developed by Facebook, which leverages the best
    parts of Jasmine while adding new capabilities. It's also easy to run Jest in
    our projects. For example, projects that depend on Webpack generally rely on NPM
    scripts to perform various tasks, as opposed to a task runner. This is easy to
    do with Jest, as we'll see in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three key aspects to Jest that will help us test our Flux architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: Jest provides a virtualized JavaScript environment, including a DOM interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jest spawns multiple worker processes to run our tests, leading to less time
    waiting for tests to complete and an overall faster development lifecycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jest can mock JavaScript modules for us, making it easier to isolate units of
    code to test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a look at a quick example to get things rolling. Suppose we have
    the following function that we''d like to test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This should be easy enough, we just need to write a unit test that checks for
    expected output. Let''s see what this test looks like in Jest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If this looks a lot like Jasmine, that's because it is. Jasmine is actually
    used under the hood to perform all the test assertions. However, at the top of
    the test module, you can see that there's a Jest function call to `unmock()`.
    This tells Jest that we don't want a mocked version of the `sayHello()` function.
    We want to test the real thing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There's actually quite a bit of tinkering involved with getting Jest set up
    to work with ES2015 module imports. But rather than try to explain that here,
    I'd recommend looking at the source code that ships along with this book. And
    now, back to the important stuff.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a `main.js` module that imports the `sayHello()` function and
    calls it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The Jest unit test that we created for the `sayHello()` function isolated the
    `sayHello()` function. That is, we didn''t have to test any other code in order
    to test this function. If we apply this same logic to the main module, we shouldn''t
    have to rely on the code that implements `sayHello()`. This is where the mocking
    capability of Jest comes in handy. Our last test turned off the mocking feature
    for the hello module, where `sayHello()` is defined. This time, we actually want
    to mock the function. Let''s see what the main test looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This time around, we're making sure that the `main.js` module is not mocked
    by Jest. This means that the `sayHello()` function that we've imported is in fact
    the mocked version. To verify that the main module is working as expected, as
    simple as the module is, we just need to verify that the `sayHello()` function
    was called once.
  prefs: []
  type: TYPE_NORMAL
- en: Testing action creators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a rough idea of how Jest works, it's time to start testing
    the various components of our Flux architecture. We'll start with action creator
    functions, since these determine the data that enters the system and are the starting
    point of the unidirectional data-flow. There are two types of action creators
    we'll want to test. First, we have the basic synchronous functions, followed by
    the asynchronous ones. Both types of actions lead to very different types of unit
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The job of an action creator is to create the necessary payload data and to
    dispatch it to stores. So to test this functionality, we''ll want the real action
    creator function and a mocked dispatcher component. Remember, the idea is to isolate
    the component as the single unit that''s being tested—we don''t want any side-effects
    from the dispatcher code to influence the test outcome. With that said, lets take
    a look at the action creator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This sort of function is probably looking familiar by now. We want our unit
    test for this function to verify whether or not the `dispatch()` method is called
    correctly. Let''s take a look at the test now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This works exactly as we expect it to. The first step is to tell Jest not to
    mock what's in the sync-func module, using the `unmock()` function. Jest will
    still mock everything else, including the dispatcher. So when this test calls
    `syncFunc()`, it's calling the mock dispatcher in-turn. When it does so, the mock
    records information about the call, which we then use in our test assertions to
    make sure that everything is working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Nice and easy, right? Things get a little trickier when we need to mock asynchronous
    action creator functions, but we'll try to simplify everything in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Jest makes it easy for us to isolate the code that a given unit test should
    be testing by mocking all the irrelevant parts. Some things are easily handled
    by the Jest mock generator. Others need our intervention, as you''ll see in this
    example. So let''s start off and take a look at the asynchronous action creator
    function that we''re trying to test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This action creator makes a request to a public JSON endpoint, and then dispatches
    the `ASYNC` action with the response as the action payload. If the `request()`
    function that we''re using to make the network request looks a lot like the global
    `fetch()` function, that''s because it is that function. The request module simply
    exports it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems pointless, but there''s really no overhead involved. This is how we''re
    able to mock all network requests in our code easily. If we mock this request
    module for our unit tests, it means that our code won''t be trying to reach a
    remote server. To mock this module, we just have to create a module by the same
    name in the `__mocks__` directory, alongside the `__tests__` directory. Jest will
    mock find this mock and substitute it for the real module when it''s imported.
    Let''s look at the source of the mock `request()` function now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If this code looks a little gross, don't worry—it's confined to this one place.
    All it's doing is replicating the interface of the native `fetch()` function that
    this module replaces (because we don't actually want to fetch anything). The tricky
    part of this approach is that any `request()` calls in our code are going to get
    the same resolved values. But this should be fine, assuming that our code can
    just ignore properties that it doesn't care about and that we can keep the test
    data in here to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have a mocked network layer, which means that we''re ready
    to implement the actual unit test now. Let''s go ahead and do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are two important things to note about this test. One, it's using the
    `pit()` function as a drop-in replacement for `it()`. Two, the `asyncFunc()` function
    itself returns a promise. These two aspects of Jest are what make writing asynchronous
    unit tests so straightforward. The difficult part of this example isn't the test,
    it's the infrastructure we need in place in order to mock things like network
    requests. Thanks to everything Jest takes care of for us, our unit test code is
    actually a lot smaller than it would otherwise be.
  prefs: []
  type: TYPE_NORMAL
- en: Testing stores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we used Jest to test action creator functions. This
    wasn't much different from testing any other JavaScript function, except that
    Flux action creators need to somehow dispatch the actions they create to stores.
    Jest helps us achieve this by automatically mocking certain components, and it
    will certainly help us test our store components.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll look at testing the basic path of an action being dispatched
    to a store and the store emitting a change event. Then, we'll think about the
    initial store state and how this can lead to bugs that unit tests should be able
    to catch. Making all of this work is going to involve thinking about implementing
    testable store code, which is something we have yet to think about in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Testing store listeners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Store components can be tricky to isolate from other components. This in turn
    makes designing unit tests for stores difficult. For example, a store will typically
    register itself with the dispatcher by passing it a callback function. This is
    the function that will change the state of the store, depending on the action
    payload that's passed to it. The reason this is a challenge is that it's tightly-coupled
    with the dispatcher.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we want the dispatcher removed from the unit test completely. We're
    only testing our store code in the unit test, so we don't want anything that's
    happening in the dispatcher to interfere with the outcome. The odds of this happening
    are slim, since the dispatcher doesn't really have much to do. However, it's better
    to be consistent with all our Flux components and somehow isolate them completely.
    We've seen how Jest can help us out in the previous section. We just need to somehow
    apply this principle to stores—to decouple them from the dispatcher during unit
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a case where we might need to reconsider how we write our store code—sometimes
    for code to be good, it needs to be changed slightly so that it''s good and testable.
    For example, the anonymous function that we would normally register with the dispatcher
    becomes a store method. This allows the test to call the method directly, skipping
    the whole dispatching mechanism, which is exactly what we want. Let''s take a
    look at the store code now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `onAction()` method is registered with the dispatcher, and
    will be called any time an action is dispatched. The `doStuff()` method breaks
    the specific state transformation that takes place in response to the `DO_STUFF`
    action out of the `onAction()` method. This isn't strictly necessary, but it does
    provide us with another target for our unit tests. For example, we could have
    just left the anonymous callback function in place and have our tests target the
    `doStuff()` method directly. However, if our tests call `onAction()` with the
    same type of payload data that comes from the dispatcher, we get better test coverage
    of the store.
  prefs: []
  type: TYPE_NORMAL
- en: 'The astute reader might have noticed that this store is importing `EventEmitter`
    from a different place than usual—`../events`. We have our own events module?
    We do now, and it''s the same idea as with the `fetch()` function in the preceding
    section. We''re providing a module of our own that Jest can mock. This is an easy
    way for Jest to mock the `EventEmitter` class. We were so busy thinking about
    the dispatcher, that we forgot to decouple our store from the event emitter for
    our test. Let''s take a look at the events module so that you can see we''re still
    exposing the good old `EventEmitter` we all know and love:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the methods inherited by our store will be mocked by Jest,
    which is perfect because now our store is completely isolated from other component
    code and we can use data collected by the mock to perform some test assertions.
    Let''s implement the unit test for this store now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: What's nice about this approach is that it closely resembles how the data flows
    through the store, but without actually depending on other components in order
    to run the test. The test data enters the store the same way it would with an
    actual dispatcher component. Likewise, we know that the correct event data is
    being emitted by the store by measuring the mock implementation. This is where
    the store's responsibilities end, and so too do the test's responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Testing initial conditions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One thing we''ll learn soon after our Flux stores grow large and complex is
    that they become increasingly difficult to test. For example, if the number of
    actions that a store responds to goes up, then the number of state configurations
    we''ll want to test with will also go up. To help accommodate the unit tests for
    our stores, it would be helpful to be able to set the initial state of the store.
    Let''s take a look at a store that allows us to set the initial state and responds
    to a couple of actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This store responds to the `POWER_ON` and `POWER_OFF` actions. If you look
    at the methods that handle the state transformations of these two actions, you
    can see that the result depends on the current state. For example, powering on
    a store requires that the store already be off. Powering off a store is even more
    restrictive—the store has to be off and cannot be busy. These types of state transformations
    need to be tested using different initial store states, to make sure that the
    happy path works as expected, as well as the edge cases. Now let''s take a look
    at the test for this store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The second test is perhaps the most interesting because it makes sure that no
    events were emitted as a result of the action, due to the way the state transformation
    logic of the store works.
  prefs: []
  type: TYPE_NORMAL
- en: Performance goals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's time to switch gears and think about testing the performance of our Flux
    architecture. Testing the performance of a particular component can be difficult
    for the same reason that testing the functionality of a component is difficult—we
    have to isolate it from other code. On the other hand, our users don't necessarily
    care about the performance of individual components—just the overall user experience.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll discuss what we're trying to achieve with our Flux architecture
    in terms of performance. We'll start with the user perceived performance of the
    application, because this is the most consequential aspect of an under-performing
    architecture. Next, we'll think about measuring the raw performance of our Flux
    components. Finally, we'll consider the benefits of putting performance requirements
    in place for when we develop new components.
  prefs: []
  type: TYPE_NORMAL
- en: User perceived performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the point of view of our users, our application either feels responsive
    or laggy. This *feeling* is called user-perceived performance, because the user
    isn't actually measuring how long something takes to complete. Generally speaking,
    user-perceived performance is about frustration thresholds. Whenever we have to
    wait for something, frustration grows because we don't feel in control of the
    situation. We can't do anything to make it hurry up, in other words.
  prefs: []
  type: TYPE_NORMAL
- en: One solution is to distract the user. There are times when our code has to process
    something and there's no way around the length of time it takes. While this is
    happening, we can keep the user updated on the task progress. We might even be
    able to show some of the output that's already been processed, depending on the
    type of task. The other answer is to write performant code, which is something
    we should always strive for anyway.
  prefs: []
  type: TYPE_NORMAL
- en: User-perceived performance is critically important for the software product
    that we're building because if it's perceived as being slow, it's also perceived
    as being of poor quality. At the end of the day, it's the user's opinion that
    matters—this is how we measure whether or not our Flux architecture scales to
    an acceptable level. The downside of user perceived performance is that it's impossible
    to quantify, at least at a granular level. This is where we need tooling in place
    to help us measure how our components perform.
  prefs: []
  type: TYPE_NORMAL
- en: Measured performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance metrics tell us specifically where the performance bottlenecks in
    our code are. If we know where the performance issues are, then we're better equipped
    to address them. From the perspective of a Flux architecture, for example, we
    would want to know whether the action creators are taking a long time to respond,
    or whether the stores are taking a long time to transform their state.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of performance testing that can help us stay on top of any
    performance issues during the development of our Flux architecture. The first
    type of testing is profiling, and we'll look at this in more detail in the next
    section. The second type of performance testing is benchmarking. This latter type
    of testing is done at a lower level and is good for comparing different implementations.
  prefs: []
  type: TYPE_NORMAL
- en: The only question is—how do we make performance measurement a fact of daily
    life, and what can we do with the results?
  prefs: []
  type: TYPE_NORMAL
- en: Performance requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given that we have the tools necessary for performance testing at our disposal,
    it would seem that it's possible to define some requirements around performance.
    For example, if someone is implementing a store, could we introduce a performance
    requirement that says a store can take no longer than x milliseconds to emit a
    change event? The plus side is that we could be reasonably confident about the
    performance of our architecture, right down to the component level. The down side
    is the complexity involved.
  prefs: []
  type: TYPE_NORMAL
- en: For one thing, development of new code would noticeably slow down, because not
    only would we have to test for functional correctness, we would also have a strict
    performance bar to clear. This takes time, and the payoff is most likely nothing.
    Let's say that we end up spending a bunch of time improving the performance of
    some component because it's barely failing the requirement. This would mean that
    we're spinning our wheels on something that's intangible to the user.
  prefs: []
  type: TYPE_NORMAL
- en: This isn't to say that performance testing cannot be automated or that it shouldn't
    be done at all. We simply have to be smart about where we invest our time testing
    the performance of our Flux code. The ultimate decider of performance is the user,
    so it's difficult to set concrete requirements that mean *good enough* performance,
    but it's really easy to waste time trying to achieve optimal performance that
    nobody will notice, least of all your customers.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The various profiling tools available to us through a web browser are often
    enough to address any performance issues in our interface. These include the components
    that make up our Flux architecture. In this section, we'll go over the three main
    tools found in browser developer tools that we'll want to use to profile our Flux
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: First are the action creator functions, specifically asynchronous functions.
    Then we'll think about the memory consumption of our Flux components. Finally,
    we'll discuss CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The network is always going to be the slowest layer of the application. Even
    if the API call we're making is relatively fast, it's still slow compared to other
    JavaScript code. If our application didn't make any network requests, it would
    be blazing fast. It also wouldn't be of much use. Generally speaking, JavaScript
    applications rely on remote API endpoints as their data resources.
  prefs: []
  type: TYPE_NORMAL
- en: To make sure that these network calls aren't causing performance issues, we
    can leverage the networking profiler of the browser developer tools. This shows
    us, in great detail, what any given request is doing, and how long it takes to
    do it. For example, if the server is taking a long time to respond to a request,
    this will be reflected in the timeline of the request.
  prefs: []
  type: TYPE_NORMAL
- en: Using this tool, we can also see the number of requests that are outstanding
    at any given point. For instance, maybe there's a page in our application that's
    hammering the server with requests and overwhelming it. In that case, we have
    to rethink the design. Each request that we look at in this tool allows us to
    drill down into the code that initiated the request. In Flux applications, this
    should always be an action creator function. With this tool, we always know which
    action creator functions are problematic from a network point of view and we can
    do something about them.
  prefs: []
  type: TYPE_NORMAL
- en: Store memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next developer tool that can help us test the performance of our Flux architecture
    is the memory profiler. Memory is obviously something that we have to be careful
    with. On the one hand, we have to be considerate of other applications running
    on the system and avoid hogging memory. On the other hand, when we try to be careful
    with memory, we end up with frequent allocations/deallocations, triggering the
    garbage collector. It's hard to put a number on the maximum amount of memory a
    component should use. The application needs what it needs.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of Flux, we're most interested in what the memory profiler can tell
    us about our stores. Remember, stores are where we're likely to face scalability
    issues as our application grows, because they'll have to handle more input data.
    Of course, we'll also want to keep an eye on the memory consumed by our view components
    as well, but ultimately it's the stores that control how much or how little memory
    views will consume.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways the memory profiler can help us better understand the memory
    consumption of our Flux stores. First, there's the memory timeline. This view
    shows how memory is allocated/deallocated over time. This is useful because it
    lets us see how memory is used as we interact with the application the same way
    a user would. Second, the memory profiler lets us take a snapshot of the current
    memory allocations. This is how we determine the type of data that's being allocated,
    and the code that's doing it. For example, with a snapshot, we can see which store
    is taking up the most memory.
  prefs: []
  type: TYPE_NORMAL
- en: CPU utilization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you saw in the previous section on the memory profiler, the frequent garbage
    collections can cause issues with responsiveness. This is because the garbage
    collector will block any other JavaScript code from running. The CPU profiler
    can actually show us how much CPU time the garbage collector is taking away from
    other code. If it's a lot, then we can figure out a better memory strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, however, we should turn our attention to the store components of
    our Flux architecture when profiling the CPU. The simple reason is that this will
    have the biggest return on investment. The scalability issues that we're likely
    to face are centered around the data transformation functions used to handle action
    payloads within stores. Unless these functions are efficient enough to handle
    the data that enters the system, the architecture won't scale because the CPU
    is being over-utilized by our code. And with that, we'll turn our attention to
    benchmarking the functions that are critical to the scalability of our systems.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On one end of the performance testing spectrum, there's user-perceived performance.
    This is where one of our customers is complaining about laggyness, and sure enough,
    it's easy for us to replicate the problem. This could be an issue with view components,
    network requests, or something in our store that's causing the suboptimal user
    experience. On the other end of the spectrum, we have raw benchmarking of code,
    where we want accurate timings to ensure that we're using the most efficient implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll briefly introduce the concept of benchmarking, and then
    we'll show an example that uses `Benchmark.js` to compare two state transformation
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we benchmark our code, we're comparing one implementation to another, or
    we can compare three or more implementations. The key is to isolate the implementations
    from any other components and to make sure that they each have the same input
    and produce the same output. Benchmarks are like unit tests in a sense, because
    we have a unit of code that we isolate as a unit and use a tool to measure and
    test its performance.
  prefs: []
  type: TYPE_NORMAL
- en: One challenge with performing these sorts of micro-benchmarks is accurate timing.
    Another challenge is creating an environment that isn't disrupted by other things.
    For example, trying to run a JavaScript benchmark in a web page is likely to face
    interference by other things, such as the DOM. `Benchmark.js` handles the nitty-gritty
    details of getting the most accurate measurement for our code. With that said,
    let's jump into an example.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike unit tests, benchmarks aren't necessarily something we want to keep around
    and maintain forever. It's simply too much of a burden, and the value of benchmarks
    tends to diminish when there's hundreds of them. There are probably a few exceptions,
    where we want to keep benchmarks in the repository for illustrative purposes.
    But generally speaking, benchmarks can safely be discarded once the code has been
    implemented or once the performance of existing code has been improved.
  prefs: []
  type: TYPE_NORMAL
- en: State transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The state transformations that happen inside of Flux stores have the potential
    to bring the system to a halt when we try to scale it up. As you know, the rest
    of the Flux components in our architecture scale well. It''s the added request
    volume and added data volume that cause problems. Low-level functions that transform
    this data need to perform well. We can use a tool like `Benchmark.js` to build
    benchmarks for the code that works with store data. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we just need to add two or more benchmark functions to the suite,
    then run it. The output is specific performance data that compares the various
    implementations. In this case, we're filtering and mapping an array of 10,000
    items. The `"for..of"` approach stands out as the best bet performance-wise.
  prefs: []
  type: TYPE_NORMAL
- en: What's important about benchmarking is that it can rule out false assumptions
    fairly easily. For example, we might assume that because `"for..of"` outperforms
    the alternative implementations, that it's automatically the best choice. Well,
    the two alternatives aren't that far behind. So if we really would rather implement
    the functionality using `reduce()`, there's probably no scaling risk in doing
    so.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code that ships with this book implements a few tricks to make this example
    work with ES2015 syntax using Babel. This is an especially good idea if you're
    transpiling your production code using Babel, so your benchmarks reflect reality.
    It's also handy to add an `npm bench` script to your `package.json` for easy access.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The focus of this chapter has been testing our Flux architectures. There are
    two types of tests that we employ to do this: functional and performance. With
    functional units, we verify that the units of code that make up our Flux architecture
    are behaving as expected. With performance units, we''re validating that the code
    is performing at the expected levels.'
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the Jest testing framework to implement unit tests for our action
    creators and our stores. We then discussed the various tools in the browser that
    can help us troubleshoot performance issues at a high-level. These are the types
    of things that impact the user experience in a tangible way.
  prefs: []
  type: TYPE_NORMAL
- en: We closed the chapter with a look at benchmarking our code. This is something
    that takes place at a low-level and is most likely related to the state transformation
    functionality of our stores. Now it's time to consider the implications a Flux
    architecture has on the overall software development lifecycle.
  prefs: []
  type: TYPE_NORMAL
