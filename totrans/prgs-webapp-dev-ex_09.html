<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Optimizing for Performance</h1>
                
            
            
                
<p class="calibre2">Performance is one of the key aspects of user experience. In fact, many experts argue that web performance creates a good user experience. There are different aspects of web performance that you should consider when you are delivering an online experience, such as:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Time to first byte</strong> (<strong class="calibre1">TTFB</strong>) and server-side latencies</li>
<li class="calibre11">Rendering processes</li>
<li class="calibre11">Interactions</li>
</ul>
<p class="calibre2">One of the primary attributes of a <strong class="calibre4">Progressive Web App</strong> (<strong class="calibre4">PWA</strong>) is speed. That's because people like pages to load fast and respond to actions or input even faster. Making fast, interactive websites is as much of an art as it is a science.</p>
<p class="calibre2">Before diving into making fast PWAs, I want to define what this chapter is designed to help with:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Goals</strong>: Defining key performance indicators to measure performance</li>
<li class="calibre11"><strong class="calibre1">Guidelines</strong>: Defining ways for you to achieve these goals</li>
<li class="calibre11"><strong class="calibre1">Demonstrate</strong>: Applying these guidelines to the PWA ticket app so that you have a reference code and workflow</li>
</ul>
<p class="calibre2">This chapter will go deep into how the browser loads and renders pages. You will also learn details of how TCP works and how you can take advantage of this knowledge to create pages that load within 1 second.</p>
<p class="calibre2">You will learn different intermediate and advanced <strong class="calibre4">web performance optimization</strong> (<strong class="calibre4">WPO</strong>) techniques and how they relate to PWA design. These techniques are woven into the natural fabric of the PWA ticket application. As this chapter evolves, you will learn techniques which rely partially on an automated build script. This automation will be carried over to the following chapter, where I will be reviewing different tools to help you build PWAs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">The importance of WPO</h1>
                
            
            
                
<p class="calibre2">DoubleClick and others websites have shown that 3 seconds is all you have. If the perceived page hasn't loaded within 3 seconds, 53% of mobile visitors abandon the page (<a href="https://www.doubleclickbygoogle.com/articles/mobile-speed-matters/" target="_blank" class="calibre9">https://www.doubleclickbygoogle.com/articles/mobile-speed-matters/</a>). Additionally, DoubleClick reported that sites loading within 5 seconds enjoyed 70% longer sessions, 35% lower bounce rates, and 25% higher advertisement viewability.</p>
<p class="calibre2">Those examples are just a small sampling of the numerous case studies and reports available, demonstrating how important page load and interaction are for a website's success. You can find many more statistics at <a href="https://wpostats.com" target="_blank" class="calibre9">https://wpostats.com</a>. This is one of the reasons web performance is continually emphasized by Google as a key ranking signal.</p>
<p class="calibre2">Human psychology is an important aspect of performance. We know how the brain perceives performance and can correlate the science to our web pages:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">0 to 16 ms</strong>: Users perceive animations as smooth, so long as 60 new frames are rendered every second or 16 ms per frame. This leaves about 10 ms to produce a frame after considering browser overheads.</li>
<li class="calibre11"><strong class="calibre1">0 to 100 ms</strong>: Users feel like the response to an action is immediate.</li>
<li class="calibre11"><strong class="calibre1">100 to 300 ms</strong>: Slight perceptible delay.</li>
<li class="calibre11"><strong class="calibre1">300 to 1000 ms</strong>: Things feel part of a natural and continuous progression of tasks.</li>
<li class="calibre11"><strong class="calibre1">&gt;= 1000 ms</strong>: (1 second), users loses focus on the task.</li>
</ul>
<p class="calibre2">Put a mental pin in those numbers because they serve as a primary benchmark for this chapter. The goal of this chapter is to modify the PWA ticket application to load within 1 second over an average 3G connection.</p>
<p class="calibre2">Loading fast gives you a competitive advantage because the average web page takes 15 seconds (<a href="https://www.thinkwithgoogle.com/marketing-resources/data-measurement/mobile-page-speed-new-industry-benchmarks/" target="_blank" class="calibre9">https://www.thinkwithgoogle.com/marketing-resources/data-measurement/mobile-page-speed-new-industry-benchmarks/</a> ) to load on mobile devices, a 7 second improvement over the 2017 standard. Not only has this improved the user's engagement, but also stats that were commonly reported when site performance is optimized. Such sites enjoy higher search rankings.</p>
<p class="calibre2">The majority of web traffic comes from mobile devices. Some businesses see as much as 95% of their traffic coming from smartphones. This is why Google is switching their primary search index from desktop to mobile by June 2018.</p>
<p class="calibre2">The Google search team has stated on multiple occasions that speed is a key ranking factor. They know that we want fast sites and that fast sites provide better user experience, which makes customers happy. Their goal is to provide the best resource to answer the user's question, which means that you need to provide a fast experience.</p>
<p class="calibre2">One of the most troubling stats is the growing size of web pages. In 2015, the average web page passed the 2.5 MB mark, which is larger than the original installation disks for the game DOOM. Consequently, website performance has suffered.</p>
<p class="calibre2">Google research found the following, disturbing stats about the average web page's size:</p>
<ul class="calibre10">
<li class="calibre11">79% &gt; 1 MB</li>
<li class="calibre11">53% &gt; 2 MB</li>
<li class="calibre11">23% &gt; 4 MB</li>
</ul>
<p class="calibre2">This is important because it takes about 5 seconds just to download a megabyte over a good 3G connection. That is not the end of the overhead, all the resources still need to be processed and the page has to be rendered.</p>
<p class="calibre2">If you consider the numbers reported by Google, this means that 79% of web pages don't even start the rendering cycle until 5 seconds after the initial request is made! At that point, the probability the user has bounced is 90%.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Reducing image payload size</h1>
                
            
            
                
<p class="calibre2">Many point to images as being the root cause and to a certain degree they are, but images do not block rendering. Images can and should be optimized, which reduces the overall page size.</p>
<p class="calibre2">Optimizing image file sizes can reduce the overall payload size by an average of 25%. If the page is 1 MB, 25% equals a 250 KB payload reduction.</p>
<p class="calibre2">Responsive images should also be used. This is where you use the <kbd class="calibre12">srcset</kbd> image and sizes attributes, or the picture element to reference images that are sized appropriately for the display:</p>
<pre class="calibre17">&lt;img srcset="img/pwa-tickets-logo-1158x559.png 1158w, 
     img/pwa-tickets-logo-700x338.png 700w, 
     img/pwa-tickets-logo-570x276.png 570w, 
     img/pwa-tickets-logo-533x258.png 533w, 
     img/pwa-tickets-logo-460x223.png 460w, 
     img/pwa-tickets-logo-320x155.png 320w"  
     src="img/pwa-tickets-logo-1158x559.png"  
     sizes="(max-width: 480px) 40vw,  
     (max-width: 720px) 20vw, 10vw" 
     alt="pwa-tickets-logo"&gt; </pre>
<p class="calibre2">Client device viewports vary widely. Instead of trying to be exact on every device, I recommend focusing on four viewport classes: phones, mini-tablets, tablets, and desktops. I will borrow the breakpoints from the Twitter bootstrap project that correspond to these viewports.</p>
<p class="calibre2">Any images above the different viewport width thresholds should be an array of images maintaining their aspect ratios at smaller widths.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">The cost of CSS and JavaScript</h1>
                
            
            
                
<p class="calibre2">The real culprit behind delaying page load time is the overuse of CSS and JavaScript. Both are rendering blocking, which means that when they are being processed, nothing else happens.</p>
<p class="calibre2">Remember from the chapters on service workers, the browser uses a single thread to manage all the rendering tasks, including processing CSS and JavaScript. While this thread is doing that processing, no rendering can take place.</p>
<p class="calibre2">Developers are often naïve about the impact CSS and JavaScript has on their pages loading. Often, this comes down to forgetting what devices real users load web pages with, that is, mobile phones.</p>
<p class="calibre2">Developers generally work on high end workstations and laptops. They also load their work on those devices while developing. Therefore, they perceive their pages as loading instantly.</p>
<p class="calibre2">The discrepancy is a combination of no network latency, high speed processors, and ample memory. This is not the case in the real world.</p>
<p class="calibre2">Most consumers use cheaper phones, typically a $200 handset, not a $1,000 iPhone or a workstation. This means lower powered devices with constrained and limited network conditions do not load and render pages as fast as desktops.</p>
<p class="calibre2">To compound these limitations, when mobile devices are using battery power, they often slow down processors and even turn off cores to reduce power consumption. This means the time to process JavaScript and CSS takes much longer.</p>
<p>Numerous reports have demonstrated how much JavaScript affects how well a page loads. <em class="calibre39">Addy Osmani</em> published a canonical study (<a href="https://medium.com/dev-channel/the-cost-of-javascript-84009f51e99e" class="calibre40">https://medium.com/dev-channel/the-cost-of-javascript-84009f51e99e</a>) showing how JavaScript gets in the way of a page loading.</p>
<p class="calibre2">A common misconception is that the primary performance impact is loading a script or style sheet over the network. This has some affect, but the bigger impact is once the file is loaded. This is where the browser must load the script in memory, parse the script, evaluate, and then execute the script.</p>
<p>"Byte-for-byte, JavaScript is more expensive for the browser to process than the equivalently sized image or Web Font" <br class="title-page-name"/>
                                                                                                                 — Tom Dale</p>
<p class="calibre2">If you use browser profiling tools, you can identify this phase of JavaScript by the presence of a yellow or a golden red color. The Chrome team calls <em class="calibre13">this a giant yellow slug</em>:</p>
<p class="cdpaligncenter1"><img src="img/00103.jpeg" class="calibre118"/></p>
<p class="calibre2">In recent years, <strong class="calibre4">single-page applications</strong> (<strong class="calibre4">SPAs</strong>) have become very popular. This has given rise to large frameworks that abstract the native APIs and provide an architecture developers and teams can follow.</p>
<p class="calibre2">You should determine if a SPA is the right solution for your needs. The primary reason SPAs have enjoyed so much popularity is that they enable seamless page transitions, similar to a native app experience.</p>
<p class="calibre2">If you have a service worker and utilize service worker caching, you can achieve the desired instant load and smooth page transitions SPAs offer. As a bonus, you won't need to load as much client-side JavaScript.</p>
<p class="calibre2">You can also learn the native APIs instead of framework abstractions, such as jQuery. For example, <kbd class="calibre12">document.querySelector</kbd> and <kbd class="calibre12">document.querySelectorAll</kbd> return references to DOM elements much faster than jQuery.</p>
<p class="calibre2">Other native APIs I leverage, replacing most of what I used jQuery for, include:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">addEventListener</kbd></li>
<li class="calibre11"><kbd class="calibre12">classList</kbd></li>
<li class="calibre11"><kbd class="calibre12">setAttribute</kbd> and <kbd class="calibre12">getAttribute</kbd></li>
</ul>
<p class="calibre2">I have tried to give you simple architectures which you can follow with the Podstr and PWA tickets applications. The benefit of not being a SPA<strong class="calibre4"> </strong>is that you can reduce the amount of JavaScript needed to run a page.</p>
<p class="calibre2">The PWA ticket application relies on very little JavaScript. localForage and Mustache accounts are used for most of the JavaScript. The application and individual pages lean on next to no script. After applying gzip compression, the typical page needs less than 14 KB of JavaScript.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Proper test devices and emulation</h1>
                
            
            
                
<p class="calibre2">I do recommend having realistic test devices. This does not mean buying an iPhone X, a Pixel 2, or Samsung 9. You should have an average phone, which can mean different things for different areas. A baseline recommendation is:</p>
<ul class="calibre10">
<li class="calibre11">North America and Europe: Motorola G4:
<ul class="calibre30">
<li class="calibre11"><em class="calibre23">Regular 3G</em> in Devtools Network Throttling</li>
</ul>
</li>
<li class="calibre11">India and Indonesia: Xiaomi Redmi 3s
<ul class="calibre30">
<li class="calibre11"><em class="calibre23">Good 2G</em> in Devtools Network Throttling</li>
</ul>
</li>
</ul>
<p class="calibre2">The general rule is that with network constrained devices, those with slow and poor cellular connections, payload is important. Lower power, CPU, and memory constrained devices have more problems parsing and evaluating scripts and style sheets.</p>
<p class="calibre2">This is amplified when a lower powered device is using a constrained network. This is why you should architect your site as if all users have this bad combination. When you do, your site will always load quickly.</p>
<p class="calibre2">Fortunately, we have numerous tools available to measure our pages' performance profiles and improve them. Different browser developer tools provide device emulation that provides a reasonable simulation of network and device constraints. WebPageTest is an example of a free online resource that can test real devices and conditions.</p>
<p class="calibre2">Key areas to focus on are server-side factors and how your pages load in the browser. The most important goal is to make your pages render as fast as possible and respond quickly.</p>
<p class="calibre2">In this chapter, we will look at different key performance indicators and how you can apply techniques and patterns to ensure that you provide a great user experience. We will also look at how these techniques relate to progressive web applications.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Testing poor conditions using developer tools</h1>
                
            
            
                
<p class="calibre2">The good news about testing for poor connectivity and average user devices is that it can be emulated. The Chrome, Edge, and FireFox developer tools all include some capacity to simulate slower connections and even lower powered devices.</p>
<p class="calibre2">Chrome has the best developed conditional testing tools. In the Developer Tools, you need to toggle the device toolbar. The keyboard shortcut is <em class="calibre13">Ctrl</em> + <em class="calibre13">Shift</em> + <em class="calibre13">M</em> and the button is located in the top left corner. It looks like a phone overlaying a tablet:</p>
<div><img src="img/00104.jpeg" class="calibre119"/></div>
<p class="calibre2">This changes the browser tab to render the page in a frame. The frame simulates the viewport of a target device. It also renders the device toolbar above the content frame:</p>
<div><img src="img/00105.jpeg" class="calibre120"/></div>
<p class="calibre2">The device toolbar consists of different drop-downs, allowing you to configure what device and connection scenario you want to emulate. The left most drop-down is a list of pre-configured devices. It contains some of the more popular devices and can be customized.</p>
<p class="calibre2">When you select a device, the width and height values are adjusted to match the device's viewport. I love this because it allows me to have a close proximity to the real device, without needing the real device.</p>
<p class="calibre2">I do recommend having a few real handsets to test your sites, but that can get expensive real fast. Personally, I have a couple of Androids, one high end and one low end, and an iPhone. Right now, I have an iPhone 6. I recommend buying either refurbished hardware or cheap, pre-paid phones, which are available at most retailers for about $50 US.</p>
<p class="calibre2">The Chrome device emulator does a fair enough approximation of real devices to allow me to complete my responsive design work. You should note that you are still using desktop Chrome, which is not exactly Android Chrome and certainly not iOS Safari.</p>
<p class="calibre2">The device emulator also has several popular Android tablets and iPads configured. Plus, you can also create your own viewports.</p>
<p class="calibre2">You can also adjust the zoom. This can be helpful if the content is too small for you to fine tune:</p>
<div><img src="img/00106.gif" class="calibre121"/></div>
<p class="calibre2">The last option is bandwidth. This is the far right drop-down. It includes options to simulate offline, middle-tier, and lower-tier connections. They try not to label these speeds by common cellular connections because that opens them up to issues of not being an exact match.</p>
<p class="calibre2">3G, 4G, and LTE all vary by location, even in the same country. Labeling these speeds by cellular speed could be very misleading.</p>
<p class="calibre2">Since the vast majority of development occurs on high powered computers on a localhost site, developers often forget that their pages are loaded on cellular connections on phones. This leads us to assume that our pages are much faster than they actually are. Instead, you should always try to experience your site as close to real world scenarios as possible.</p>
<p class="calibre2">A big reason I encourage developers to not use JavaScript frameworks is after experiencing a mobile first application on 3G days before launching. It took about 30 seconds for each page to load. I found not only was the poor 3G connectivity to be a problem, but the amount of JavaScript to be the bottleneck.</p>
<p class="calibre2">If I had not started using our application on my 3G handset, I would not have known how poor the user experience was. Back then, browser developer tools did not have these simulation features, which made the real device mandatory. So, be thankful that these tools exist, which can save you hours of time rearchitecting your sites.</p>
<p class="calibre2">I utilize these emulation features to develop my sites, especially for responsive design work. The speed emulation helps me feel issues my customers may experience, which allows me to have more empathy for them.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Performing performance and PWA testing with Lighthouse</h1>
                
            
            
                
<p class="calibre2">Chrome includes a powerful tool to test how well your site performs and meets progressive web application criteria. This tool is called <strong class="calibre4">Lighthouse</strong>. The tool is integrated into the developer tools Audit tab and is available as a node module and command-line utility:</p>
<div><img src="img/00107.jpeg" class="calibre122"/></div>
<p class="calibre2">I will focus on using Lighthouse in the developer tool here and follow up with command line usage in the following chapter.</p>
<p class="calibre2">To perform an audit, press the Perform an audit... button, as seen in the preceding screenshot. You will then see a dialog that gives you high level configuration options.</p>
<p class="calibre2">There are five areas Lighthouse audits:</p>
<ul class="calibre10">
<li class="calibre11">Performance</li>
<li class="calibre11">PWAs</li>
<li class="calibre11">Best practices</li>
<li class="calibre11">Accessibility</li>
<li class="calibre11">SEO</li>
</ul>
<p class="calibre2">You can run tests in all of these areas or only execute tests in selected areas. Lighthouse runs the audit and produces a scorecard and report:</p>
<div><img src="img/00108.jpeg" class="calibre52"/></div>
<p class="calibre2">The report highlights specific areas where you can improve the page. In the preceding screenshot, I ran just a performance audit and you can see some of the specific areas to improve, including the perceptual speed index. I also had the tool take screenshots as the page loaded, so you can see how the page renders over time.</p>
<p class="calibre2">The developer tools make it easy to run Lighthouse audits on a single page. You can run them from any Chrome instance, but I recommend opening an incognito instance. When you do this, you load the page in a clean browser, with no cache, cookies, or extensions.</p>
<p class="calibre2">Because Chrome extensions run in the same process as the browser tab, they often interfere with pages and tools, such as Lighthouse. I found a page that normally loads fast and scores well suffers when extensions are included. They delay page loads and often execute well after the page completes loading.</p>
<p class="calibre2">It takes between 30-90 seconds to run a full audit. It depends on how many tests are being executed and the response time of your site.</p>
<p class="calibre2">The battery of performance audits that are run are very thorough, covering not only a desktop and high speed connection, but use emulation to simulate low powered phones over 3G connections. It is these conditions that expose your weaknesses.</p>
<p class="calibre2">You can use the report to pinpoint specific areas to correct, many of which are discussed in this chapter.</p>
<p>Each test has online documentation to explain what the test is and the actions you can take: <a href="https://developers.google.com/web/tools/lighthouse/audits/consistently-interactive" class="calibre40">https://developers.google.com/web/tools/Lighthouse/audits/consistently-interactive</a>.</p>
<p class="calibre2">Because the PWA ticket application is fairly optimized, there are not many areas to address. This test was run on the home page, after the user was authenticated. The one area that presents a delay is the perceptual speed index.</p>
<p class="calibre2">This measures how long the page content takes to load. In this example, we scored 47, which is very low. This is because the UI thread is unresponsive while making the API call and rendering the markup for the upcoming events and the user's tickets.</p>
<p class="calibre2">We can improve this score by passing the API call and rendering to the service worker or even a web worker. This would take the work out of the UI thread and place it in a background thread. This will require a shift in page and site architecture.</p>
<p class="calibre2">Another recommendation is to use next generation image formats such as WebP and JPEG 2000. While these image formats are more efficient, they are not broadly supported. This is partially due to their young age and partially due to licensing concerns by different user agents. So, for now, I tend to ignore these recommendations and hold out hope that these formats will be commonly supported in the near future.</p>
<p class="calibre2">You could leverage a complex solution using the <kbd class="calibre12">PICTURE</kbd> element, but I find that it requires more management and responsibility than the payoff warrants.</p>
<p class="calibre2">It was announced at the recent Google I/O that Lighthouse version 3 will be available soon. They previewed an updated UI and a few new tests. You can read more about those announcements on the Google Developer's site: <a href="https://developers.google.com/web/tools/lighthouse/v3/scoring" class="calibre9">https://developers.google.com/web/tools/Lighthouse/v3/scoring</a>.</p>
<p class="calibre2">As a word of caution, Lighthouse is an opinionated tool. This means that it looks for things Google and the Chrome team think are important. It is not a test running tool you can add custom tests or rules to based on your specific requirements.</p>
<p class="calibre2">At the 2017 Microsoft Edge Web Summit, they announced a similar tool called Sonar (<a href="https://sonarwhal.com" class="calibre9">https://sonarwhal.com</a>). It is also a node module and command-line tool that exercises tests against a supplied URL. The difference with Lighthouse is the ability to expand the test suite as you want. Not only can you add publicly available tests or rules, you can author your own.</p>
<p class="calibre2">Sonar can perform and does use the same test suites as Lighthouse, but allows you to add more. At the time of writing this book, it is not available in the Edge developer tools like Lighthouse. They do offer an online instance where you can test public URLs and of course run it locally as part of your test suite.</p>
<p class="calibre2">You should make Lighthouse and Sonar part of your routine developer work flow. You can quickly spot not only performance issues, but missing progressive web application requirements, best practices, basic SEO issues, and bad server configurations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Using WebPageTest to benchmark performance</h1>
                
            
            
                
<p class="calibre2"><strong class="calibre4">WebPageTest</strong> (<a href="https://webpagetest.org/" target="_blank" class="calibre9">https://webpagetest.org/</a>) is a free tool you can use to give you web performance details. It works much like developer tools, but adds even more value:</p>
<div><img src="img/00109.jpeg" class="calibre123"/></div>
<p class="calibre2">Not only does it provide a detailed waterfall, it provides testing from different locations, varying speeds, and devices. Sometimes, the truths it reveals can be hard to swallow, but it provides you with targeted areas so that you can improve your performance:</p>
<div><img src="img/00110.jpeg" class="calibre124"/></div>
<p class="calibre2">To perform a test, visit <a href="https://webpagetest.org" class="calibre9">https://webpagetest.org</a>. If your site has a public URL, enter it in the form and select a location, device/browser, and a speed to test. Once you submit the request, your site is evaluated using real hardware. After a minute or two, assuming it is not added to a queue to be processed, you receive a report:</p>
<div><img src="img/00111.jpeg" class="calibre125"/></div>
<p class="calibre2">Just like the browser developer tools provide a network waterfall, WebPageTest can too, but with even more details:</p>
<div><img src="img/00112.jpeg" class="calibre126"/></div>
<p class="calibre2">A key metric I always check is the speed index. This is a performance indicator created by Patrick Meenan, the mind behind WebPageTest, that measures how visually complete a page is over its load time:</p>
<div><img src="img/00113.jpeg" class="calibre127"/></div>
<p class="calibre2">It measures how much white space is visible compared to the final render. The goal is to minimize the time to a complete render. Speed Index is a way to measure the time to the first interaction or perceived rendering.</p>
<p class="calibre2">A good number to target is 1,000 or less. This indicates that it took 1 second or less to render the page. For reference, most pages I evaluate score well over 10,000, indicating that it takes at least 10 seconds to render. These poor scores are over a broadband connection, so the value is much worse for cellular connections:</p>
<div><img src="img/00114.gif" class="calibre128"/></div>
<p class="calibre2">There are many advanced features and settings that can be used to execute WebPageTest, including custom scripts. You can even stand up your own virtual machine either locally or in Amazon AWS. This is helpful when you have an enterprise application that's hidden behind a firewall.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Key performance indicators</h1>
                
            
            
                
<p class="calibre2">The first step to creating a fast site or improving an existing site is to use tools to measure your performance and knowing what to measure. You should create a performance baseline and make iterative changes to improve your performance profile.</p>
<p class="calibre2">In this section, I will review different metrics you should measure, why you need to track them, and how to improve them.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Time to first byte</h1>
                
            
            
                
<p class="calibre2">The time it takes to retrieve the first response byte is the time to first byte. This moment starts the response download. The most important network request is for the document. Once downloaded, the rest of the network resources (images, scripts, style sheets, and so on) can be downloaded.</p>
<p class="calibre2">You can break down the time to first byte process into different steps:</p>
<ul class="calibre10">
<li class="calibre11">Time to create a connection between the browser and server</li>
<li class="calibre11">The time it takes to retrieve and possibly render the file on the server</li>
<li class="calibre11">The time it takes to send the bytes to the browser</li>
</ul>
<p class="calibre2">The easiest way to measure time to first byte is by opening the browser developer tools by pressing <em class="calibre13">F12</em> or <em class="calibre13">Ctrl</em> + <em class="calibre13">Shift</em> + <em class="calibre13">I</em>. Each browser's developer tools have a network tab. Here, you can see a page's waterfall. You may need to refresh the page to generate the report.</p>
<p class="calibre2">I recommend performing both a primed and unprimed request. The difference is when loading the page as if this is the first time you have visited the site, which is called unprimed. In this state, there is nothing being persisted in the browser cache. You should also clear or bypass your service worker.</p>
<p class="calibre2">You can trigger an unprimed request by doing a hard reload, <em class="calibre13">Ctrl</em> + <em class="calibre13">F5</em>.</p>
<p class="calibre2">If you look at the following waterfall example, you will notice that the first request, the one for the document or HTML, completes first. The browser then parses the markup, identifying additional assets to load. This is when those requests begin.</p>
<p class="calibre2">You should be able to notice this pattern for all pages, even when assets are locally cached. This is why there is a slight time gap between the initial document request and the supporting resources:</p>
<div><img src="img/00115.jpeg" class="calibre52"/></div>
<p class="calibre2">A primed request assumes that you have visited the site or page before, and the browser and possibly the service worker cache contain valid responses. This means those requests are made locally, with no network activity. In theory, the page should load faster thanks to the cache.</p>
<p class="calibre2">The waterfall is composed of each file request required to compose the page. You should be able to select an individual request (double click the request in the waterfall) to see how much time each step took:</p>
<div><img src="img/00116.jpeg" class="calibre129"/></div>
<p class="calibre2">Chrome, FireFox, and Edge let you visualize the time to first byte. Each have a Timings panel that breaks apart the different parts of the request and the time allocated. It refines the parts a little more, showing you time to perform DNS resolution, creating the connection to the server, and the time it took the server to send the bytes to the browser.</p>
<p class="calibre2">Before a network request is made, it is added to a browser queue. This queue is a collection of requests the browser needs to make. Each browser determines how this queue is processed, which depends on available resources, HTTP/2 versus HTTP/1 support, and so on.</p>
<p class="calibre2">Next, if needed, the browser triggers a DNS resolution. If the device has a cache domain resolution or IP address, this step is skipped. You can speed this up by using the <kbd class="calibre12">dns-prefetch</kbd>, which I will cover a little later.</p>
<p class="calibre2">The browser then makes the network request. At that point, it is up to the server to send a response. If the server has any bottlenecks, you should address those issues.</p>
<p class="calibre2">Don't forget TLS negotiation. There is a slight performance hit for HTTPS, but when using HTTP/2, this hit is typically washed out by additional performance enhancements offered by HTTP/2.</p>
<p class="calibre2">You can reduce your time to first byte by optimizing your server configuration. You should look for opportunities to reduce disk I/O by caching responses in memory. In ASP.NET, this is done by implementing the Output cache. Other web platforms provide similar capabilities.</p>
<p class="calibre2">Database queries are another common bottleneck. If you can eliminate them, you should. Evaluate the page's data and find the data that could be retrieved ahead of time. I like to create JSON data files or objects in memory to avoid these expensive queries.</p>
<p class="calibre2">This is the main reason no-SQL, document databases such as MongoDB and ElasticSearch, and cloud services such as DynamoDB have grown in popularity. These databases as designed to have pre-selected and formatted data ready to go on demand. These solutions have helped popular online sites such as Twitter, Facebook, and so on grow and scale very quickly.</p>
<p class="calibre2">Another tactic is to avoid on-demand rendering as much as possible. Most websites are rendered by a server process such as ASP.NET, PHP, Ruby, Node, and so on. These all add overhead to the request process. By pre-rendering markup where possible, you reduce the opportunities for these processes to slow down the response.</p>
<p class="calibre2">I try to use a static website solution when possible because they offer the fastest response pipeline. Static sites have the advantage over runtime rendering because the rendering cycle is removed. You can create your own engine to pre-render content or use a tool like Varnish to manage the task. You don't have to abandon your existing processor, but instead add a static engine on top to maintain the static files so that your pages load faster.</p>
<p class="calibre2">The only remaining point of friction is the speed of the networks. Unfortunately, these are typically out of your control. Routers, proxies, and cell towers can all cause issues.</p>
<p class="calibre2">At this point, the response bytes start streaming into the browser for processing. The larger the file, the longer, typically, the delay.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">The PRPL pattern</h1>
                
            
            
                
<p class="calibre2">We have looked at both the time to first byte and runtime performance issues. The best way to make sure that your site is performing its best is by implementing architecture best practices. The PRPL pattern was created to help modern web applications achieve top performance values.</p>
<p class="calibre2">The Google Polymer team developed PRPL as a guideline to follow to help websites perform better. It should be considered an architecture you can implement, but it is not all about technical specifics. To quote the PRPL documentation:</p>
<p>"PRPL is more about a mindset and a long-term vision for improving the performance of the mobile web than it is about specific technologies or techniques."</p>
<p class="calibre2">PRPL goes back to the principle of putting performance as a first-class feature of any website.</p>
<p class="calibre2">PRPL stands for:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">P</strong>ush critical resources for the initial URL route using <kbd class="calibre12">&lt;link preload&gt;</kbd> and HTTP/2</li>
<li class="calibre11"><strong class="calibre1">R</strong>ender initial route</li>
<li class="calibre11"><strong class="calibre1">P</strong>re-cache remaining routes</li>
<li class="calibre11"><strong class="calibre1">L</strong>azy-load and create remaining routes on demand</li>
</ul>
<p class="calibre2">Even though PRPL was created with modern single page apps in mind, progressive web applications can benefit from following the PRPL pattern. Service workers are a valuable tool for implementing the PRPL pattern because you can leverage the Cache API to implement the pattern. You just have to adjust how the different principles are applied to improve your apps' performance.</p>
<p class="calibre2">The primary goals of the PRPL pattern are:</p>
<ul class="calibre10">
<li class="calibre11">Minimum time-to-interactive:
<ul class="calibre30">
<li class="calibre11">Especially on first use (regardless of entry point)</li>
<li class="calibre11">Especially on real-world mobile devices</li>
</ul>
</li>
<li class="calibre11">Maximum caching efficiency, especially over time as updates are released</li>
<li class="calibre11">Simplicity of development and deployment</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Implementing push with browser hints and the service worker cache</h1>
                
            
            
                
<p class="calibre2">The first concept of push relies on implementing the HTTP/2 server-side push. I have found this to be difficult to configure as most servers have not yet implemented HTTP/2 push.</p>
<p class="calibre2">This is where service workers can offer a solution I feel is even better. We looked at how to implement pre-caching, an excellent alternative to using HTTP/2 Push. By using pre-caching, you are effectively pushing those critical assets to the browser before they are needed.</p>
<p class="calibre2">Remember that the resources you pre-cache should be critical and common application assets. These assets should mirror what you might want to configure HTTP/2 push to send.</p>
<p class="calibre2">Combining service worker caching with the preload resource hint can recreate most of what HTTP/2 push does. Browsers use preload hint to initialize resource requests before they are encountered in the code. When paired with pre-cached assets, the loading process is lightning fast.</p>
<p class="calibre2">On the surface, resource hints like preload may not seem to provide much advantage. But as a page's composition grows complex, these hints can improve page load and rendering time significantly.</p>
<p class="calibre2">The browser does not initiate a request until it is parsed from the HTML or initiated from a script or style sheet.</p>
<p class="calibre2">Custom font files are a perfect example. Their downloads are not initiated until the browser parses the style sheet and finds the font reference. If the files are included as a preload resource hint, the browser has already loaded or at least started the request, making the file available sooner:</p>
<pre class="calibre17">    &lt;link rel="preload" href="css/webfonts/fa-brands-400.eot " <br class="title-page-name"/>     as="font"&gt; 
     ... 
    &lt;link rel="preload" href="js/libs/utils.js" as="script"&gt; 
    &lt;link rel="preload" href="js/libs/localforage.min.js" as="script"&gt; 
    &lt;link rel="preload" href="js/libs/mustache.min.js" as="script"&gt; 
    &lt;link rel="preload" href="js/app/events.js" as="script"&gt; 
    &lt;link rel="preload" href="js/app/tickets.js" as="script"&gt; 
    &lt;link rel="preload" href="js/app/user.js" as="script"&gt; 
    &lt;link rel="preload" href="js/app/app.js" as="script"&gt; </pre>
<p class="calibre2">Specifying the resource content type allows the browser to:</p>
<ul class="calibre10">
<li class="calibre11">Prioritize resource loading</li>
<li class="calibre11">Match future requests and reusing the same resource</li>
<li class="calibre11">Apply the resource's content security policy</li>
<li class="calibre11">Set the resource's correct Accept request headers</li>
</ul>
<p class="calibre2">You can also add the resource MIME type. When you do this, the browser can determine if it supports the resource type before it attempts to download the file:</p>
<pre class="calibre17">&lt;link rel="preload" href="js/app/app.js" as="script" type="application/javascript"&gt; </pre>
<p class="calibre2">The idea is to make the page or application's assets available before the DOM parsing triggers a request. Since these assets are available in the service worker cache, they are already stored locally and can be loaded instantly.</p>
<p class="calibre2">You can notice the difference in the waterfall where <kbd class="calibre12">preload</kbd> hints are used. Do you remember that I pointed out a slight time gap between the initial markup being loaded and the assets in earlier waterfalls?</p>
<p class="calibre2">If you look at the following waterfall, you will notice that the dependencies are initiated before the markup has finished loading:</p>
<div><img src="img/00117.jpeg" class="calibre52"/></div>
<p class="calibre2">This is due to the browser identifying these assets with the preload hint applied. It starts downloading the resource as soon as the item is parsed from the markup, not after the entire document is completely parsed.</p>
<p class="calibre2">You should also note how fast these resources are loaded. This is due to them being cached using service worker caching. This eliminates the network bottleneck, which in some cases means that the files are loaded even before the markup is completely parsed.</p>
<p class="calibre2">It's a slight page load advantage, not a major improvement. Every little bit helps as milliseconds quickly add up.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Using the app shell model and service worker to render the initial route</h1>
                
            
            
                
<p class="calibre2">Since the PRPL pattern was designed from a SPA first perspective, the language speaks to that architecture. But as you have seen in the previous chapters, app shells are useful for progressive web apps.</p>
<p class="calibre2">Even when you don't have a page cached, you should have at least your application's markup shell cached locally. This can serve as your initial render, giving the user a sense of response. Meanwhile, you can retrieve any assets from the network to complete the page.</p>
<p class="calibre2">The PWA tickets app uses the service worker to render pages using Mustache templates and JSON data retrieved from the API. This is an example of how you can return the app shell as a valid response to a request and then update the content once it is available.</p>
<p class="calibre2">My rule is to give the user everything I have at the moment and fill in the blanks as I have more to give. This could be a combination of supplying the app shell and replacing it later or injecting the page-specific markup once it is available.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Service worker pre-caching important routes</h1>
                
            
            
                
<p class="calibre2">At this point in this book, it should be obvious how a good service worker pre-caching strategy applies to the PRPL pre-caching point, but it never hurts to review the concept.</p>
<p class="calibre2">The second P in PRPL stands for pre-caching common routes. This includes the HTML and their supporting files such as styles, images, and scripts. This is exactly how your service worker's pre-cache strategy should be designed.</p>
<p class="calibre2">Important assets are commonly visited pages, but can also be the markup templates required to render the pages in the service worker. The common styles, scripts, images, and other support assets should be pre-cached.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Lazy-loading non-critical and dynamic routes</h1>
                
            
            
                
<p class="calibre2">Not every page in every website can or should be cached ahead of time. As you saw in  <a target="_blank" href="part0114.html#3CN040-f12cdcca08b54960b3d271452dc7667d" class="calibre9">Chapter 5</a>, <em class="calibre13">The Service Worker Life Cycle</em>, you should also have cache invalidation logic in place to ensure that you supply the freshest content.</p>
<p class="calibre2">Dynamic content, like the available ticket events or even an updated list of podcast episodes, is difficult to cache long-term. But you can provide a better experience than just waiting to download all of the page's resources.</p>
<p class="calibre2">This is where employing one or more of the common caching strategies is helpful. You can also combine your rendering strategy with the app shell concept and build the page as assets are loaded or updated.</p>
<p class="calibre2">You can also pre-cache and update common support assets as needed. This is one of the underestimated powers of the web, dynamically updating what is rendered and how it is rendered. You can update not just the markup in a page, but change style sheets and scripts on the fly too.</p>
<p class="calibre2">As you have also learned, you can cache assets in the service worker without affecting the UI thread. This can be used to pre-cache non-critical assets and update previously cached content.</p>
<p class="calibre2">As you can see, service worker caching makes implementing the PRPL pattern very natural. Its ability to cache resources makes all four PRPL principles easy to implement. If you have followed the examples and guidelines in the previous chapters, then you have seen how to design PRPL compliant progressive web applications.</p>
<p class="calibre2">I think the number one PRPL principle is to cache as much of your application's assets in the client as possible. This makes the network nice to have and not a potential source of delay and uncertainty. This is exactly what service worker caching was designed to do: make your assets close to the user's glass.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">The RAIL pattern</h1>
                
            
            
                
<p class="calibre2">The RAIL pattern is an acronym used by the Google Chrome team to define one of the many WPO patterns you should try to follow. Its goal is to ensure your user experience is responsive:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">Response</strong>: How quickly there is a response when there is any input</li>
<li class="calibre11"><strong class="calibre1">Animation</strong>: Includes visual animation, scrolling, and dragging</li>
<li class="calibre11"><strong class="calibre1">Idle</strong>: Background work</li>
<li class="calibre11"><strong class="calibre1">Load</strong>: How quickly a page can achieve the first meaningful paint</li>
</ul>
<p class="calibre2">Where the PRPL pattern is concerned with resource loading, RAIL is about the runtime user experience or what happens once the resources are loaded.</p>
<p class="calibre2">The pattern is designed to be user centric, focusing on performance first. The four aspects that make up the acronym are distinct areas of a web application and page's life cycle, or what happens once the bytes are loaded.</p>
<p class="calibre2">Consider the different areas where performance is important: loading, rendering and responding to actions. There is more than just the page load phase. Page load is how fast the browser can load the assets, but many forget it still needs to process those assets and render the content. Then, once rendered, you can respond to user interactions.</p>
<p class="calibre2">You also need to consider how quickly the page response is to a click or tap. Is scrolling smooth? Are notifications prompt? What about any background activities, are they efficient?</p>
<p class="calibre2">The first critical factor to the average user is how long it takes for a page to become interactive, not how long it takes to download the files.</p>
<p class="calibre2">At the recent Google I/O, the team announced new metrics that Lighthouse will report, specifically <strong class="calibre4">First Contentful Paint</strong> (<strong class="calibre4">FCP</strong>). This is the point at which the browser renders the first pixel of a new page's DOM. The measurement starts from the time of navigation to the point that the first pixel is rendered.</p>
<p class="calibre2">The reason this is a key performance indicator is that this is the first visual queue to the user where their requested action or navigation is being handled. I like to translate that into saying that this is the point where the user knows the page is coming and was not lost in the ether, causing them to try and reload the page or give up.</p>
<p class="calibre2">The FCP is available from the Paint Timing API (<a href="https://w3c.github.io/paint-timing/" target="_blank" class="calibre9">https://w3c.github.io/paint-timing/</a>), one of the modern performance APIs available in modern browsers.</p>
<p class="calibre2">The next KPI you should focus on is <strong class="calibre4">Time to Interactive</strong> (<strong class="calibre4">TTI</strong>). This is the point where the page is fully rendered and capable of responding to user input. Often, even though the page appears to be rendered, it cannot respond to the user due to background processing.</p>
<p class="calibre2">For example, the page is still processing JavaScript, which locks the UI thread and the page cannot be scrolled.</p>
<p class="calibre2">RAIL focuses on the user; the goal is not to necessarily make the site perform fast on a specific device, but to make your user happy. Any time a user interacts with your content, you should have a response within 100 ms. Any sort of animation or scrolling should also respond within 10 ms.</p>
<p class="calibre2">Because modern web pages tend to do a lot of background processing, you should maximize idle time to perform these tasks, not blocking interaction and rendering.</p>
<p class="calibre2">If you need to perform any non-UI processing, such as data transformations, service workers or web workers provide a channel for you to offload those processes to background threads. This frees the UI thread to focus on UI tasks, like layout and painting. It also frees the UI to immediately respond to user interaction.</p>
<p class="calibre2">Focus on delivering interactive content within one second. This can be very difficult to achieve over cellular networks and average mobile devices, but not impossible.</p>
<p class="calibre2">As I mentioned before, server load time is not the majority of your web performance profile, it's the resource processing that creates the bigger bottlenecks. That's because scripts and style sheets block the critical rendering path, leaving your page partially rendered or appearing to be hung.</p>
<p class="calibre2">If you have never heard of the critical rendering path, it is the workflow that browsers use to compose and render a page:</p>
<div><img src="img/00118.jpeg" class="calibre130"/></div>
<p class="calibre2">These are the main steps:</p>
<ol class="calibre14">
<li value="1" class="calibre11"><strong class="calibre1">Document Object Model</strong> (<strong class="calibre1">DOM</strong>)</li>
<li value="2" class="calibre11"><strong class="calibre1">CSS object model </strong>(<strong class="calibre1">CSSOM</strong>)</li>
<li value="3" class="calibre11">Render Tree</li>
<li value="4" class="calibre11">Layout</li>
<li value="5" class="calibre11">Paint</li>
</ol>
<p class="calibre2">To compose the DOM, the browser must complete these substeps:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Convert bytes to characters</li>
<li value="2" class="calibre11">Identify tokens</li>
<li value="3" class="calibre11">Convert tokens to nodes</li>
<li value="4" class="calibre11">Build the DOM Tree</li>
</ol>
<p class="calibre2">Similar to building the DOM, the browser follows a similar series of steps to compose the CSSOM or process styles:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Convert bytes to characters</li>
<li value="2" class="calibre11">Identify tokens</li>
<li value="3" class="calibre11">Convert tokens to nodes</li>
<li value="4" class="calibre11">Build CSSOM</li>
</ol>
<p class="calibre2">The important takeaway for CSS is just like JavaScript: it is rendering blocking. The browser must process the page's styles before it can be rendered. Multiple large CSS files cause the CSSOM process to repeat. The larger the style sheet, the longer this step takes.</p>
<p class="calibre2">Once the DOM and CSSOM are created, the browser then combines the two to compose the render tree. This is followed by the layout step, where all the size and color attributes are calculated for all the page elements.</p>
<p class="calibre2">Finally, the pixels are painted to the screen. This is not always 'instant', as different styles and style combinations take different amounts of processing to render.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">How JavaScript clogs the pipeline</h1>
                
            
            
                
<p class="calibre2">The DOM and CSSOM processes work together to produce what is rendered on the screen, but there is a third part of the rendering cycle, that is, processing JavaScript. Not only is JavaScript a render blocker, it is also a parser blocking process.</p>
<p class="calibre2">This is the primary reason we add script references at the end of the HTML. By doing so, the browser has a chance to parse the HTML and CSS before attempting to load scripts, which blocks the parsing process:</p>
<div><img src="img/00119.gif" class="calibre131"/></div>
<p class="calibre2">When the browser encounters a script, it stops the DOM and CSS parsers to load, parse, and evaluate the JavaScript. This is one reason I make intentional efforts to minimize my JavaScript size.</p>
<p class="calibre2">There are some tricks you can employ to minimize this behavior. The first is to mark any script you can as async. This causes the browser to casually load the script file.</p>
<p class="calibre2">This sounds great at first, but most of the time, I have found this to be a more optimistic than practical. There always seems to be at least one critical script that requires execution as the page is being rendered.</p>
<p class="calibre2">You can mark all your scripts as asynchronous and see if there are any issues when running your application. Just be thorough with your testing to flesh out any edge cases:</p>
<pre class="calibre17">&lt;script src="img/config.js" async&gt;&lt;/script&gt; </pre>
<p class="calibre2">Another solution is to minify your scripts and then inline them in your markup. This can also help your rendering cycle. One of the main benefits is not waiting on an additional file to download.</p>
<p class="calibre2">However, if you are using HTTP/2, the multiplexing capabilities will probably offer more benefits. With HTTP/2, you are often better off using small files that can be individually cached than large file bundles.</p>
<p class="calibre2">When you inline scripts, the size of your HTML grows, which can delay its processing. However, as you are about to learn, inlining CSS is highly beneficial. It is a matter of testing to see what works best for your page and application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Why 14 KB is the magic number</h1>
                
            
            
                
<p class="calibre2">In an effort to control network traffic, TCP implements a pattern called slow start. It does this to keep the network from being overwhelmed with requests. The details are specified in RFC 5681 (<a href="https://tools.ietf.org/html/rfc5681" target="_blank" class="calibre9">https://tools.ietf.org/html/rfc5681</a>).</p>
<p class="calibre2">The protocol works where the sender or initiator sends an initial, small packet. When it receives a response, it then doubles the packet size. This volley is repeated until the sender receives a congested response.</p>
<p class="calibre2">The initial packet size is 14 KB. This back and forth is a series of round trips. If you can fit an entire page or response within 14 KB, it only needs one round trip to be completely downloaded:</p>
<div><img src="img/00120.jpeg" class="calibre132"/></div>
<p class="calibre2">In this example, the response is 10.5 KB, so only 1 round trip is required. You should note that this example is not compressed, which would reduce the size significantly. This is another point I want you to remember as we apply resource inlining a little later.</p>
<p class="calibre2">The initial TCP data packet is actually 16 KB, but the first 2 KB are reserved for request header data. The remaining 14 KB are where your content or data are transferred. If the content is more than 14 KB, then a second round trip is initiated, and this time the packet size is doubled to 32 KB. This repeats until there is a network congestion message.</p>
<p class="calibre2">By limiting the request to a single round trip, you are able to load the entire response almost instantly. The more round trips, the longer the data takes to load.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Inline critical CSS</h1>
                
            
            
                
<p class="calibre2">When you inline CSS, you eliminate the required round trips to retrieve the styles, and they are immediately available to the browser as it parses the DOM. This makes these two critical steps much faster.</p>
<p class="calibre2">To refresh, when the browser encounters external style sheets, it blocks any rendering until the style sheets are fully loaded.</p>
<p class="calibre2">As I mentioned earlier, you want to limit the size of a page's CSS to just the CSS required to render the page. By limiting the styles to just those used by the page, you can typically reduce the amount of CSS to a handful of kilobytes.</p>
<p class="calibre2">Because the amount of real CSS is minimal, you can inline those styles in the document's <kbd class="calibre12">head</kbd> element. Now, the browser has no external file to download and a minimal amount of CSS to load. Plus, you have the critical styles required to render the app shell.</p>
<p class="calibre2">The PWA ticket app has a very standard app shell: <kbd class="calibre12">header</kbd>, <kbd class="calibre12">body</kbd>, and <kbd class="calibre12">footer</kbd>. Each individual page requires a minimal amount of custom CSS to render its content.</p>
<p class="calibre2">The good news is that there are tools available to help you identify the CSS each page requires. There are multiple node modules available, but I have focused on the UnCSS module (<a href="https://www.npmjs.com/package/uncss" class="calibre9">https://www.npmjs.com/package/uncss</a>). It was one of the first modules created to identify the required CSS.</p>
<p class="calibre2">Because these are node modules, you can include them in a build script. The PWA ticket application has a build script in the project's <kbd class="calibre12">utils</kbd> folder called <kbd class="calibre12">render-public.js</kbd>. I won't go into all of the script's details, but it runs over the site's source to produce the site's pages and support files.</p>
<p class="calibre2">The <kbd class="calibre12">extractCSS</kbd> function handles extracting a page's styles, minimizing them, and injecting them into the <kbd class="calibre12">head</kbd> element.</p>
<p class="calibre2">There are additional node modules being used to help. Cheerio loads HTML and creates an object with the jQuery API, just like you were using jQuery in the browser. This makes manipulating the markup much easier.</p>
<p class="calibre2">The second module is CleanCSS. This module minifies styles, removing unnecessary white space, thus making the code take up less space:</p>
<pre class="calibre17">function extractCSS($, callback) { 
 
    let options = { 
            ignore: [".page-content .card", ".page-content .card-<br class="title-page-name"/>            title", ".page-content .ticket-card"], 
            media: ['@media (max-width:480px)', '@media (min-<br class="title-page-name"/>           width:768px)', '@media (max-width:992px)', '@media (max-<br class="title-page-name"/>           width:1199px)'], 
            stylesheets: [path.resolve(publicPath, <br class="title-page-name"/>            'css/libs/bootstrap.min.css'), 
            path.resolve(publicPath, 'css/app/site.css') 
            ], 
            timeout: 1000, 
            report: true, 
            banner: false 
        }, 
        html = $.html(); 
 
    let $html = cheerio.load(html); 
 
    $html("body").append(templates); 
    $html("script").remove(); 
 
    $("script").remove(); 
 
    //run uncss 
    uncss($html.html(), options, function (error, output) { 
 
        if (error) { 
            console.log(error); 
        } 
 
        let minCSS = new CleanCSS({ 
            level: 2 
        }).minify(output); 
 
        $("head").append("&lt;style&gt;" + minCSS.styles + "&lt;/style&gt;"); 
 
        callback($); 
 
    }); 
} </pre>
<p class="calibre2">UnCSS has a long list of configuration options you can use to control how the module executes. I have supplied the most common settings I use, like media query breakpoints and eliminating banner comments.</p>
<p class="calibre2">Sometimes, I find that I still need to include a list of selectors that should not be removed:</p>
<pre class="calibre17">ignore: [".page-content .card", ".page-content .card-title", ".page-content .ticket-card"] </pre>
<p class="calibre2">I have also found that removing any script references from the markup will help the module. When the module finds a script, it does try to load it and execute it. This is because UnCSS exercises the page in a headless browser, which loads the page just as if it were a normal browser.</p>
<p class="calibre2">UnCSS can either process raw HTML, which is how I use it, or load a page via a URL or local path. It utilizes the standard node callback pattern, so you should write your code accordingly.</p>
<p class="calibre2">Another thing I try to do is inject potential content templates in the HTML to be processed. This should help UnCSS isolate all the styles needed, even when they are dynamically rendered.</p>
<p class="calibre2">Like UnCSS, CleanCSS also uses the callback pattern. You can supply the filtered CSS and it will return a minified version.</p>
<p class="calibre2">At this point, you can inject the minified styles into the HTML <kbd class="calibre12">head</kbd>:</p>
<pre class="calibre17">$("head").append("&lt;style&gt;" + minCSS.styles + "&lt;/style&gt;"); </pre>
<p class="calibre2">At this point, you have the page's HTML with all its required CSS, inline in the markup HEAD. For the PWA ticket application, the typical page is around 30 KB, which does not meet the 14 KB goal.</p>
<p class="calibre2">Fortunately, we are not done.</p>
<p class="calibre2">Static resources, like HTML, should be compressed. You can use gzip or deflate compression. Brotli is another option, but is not universally supported by all browsers. Once you compress these files, they typically reduce to around 8 KB, well within our 14 KB goal!</p>
<p class="calibre2">Most web servers can be configured to compress text files on demand. But as you can imagine, I like to do this as part of my deploy process. This can be done with a node, but you should check with your devops team to make sure that this is being done for your site:</p>
<pre class="calibre17">Body = fs.createReadStream(src).pipe(zlib.createGzip({ 
            level: 9 
        })); </pre>
<p class="calibre2">Make sure any compressed files are served with the Content-Encoding header set to gzip or deflate so that the browser knows to decompress the response.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Minifying scripts with uglify</h1>
                
            
            
                
<p class="calibre2">Just like CSS, you should also minimize JavaScript files. Just like we used Clean-CSS to minify CSS, you can use uglify to do the same for your JavaScript. Instead of inlining the script, I like to keep it in individual files.</p>
<p class="calibre2">In the past, I would have also bundled multiple script files together. HTTP/2 utilizes request multiplexing to optimize content delivery. By keeping each script in individual files, you can take advantage of long term caching and make small changes without requiring a complete download.</p>
<p class="calibre2">In addition to minimizing the scripts, I am also going to show you how to create unique file names using an MD5 hash on the content. This will allow you to apply a very long caching time without worrying about browser caches retaining stale copies. This technique is advanced and does require some planning and, of course, an intelligent build or rendering process.</p>
<p class="calibre2">There are multiple uglifier node modules. I chose <kbd class="calibre12">uglify-js</kbd> for the PWA ticket application. The way I tend to pick modules like this is to look at popularity, but also what popular task runners such as Grunt, Gulp, and WebPack plugins rely upon.</p>
<p class="calibre2">As a word of warning, <kbd class="calibre12">uglify-js</kbd> does not handle ES6 syntax, like <kbd class="calibre12">let</kbd> and <kbd class="calibre12">const</kbd>, and will throw errors when encountered. But I warn against using ES6 syntax in the browser since there are still many browsers that do not support it, such as Internet Explorer.</p>
<p class="calibre2">For the build script, I chose to create a simple uglify module to reference in the overall build script. It references <kbd class="calibre12">uglify-js</kbd> and creates an <kbd class="calibre12">uglify</kbd> class:</p>
<pre class="calibre17">const UglifyJS = require("uglify-js"), 
    uglifyOptions = { 
        parse: { 
            html5_comments: false, 
            shebang: false 
        }, 
        compress: { 
            drop_console: true, 
            keep_fargs: false 
        } 
    }, 
   ...; 
 
class uglify { 
 
    constructor(src) {} 
    transformSrc(srcFiles) {} 
    minify() {} 
 
} </pre>
<p class="calibre2">The class <kbd class="calibre12">constructor</kbd> and <kbd class="calibre12">transformSrc</kbd> methods are used to set up before minification. They are set up to allow you to pass either a single script reference or an array of scripts to uglify and concatenate.</p>
<p class="calibre2">Just like UnCSS, uglify allows you to customize the process. This is where the options allow you to configure the module. For this, I chose some simple settings I like to use to optimize the process:</p>
<pre class="calibre17">    minify() { 
 
        let src = this.transformSrc(srcFiles); 
        return UglifyJS.minify(src, uglifyOptions); 
 
    } </pre>
<p class="calibre2">The render script not only uglifies each script; it also creates a unique hash name:</p>
<pre class="calibre17">function uglifyScripts() { 
 
    scripts.forEach(script =&gt; { 
 
        let ug = new uglify(path.resolve(publicPath, script)); 
        let min = ug.minify(); 
 
        if (min.code &amp;&amp; min.code !== "") { 
 
            let hashName = utils.getHash(min.code); 
 
            fs.writeFileSync(path.join(publicPath, <br class="title-page-name"/>            path.dirname(script), hashName + ".min.js"), min.code); 
 
            scriptsObjs.push({ 
                src: script, 
                hash: hashName + ".min.js" 
            }); 
        } else { 
            console.log("uglify error ", min.error); 
        } 
    });  
} </pre>
<p class="calibre2">The file is calculated by passing the script's contents to the nodejs crypto object. The crypto object makes calculating hashes simple. In this case, I want an md5 hash value, so when the <kbd class="calibre12">createHash</kbd> method is called, you supply the <kbd class="calibre12">'md5'</kbd> value.</p>
<p class="calibre2">If you are not familiar with md5 hashes, they are a cryptographic way of generating a checksum to verify data integrity. They are not good for cryptography, but provide a unique value based on the data. That unique value is helpful for creating a unique file name:</p>
<pre class="calibre17">function  getHash(data) { 
    var md5 = crypto.createHash('md5'); 
    md5.update(data); 
<br class="title-page-name"/>    return md5.digest('hex'); 
} </pre>
<p class="calibre2">The uniqueness is good enough to have faith that the script file name won't be duplicated within your application. The build script needs to not only generate unique hash values, but save the file with the hash name. It could also just rename the source file.</p>
<p class="calibre2">Even after you create the file with the unique file name, you still need to integrate it into the HTML files. The render script takes care of that task. The product looks something like this:</p>
<pre class="calibre17">&lt;script src="img/470bb9da4a68c224d0034b1792dcbd77.min.js"&gt;&lt;/script&gt; 
&lt;script src="img/ca901f49ff220b077f4252d2f1140c68.min.js"&gt;&lt;/script&gt; 
&lt;script src="img/2ae25530a0dd28f30ca44f5182f0de61.min.js"&gt;&lt;/script&gt; 
&lt;script src="img/aa0a8a25292f1dc72b1bee3bd358d477.min.js"&gt;&lt;/script&gt; 
&lt;script src="img/470bb9da4a68c224d0034b1792dcbd77.min.js"&gt;&lt;/script&gt; 
&lt;script src="img/e392a867bee507b90b366637460259aa.min.js"&gt;&lt;/script&gt; 
&lt;script src="img/8fd5a965abed65cd11ef13e6a3408641.min.js"&gt;&lt;/script&gt; 
&lt;script src="img/512df4f42ca96bc22908ff3a84431452.min.js"&gt;&lt;/script&gt; 
&lt;script src="img/bc8ffbb70c5786945962ce782fae415c.min.js"&gt;&lt;/script&gt; </pre>
<p class="calibre2">I also added <kbd class="calibre12">.min</kbd> to each one of the files because the scripts have been minimized. This is done out of convention rather than a requirement. The benefit is for tools, like browser developer tools, that understand that the script is minimized. Edge allows you to choose to bypass the script when you are debugging because <kbd class="calibre12">.min</kbd> is appended to the file name.</p>
<p class="calibre2">Because each page also sets a preloaded hint for the script files, those references must also be updated:</p>
<pre class="calibre17">&lt;link rel="preload" href="js/libs/470bb9da4a68c224d0034b1792dcbd77.min.js" as="script" type="application/javascript"&gt; 
&lt;link rel="preload" href="js/libs/ca901f49ff220b077f4252d2f1140c68.min.js" as="script" type="application/javascript"&gt; 
&lt;link rel="preload" href="js/libs/2ae25530a0dd28f30ca44f5182f0de61.min.js" as="script" type="application/javascript"&gt; 
&lt;link rel="preload" href="js/libs/aa0a8a25292f1dc72b1bee3bd358d477.min.js" as="script" type="application/javascript"&gt; 
&lt;link rel="preload" href="js/app/pages/bc8ffbb70c5786945962ce782fae415c.min.js" as="script" type="application/javascript"&gt; 
&lt;link rel="preload" href="js/app/512df4f42ca96bc22908ff3a84431452.min.js" as="script" type="application/javascript"&gt; </pre>
<p class="calibre2">So, why did I add this complicated renaming step to the build and render processes? To enable long cache time frames. This tells the browser to attempt to cache the response locally in the built-in browser cache, not the service worker cache.</p>
<p class="calibre2">The recommended time to live is at least a year. Most scripts can and will be updated in that time frame and the hash name technique gives you a guaranteed cache busting technique. Other techniques, like appending a unique <kbd class="calibre12">QueryString</kbd> parameter, may not always work.</p>
<p class="calibre2">You can set the long time to live by setting the <kbd class="calibre12">cache-control</kbd> header. This needs to be done in your web server, so that it will be part of your devops workflow:</p>
<pre class="calibre17">cache-control: public, max-age=31536000 </pre>
<p class="calibre2">I won't dive into the art of configuring Cache-Control headers, but you can use the preceding example as a reference. Files such as scripts, style sheets, and even images are candidates for the hash naming trick. Just be sure to update any references to the files to the new name.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Using feature detection to conditionally load JavaScript polyfils</h1>
                
            
            
                
<p class="calibre2">The PWA ticket application uses many modern APIs, but some are not supported by older browsers. There are two browser scenarios you should be most concerned with: Internet Explorer and older Android phones. UC Browser is another popular browser that does not support all newer features yet.</p>
<p class="calibre2">Internet Explorer is the now deprecated Microsoft Browser. The only supported version is IE 11, and right now only lives on Windows 7 and in enterprises. Enterprises use many line of business applications, and many were created against old and obsolete web standards. Often, it is expensive for them to update or replace these applications.</p>
<p class="calibre2">Internet Explorer provides a legacy browser channel for them to continue running these applications. However, when they upgrade to Windows 10, they should configure these line of business applications to trigger Internet Explorer from Edge as needed and not as a default browser.</p>
<p class="calibre2">This means that the default behavior you should expect is Edge, not Internet Explorer on Windows 10. However, human nature and habits often override recommended practice, which means IE is still a very popular browser.</p>
<p class="calibre2">Fortunately, most of the modern APIs PWA tickets use can be polyfiled. This is where a script can be loaded, on demand, to implement the new API. Other APIs can be safely used behind a feature detection gate, like we do before registering a service worker, or be trusted to be gracefully ignored. The latter is how modern CSS properties are handled.</p>
<p class="calibre2">Loading a feature polyfil can be done as needed using feature detection and a simple technique I call toggling a script.</p>
<p class="calibre2">The PWA tickets application uses 4 polyfils:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">Object.Assign</kbd></li>
<li class="calibre11">Promises</li>
<li class="calibre11">The Fetch API</li>
<li class="calibre11"><kbd class="calibre12">IntersectionObserver</kbd></li>
</ul>
<p class="calibre2">The trick relies on these script references applying a type attribute other than script. This tells the browser that even though it is a script element, the src is not a script. Of course, the src files are scripts, but by setting the type to something else, the browser does not download the scripts.</p>
<p class="calibre2">The <kbd class="calibre12">toggleScript</kbd> method takes a supplied ID to reference the polyfil's <kbd class="calibre12">script</kbd> element. It then toggle's the script's type from <kbd class="calibre12">script-polyfil</kbd> to text/JavaScript. When this toggle happens, the browser downloads and processes the polyfil script:</p>
<pre class="calibre17">&lt;script type="script-polyfil" id="polyfilassign" <br class="title-page-name"/>    src="img/object.assign.js"&gt;&lt;/script&gt; 
    &lt;script type="script-polyfil" id="polyfilpromise"   <br class="title-page-name"/>    src="img/es6-promise.min.js"&gt;&lt;/script&gt; 
    &lt;script type="script-polyfil" id="polyfilfetch" <br class="title-page-name"/>    src="img/fetch.js"&gt;&lt;/script&gt; 
    &lt;script type="script-polyfil" id="polyfilintersection" <br class="title-page-name"/>    src="img/intersection-observer.js"&gt;&lt;/script&gt; 
    &lt;script&gt; 
        //wrap in IIFE to keep out of global scope 
        (function () { 
            function toggleScript(id) { 
                var target = document.getElementById("polyfil" + id); 
                target.setAttribute("type", "text/javascript"); 
            } 
 
            if (typeof Object.assign != 'function') { 
                toggleScript("assign"); 
            } 
 
            if (typeof Promise === "undefined" || <br class="title-page-name"/>            Promise.toString().indexOf("[native code]") === -1) { 
                toggleScript("promise"); 
            } 
 
            if (typeof fetch === "undefined" || <br class="title-page-name"/>            fetch.toString().indexOf("[native code]") === -1) { 
                toggleScript("fetch"); 
            } 
         }()); 
    &lt;/script&gt; </pre>
<p class="calibre2">All this depends on the polyfil being needed. Each API or feature support can be detected with a simple test. If the test fails, the feature is not supported, and the toggleScript method is called to load the polyfil.</p>
<p class="calibre2">You should put this code before you load any of the application-specific code or any code that might depend on these APIs.</p>
<p class="calibre2">Dynamically loading polyfils is important because it means you will use the native APIs when present and avoid loading these expensive files when the native API is present.</p>
<p class="calibre2">Any time you need to load these polyfils, the page load suffers. however, I think that this is a reasonable trade off because these older browsers will run slower in the first place and the user may not have the same expectations as someone using a modern browser.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Lazy loading images</h1>
                
            
            
                
<p class="calibre2">Images can delay your overall page load experience due to image numbers and their size. There are different strategies for optimizing image delivery. The first one you should consider is lazy loading images below the fold.</p>
<p class="calibre2">This could be a very tricky technique to execute. Modern APIs can help you, the <kbd class="calibre12">IntersectionObserver</kbd> (<a href="https://developer.mozilla.org/en-US/docs/Web/API/Intersection_Observer_API" target="_blank" class="calibre9">https://developer.mozilla.org/en-US/docs/Web/API/Intersection_Observer_API</a>) API in particular gives you the ability to detect when elements are entering the viewport. You can designate the distance and time estimated threshold for an element to appear.</p>
<p class="calibre2">The <kbd class="calibre12">IntersectionObserver</kbd> API will trigger an event to let you know when an element is about to be displayed. At this point, you can initiate an image download if necessary. This means that your pages images will not be loaded in the initial render process, but be loaded as needed. This can conserve valuable bandwidth and network connections in that initial page load:</p>
<pre class="calibre17">    lazyDisplay: function () { 
 
        var images = document.querySelectorAll('.lazy-image'); 
        var config = { 
   // If the image gets within 50px in the Y axis, start the download. 
            rootMargin: '50px 0px', 
            threshold: 0.01 
        }; 
 
        // The observer for the images on the page 
       var observer = new IntersectionObserver(this.showImage, config); 
 
        images.forEach(function (image) { 
            observer.observe(image); 
        }); 
 
    } </pre>
<p class="calibre2">Instead of using the images <kbd class="calibre12">src</kbd> attribute, you can designate the image source as a data attribute (<kbd class="calibre12">data-src</kbd>). You should also do the same thing for the <kbd class="calibre12">srcset</kbd> attribute:</p>
<pre class="calibre17">&lt;img class="lazy-image" data-src="img/venue.jpg" 
    data-srcset=" img/venues/venue-1200x900.jpg 1200vw, img/venues/venue -992x744.jpg 992vw, img/venues/venue -768x576.jpg 768vw, img/venues/venue -576x432.jpg 576vw" 
    sizes=" (max-width: 577px) 90vw, (max-width: 769px) 45vw, (max-width: 769px) 30vw, 20vw" &gt;</pre>
<p class="calibre2">The <kbd class="calibre12">showImage</kbd> function handles toggling the <kbd class="calibre12">data-src</kbd> and <kbd class="calibre12">data-srcset</kbd> values to the corresponding <kbd class="calibre12">src</kbd> and <kbd class="calibre12">srcset</kbd> values. This causes the browser to load the images just before they come into view, or on demand:</p>
<pre class="calibre17">showImage: function (entries, observer) { 
  entries.forEach(function (io) { 
          if (io.isIntersecting) { 
               var image = io.target, 
               src = image.getAttribute("data-src"), 
               srcSet = image.getAttribute("data-srcset"); 
 
               if (srcSet) { 
                  image.setAttribute("srcset", srcSet); 
               } 
 
               if (src) { 
                  image.setAttribute("src", src); 
               } 
            } 
        }); 
    } </pre>
<p class="calibre2">If you're worried about legacy browsers supporting <kbd class="calibre12">IntersectionObserver</kbd>, don't worry: there is a polyfil (<a href="https://github.com/w3c/IntersectionObserver/tree/master/polyfill" target="_blank" class="calibre9">https://github.com/w3c/IntersectionObserver/tree/master/polyfill</a>). Right now, Chrome, Firefox, and Edge have native <kbd class="calibre12">IntersectionObserver</kbd> support. The polyfil allows you to use this technique in other browsers.</p>
<p class="calibre2">You should use feature detection to determine if you need to load the polyfil and the polyfil technique that was described previously.</p>
<p class="calibre2">The PWA ticket application uses the <kbd class="calibre12">IntersectionObserver</kbd> pattern to lazy-load images. I also want to point out a key aspect of this technique, which is specifying the images' render size as a placeholder.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="calibre2">This chapter has focused on improving your progressive web app's performance. You have learned about key performance indicators you can measure as well as tools to measure your application.</p>
<p class="calibre2">Once you have identified items to improve, you can attack them to improve your page load time and response times.</p>
<p class="calibre2">You have also been given some techniques and been exposed to code to help you build your pages to provide a better user experience. You have seen how to minimize the amount of code each page needs, improve caching, and reduce the initial page payload.</p>
<p class="calibre2">In the next chapter, you will see how to automate your progressive web application workflow to ensure that you have a consistently performing and qualifying PWA.</p>


            

            
        
    </body></html>