<html><head></head><body>
		<div><h1 id="_idParaDest-196" class="chapter-number"><a id="_idTextAnchor247"/>13</h1>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor248"/>The Multiverses – Cross-Browser Testing and Cross-Environment Testing</h1>
			<p>In this chapter, we will begin adding the mutant power of horizontal scaling to browser operating systems and other platforms. This is in contrast to vertical scaling, which involves adding more tests to our suites, such as adding more floors to a superhero base that’s hiding in plain sight. Horizontal scaling is like expanding to more buildings up and down the city block. Our tests can run in multiple browsers, versions, operating systems, and other platforms. What this means is that if we are using a Mac as opposed to a Windows PC, then we will be confident that our applications and tests run well on our chosen browser. Chrome is typically the target browser because of the large number of users on both Windows and Mac. But many Mac users prefer Safari and Windows users prefer Edge. So, how do we ensure these combinations get tested?</p>
			<p>That’s where the standalone Selenium WebDriver service becomes useful. This service is used to automate the testing process across various browsers and platforms, which helps in identifying issues that might occur in specific environments. Utilizing this service can be a creative solution to streamline the test automation framework as it allows for more comprehensive testing coverage with less manual effort. However, it can also become quickly overwhelming.</p>
			<p>Think of this as a crossover between the multiple superhero universes. We will be extending testing beyond Chrome to Edge on a Windows machine as well as extending Chrome to Safari on a Mac. Then, we will use cloud-based solutions for various combinations.</p>
			<p>The main topics in this chapter are:</p>
			<ul>
				<li>Horizontal scaling</li>
				<li>Using built-in functionality via the wdio config file</li>
				<li>Using LambdaTest online to automate browser testing grid</li>
				<li>Using Selenium Standalone server to locally build the testing grid</li>
				<li>Avoiding the rabbit hole of horizontal scaling</li>
				<li>Handling environment-specific logic</li>
			</ul>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor249"/>Horizontal scaling – cross-browser testing</h1>
			<p>There are three ways in<a id="_idIndexMarker500"/> which you can do<a id="_idIndexMarker501"/> cross-browser testing for your projects:</p>
			<ul>
				<li>Using the built-in functionality via the <code>wdio</code> config file</li>
				<li>Using LambdaTest online to automate the browser testing grid</li>
				<li>Using Selenium Standalone server to locally build the testing grid</li>
			</ul>
			<p>Although we will discuss all three ways, in this book, our examples will be completed using the built-in functionality provided by the <code>wdio</code> config file.</p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor250"/>Using built-in functionality via the wdio config file</h1>
			<p>Cross-browser testing <a id="_idIndexMarker502"/>involves setting up the testing environment, writing tests using Jasmine syntax in TypeScript, and running the tests on different browsers. This is accomplished in the config file of WebdriverIO in the capabilities section. We will extend from Chrome to Edge in the capabilities <a id="_idIndexMarker503"/>section. This also controls how many<a id="_idIndexMarker504"/> concurrent browsers will be launched in parallel with the <strong class="bold">maxInstances</strong> parameter.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor251"/>Extending the wdio config file so that it supports multiple browsers</h2>
			<p>Set up <code>wdio.conf.ts</code> so that it<a id="_idIndexMarker505"/> defines your test settings and browser capabilities:</p>
			<pre class="console">
   // wdio.conf.ts
   exports.config = {
     specs: ['./tests/**/*.spec.ts'],
maxInstances: 2,
capabilities: [
       {
         browserName: 'chrome',
       },
       {
         browserName: 'safari',
       },
       {
         browserName: 'edge',
       },
     ],
     framework: 'jasmine',
     jasmineOpts: {
       defaultTimeoutInterval: 60000,
     },
     Services:[
"chromedriver",
"safaridriver",
"edgedriver"
     ]
   };</pre>			<p>In the <code>Services</code> section, we must <a id="_idIndexMarker506"/>provide the drivers to interact with the browsers. <code>chromedriver</code> runs the Chrome browser, which we have been using all along. To drive Safari, <code>safaridriver</code> will be used. Keep in mind that the number of concurrent browsers that can be used is limited to the resources available to the local machine.</p>
			<p>The following is an example of the type of test that can be run:</p>
			<pre class="console">
   // test/example.spec.ts
   import { browser } from '@wdio/globals';
   describe('Example Test', () =&gt; {
     it('should open a website', async () =&gt; {
       await browser.url('https://example.com');
       const title = await browser.getTitle();
       expect(title).toContain('Example Domain');
     });
   });
Yarn</pre>			<p>Finally, we must execute the test in multiple browsers by running this command:</p>
			<pre class="source-code">
yarn wdio wdio.conf.ts --spec ./test/example.spec.ts</pre>			<p>This will execute the preceding<a id="_idIndexMarker507"/> example test on all browsers configured in the <code>wdio.conf</code> file’s capabilities section, namely Chrome, Safari, and Edge.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor252"/>Handling browser-specific issues</h2>
			<p>If your application has <a id="_idIndexMarker508"/>browser-specific code or issues, you can use conditional checks or feature detection to handle them gracefully.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor253"/>Test responsiveness</h2>
			<p>Besides functional testing, ensure <a id="_idIndexMarker509"/>that your application is responsive and works well on different screen sizes and devices. This will require some next-level platform support. Companies such as LambdaTest, Browser Stack, and Sauce Labs provide custom environment configurations to ensure our application runs correctly under different architectures. These include iOS and Android mobile devices, tablets, and laptops of differing screen sizes. It is here that trying to maintain all these physical devices with the latest updates can become unfeasible.</p>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor254"/>Using LambdaTest online to automate the browser testing grid</h1>
			<p>Cross-browser testing with <a id="_idIndexMarker510"/>LambdaTest allows you to test your web applications or websites across a wide range of browsers and operating systems. LambdaTest is a cloud-based platform that provides <a id="_idIndexMarker511"/>real browsers that run on virtual <a id="_idIndexMarker512"/>machines, enabling you to perform comprehensive testing without the need to set up physical devices or virtual machines locally.</p>
			<p>To perform cross-browser testing with LambdaTest, follow these steps:</p>
			<ol>
				<li>First, you need to sign up for a LambdaTest account. Once you’ve registered, you can access the LambdaTest dashboard:</li>
			</ol>
			<div><div><img src="img/B19395_13_1.jpg" alt="Figure 13. 1 – LambdaTest dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13. 1 – LambdaTest dashboard</p>
			<ol>
				<li value="2">On the <a id="_idIndexMarker513"/>LambdaTest dashboard, you<a id="_idIndexMarker514"/> can select the browsers <a id="_idIndexMarker515"/>and operating systems you want to test your website on. A large variety of browsers and versions are available, including Chrome, Safari, and Edge on different operating systems such as Windows and macOS, as well as iOS and Android mobile devices:</li>
			</ol>
			<div><div><img src="img/B19395_13_2.jpg" alt="Figure 13.2 – LambdaTest browser, operating system, and screen resolution selections"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – LambdaTest browser, operating system, and screen resolution selections</p>
			<ol>
				<li value="3">You can<a id="_idIndexMarker516"/> choose to run tests on<a id="_idIndexMarker517"/> either the <em class="italic">live interactive testing</em> environment<a id="_idIndexMarker518"/> or the <em class="italic">automated screenshot </em><em class="italic">testing</em> environment.</li>
			</ol>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor255"/>Live interactive testing</h2>
			<p>In this mode, you can interact with <a id="_idIndexMarker519"/>browsers in real time, just like using a physical device:</p>
			<div><div><img src="img/B19395_13_3.jpg" alt="Figure 13.3 – LambdaTest live interactive testing for manual testers"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – LambdaTest live interactive testing for manual testers</p>
			<p> You can <a id="_idIndexMarker520"/>navigate your website, perform actions, and manually check for multiple issues.  Interactive live testing is a pivotal feature in modern test automation frameworks that aligns well with a focus on inspecting tests mid-execution.</p>
			<p>The live interactive testing feature provided by LambdaTest allows testers to interact with a website or web application in a real-time environment. This mirrors the experience a user would have on a physical device.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor256"/>Automated screenshot testing</h2>
			<p>In this mode, LambdaTest takes screenshots of your<a id="_idIndexMarker521"/> website on different browsers and operating systems automatically. This is useful for quick checks and to see how your website looks on various configurations:</p>
			<div><div><img src="img/B19395_13_4.jpg" alt="Figure 13.4 – Automated screenshot testing"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Automated screenshot testing</p>
			<p>Once you’ve selected the browsers and testing mode, you can enter your website’s URL in LambdaTest and start the testing process. The platform will open virtual machines with the chosen browsers and load your website for testing.</p>
			<p>During the testing process, you can inspect elements, use developer tools, and debug any issues you encounter. You can also take screenshots and save them for further analysis and reporting.</p>
			<p>LambdaTest<a id="_idIndexMarker522"/> provides detailed test reports, including screenshots and logs, which can help you identify any discrepancies across browsers and operating system configurations. You can share these with your team to discuss and address any issues that are found during cross-browser testing.</p>
			<p>They also offer<a id="_idIndexMarker523"/> integrations with various testing and collaboration tools, making it easier to incorporate cross-browser testing seamlessly into your existing development workflow. By using LambdaTest for cross-browser testing, you can ensure that your web application performs consistently and optimally across different browsers and operating systems.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor257"/>Using Selenium Standalone server to locally build the testing grid</h1>
			<p>Cross-browser testing with <a id="_idIndexMarker524"/>Selenium <a id="_idIndexMarker525"/>Standalone server allows you to test web applications or websites across multiple browsers and operating systems using the Selenium WebDriver API. The standalone server acts as a hub that connects to different browsers and executes test scripts on them.</p>
			<p>To perform cross-browser testing with the Selenium Standalone server, follow these steps:</p>
			<ol>
				<li>Download the Selenium Standalone server JAR file from the official Selenium website and run it on your machine or a dedicated server. This server acts as a central hub that manages browser sessions and receives test commands from your test scripts.</li>
				<li>Install the browsers you want to test on the machine where the Selenium Standalone server is running. Ensure that you have the necessary browser drivers installed for each browser (for example, ChromeDriver for Chrome, GeckoDriver for Edge) and that they have been added to your system’s PATH.</li>
				<li>Develop your test scripts using your preferred programming language and Selenium WebDriver bindings (for example, JavaScript, Python, C#, and so on). In your test scripts, set the desired capabilities to specify the browser and operating system configurations you want to test. The desired capabilities define which browser, browser version, and operating system Selenium Standalone server should use for the test. Use the Selenium WebDriver API to request a new browser session from the Selenium Standalone server, specifying the desired capabilities. The server will then launch the specified browser on the configured machine.</li>
			</ol>
			<p>Once the<a id="_idIndexMarker526"/> browser<a id="_idIndexMarker527"/> session has been established, your test scripts can interact with the web elements by using WebDriver commands. You can navigate pages, click buttons, fill out forms, and perform other actions to test the functionality and user interface of your web application. During the test’s execution, the server will collect test results, logs, and any errors that were encountered during cross-browser testing.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor258"/>Cross-environment testing with a shared configuration file</h2>
			<p>Cross-environment testing <a id="_idIndexMarker528"/>involves configuring WebdriverIO to run tests on different environments, such as test and staging. Occasionally, this might include development as well as production environments. This approach allows you to ensure compatibility and functionality across different environments, helping you catch potential issues early in the development process:</p>
			<div><div><img src="img/B19395_13_5.jpg" alt="Figure 13.5 – Three wdio conf files sharing a common config file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Three wdio conf files sharing a common config file</p>
			<p>But we don’t want to <a id="_idIndexMarker529"/>repeat all the settings in multiple files. Fortunately, WebdriverIO allows us to share values across all environments. We created a <code>shared.conf</code> file that holds all the settings that are shared across all environments. If any settings need to be changed, we can make the necessary changes in a single location.</p>
			<p>The way this is accomplished is by creating individual files for each operating system and environment, such as <code>windows.conf</code> and <code>mac.conf</code>. We will do this in a cloud environment with <code>lambdatest.conf</code> shortly.</p>
			<p>In the <code>wdio.shared.conf.ts</code> configuration file, define multiple environments (for example, development, test, and production) with the appropriate settings for each environment. Here’s an example:</p>
			<pre class="console">
  // wdio.shared.conf.ts
/**
*  The baseUrl will only be used if you don't specify a url in your script
*  loadPage('/')
*  if you specify on then its ignored
*  loadPage('https://candymapper.com/')
*/
let baseUrl: string
let env = process.env.Env
let urls = {
    uat: 'https://the-internet.herokuapp.com',
    dev: 'https://candymapperr2.com/',
    prod: 'https://candymapper.com/'
}
baseUrl = urls[env]
   exports.config = {
     // ... other configurations ...
     baseUrl: baseUrl,
     // ... other configurations ...
   };</pre>			<p>Regardless of the operating <a id="_idIndexMarker530"/>system, every browser will navigate to the same URLs without having the information copied multiple times.</p>
			<p>This <a id="_idIndexMarker531"/>can be quite complex for a project on local machines with potentially different resources and configurations. So, the next step is to leverage cloud resources to ensure all testing configurations are consistent, such as on LambdaTest. This is how the <code>shared.conf</code> file is used in <code>windows.conf</code>, <code>mac.conf</code>, and a cloud-based service such as <code>lambdatest.conf</code>.</p>
			<p>The following is an example of a <code>windows.conf.ts</code> or <code>mac.conf.ts</code> file using the <code>shared.conf.ts</code> file:</p>
			<pre class="source-code">
import { config as sharedConfig } from './wdio.shared.conf'
export const config: WebdriverIO.Config = {
    ...sharedConfig,
    ...{
        capabilities: [
            {
                browserName: 'chrome',
                'goog:chromeOptions': {
                    args: ['--disable-gpu']
 },
      acceptInsecureCerts: true,
      },
      {
       browserName: 'safari'
      }
      ]
    }
}</pre>			<p>However, <code>LambdaTest.conf.ts</code> or other cloud-based services (SauceLabs, BrowserStack, and so on) will require different sets of configurations.</p>
			<p>The<a id="_idIndexMarker532"/> following is an example of a cloud-based service using the <code>shared.conf</code> file:</p>
			<pre class="source-code">
import { config as sharedConfig } from './wdio.shared.conf';
export const config = {
    ...sharedConfig,
    ...{
        services: [
            ["lambdatest",
                {
                    tunnel: false,
                    lambdatestOpts: {
                        logFile: "tunnel.log"
                    }
                }
            ]
        ],
        user: process.env.LT_USERNAME,
        key: process.env.LT_ACCESS_KEY,
        capabilities: [
            {
                "LT:Options": {
                    browserName: "Edge",
                    version: "latest",
                    name: "Test WebdriverIO Single",
                    build: "WebDriver Selenium Sample"
                }
            },
        ],
        logLevel: "info",
        coloredLogs: true,
        screenshotPath: "./errorShots/",
        waitforTimeout: 100000,
        connectionRetryTimeout: 90000,
        connectionRetryCount: 1,
        path: "/wd/hub",
        hostname: process.env.LT_HOST_URL,
        port: 80
    }
}</pre>			<p>In <a id="_idIndexMarker533"/>this example, we use the <code>baseUrl</code> variable to select the appropriate environment based on the<code>"Env=uat"</code> environment variable that’s set when running the tests.</p>
			<p>Use <code>baseUrl</code> from<a id="_idIndexMarker534"/> the configuration to navigate to different URLs for each environment:</p>
			<pre class="console">
   // tests/ch13.spec.ts
   describe('Cross-Environment Test', () =&gt; {
     it('should open the website', () =&gt; {
       browser.url('/');
       const title = browser.getTitle();
       expect(title).toContain('Example Domain');
     });
   });</pre>			<p>From the command line, we can change the environments the tests run against. In this example, we are running against <code>uat</code>, which is <code>the-internet</code>, and <code>dev</code>, which is <code>candymapperr2.com</code> on Windows on Chrome and Edge browsers. Lastly, the <code>prod</code> example runs against <code>candymapper.com</code> on Mac on Chrome and Safari:</p>
			<pre class="console">
   Env=uat wdio wdio.conf.ts --spec ./test/specs/ch13.ts
   Env=dev wdio wdio.dev.conf.ts
   Env=prod wdio wdio.prod.conf.ts
   Env=uat wdio wdio.lambdatest.conf.ts --spec ./test/specs/ch13.ts</pre>			<p>From this, we can see how we might start getting to a point where we’re trying to support large combinations of operating systems, browsers, and even older versions. This level of architecture support alone will not be sustainable, so the next logical step is to move testing to the cloud. This brings us some unique advantages. The console output of the tests is still available when it’s run in a cloud environment:</p>
			<div><div><img src="img/B19395_13_6.jpg" alt="Figure 13.6 – Results from the terminal window in LambdaTest"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – Results from the terminal window in LambdaTest</p>
			<p>In the cloud, the <a id="_idIndexMarker535"/>test cases can be assigned to run in multiple browsers, versions, and operating systems, but without the need to configure and support the underlying architecture:</p>
			<div><div><img src="img/B19395_13_7.jpg" alt=" Figure 13.7 – Results of the test cases in multiple operating systems and browsers in the cloud"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 13.7 – Results of the test cases in multiple operating systems and browsers in the cloud</p>
			<p>The following <a id="_idIndexMarker536"/>example shows the multiple browsers and operating systems that we can run against. Now, if we were to click on a single item, we could dive deeper into the details of a particular system and run results:</p>
			<div><div><img src="img/B19395_13_8.jpg" alt="Figure 13.8 – Test run on Safari V.15 on Mac Monteray in the cloud"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Test run on Safari V.15 on Mac Monteray in the cloud</p>
			<p>And while screen <a id="_idIndexMarker537"/>captures are nice, it’s even better to watch an entire video that’s been recorded. This provides a clear look into the interactions of a test run:</p>
			<div><div><img src="img/B19395_13_9.jpg" alt="Figure 13.9 – A video still of the test case running in LambdaTest"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.9 – A video still of the test case running in LambdaTest</p>
			<p>Again, video storage <a id="_idIndexMarker538"/>space and cleanup are less time-consuming. The costs can be compared to having one or two team members dedicated to developing, enhancing, and maintaining such large files generated on-site becoming prohibitive when they could be spending more time writing more test cases, analyzing results, and writing defects.</p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor259"/>Avoiding the rabbit hole of horizontal scaling</h1>
			<p>It is <a id="_idIndexMarker539"/>important to the 80/20 rule and the rule of threes in mind. We do not want to try to support 80% of the popular browser and operating system combinations when our customers are using only 20%. It may sound pro-active to try to support Safari on Mac when our customers only use Chrome on Windows. Attempting to do a regression test in a new browser on every environment becomes logarithmically impossible. You may not have the time to execute all test cases on all browsers and all environments. We only want to test on the browsers that are used by more of our users, so that might be a maximum combination of three: one browser in two operating systems or two browsers in one operating system. In addition, time can be taken away from creating new tests if we are trying to determine the root cause of why one test runs in one browser or operating system and fails in another.</p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor260"/>Handling environment-specific logic</h1>
			<p>If your application has <a id="_idIndexMarker540"/>environment-specific code or issues, use conditional checks or feature detection to handle them gracefully:</p>
			<pre class="source-code">
If (process.platform === 'mac'){
  // do something specific thats mac only
} else {
 // contine as usual
}</pre>			<p class="callout-heading">Rule of thumb</p>
			<p class="callout">Try not to get bogged down in getting All-Pass on every browser and operating system. Expand to one additional browser, then one additional operating system. It is best to only perform smoke testing on peripheral configurations. It can easily consume your time supporting logarithmically.</p>
			<p>What if we have a new field that has been added to our testing environment but does not exist in production? Can we build a test that will support both? At this point, we can introduce a new set of <code>IfExist()</code> custom commands. Each base method, including <code>click()</code>, <code>setValue()</code>, and <code>select()</code>, will have a corresponding function: <code>clickIfExist()</code>, <code>setValueIfExist()</code>, and <code>selectIfExist()</code>, respectively. We can also add a <code>verifyIfExist()</code> method. The goal is that rather than have separate versions of every test for each environment, we have one set of tests that is highly likely to reach the endpoint of the journey, even if there are minor differences along the way.</p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor261"/>The multiverse – one test, two environments</h1>
			<p>The <a id="_idIndexMarker541"/>advantage is that these <code>IfExist()</code> methods will not stop the test if the object doesn’t exist. Our tests can now be executed in a test environment where new functionality exists, as well as a production environment where the functionality is yet to be pushed. For example, a page may ask for a month to be selected from a list on a long survey navigation path. In the staging environment, this requires the <strong class="bold">Next</strong> button to be explicitly clicked to move to the page. However, in QA, the <strong class="bold">Next</strong> button is removed and the page implicitly moves on once the user selects an item from the list:</p>
			<pre class="console">
Helpers.clickIfExists(await this.btnNext);</pre>			<p>There are two approaches to this implementation. First, we could enhance the <code>clickadv()</code> method with an optional property:</p>
			<pre class="console">
export async function clickAdv(element: WebdriverIO.Element, ifExists: boolean = false) {
// isExist code branch here ...
}</pre>			<p>However, this leads to code that is less clear about the intention, with the potential of a magic Boolean argument being used:</p>
			<pre class="console">
await Helpers.clickAdv(this.btnNext, true); // may not exist</pre>			<p>Instead, let’s <a id="_idIndexMarker542"/>create an alternative function with <code>ifExists</code> appended. This function uses the automation switchboard to tell the initial wrapper to act differently if the element does not exist:</p>
			<pre class="console">
const IF_EXISTS = "IF_EXISTS";
export async function clickAdvIfExists(element: WebdriverIO.Element) {
ABS(IF_EXISTS) = true;
let result = await this.clickAdv(element);
ASB(IF_EXISTS) = false;
return result;
}</pre>			<p>Second, we store the state of the element when we check that it is valid. We will also save the locator of the element if it has not already been saved in the <code>beforeCommand</code> hook:</p>
			<pre class="console">
export async function getValidElement(
  element: WebdriverIO.Element,
  elementType: string
): Promis<a id="_idTextAnchor262"/>e&lt;WebdriverIO.Element&gt; {
...
  if (!found) {
    ABS.set ("ELEMENT_SELECTO<a id="_idTextAnchor263"/>R") <a id="_idTextAnchor264"/>= element.selector)
    await log(`  ERROR: Unable to find ${selector}`);
  }
  ASB.set ("ELEMENT_EXISTS") = found;
  return newElement;
}</pre>			<p>Lastly, we return immediately from the <code>clickAdv()</code> method:<a id="_idTextAnchor265"/></p>
			<pre class="console">
if (ASB.get("<a id="_idTextAnchor266"/>ELEMENT_EXISTS") == false){
await log(`  IfExist: Skipping cli<a id="_idTextAnchor267"/>cking
${ASB.get("ELEMENT_SELETOR")}`);
return true;
}</pre>			<p>Now, we can add the<a id="_idIndexMarker543"/> feature just by adding <code>IfExists</code>:</p>
			<pre class="console">
await Helpers.clickAdvIfExists(this.btnNext); // may not exist</pre>			<p>We can do the same to enhance the <code>setValueAdv()</code> method:</p>
			<pre class="source-code">
export async function setValueAdvIfExists(
element: WebdriverIO.Element),
text: string
)
ABS(IF_EXISTS) = true;
let result = await this.setValueAdv(element, text);
c;
return result;
}
export async function setValueAdv(
  inputField: WebdriverIO.Element,
  text: string
) {
If (ABS(IF_EXISTS) == true)
return true;
}</pre>			<p>We must do the<a id="_idIndexMarker544"/> same to create <code>selectValueAdvIfExists</code>:</p>
			<pre class="console">
export async function selectAdvIfExists(
element: WebdriverIO.Element),
text: string
)
ABS(IF_EXISTS) = true;
let result = await this.clickAdv(element);
ASB(IF_EXISTS) = false; // Reset for next element
return result;
}</pre>			<p>Now, we can have tests that are robust enough to run in slightly different test environments and still get to the conclusion of an end-to-end test.</p>
			<p>For example, in the following figure, we<a id="_idIndexMarker545"/> have two websites:</p>
			<div><div><img src="img/B19395_13_10.jpg" alt="Figure 13.10 – Production versus pre-production environments where a button element has been removed"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.10 – Production versus pre-production environments where a button element has been removed</p>
			<p>The production site on the left has a <strong class="bold">GET IN TOUCH</strong> button that scrolls down the page to a customer detail input support page.</p>
			<p>On the right is the new release of the site. Note that this site doesn’t include the <strong class="bold">GET IN </strong><strong class="bold">TOUCH</strong> button.</p>
			<p>With the option to click the button only if it exists without failing the test, we can begin to have tests that are more flexible in slightly differing environments. If the button only exists in one <a id="_idIndexMarker546"/>environment, the test can continue to execute without failing in both. This changes our focus from maintaining test cases to having an increased chance of reaching the end path. Finally, even if the method fails because the locator is different, the next few steps will execute on the wrong page and still bring the test to a halt for maintenance.</p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor268"/>Summary</h1>
			<p>In our epic journey, we’ve unlocked a new superpower for our scripts by integrating the automation switchboard. This newfound capability ensures that our scripts remain as adaptable as the ever-evolving world of superheroes. They can now seamlessly operate across various browsers and operating systems, making them as versatile as a superhero’s toolkit.</p>
			<p>As we turn the page to the next thrilling chapter, get ready to witness our web hero, WebdriverIO, taking flight into the clouds of cloud-based test automation and scheduling. Just like a superhero soaring through the skies, we’ll delve into the extraordinary realm of executing tests in the cloud. This chapter promises to be a riveting adventure, showcasing the incredible potential of our superheroic scripts as they conquer new heights and challenges in the world of testing.</p>
		</div>
	</body></html>