- en: Unit Testing and Functional Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: nit testing has become a primary part of good software development practice.
    It is a method by which individual units of source code are tested to ensure proper
    functioning. Each unit is theoretically the smallest testable part of an application.
    In a Node.js application, you might consider each module as a unit.
  prefs: []
  type: TYPE_NORMAL
- en: In unit testing, each unit is tested separately, isolating the unit under test
    as much as possible from other parts of the application. If a test fails, you
    would want it to be due to a bug in your code rather than a bug in the package
    that your code happens to use. A common technique is to use mock objects or mock
    data to isolate individual parts of the application from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Functional testing, on the other hand, doesn't try to test individual components,
    but instead it tests the whole system. Generally speaking, unit testing is performed
    by the development team, and functional testing is performed by a **Quality Assurance**
    (**QA**) or **Quality Engineering** (**QE**) team. Both testing models are needed
    to fully certify an application. An analogy might be that unit testing is similar
    to ensuring that each word in a sentence is correctly spelled, while functional
    testing ensures that the paragraph containing that sentence has a good structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Assertions as the basis of software tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mocha unit testing framework and the Chai assertions library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using tests to find bugs and fixing the bug
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Docker to manage test infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing a REST backend service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UI testing in a real web browser using Puppeteer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving UI testability with element ID attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assert – the basis of testing methodologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Node.js has a useful built-in testing tool, the `assert` module. Its functionality
    is similar to assert libraries in other languages. Namely, it's a collection of
    functions for testing conditions, and if the conditions indicate an error, the `assert`
    function throws an exception.
  prefs: []
  type: TYPE_NORMAL
- en: At its simplest, a test suite is a series of `assert` calls to validate the
    behavior of a thing being tested. For example, a test suite could instantiate the
    user authentication service, then make an API call, using `assert` methods to
    validate the result, then make another API call, validating its results, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a code snippet like this, which you could save in a file named `deleteFile.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to notice is this contains several layers of asynchronous callback
    functions. That presents a couple of challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Capturing errors from deep inside a callback, to ensure the test scenario fails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting conditions where the callbacks are never called
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example of using `assert` for testing. Create a file named `test-deleteFile.js` containing
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is what's called a negative test scenario, in that it's testing whether
    requesting to delete a nonexistent file throws an error.
  prefs: []
  type: TYPE_NORMAL
- en: If you are looking for a quick way to test, the `assert` module can be useful
    when used this way. If it runs and no messages are printed, then the test passes.
    But, did it catch the instance of the `deleteFile` callback never being called?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `assert` module is used by many of the test frameworks as a core tool for
    writing test cases. What the test frameworks do is create a familiar test suite
    and test case structure to encapsulate your test code.
  prefs: []
  type: TYPE_NORMAL
- en: There are many styles of assertion libraries available in Node.js. Later in
    this chapter, we'll use the Chai assertion library ([http://chaijs.com/](http://chaijs.com/))
    which gives you a choice between three different assertion styles (should, expect,
    and assert).
  prefs: []
  type: TYPE_NORMAL
- en: Testing a Notes model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start our unit testing journey with the data models we wrote for the Notes
    application. Because this is unit testing, the models should be tested separately from
    the rest of the Notes application.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of most of the Notes models, isolating their dependencies implies
    creating a mock database. Are you going to test the data model or the underlying
    database? Mocking out a database means creating a fake database implementation, which
    does not look like a productive use of our time. You can argue that testing a
    data model is really about testing the interaction between your code and the database,
    that mocking out the database means not testing that interaction, and therefore
    we should test our code against the database engine used in production.
  prefs: []
  type: TYPE_NORMAL
- en: With that line of reasoning in mind, we'll skip mocking out the database, and
    instead run the tests against a database containing test data. To simplify launching
    the test database, we'll use Docker to start and stop a version of the Notes application
    stack that's set up for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Mocha and Chai­ – the chosen test tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you haven't already done so, duplicate the source tree to use in this chapter.
    For example, if you had a directory named `chap10`, create one named `chap11`
    containing everything from `chap10`.
  prefs: []
  type: TYPE_NORMAL
- en: In the `notes` directory, create a new directory named `test`.
  prefs: []
  type: TYPE_NORMAL
- en: Mocha ([http://mochajs.org/](http://mochajs.org/)) is one of many test frameworks
    available for Node.js. As you'll see shortly, it helps us write test cases and
    test suites, and it provides a test results reporting mechanism. It was chosen
    over the alternatives because it supports Promises. It fits very well with the
    Chai assertion library mentioned earlier. And, we'll need to use ES6 modules from
    test suites written in CommonJS, and therefore we must use the `esm` module.
  prefs: []
  type: TYPE_NORMAL
- en: You may find references to an earlier `@std/esm` module. That module has been
    deprecated, with `esm` put in its place.
  prefs: []
  type: TYPE_NORMAL
- en: 'While in the `notes/test` directory, type this to install Mocha, Chai, and
    `esm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notes model test suite
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because we have several Notes models, the test suite should run against any
    model. We can write tests using the Notes model API we developed, and an environment
    variable should be used to declare the model to test.
  prefs: []
  type: TYPE_NORMAL
- en: Because we've written the Notes application using ES6 modules, we have a small
    challenge to overcome. Mocha only supports running tests in CommonJS modules,
    and Node.js (as of this writing) does not support loading an ES6 module from a
    CommonJS module. An ES6 module can use `import` to load a CommonJS module, but
    a CommonJS module cannot use `require` to load an ES6 module. There are various
    technical reasons behind this, the bottom line is that we're limited in this way.
  prefs: []
  type: TYPE_NORMAL
- en: Because Mocha requires that tests be CommonJS modules, we're in the position
    of having to load an ES6 module into a CommonJS module.  A module, `esm`, exists
    which allows that combination to work. If you'll refer back, we installed that
    module in the previous section. Let's see how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `test` directory, create a file named `test-model.js` containing this
    as the outer shell of the test suite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The support to load ES6 modules is enabled by the `require('esm')` statement
    shown here. It replaces the standard `require` function with one from the `esm`
    module. That parameter list at the end enables the feature to load ES6 modules
    in a CommonJS module. Once you've done this, your CommonJS module can load an
    ES6 module as evidenced by `require('../models/notes')` a couple of lines later.
  prefs: []
  type: TYPE_NORMAL
- en: The Chai library supports three flavors of assertions. We're using the `assert`
    style here, but it's easy to use a different style if you prefer. For the other
    styles supported by Chai, see [http://chaijs.com/guide/styles/](http://chaijs.com/guide/styles/).
  prefs: []
  type: TYPE_NORMAL
- en: Chai's assertions include a very long list of useful assertion functions, see [http://chaijs.com/api/assert/](http://chaijs.com/api/assert/).
  prefs: []
  type: TYPE_NORMAL
- en: The Notes model to test must be selected with the `NOTES_MODEL` environment
    variable. For the models that also consult environment variables, we'll need to
    supply that configuration as well.
  prefs: []
  type: TYPE_NORMAL
- en: With Mocha, a test suite is contained within a `describe` block. The first argument
    is descriptive text, which you use to tailor the presentation of test results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than maintaining a separate test database, we can create one on the
    fly while executing tests. Mocha has what are called hooks, which are functions
    executed before or after test case execution. The hook functions let you, the
    test suite author, set up and tear down required conditions for the test suite
    to operate as desired. For example, to create a test database with known test
    content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This defines a `beforeEach` hook, which is executed before every test case.
    The other hooks are `before`, `after`, `beforeEach`, and `afterEach`. The each
    hooks are triggered before or after each test case execution.
  prefs: []
  type: TYPE_NORMAL
- en: This is meant to be a cleanup/preparation step before every test. It uses our
    Notes API to first delete all notes from the database (if any) and then create
    a set of new notes with known characteristics. This technique simplifies tests
    by ensuring that we have known conditions to test against.
  prefs: []
  type: TYPE_NORMAL
- en: We also have a side effect of testing the `model.keylist` and `model.create`
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Mocha, a series of test cases are encapsulated with a `describe` block,
    and written using an `it` block. The `describe` block is meant to describe that
    group of tests, and the `it` block is for checking assertions on a specific aspect
    of the thing being tested. You can nest the `describe` blocks as deeply as you
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The idea is to call Notes API functions, then to test the results to check whether
    they matched the expected results.
  prefs: []
  type: TYPE_NORMAL
- en: This `describe` block is within the outer `describe` block. The descriptions
    given in the describe and `it` blocks are used to make the test report more readable.
    The `it` block forms a pseudo-sentence along the lines of *it (the thing being
    tested) should do this or that*.
  prefs: []
  type: TYPE_NORMAL
- en: It is important with Mocha to not use arrow functions in the `describe` and `it` blocks.
    By now, you will have grown fond of arrow functions because of how much easier
    they are to write. But, Mocha calls these functions with a `this` object containing
    useful functions for Mocha. Because arrow functions avoid setting up a `this` object,
    Mocha would break.
  prefs: []
  type: TYPE_NORMAL
- en: Even though Mocha requires regular functions for the `describe` and `it` blocks,
    we can use arrow functions within those functions.
  prefs: []
  type: TYPE_NORMAL
- en: How does Mocha know whether the test code passes? How does it know when the
    test finishes? This segment of code shows one of the three methods.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, Mocha is looking to see if the function throws an exception, or whether
    the test case takes too long to execute (a timeout situation). In either case,
    Mocha will indicate a test failure. That's of course simple to determine for non-asynchronous
    code. But, Node.js is all about asynchronous code, and Mocha has two models for
    testing asynchronous code.
  prefs: []
  type: TYPE_NORMAL
- en: In the first (not seen here), Mocha passes in a callback function, and the test
    code is to call the callback function. In the second, as seen here, it looks for
    a Promise being returned by the test function, and determines pass/fail on whether
    the Promise is in the *resolve* or *reject* state.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we're using `async` functions, because they automatically return
    a Promise. Within the functions, we're calling asynchronous functions using `await,` ensuring
    any thrown exception is indicated as a rejected Promise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another item to note is the question asked earlier: what if the callback function
    we''re testing is never called?  Or, what if a Promise is never resolved?  Mocha
    starts a timer and if the test case does not finish before the timer expires,
    Mocha fails the test case.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and running tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have more tests to write, but let''s first get set up to run the tests.
    The simplest model to test is the in-memory model. Let''s add this to the `scripts`
    section of `notes/test/package.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To install dependencies, we must run `npm install` in both the `notes/test`
    and `notes` directories. That way both the dependencies for the test code, and
    the dependencies for Notes, are installed in their correct place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can run it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `mocha` command is used to run the test suite.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of the output follows the structure of the `describe` and `it`
    blocks. You should set up the descriptive text strings so it reads nicely.
  prefs: []
  type: TYPE_NORMAL
- en: More tests for the Notes model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'That wasn''t enough to test much, so let''s go ahead and add some more tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Notice that for the negative tests – where the test passes if an error is thrown
    – we run it in a `try/catch` block. The `throw new Error` line in each case should
    not execute because the preceding code should throw an error. Therefore, we can
    check if the message in that thrown error is the message which arrives, and fail
    the test if that's the case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the test report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In these additional tests, we have a couple of negative tests. In each test
    that we expect to fail, we supply a `notekey` that we know is not in the database,
    and we then ensure that the model gives us an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Chai Assertions API includes some very expressive assertions. In this case,
    we''ve used the `deepEqual` method which does a deep comparison of two objects.
    In our case, it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This reads nicely in the test code, but more importantly a reported test failure
    looks very nice. Since these are currently passing, try introducing an error by
    changing one of the expected value strings. Upon rerunning the test, you''ll see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At the top is the status report of each test case. For one test, instead of
    a check mark is a number, and the number corresponds to the reported details at
    the bottom. Mocha presents test failures this way when the `spec` reporter is
    used. Mocha supports other test report formats, some of which produce data that
    can be sent into test status reporting systems. For more information, see [https://mochajs.org/#reporters](https://mochajs.org/#reporters).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the failure was detected by a `deepEqual` method, which presents
    the detected object inequality in this way.
  prefs: []
  type: TYPE_NORMAL
- en: Testing database models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That was good, but we obviously won't run Notes in production with the in-memory
    Notes model. This means that we need to test all the other models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing the LevelUP and filesystem models is easy, just add this to the scripts
    section of `package.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will produce a successful test result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest database to test is SQLite3, since it requires zero setup. We
    have two SQLite3 models to test, let''s start with `notes-sqlite3.js`. Add the
    following to the scripts section of `package.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This command sequence puts the test database in the `chap11.sqlite3` file. It
    first initializes that database using the `sqlite3` command-line tool. Note that
    we've connected its input to `/dev/null` because the `sqlite3` command will prompt
    for input otherwise. Then, it runs the test suite passing in environment variables
    required to run against the SQLite3 model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the test suite does find two errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The failing test calls `model.read("badkey12")`, a `key` which we know does
    not exist. Writing negative tests paid off. The failing line of code at `models/notes-sqlite3.mjs`
    (line 64) reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It's easy enough to insert `console.log(util.inspect(row));` just before this
    and learn that, for the failing call, SQLite3 gave us `undefined` for `row`, explaining
    the error message.
  prefs: []
  type: TYPE_NORMAL
- en: The test suite calls the `read` function multiple times with a `notekey` value
    that does exist. Obviously, when given an invalid `notekey` value, the query gives
    an empty results set and SQLite3 invokes the callback with both the `undefined`
    error and the `undefined` row values. This is common behavior for database modules.
    An empty result set isn't an error, and therefore we received no error and an
    undefined `row`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, we saw this behavior earlier with `models/notes-sequelize.mjs`. The
    equivalent code in `models/notes-sequelize.mjs` does the right thing, and it has
    a check, which we can adapt. Let''s rewrite the `read` function in `models/notes-sqlite.mjs`
    to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is simple, we just check whether `row` is `undefined` and, if so, throw
    an error. While the database doesn't see an empty results set as an error, Notes
    does. Furthermore, Notes already knows how to deal with a thrown error in this
    case. Make this change and that particular test case passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a second similar error in the `destroy` logic. The test to destroy
    a nonexistent note fails to produce an error at this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If we inspect the other models, they''re throwing errors for a nonexistent
    key. In SQL, it obviously is not an error if this SQL (from `models/notes-sqlite3.mjs`)
    does not delete anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, there isn''t a SQL option to make this SQL statement fail if
    it does not delete any records. Therefore, we must add a check to see if a record
    exists. Namely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, we read the note and as a byproduct we verify the note exists. If
    the note doesn't exist, `read` will throw an error, and the `DELETE` operation
    will not even run.
  prefs: []
  type: TYPE_NORMAL
- en: These are the bugs we referred to in Chapter 7, *Data Storage and Retrieval*.
    We simply forgot to check for these conditions in this particular model. Thankfully,
    our diligent testing caught the problem. At least, that's the story to tell the
    managers rather than telling them that we forgot to check for something we already
    knew could happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve fixed `models/notes-sqlite3.mjs`, let''s also test `models/notes-sequelize.mjs`
    using the SQLite3 database. To do this, we need a connection object to specify
    in the `SEQUELIZE_CONNECT` variable. While we can reuse the existing one, let''s
    create a new one. Create a file named `test/sequelize-sqlite.yaml` containing
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This way, we don't overwrite the production database instance with our test
    suite. Since the test suite destroys the database it tests, it must be run against
    a database we are comfortable destroying. The logging parameter turns off the
    voluminous output `Sequelize` produces so that we can read the test results report.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following to the scripts section of `package.json`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then run the test suite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We pass with flying colors!  We've been able to leverage the same test suite
    against multiple Notes models. We even found two bugs in one model. But, we have
    two test configurations remaining to test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test results matrix reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`models-fs`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-memory`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-levelup`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-sqlite3`: 2 failures, now fixed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-sequelize`: with SQLite3: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-sequelize`: with MySQL: untested'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-mongodb`: untested'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two untested models both require the setup of a database server. We avoided
    testing these combinations, but our manager won't accept that excuse because the
    CEO needs to know we've completed the test cycles. Notes must be tested in a similar
    configuration to the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: In production, we'll be using a regular database server, of course, with MySQL
    or MongoDB being the primary choices. Therefore, we need a way that incurs a low
    overhead to run tests against those databases. Testing against the production
    configuration must be so easy that we should feel no resistance in doing so, to
    ensure that tests are run often enough to make the desired impact.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we've already had experience of a technology that supports easily
    creating and destroying the deployment infrastructure. Hello, Docker!
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker to manage test infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One advantage Docker gives is the ability to install the production environment
    on our laptop. It's then very easy to push the same Docker setup to the cloud-hosting
    environment for staging or production deployment.
  prefs: []
  type: TYPE_NORMAL
- en: What we'll do in this section is demonstrate reusing the Docker Compose configuration
    defined previously for test infrastructure, and to automate executing the Notes
    test suite inside the containers using a shell script. Generally speaking, it's
    important to replicate the production environment when running tests. Docker can
    make this an easy thing to do.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker, we'll be able to easily test against a database, and have a simple
    method for starting and stopping a test version of our production environment. Let's
    get started.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose to orchestrate test infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We had a great experience using Docker Compose to orchestrate Notes application
    deployment. The whole system, with four independent services, is easily described
    in `compose/docker-compose.yml`. What we'll do is duplicate the Compose file,
    then make a couple of small changes required to support test execution.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by making a new directory, `test-compose`, as a sibling to the `notes`,
    `users`, and `compose` directories. Copy `compose/docker-compose.yml` to the newly
    created `test-compose` directory. We'll be making several changes to this file
    and a couple of small changes to the existing Dockerfiles.
  prefs: []
  type: TYPE_NORMAL
- en: We want to change the container and network names so our test infrastructure
    doesn't clobber the production infrastructure. We'll constantly delete and recreate
    the test containers, so as to keep the developers happy, we'll leave development
    infrastructure alone and perform testing on separate infrastructure. By maintaining
    separate test containers and networks, our test scripts can do anything they like
    without disturbing the development or production containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this change to the `db-auth` and `db-notes` containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This is the same as earlier, but with `-test` appended to container and network
    names.
  prefs: []
  type: TYPE_NORMAL
- en: That's the first change we must make, append `-test` to every container and
    network name in `test-compose/docker-compose.yml`. Everything we'll do with tests
    will run on completely separate containers, hostnames, and networks from those
    of the development instance.
  prefs: []
  type: TYPE_NORMAL
- en: This change will affect the `notes-test` and `userauth-test` services because
    the database server hostnames are now `db-auth-test` and `db-notest-test`. There
    are several environment variables or configuration files to update.
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration is the environment variables required to configure the
    services.  Previously, we defined all environment variables in the Dockerfiles. 
    It's extremely useful to reuse those Dockerfiles so we know we're testing the
    same deployment as is used in production.  But we need to tweak the configuration
    settings to match the test infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The database configuration shown here is an example. The same Dockerfiles are
    used, but we also define environment variables in `test-compose/docker-compose.yml`.
    As you might expect, this overrides the Dockerfile environment variables with
    the values set here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Again, we changed the container and network names to append `-test`. We moved
    some of the environment variables from Dockerfile to `test-compose/docker-compose.yml`.
    Finally, we added some data volumes to mount host directories inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to do is to set up directories to store test code. A common practice
    in Node.js projects is to put test code in the same directory as the application
    code. Earlier in this chapter, we did so, implementing a small test suite in the `notes/test`
    directory. As it stands, `notes/Dockerfile` does not copy that directory into
    the container. The test code must exist in the container to execute the tests.
    Another issue is it's helpful to not deploy test code in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we can do is to ensure that `test-compose/docker-compose.yml` mounts `notes/test`
    into the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This gives us the best of both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: The test code is in `notes/test` where it belongs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test code is not copied into the production container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In test mode, the `test` directory appears where it belongs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a couple of configuration files remaining for the `Sequelize` database
    connection to set up.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `userauth-test` container, the `SEQUELIZE_CONNECT` variable now refers
    to a configuration file that does not exist, thanks to overriding the variable
    in `user/Dockerfile`. Let''s create that file as `test-compose/userauth/sequelize-docker-mysql.yaml`,
    containing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The values match the variables passed to the `db-userauth-test` container.
    Then we must ensure this configuration file is mounted into the `userauth-test`
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For `notes-test` we have a configuration file, `test/sequelize-mysql.yaml`,
    to put in the `notes/test` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Again, this matches the configuration variables in `db-notes-test`. In `test-compose/docker-compose.yml`,
    we mount that file into the container.
  prefs: []
  type: TYPE_NORMAL
- en: Executing tests under Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we're ready to execute some of the tests inside a container. We've used
    a Docker Compose file to describe the test environment for the Notes application,
    using the same architecture as in the production environment. The test scripts
    and configuration has been injected into the containers. The question is, how
    do we automate test execution?
  prefs: []
  type: TYPE_NORMAL
- en: The technique we'll use is to run a shell script, and use `docker exec -it` to
    execute commands to run the test scripts. This is somewhat automated, and with
    some more work it can be fully automated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `test-compose`, let''s make a shell script called `run.sh` (on Windows,
    `run.ps1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: It's common practice to run tests out of a continuous integration system such
    as Jenkins. Continuous integration systems automatically run builds or tests against
    software products. The build and test results data is used to automatically generate
    status pages.  Visit [https://jenkins.io/index.html](https://jenkins.io/index.html),
    which is a good starting point for a Jenkins job.
  prefs: []
  type: TYPE_NORMAL
- en: That makes the first real step to building the containers, followed by bringing
    them up. The script sleeps for a few seconds to give the containers time to fully
    instantiate themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The subsequent commands all follow a particular pattern that is important to
    understand. The commands are executed in the `/notesapp/test` directory thanks
    to the `--workdir` option. Remember that directory is injected into the container
    by the Docker Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Using `-e DEBUG=` we've disabled the `DEBUG` options. If those options are set,
    we'd have excess unwanted output in the test results, so using this option ensures
    that debugging output doesn't occur.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the options, you can see that the subsequent commands
    are all executed in the `test` directory using the `package.json` in that directory.
    It starts by running `npm install`, and then running each of the scenarios in
    the test matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the tests, simply type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: That's good, we've got most of our test matrix automated and pretty well squared
    away. There is a glaring hole in the test matrix and plugging that hole will let
    us see how to set up MongoDB under Docker.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB setup under Docker and testing Notes against MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Chapter 7,* Data Storage and Retrieval*, we developed MongoDB support for
    Notes, and since then we've focused on `Sequelize`. To make up for that slight,
    let's make sure we at least test our MongoDB support. Testing on MongoDB would
    simply require defining a container for the MongoDB database and a little bit
    of configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://hub.docker.com/_/mongo/](https://hub.docker.com/_/mongo/) for
    the official MongoDB container. You'll be able to retrofit this to allow deploying
    the Notes application running on MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add this to `test-compose/docker-compose.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: That's all that's required to add a MongoDB container to a Docker Compose file.
    We've connected it to `frontnet` so that the `notes` (`notes-test`) container
    can access the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then in `notes/test/package.json` we add a line to facilitate running tests
    on MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Simply by adding the MongoDB container to `frontnet-test`, the database is available
    at the URL shown here. Hence, it's simple to now run the test suite using the
    Notes MongoDB model.
  prefs: []
  type: TYPE_NORMAL
- en: The `--no-timeouts` option was necessary to avoid a spurious error while testing
    the suite against MongoDB. This option instructs Mocha to not check whether a
    test case execution takes too long.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final requirement is to add this line in `run.sh` (or `run.ps1` for Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: That, then, ensures MongoDB is tested during every test run.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now report to the manager the final test results matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '`models-fs`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-memory`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-levelup`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-sqlite3`: Two failures, now fixed, PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-sequelize` with SQLite3: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-sequelize` with MySQL: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`models-mongodb`: PASS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The manager will tell you "good job" and then remember that the Models are
    only a portion of the Notes application. We''ve left two areas completely untested:'
  prefs: []
  type: TYPE_NORMAL
- en: The REST API for the user authentication service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional testing of the user interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get on with those testing areas.
  prefs: []
  type: TYPE_NORMAL
- en: Testing REST backend services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's now time to turn our attention to the user authentication service. We've
    mentioned tests of this service, saying that we'll get to them later. We had developed
    some scripts for ad hoc testing, which have been useful all along. But later is
    now, and it's time to get cracking on some real tests.
  prefs: []
  type: TYPE_NORMAL
- en: There's a question of which tool to use for testing the authentication service.
    Mocha does a good job of organizing a series of test cases, and we should reuse
    it here. But the thing we have to test is a REST service. The customer of this
    service, the Notes application, uses it through the REST API, giving us a perfect
    rationalization to test at the REST interface. Our ad hoc scripts used the SuperAgent
    library to simplify making REST API calls. There happens to be a companion library,
    SuperTest, that is meant for REST API testing. Read its documentation here: [https://www.npmjs.com/package/supertest](https://www.npmjs.com/package/supertest).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve already made the `test-compose/userauth` directory. In that directory,
    create a file named `test.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This sets up Mocha and the SuperTest client. The `URL_USERS_TEST` environment
    variable specifies the base URL of the server to run the test against. You'll
    almost certainly be using `http://localhost:3333` given the configuration we've
    used earlier in the book. SuperTest initializes itself a little differently to
    SuperAgent. The SuperTest module exposes a function that we call with the  `URL_USERS_TEST` environment
    variable, then we use THAT `request` object throughout the rest of the script
    to make REST API requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'This variable was already set in `test-compose/docker-compose.yml` with the
    required value. The other thing of importance is a pair of variables to store
    the authentication user ID and key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If you remember, the `beforeEach` function is run immediately before every
    test case, and `afterEach` is run afterward. These functions use the REST API
    to create our test user before running the test, and then afterward to destroy
    the test user. That way our tests can assume this user will exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can turn to testing some API methods, such as the `/list` operation.
  prefs: []
  type: TYPE_NORMAL
- en: We have already guaranteed that there is an account, in the `beforeEach` method,
    so `/list` should give us an array with one entry.
  prefs: []
  type: TYPE_NORMAL
- en: 'This follows the general pattern for using Mocha to test a REST API method.
    First, we use SuperTest''s `request` object to call the API method, and `await`
    its result. Once we have the result, we use `assert` methods to validate it is
    what is expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We are checking the `/find` operation in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking for the account we know exists – failure is indicated if the user account
    is not found
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking for the one we know does not exist – failure is indicated if we receive
    something other than an error or an empty object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add this test case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we should check the `/destroy` operation. We already check this operation
    in the `afterEach` method, where we `destroy` a known user account. We need to
    also perform the negative test and verify its behavior against an account we know
    does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: The desired behavior is that either an error is thrown, or the result shows
    an HTTP `status` indicating an error. In fact, the current authentication server
    code gives a 500 status code along with some other information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `test-compose/docker-compose.yml`, we need to inject this script, `test.js`,
    into the `userauth-test` container. We''ll add that here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a test script, and have injected that script into the desired container
    (`userauth-test`). The next step is to automate running this test. One way is
    to add this to `run.sh` (aka `run.ps1` on Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now, if you run the `run.sh` test script you'll see the required packages get
    installed, and then this test suite execution.
  prefs: []
  type: TYPE_NORMAL
- en: Automating test results reporting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's cool we have automated test execution, and Mocha makes the test results
    look nice with all those check marks. What if the management wants a graph of
    test failure trends over time?  Or there could be any number of reasons to report
    test results as data rather than a user-friendly printout on the console.
  prefs: []
  type: TYPE_NORMAL
- en: Mocha uses what's called a Reporter to report test results. A Mocha Reporter
    is a module that prints data in whatever format it supports. Information is on
    the Mocha website:  [https://mochajs.org/#reporters](https://mochajs.org/#reporters).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find the current list of available `reporters` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you use a specific `reporter` like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '**Test Anything Protocol** (**TAP**) is a widely used test results format,
    increasing the possibility of finding higher level reporting tools. Obviously,
    the next step would be to save the results into a file somewhere, after mounting
    a host directory into the container.'
  prefs: []
  type: TYPE_NORMAL
- en: Frontend headless browser testing with Puppeteer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A big cost area in testing is manual user interface testing. Therefore, a wide
    range of tools have been developed to automate running tests at the HTTP level.
    Selenium is a popular tool implemented in Java, for example. In the Node.js world,
    we have a few interesting choices. The *chai-http* plugin to Chai would let us
    interact at the HTTP level with the Notes application, while staying within the
    now-familiar Chai environment.
  prefs: []
  type: TYPE_NORMAL
- en: However, for this section, we'll use Puppeteer ([https://github.com/GoogleChrome/puppeteer](https://github.com/GoogleChrome/puppeteer)).
    This tool is a high-level Node.js module to control a headless Chrome or Chromium
    browser, using the DevTools protocol. That protocol allows tools to instrument,
    inspect, debug, and profile Chromium or Chrome.
  prefs: []
  type: TYPE_NORMAL
- en: Puppeteer is meant to be a general purpose test automation tool, and has a strong
    feature set for that purpose. Because it's easy to make web page screenshots with
    Puppeteer, it can also be used in a screenshot service.
  prefs: []
  type: TYPE_NORMAL
- en: Because Puppeteer is controlling a real web browser, your user interface tests
    will be very close to live browser testing without having to hire a human to do
    the work. Because it uses a headless version of Chrome, no visible browser window
    will show on your screen, and tests can be run in the background, instead. A downside
    to this attractive story is that Puppeteer only works against Chrome. Meaning
    that an automated test against Chrome does not test your application against other
    browsers, such as Opera or Firefox.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Puppeteer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first set up the directory and install the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'During installation, you''ll see that Puppeteer causes the download of Chromium
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The `puppeteer` module will launch that Chromium instance as needed, managing
    it as a background process, and communicating with it using the DevTools protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the script we''re about to write, we need a user account that we can use
    to log in and perform some actions. Fortunately, we already have a script to set
    up a test account. In `users/package.json`, add this line to the scripts section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re about to write this test script, but let''s finish the setup, the final
    bit of which is adding these lines to `run.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: When executed, these two lines ensure that the test user is set up, and it then
    runs the user interface tests.
  prefs: []
  type: TYPE_NORMAL
- en: Improving testability in the Notes UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the Notes application displays well in the browser, how do we write test
    software to distinguish one page from another?  The key requirement is for test
    scripts to inspect the page, determine which page is being displayed, and read
    the data on the page. That means each HTML element must be easily addressable
    using a CSS selector.
  prefs: []
  type: TYPE_NORMAL
- en: While developing the Notes application, we forgot to do that, and the **Software
    Quality Engineering** (**SQE**) manager has requested our assistance. At stake
    is the testing budget, which will be stretched further the more the SQE team can
    automate their tests.
  prefs: []
  type: TYPE_NORMAL
- en: All that's necessary is to add a few `id` or `class` attributes to HTML elements
    to improve testability. With a few identifiers, and a commitment to maintain those
    identifiers, the SQE team can write repeatable test scripts to validate the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `notes/partials/header.hbs`, change these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'In `notes/views/index.hbs`, make these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In `notes/views/login.hbs`, make these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In `notes/views/notedestroy.hbs`, make these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In `notes/views/noteedit.hbs`, make these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In `notes/views/noteview.hbs`, make these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: What we've done is add `id=` attributes to selected elements in the templates.
    We can now easily write CSS selectors to address any element. The engineering
    team can also start using these selectors in UI code.
  prefs: []
  type: TYPE_NORMAL
- en: Puppeteer test script for Notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In `test-compose/notesui`, create a file named `uitest.js` containing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: This is the start of a Mocha test suite. In the `before` function, we set up
    Puppeteer by launching a Puppeteer instance, starting a new Page object, and telling
    that Page to go to the Notes application home page. That URL is passed in using
    the named environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s useful to first think about scenarios we might want to verify with the
    Notes applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Log into the Notes application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a note to the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: View an added note
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete an added note
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s code for an implementation of the Login scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This test sequence handles the Login Scenario. It shows you a few of the Puppeteer
    API methods. Documentation of the full API is at [https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md](https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md).
    The Page object encapsulates the equivalent of a browser tab in Chrome/Chromium.
  prefs: []
  type: TYPE_NORMAL
- en: The `waitForSelector` function does what it says – it waits until an HTML element
    matching the CSS selector appears, and it will wait over one or more page refreshes.
    There are several variants of this function to allow waiting for several kinds
    of things. This function returns a Promise, making it worth our time to use async
    functions in our test code. The Promise will resolve to an `ElementHandle`, which
    is a wrapper around an HTML element, or else throw an exception, which would conveniently
    make the test fail.
  prefs: []
  type: TYPE_NORMAL
- en: The named element, `#btnloginlocal`, is in `partials/header.hbs`, and will show
    up only when a user is not logged in. Hence, we will have determined that the
    browser is currently displaying a Notes page, and that it is not logged in.
  prefs: []
  type: TYPE_NORMAL
- en: The `click` method does what it suggests, and causes a mouse button click on
    the referenced HTML element. If you want to emulate a tap, such as for a mobile
    device, there is a `tap` method for that purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next stage of the test sequence picks up from that click. The browser should
    have gone to the Login page, and therefore this CSS selector should become valid: 
     `#notesLoginPage #notesLoginForm`. What we do next is type text for our test
    user ID and password into the corresponding form elements, and then click on the
    Log In button.'
  prefs: []
  type: TYPE_NORMAL
- en: The next test stage picks up from there, and the browser should be on the home
    page as determined by this CSS selector: `#notesHomePage`. If we were logged in
    successfully, the page should have Log Out (`#btnLogout`) and ADD Note buttons
    (`#btnAddNote`).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we've used a different function, `$`, to check if the ADD Note
    button exists. Unlike the `wait` functions, `$` simply queries the current page
    without waiting. If the named CSS Selector is not in the current page, it simply
    returns `null` rather than throwing an exception. Therefore, to determine that
    the element exists, we use `assert.exists` rather than relying on the thrown exception.
  prefs: []
  type: TYPE_NORMAL
- en: Running the login scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have one test scenario entered, let''s give it a whirl. In one
    window, start the Notes test infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Then in another window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The `NOTES_HOME_URL` variable is what the script looks for to direct the Chromium
    browser to use the Notes application. To run the tests, we should use Docker Compose
    to launch the test infrastructure, and then ensure the test user is installed
    in the user database.
  prefs: []
  type: TYPE_NORMAL
- en: The Add Note scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add this to `uitest.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a more involved scenario, in which we:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the ADD Note button
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait for the note edit screen to show up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in the text for the note and click the Save button
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the note view page to ensure that's correct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the home page to ensure that's correct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of this is using the same Puppeteer functions as before, but with a couple
    of additions.
  prefs: []
  type: TYPE_NORMAL
- en: The `$eval` function looks for the element matching the CSS selector, and invokes
    the callback function on that element. If no element is found an error is thrown
    instead. As used here, we are retrieving the text from certain elements on the
    screen, and validating that it matches what the test entered as the note. That's
    an end-to-end test of adding and retrieving notes.
  prefs: []
  type: TYPE_NORMAL
- en: The next difference is using `goto` instead of clicking on `#btnGoHome`.
  prefs: []
  type: TYPE_NORMAL
- en: As you add test scenarios to the test script, you'll find it easy for Puppeteer
    to have a spurious timeout, or for the login process to mysteriously not work,
    or other spurious errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than go over the remaining scenarios, we''ll spend the next section
    discussing how to mitigate such issues. But first we need to prove the scenario
    does work even if we have to run the test 10 times to get this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Mitigating/preventing spurious test errors in Puppeteer scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal is to fully automate the test run, in order to avoid having to hire
    a human to babysit the test execution and spend time rerunning tests because of
    spurious errors. To do so, the tests need to be repeatable without any spurious
    errors. Puppeteer is a complex system – there is a Node.js module communicating
    with a Chromium instance running Headless in the background – and it seems easy
    for timing issues to cause a spurious error.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring timeouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Both Mocha and Puppeteer allow you to set timeout values, and a long timeout
    value can avoid triggering an error, if some action simply requires a long time
    to run. At the top of the test suite, we used this Mocha function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: That gives 10 seconds for every test case. If you want to use a longer timeout,
    increase that number.
  prefs: []
  type: TYPE_NORMAL
- en: The `puppeteer.launch` function can take a timeout value in its options object.
    By default, Puppeteer uses a 30-second timeout on most operations, and they all
    take an options object with a setting to change that timeout period. In this case,
    we've added the `slowMo` option to slow down operations on the browser.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing events on the Page and the Puppeteer instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another useful tactic is to generate a trace of what happened so you can puzzle
    away. Inserting `console.log` statements is tedious and makes your code look a
    little ugly. Puppeteer offers a couple of methods to trace the actions and to
    dynamically turn off tracing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `uitest.js,` add this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: That is, the Page object offers several event listeners in which we can output
    details about various events, including HTTP requests and responses. We can even
    print out the HTML text of the response. The `ignoreURL` function lets us suppress
    a few select URLs so we're not inundated with unimportant requests and responses.
  prefs: []
  type: TYPE_NORMAL
- en: You can trace Puppeteer itself using its DEBUG environment variable. See the
    README for more information: [https://github.com/GoogleChrome/puppeteer.](https://github.com/GoogleChrome/puppeteer)
  prefs: []
  type: TYPE_NORMAL
- en: Inserting pauses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It can be useful to insert a long pause at certain points to give the browser
    time to do something. Try this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This is how we implement the equivalent of a `sleep` function using Promises.
    Using `setTimeOut` this way, along with a timeout value, simply causes a delay
    for the given number of milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this function, simply insert this into the test scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'A variant on this is to wait for things to fully render in the browser. For
    example, you might have seen a pause before the Home icon in the upper-left corner
    fully renders. That pause can cause spurious errors, and this function can wait
    until that button fully renders itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'To use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: If you don't want to maintain this extra function, it's easy enough to add the
    `waitForSelector` call into your test cases instead.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding WebSockets conflicts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An error, `Cannot find context with specified id undefined`, can be thrown by
    Puppeteer. According to an issue in the Puppeteer issue queue, this can arise
    from unplanned interactions between Puppeteer and WebSockets:  [https://github.com/GoogleChrome/puppeteer/issues/1325](https://github.com/GoogleChrome/puppeteer/issues/1325) 
    This issue in turn affects the Socket.IO support in the Notes application, and
    therefore it may be useful to disable Socket.IO support during test runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s fairly simple to allow disabling of Socket.IO. In `app.mjs,` add this
    exported function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This looks for an environment variable to cause the function to return `true`
    or `false`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `routes/index.mjs` and `routes/notes.mjs,` add this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We do this to import the preceding function. It also demonstrates some of the
    flexibility we get from ES6 Modules, because we can import just the required functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `routes/index.mjs` and `routes/notes.mjs`, for every router function that
    calls `res.render` to send results, use the `enableSocketio` function as so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Hence, we've imported the function and for every view we pass `enableSocketio`
    as data to the view template.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `views/index.hbs` and `views/noteview.hbs`, we have a section of JavaScript
    code to implement SocketIO-based semi-real-time features. Surround each such section
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: By eliminating the client-side SocketIO code, we ensure the user interface does
    not open a connection to the SocketIO service. The point of this exercise was
    to avoid using WebSockets to avoid issues with Puppeteer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in `views/noteview.hbs` support disabling the Comment button like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: The final step would be to set the environment variable, `NOTES_DISABLE_SOCKETIO`,
    in the Docker Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Taking screenshots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of Puppeteer''s core features is to take screenshots, either as PNG or
    PDF files. In our test scripts, we can take screenshots to track what was on the
    screen at any given time during the test. For example, if the Login scenario spuriously
    fails to log in, we can see that in the screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Simply add code snippets like this throughout your test script. The filename
    shown here follows a convention where the first segment names the test scenario,
    the number is a sequence number within the test scenario, and the last describes
    the step within the test scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Taking screenshots also provides another stage of validation. You may want to
    do visual validation of your application as well. The `pixelmatch` module can
    compare two PNG files, and therefore a set of so-called Golden Images can be maintained
    for comparison during test runs.
  prefs: []
  type: TYPE_NORMAL
- en: For an example of using Puppeteer this way, see: [https://meowni.ca/posts/2017-puppeteer-tests/](https://meowni.ca/posts/2017-puppeteer-tests/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve covered a lot of territory in this chapter, looking at three distinct
    areas of testing: unit testing, REST API testing, and UI functional tests. Ensuring
    that an application is well tested is an important step on the road to software
    success. A team that does not follow good testing practices is often bogged down
    with fixing regression after regression.'
  prefs: []
  type: TYPE_NORMAL
- en: We've talked about the potential simplicity of simply using the assert module
    for testing. While the test frameworks, such as Mocha, provide great features,
    we can go a long way with a simple script.
  prefs: []
  type: TYPE_NORMAL
- en: There is a place for test frameworks, such as Mocha, if only to regularize our
    test cases, and to produce test results reports. We used Mocha and Chai for this,
    and these tools were quite successful. We even found a couple of bugs with a small
    test suite.
  prefs: []
  type: TYPE_NORMAL
- en: When starting down the unit testing road, one design consideration is mocking
    out dependencies. But it's not always a good use of our time to replace every
    dependency with a mock version.
  prefs: []
  type: TYPE_NORMAL
- en: To ease the administrative burden of running tests, we used Docker to automate
    setting up and tearing down the test infrastructure. Just as Docker was useful
    in automating deployment of the Notes application, it's also useful in automating
    test infrastructure deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we were able to test the Notes web user interface in a real web browser.
    We can't trust that unit testing will find every bug; some bugs will only show
    up in the web browser. Even so, we've only touched the beginning of what could
    be tested in Notes.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we've covered the gamut of Node.js development, giving you a strong
    foundation from which to start developing Node.js applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll explore RESTful web services.
  prefs: []
  type: TYPE_NORMAL
